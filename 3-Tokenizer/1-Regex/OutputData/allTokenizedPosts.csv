Tokens
Python
uses
the
reference
count
method
to
handle
object
life
time
.
So
an
object
that
has
no
more
use
will
be
immediately
destroyed
.
But
","
in
Java
","
the
GC(garbage collector)
destroys
objects
which
are
no
longer
used
at
a
specific
time
.
Why
does
Java
choose
this
strategy
and
what
is
the
benefit
from
this
?
Is
this
better
than
the
Python
approach
?
There
are
drawbacks
of
using
reference
counting
.
One
of
the
most
mentioned
is
circular
references
:
Suppose
A
references
B
","
B
references
C
and
C
references
B
.
If
A
were
to
drop
its
reference
to
B
","
both
B
and
C
will
still
have
a
reference
count
of
1
and
won't
be
deleted
with
traditional
reference
counting
.
CPython
(
reference
counting
is
not
part
of
python
itself
","
but
part
of
the
C
implementation
thereof
)
catches
circular
references
with
a
separate
garbage
collection
routine
that
it
runs
periodically
...
Another
drawback
:
Reference
counting
can
make
execution
slower
.
Each
time
an
object
is
referenced
and
dereferenced
","
the
interpreter
/
VM
must
check
to
see
if
the
count
has
gone
down
to
0
(
and
then
deallocate
if
it
did
)
.
Garbage
Collection
does
not
need
to
do
this
.
Also
","
Garbage
Collection
can
be
done
in
a
separate
thread
(
though
it
can
be
a
bit
tricky
)
.
On
machines
with
lots
of
RAM
and
for
processes
that
use
memory
only
slowly
","
you
might
not
want
to
be
doing
GC
at
all
!
Reference
counting
would
be
a
bit
of
a
drawback
there
in
terms
of
performance
...
I
think
the
article
""""
Java
theory
and
practice
:
A
brief
history
of
garbage
collection
""""
from
IBM
should
help
explain
some
of
the
questions
you
have
.
The
latest
Sun
Java
VM
actually
have
multiple
GC
algorithms
which
you
can
tweak
.
The
Java
VM
specifications
intentionally
omitted
specifying
actual
GC
behaviour
to
allow
different
(
and
multiple
)
GC
algorithms
for
different
VMs
.
For
example
","
for
all
the
people
who
dislike
the
""""
stop-the-world
""""
approach
of
the
default
Sun
Java
VM
GC
behaviour
","
there
are
VM
such
as
IBM's
WebSphere
Real
Time
which
allows
real-time
application
to
run
on
Java
.
Since
the
Java
VM
spec
is
publicly
available
","
there
is
(
theoretically
)
nothing
stopping
anyone
from
implementing
a
Java
VM
that
uses
CPython's
GC
algorithm
.
Reference
counting
is
particularly
difficult
to
do
efficiently
in
a
multi-threaded
environment
.
I
don't
know
how
you'd
even
start
to
do
it
without
getting
into
hardware
assisted
transactions
or
similar
(
currently
)
unusual
atomic
instructions
.
Reference
counting
is
easy
to
implement
.
JVMs
have
had
a
lot
of
money
sunk
into
competing
implementations
","
so
it
shouldn't
be
surprising
that
they
implement
very
good
solutions
to
very
difficult
problems
.
However
","
it's
becoming
increasingly
easy
to
target
your
favourite
language
at
the
JVM
.
One
big
disadvantage
of
Java's
tracing
GC
is
that
from
time
to
time
it
will
""""
stop
the
world
""""
and
freeze
the
application
for
a
relatively
long
time
to
do
a
full
GC
.
If
the
heap
is
big
and
the
the
object
tree
complex
","
it
will
freeze
for
a
few
seconds
.
Also
each
full
GC
visits
the
whole
object
tree
over
and
over
again
","
something
that
is
probably
quite
inefficient
.
Another
drawback
of
the
way
Java
does
GC
is
that
you
have
to
tell
the
jvm
what
heap
size
you
want
(
if
the
default
is
not
good
enough
)
;
the
JVM
derives
from
that
value
several
thresholds
that
will
trigger
the
GC
process
when
there
is
too
much
garbage
stacking
up
in
the
heap
.
I
presume
that
this
is
actually
the
main
cause
of
the
jerky
feeling
of
Android
(
based
on
Java
)
","
even
on
the
most
expensive
cellphones
","
in
comparison
with
the
smoothness
of
iOS
(
based
on
ObjectiveC
","
and
using
RC
)
.
I'd
love
to
see
a
jvm
option
to
enable
RC
memory
management
","
and
maybe
keeping
GC
only
to
run
as
a
last
resort
when
there
is
no
more
memory
left
.
Late
in
the
game
","
but
I
think
one
significant
rationale
for
RC
in
python
is
its
simplicity
.
See
this
email
by
Alex
Martelli
","
for
example
.
(
I
could
not
find
a
link
outside
google
cache
","
the
email
date
from
13th
october
2005
on
python
list
)
.
Actually
reference
counting
and
the
strategies
used
by
the
Sun
JVM
are
all
different
types
of
garbage
collection
algorithms
.
There
are
two
broad
approaches
for
tracking
down
dead
objects
:
tracing
and
reference
counting
.
In
tracing
the
GC
starts
from
the
""""
roots
""""
-
things
like
stack
references
","
and
traces
all
reachable
(
live
)
objects
.
Anything
that
can't
be
reached
is
considered
dead
.
In
reference
counting
each
time
a
reference
is
modified
the
object's
involved
have
their
count
updated
.
Any
object
whose
reference
count
gets
set
to
zero
is
considered
dead
.
With
basically
all
GC
implementations
there
are
trade
offs
but
tracing
is
usually
good
for
high
through
put
(
i.e.
fast
)
operation
but
has
longer
pause
times
(
larger
gaps
where
the
UI
or
program
may
freeze
up
)
.
Reference
counting
can
operate
in
smaller
chunks
but
will
be
slower
overall
.
It
may
mean
less
freezes
but
poorer
performance
overall
.
Additionally
a
reference
counting
GC
requires
a
cycle
detector
to
clean
up
any
objects
in
a
cycle
that
won't
be
caught
by
their
reference
count
alone
.
Perl
5
didn't
have
a
cycle
detector
in
its
GC
implementation
and
could
leak
memory
that
was
cyclic
.
Research
has
also
been
done
to
get
the
best
of
both
worlds
(
low
pause
times
","
high
throughput
)
:
http://cs.anu.edu.au/~Steve.Blackburn/pubs/papers/urc-oopsla-2003.pdf
Garbage
collection
is
faster
(
more
time
efficient
)
than
reference
counting
","
if
you
have
enough
memory
.
For
example
","
a
copying
gc
traverses
the
""""
live
""""
objects
and
copies
them
to
a
new
space
","
and
can
reclaim
all
the
""""
dead
""""
objects
in
one
step
by
marking
a
whole
memory
region
.
This
is
very
efficient
","
if
you
have
enough
memory
.
Generational
collections
use
the
knowledge
that
""""
most
objects
die
young
""""
;
often
only
a
few
percent
of
objects
have
to
be
copied
.
[
This
is
also
the
reason
why
gc
can
be
faster
than
malloc
/
free
]
Reference
counting
is
much
more
space
efficient
than
garbage
collection
","
since
it
reclaims
memory
the
very
moment
it
gets
unreachable
.
This
is
nice
when
you
want
to
attach
finalizers
to
objects
(
e.g
.
to
close
a
file
once
the
File
object
gets
unreachable
)
.
A
reference
counting
system
can
work
even
when
only
a
few
percent
of
the
memory
is
free
.
But
the
management
cost
of
having
to
increment
and
decrement
counters
upon
each
pointer
assignment
cost
a
lot
of
time
","
and
some
kind
of
garbage
collection
is
still
needed
to
reclaim
cycles
.
So
the
trade-off
is
clear
:
if
you
have
to
work
in
a
memory-constrained
environment
","
or
if
you
need
precise
finalizers
","
use
reference
counting
.
If
you
have
enough
memory
and
need
the
speed
","
use
garbage
collection
.
Darren
Thomas
gives
a
good
answer
.
However
","
one
big
difference
between
the
Java
and
Python
approaches
is
that
with
reference
counting
in
the
common
case
(
no
circular
references
)
objects
are
cleaned
up
immediately
rather
than
at
some
indeterminate
later
date
.
For
example
","
I
can
write
sloppy
","
non-portable
code
in
CPython
such
as
and
the
file
descriptor
for
that
file
I
opened
will
be
cleaned
up
immediately
because
as
soon
as
the
reference
to
the
open
file
goes
away
","
the
file
is
garbage
collected
and
the
file
descriptor
is
freed
.
Of
course
","
if
I
run
Jython
or
IronPython
or
possibly
PyPy
","
then
the
garbage
collector
won't
necessarily
run
until
much
later
;
possibly
I'll
run
out
of
file
descriptors
first
and
my
program
will
crash
.
So
you
SHOULD
be
writing
code
that
looks
like
but
sometimes
people
like
to
rely
on
reference
counting
to
always
free
up
their
resources
because
it
can
sometimes
make
your
code
a
little
shorter
.
I'd
say
that
the
best
garbage
collector
is
the
one
with
the
best
performance
","
which
currently
seems
to
be
the
Java-style
generational
garbage
collectors
that
can
run
in
a
separate
thread
and
has
all
these
crazy
optimizations
","
etc.
The
differences
to
how
you
write
your
code
should
be
negligible
and
ideally
non-existent
.
Phil's
answer
really
solved
it
","
but
I'll
elaborate
a
little
more
.
Since
the
epoch
is
in
UTC
","
if
I
want
to
compare
other
times
to
the
epoch
","
I
need
to
interpret
them
as
UTC
as
well
.
By
converting
the
time
tuple
to
a
timestamp
treating
is
as
UTC
time
","
I
get
a
number
which
is
evenly
divisible
by
the
number
of
seconds
in
a
day
.
I
can
use
this
to
convert
a
date
to
a
days-from-the-epoch
representation
which
is
what
I'm
ultimately
after
.
Short
answer
:
Because
of
timezones
.
The
Epoch
is
in
UTC
.
For
example
","
I'm
on
IST
(
Irish
Standard
Time
)
or
UTC+1
.
time.mktime()
is
relative
to
my
timezone
","
so
on
my
system
this
refers
to
Because
you
got
the
result
1233378000
","
that
would
suggest
that
you're
5
hours
behind
me
Have
a
look
at
the
time.gmtime()
function
which
works
off
UTC
.
Interesting
.
I
don't
know
","
but
I
did
try
this
:
which
is
what
you
expected
.
My
guess
?
Maybe
some
time
correction
was
done
since
the
epoch
.
This
could
be
only
a
few
seconds
","
something
like
a
leap
year
.
I
think
I
heard
something
like
this
before
","
but
can't
remember
exactly
how
and
when
it
is
done
...
local
time
...
fancy
that
.
The
time
tuple
:
Incidentally
","
we
seem
to
be
6
hours
apart
:
time.mktime
should
return
the
number
of
seconds
since
the
epoch
.
Since
I'm
giving
it
a
time
at
midnight
and
the
epoch
is
at
midnight
","
shouldn't
the
result
be
evenly
divisible
by
the
number
of
seconds
in
a
day
?
Michal
Finkelstein
from
OpenCalais
here
.
First
","
thanks
for
your
interest
.
I'll
reply
here
but
I
also
encourage
you
to
read
more
on
OpenCalais
forums
;
there's
a
lot
of
information
there
including
-
but
not
limited
to
:
http://opencalais.com/tagging-information
http://opencalais.com/how-does-calais-learn
Also
feel
free
to
follow
us
on
Twitter
(
@OpenCalais
)
or
to
email
us
at
team@opencalais.com
Now
to
the
answer
:
OpenCalais
is
based
on
a
decade
of
research
and
development
in
the
fields
of
Natural
Language
Processing
and
Text
Analytics
.
We
support
the
full
""""
NLP
Stack
""""
(
as
we
like
to
call
it
)
:
From
text
tokenization
","
morphological
analysis
and
POS
tagging
","
to
shallow
parsing
and
identifying
nominal
and
verbal
phrases
.
Semantics
come
into
play
when
we
look
for
Entities
(
a.k.a
.
Entity
Extraction
","
Named
Entity
Recognition
)
.
For
that
purpose
we
have
a
sophisticated
rule-based
system
that
combines
discovery
rules
as
well
as
lexicons
/
dictionaries
.
This
combination
allows
us
to
identify
names
of
companies
/
persons
/
films
","
etc.
","
even
if
they
don't
exist
in
any
available
list
.
For
the
most
prominent
entities
(
such
as
people
","
companies
)
we
also
perform
anaphora
resolution
","
cross-reference
and
name
canonization
/
normalization
at
the
article
level
","
so
we'll
know
that
'
John
Smith
'
and
'
Mr.
Smith
'
","
for
example
","
are
likely
referring
to
the
same
person
.
So
the
short
answer
to
your
question
is
-
no
","
it's
not
just
about
matching
against
large
databases
.
Events
/
Facts
are
really
interesting
because
they
take
our
discovery
rules
one
level
deeper
;
we
find
relations
between
entities
and
label
them
with
the
appropriate
type
","
for
example
M&As
(
relations
between
two
or
more
companies
)
","
Employment
Changes
(
relations
between
companies
and
people
)
","
and
so
on
.
Needless
to
say
","
Event
/
Fact
extraction
is
not
possible
for
systems
that
are
based
solely
on
lexicons
.
For
the
most
part
","
our
system
is
tuned
to
be
precision-oriented
","
but
we
always
try
to
keep
a
reasonable
balance
between
accuracy
and
entirety
.
By
the
way
there
are
some
cool
new
metadata
capabilities
coming
out
later
this
month
so
stay
tuned
.
Regards
","
Michal
Open
Calais
probably
use
language
parsing
technology
and
language
statics
to
guess
which
words
or
phrases
are
Names
","
Places
","
Companies
","
etc.
Then
","
it
is
just
another
step
to
do
some
kind
of
search
for
those
entities
and
return
meta
data
.
Zementa
probably
does
something
similar
","
but
matches
the
phrases
against
meta-data
attached
to
images
in
order
to
acquire
related
results
.
It
certainly
isn't
easy
.
I
was
wondering
how
as
semantic
service
like
Open
Calais
figures
out
the
names
of
companies
","
or
people
","
tech
concepts
","
keywords
","
etc.
from
a
piece
of
text
.
Is
it
because
they
have
a
large
database
that
they
match
the
text
against
?
How
would
a
service
like
Zemanta
know
what
images
to
suggest
to
a
piece
of
text
for
instance
?
I'm
not
familiar
with
the
specific
services
listed
","
but
the
field
of
natural
language
processing
has
developed
a
number
of
techniques
that
enable
this
sort
of
information
extraction
from
general
text
.
As
Sean
stated
","
once
you
have
candidate
terms
","
it's
not
to
difficult
to
search
for
those
terms
with
some
of
the
other
entities
in
context
and
then
use
the
results
of
that
search
to
determine
how
confident
you
are
that
the
term
extracted
is
an
actual
entity
of
interest
.
OpenNLP
is
a
great
project
if
you'd
like
to
play
around
with
natural
language
processing
.
The
capabilities
you've
named
would
probably
be
best
accomplished
with
Named
Entity
Recognizers
(
NER
)
(
algorithms
that
locate
proper
nouns
","
generally
","
and
sometimes
dates
as
well
)
and
/
or
Word
Sense
Disambiguation
(
WSD
)
(
eg
:
the
word
'
bank
'
has
different
meanings
depending
on
it's
context
","
and
that
can
be
very
important
when
extracting
information
from
text
.
Given
the
sentences
:
""""
the
plane
banked
left
""""
","
""""
the
snow
bank
was
high
""""
","
and
""""
they
robbed
the
bank
""""
you
can
see
how
dissambiguation
can
play
an
important
part
in
language
understanding
)
Techniques
generally
build
on
each
other
","
and
NER
is
one
of
the
more
complex
tasks
","
so
to
do
NER
successfully
","
you
will
generally
need
accurate
tokenizers
(
natural
language
tokenizers
","
mind
you
-
-
statistical
approaches
tend
to
fare
the
best
)
","
string
stemmers
(
algorithms
that
conflate
similar
words
to
common
roots
:
so
words
like
informant
and
informer
are
treated
equally
)
","
sentence
detection
(
'
Mr.
Jones
was
tall
.
'
is
only
one
sentence
","
so
you
can't
just
check
for
punctuation
)
","
part-of-speech
taggers
(
POS
taggers
)
","
and
WSD
.
There
is
a
python
port
of
(
parts
of
)
OpenNLP
called
NLTK
(
http://nltk.sourceforge.net
)
but
I
don't
have
much
experience
with
it
yet
.
Most
of
my
work
has
been
with
the
Java
and
C
#
ports
","
which
work
well
.
All
of
these
algorithms
are
language-specific
","
of
course
","
and
they
can
take
significant
time
to
run
(
although
","
it
is
generally
faster
than
reading
the
material
you
are
processing
)
.
Since
the
state-of-the-art
is
largely
based
on
statistical
techniques
","
there
is
also
a
considerable
error
rate
to
take
into
account
.
Furthermore
","
because
the
error
rate
impacts
all
the
stages
","
and
something
like
NER
requires
numerous
stages
of
processing
","
(
tokenize
->
sentence
detect
->
POS
tag
->
WSD
->
NER
)
the
error
rates
compound
.
Meanwhile
","
I
did
a
refined
research
to
verify
what
the
internal
representation
in
Python
is
","
and
also
what
its
limits
are
.
""""
The
Truth
About
Unicode
In
Python
""""
is
a
very
good
article
which
cites
directly
from
the
Python
developers
.
Apparently
","
internal
representation
is
either
UCS-2
or
UCS-4
depending
on
a
compile-time
switch
.
So
Jon
","
it's
not
UTF-16
","
but
your
answer
put
me
on
the
right
track
anyway
","
thanks
.
so
what
is
a
""""
Unicode
string
""""
in
Python
?
Python
'
knows
'
that
your
string
is
Unicode
.
Hence
if
you
do
regex
on
it
","
it
will
know
which
is
character
and
which
is
not
etc
","
which
is
really
helpful
.
If
you
did
a
strlen
it
will
also
give
the
correct
result
.
As
an
example
if
you
did
string
count
on
Hello
","
you
will
get
5
(
even
if
it's
Unicode
)
.
But
if
you
did
a
string
count
of
a
foreign
word
and
that
string
was
not
a
Unicode
string
than
you
will
have
much
larger
result
.
Pythong
uses
the
information
form
the
Unicode
Character
Database
to
identify
each
character
in
the
Unicode
String
.
Hope
that
helps
.
what
is
a
""""
Unicode
string
""""
in
Python
?
Does
that
mean
UCS-2
?
Unicode
strings
in
Python
are
stored
internally
either
as
UCS-2
(
fixed-length
16-bit
representation
","
almost
the
same
as
UTF-16
)
or
UCS-4
/
UTF-32
(
fixed-length
32-bit
representation
)
.
It's
a
compile-time
option
;
on
Windows
it's
always
UTF-16
whilst
many
Linux
distributions
set
UTF-32
(
‘
wide
mode
’
)
for
their
versions
of
Python
.
You
are
generally
not
supposed
to
care
:
you
will
see
Unicode
code-points
as
single
elements
in
your
strings
and
you
won't
know
whether
they're
stored
as
two
or
four
bytes
.
If
you're
in
a
UTF-16
build
and
you
need
to
handle
characters
outside
the
Basic
Multilingual
Plane
you'll
be
Doing
It
Wrong
","
but
that's
still
very
rare
","
and
users
who
really
need
the
extra
characters
should
be
compiling
wide
builds
.
plain
wrong
","
or
is
it
?
Yes
","
it's
quite
wrong
.
To
be
fair
I
think
that
tutorial
is
rather
old
;
it
probably
pre-dates
wide
Unicode
strings
","
if
not
Unicode
3.1
(
the
version
that
introduced
characters
outside
the
Basic
Multilingual
Plane
)
.
There
is
an
additional
source
of
confusion
stemming
from
Windows's
habit
of
using
the
term
“
Unicode
”
to
mean
","
specifically
","
the
UTF-16LE
encoding
that
NT
uses
internally
.
People
from
Microsoftland
may
often
copy
this
somewhat
misleading
habit
.
Python
stores
Unicode
as
UTF-16
.
str()
will
return
the
UTF-8
representation
of
the
UTF-16
string
.
From
Wikipedia
on
UTF-8
:
UTF-8
(
8-bit
UCS
/
Unicode
Transformation
Format
)
is
a
variable-length
character
encoding
for
Unicode
.
It
is
able
to
represent
any
character
in
the
Unicode
standard
","
yet
the
initial
encoding
of
byte
codes
and
character
assignments
for
UTF-8
is
backwards
compatible
with
ASCII
.
For
these
reasons
","
it
is
steadily
becoming
the
preferred
encoding
for
e-mail
","
web
pages
[1]
","
and
other
places
where
characters
are
stored
or
streamed
.
So
","
it's
anywhere
between
one
and
four
bytes
depending
on
which
character
you
wish
to
represent
within
the
realm
of
Unicode
.
From
Wikipedia
on
Unicode
:
In
computing
","
Unicode
is
an
industry
standard
allowing
computers
to
consistently
represent
and
manipulate
text
expressed
in
most
of
the
world's
writing
systems
.
So
it's
able
to
represent
most
(
but
not
all
)
of
the
world's
writing
systems
.
I
hope
this
helps
:
)
I
stumbled
over
this
passage
in
the
Django
tutorial
:
Django
models
have
a
default
str()
method
that
calls
unicode()
and
converts
the
result
to
a
UTF-8
bytestring
.
This
means
that
unicode(p)
will
return
a
Unicode
string
","
and
str(p)
will
return
a
normal
string
","
with
characters
encoded
as
UTF-8
.
Now
","
I'm
confused
because
afaik
Unicode
is
not
any
particular
representation
","
so
what
is
a
""""
Unicode
string
""""
in
Python
?
Does
that
mean
UCS-2
?
Googling
turned
up
this
""""
Python
Unicode
Tutorial
""""
which
boldly
states
Unicode
is
a
two-byte
encoding
which
covers
all
of
the
world's
common
writing
systems
.
which
is
plain
wrong
","
or
is
it
?
I
have
been
confused
many
times
by
character
set
and
encoding
issues
","
but
here
I'm
quite
sure
that
the
documentation
I'm
reading
is
confused
.
Does
anybody
know
what's
going
on
in
Python
when
it
gives
me
a
""""
Unicode
string
""""
?
In
Python
2.6
+
","
it
is
possible
to
use
the
format()
function
","
so
in
your
case
you
can
use
:
There
are
multiple
ways
of
using
this
function
","
so
for
further
information
you
can
check
the
documentation
.
You
can
use
C
style
string
formatting
:
See
here
","
especially
:
https://web.archive.org/web/20120415173443/http://diveintopython3.ep.io/strings.html
Formatting
in
Python
is
done
via
the
string
formatting
(
%
)
operator
:
/
Edit
:
There's
also
strftime
.
str()
in
python
on
an
integer
will
not
print
any
decimal
places
.
If
you
have
a
float
that
you
want
to
ignore
the
decimal
part
","
then
you
can
use
str(int(floatValue)
)
.
Perhaps
the
following
code
will
demonstrate
:
I
need
to
find
out
how
to
format
numbers
as
strings
.
My
code
is
here
:
Hours
and
minutes
are
integers
","
and
seconds
is
a
float
.
the
str()
function
will
convert
all
of
these
numbers
to
the
tenths
(
0.1
)
place
.
So
instead
of
my
string
outputting
""""
5:30:59.07
pm
""""
","
it
would
display
something
like
""""
5.0:30.0:59.1
pm
""""
.
Bottom
line
","
what
library
/
function
do
I
need
to
do
this
for
me
?
If
you
have
a
value
that
includes
a
decimal
","
but
the
decimal
value
is
negligible
(
ie
:
100.0
)
and
try
to
int
that
","
you
will
get
an
error
.
It
seems
silly
","
but
calling
float
first
fixes
this
.
str(int(float([variable])
)
)
You
can
use
following
to
achieve
desired
functionality
Starting
in
Python
2.6
","
there
is
an
alternative
:
the
str.format()
method
.
Here
are
some
examples
using
the
existing
string
format
operator
(
%
)
:
Here
are
the
equivalent
snippets
but
using
str.format()
:
Like
Python
2.6
+
","
all
Python
3
releases
(
so
far
)
understand
how
to
do
both
.
I
shamelessly
ripped
this
stuff
straight
out
of
my
hardcore
Python
intro
book
and
the
slides
for
the
Intro+Intermediate
Python
courses
I
offer
from
time-to-time
.
:
-
)
You
can
get
the
progress
feedback
with
urlretrieve
as
well
:
Source
code
can
be
:
This
may
be
a
little
late
","
But
I
saw
pabloG's
code
and
couldn't
help
adding
a
os.system('cls')
to
make
it
look
AWESOME
!
Check
it
out
:
If
running
in
an
environment
other
than
Windows
","
you
will
have
to
use
something
other
then
'
cls
'
.
In
MAC
OS
X
and
Linux
it
should
be
'
clear
'
.
use
wget
module
:
Wrote
wget
library
in
pure
Python
just
for
this
purpose
.
It
is
pumped
up
urlretrieve
with
these
features
as
of
version
2.0
.
Following
are
the
most
commonly
used
calls
for
downloading
files
in
python
:
urllib.urlretrieve
(
'
url_to_file
'
","
file_name
)
urllib2.urlopen('url_to_file')
requests.get(url)
"wget.download('url', file_name)"
Note
:
urlopen
and
urlretrieve
are
found
to
perform
relatively
bad
with
downloading
large
files
(
size
>
500
MB
)
.
requests.get
stores
the
file
in-memory
until
download
is
complete
.
Here's
how
to
do
it
in
Python
3
using
the
standard
library
:
urllib.request.urlopen
urllib.request.urlretrieve
If
you
have
wget
installed
","
you
can
use
parallel_sync
.
pip
install
parallel_sync
Doc
:
https://pythonhosted.org/parallel_sync/pages/examples.html
This
is
pretty
powerful
.
It
can
download
files
in
parallel
","
retry
upon
failure
","
and
it
can
even
download
files
on
a
remote
machine
.
In
2012
","
use
the
python
requests
library
You
can
run
pip
install
requests
to
get
it
.
Requests
has
many
advantages
over
the
alternatives
because
the
API
is
much
simpler
.
This
is
especially
true
if
you
have
to
do
authentication
.
urllib
and
urllib2
are
pretty
unintuitive
and
painful
in
this
case
.
2015-12-30
People
have
expressed
admiration
for
the
progress
bar
.
It's
cool
","
sure
.
There
are
several
off-the-shelf
solutions
now
","
including
tqdm
:
This
is
essentially
the
implementation
@kvance
described
30
months
ago
.
I
have
a
small
utility
that
I
use
to
download
a
MP3
from
a
website
on
a
schedule
and
then
builds
/
updates
a
podcast
XML
file
which
I've
obviously
added
to
iTunes
.
The
text
processing
that
creates
/
updates
the
XML
file
is
written
in
Python
.
I
use
wget
inside
a
Windows
.
bat
file
to
download
the
actual
MP3
however
.
I
would
prefer
to
have
the
entire
utility
written
in
Python
though
.
I
struggled
though
to
find
a
way
to
actually
down
load
the
file
in
Python
","
thus
why
I
resorted
to
wget
.
So
","
how
do
I
download
the
file
using
Python
?
urlretrieve
and
requests.get
is
simple
","
however
the
reality
not
.
I
have
fetched
data
for
couple
sites
","
including
text
and
images
","
the
above
two
probably
solve
most
of
the
tasks
.
but
for
a
more
universal
solution
I
suggest
the
use
of
urlopen
.
As
it
is
included
in
Python
3
standard
library
","
your
code
could
run
on
any
machine
that
run
Python
3
without
pre-installing
site-par
This
answer
provides
a
solution
to
HTTP
403
Forbidden
when
downloading
file
over
http
using
Python
.
I
have
tried
only
requests
and
urllib
modules
","
the
other
module
may
provide
something
better
","
but
this
is
the
one
I
used
to
solve
most
of
the
problems
.
The
wb
in
"open('test.mp3','wb')"
opens
a
file
(
and
erases
any
existing
file
)
in
binary
mode
so
you
can
save
data
with
it
instead
of
just
text
.
I
wrote
the
following
","
which
works
in
vanilla
Python
2
or
Python
3
.
Notes
:
Supports
a
""""
progress
bar
""""
callback
.
Download
is
a
4
MB
test
.
zip
from
my
website
.
Simple
yet
Python
2
&
Python
3
compatible
way
:
I
agree
with
Corey
","
urllib2
is
more
complete
than
urllib
and
should
likely
be
the
module
used
if
you
want
to
do
more
complex
things
","
but
to
make
the
answers
more
complete
","
urllib
is
a
simpler
module
if
you
want
just
the
basics
:
Will
work
fine
.
Or
","
if
you
don't
want
to
deal
with
the
""""
response
""""
object
you
can
call
read()
directly
:
One
more
","
using
urlretrieve
:
(
for
Python
3
+
use
'
import urllib
.
request
'
and
urllib.request.urlretrieve
)
Yet
another
one
","
with
a
""""
progressbar
""""
An
improved
version
of
the
PabloG
code
for
Python
2
/
3
:
In
Python
2
","
use
urllib2
which
comes
with
the
standard
library
.
This
is
the
most
basic
way
to
use
the
library
","
minus
any
error
handling
.
You
can
also
do
more
complex
stuff
such
as
changing
headers
.
The
documentation
can
be
found
here
.
The
Threading
example
from
Eli
will
run
the
thread
","
but
not
do
any
of
the
work
after
that
line
.
I'm
going
to
look
into
the
processing
module
and
the
subprocess
module
.
I
think
the
com
method
I'm
running
needs
to
be
in
another
process
","
not
just
in
another
thread
.
Have
a
look
at
the
process
management
functions
in
the
os
module
.
There
are
function
for
starting
new
processes
in
many
different
ways
","
both
synchronously
and
asynchronously
.
I
should
note
also
that
Windows
doesn't
provide
functionality
that
is
exactly
like
fork()
on
other
systems
.
To
do
multiprocessing
on
Windows
","
you
will
need
to
use
the
threading
module
.
In
addition
to
the
process
management
code
in
the
os
module
that
Greg
pointed
out
","
you
should
also
take
a
look
at
the
threading
module
:
https://docs.python.org/library/threading.html
Use
the
python
multiprocessing
module
which
will
work
everywhere
.
Here
is
a
IBM
developerWords
article
that
shows
how
to
convert
from
os.fork()
to
the
multiprocessing
module
.
How
do
I
implement
some
logic
that
will
allow
me
to
reproduce
on
Windows
the
functionality
that
I
have
on
Linux
with
the
fork()
system
call
","
using
Python
?
I'm
specifically
trying
to
execute
a
method
on
the
SAPI
Com
component
","
while
continuing
the
other
logic
in
the
main
thread
without
blocking
or
waiting
.
You
might
also
like
using
the
processing
module
(
http://pypi.python.org/pypi/processing
)
.
It
has
lot's
of
functionality
for
writing
parallel
systems
with
the
same
API
as
the
threading
module
...
fork()
has
in
fact
been
duplicated
in
Windows
","
under
Cygwin
","
but
it's
pretty
hairy
.
The
fork
call
in
Cygwin
is
particularly
interesting
because
it
does
not
map
well
on
top
of
the
Win32
API
.
This
makes
it
very
difficult
to
implement
correctly
.
See
the
The
Cygwin
User's
Guide
for
a
description
of
this
hack
.
Possibly
a
version
of
spawn()
for
python
?
http://en.wikipedia.org/wiki/Spawn_
(
operating_system
)
http://github.com/ITikhonov/git-loc
worked
right
out
of
the
box
for
me
.
You
might
also
consider
gitstats
","
which
generates
this
graph
as
an
html
file
.
You
may
get
both
added
and
removed
lines
with
git
log
","
like
:
From
this
","
you
can
write
a
similar
script
to
the
one
you
did
using
this
info
.
In
python
:
The
first
thing
that
jumps
to
mind
is
the
possibility
of
your
git
history
having
a
nonlinear
history
.
You
might
have
difficulty
determining
a
sensible
sequence
of
commits
.
Having
said
that
","
it
seems
like
you
could
keep
a
log
of
commit
ids
and
the
corresponding
lines
of
code
in
that
commit
.
In
a
post-commit
hook
","
starting
from
the
HEAD
revision
","
work
backwards
(
branching
to
multiple
parents
if
necessary
)
until
all
paths
reach
a
commit
that
you've
already
seen
before
.
That
should
give
you
the
total
lines
of
code
for
each
commit
id
.
Does
that
help
any
?
I
have
a
feeling
that
I've
misunderstood
something
about
your
question
.
Basically
I
want
to
get
the
number
of
lines-of-code
in
the
repository
after
each
commit
.
The
only
(
really
crappy
)
ways
I
have
found
is
to
use
git
filter-branch
to
run
wc
-
l
*
","
and
a
script
that
runs
git
reset
-
-
hard
on
each
commit
","
then
runs
wc
-
l
To
make
it
a
bit
clearer
","
when
the
tool
is
run
","
it
would
output
the
lines
of
code
of
the
very
first
commit
","
then
the
second
and
so
on
.
This
is
what
I
want
the
tool
to
output
(
as
an
example
)
:
I've
played
around
with
the
ruby
'
git
'
library
","
but
the
closest
I
found
was
using
the
.
lines()
method
on
a
diff
","
which
seems
like
it
should
give
the
added
lines
(
but
does
not
:
it
returns
0
when
you
delete
lines
for
example
)
I've
been
having
a
hard
time
trying
to
understand
PyPy's
translation
.
It
looks
like
something
absolutely
revolutionary
from
simply
reading
the
description
","
however
I'm
hard-pressed
to
find
good
documentation
on
actually
translating
a
real
world
piece
of
code
to
something
such
as
LLVM
.
Does
such
a
thing
exist
?
The
official
PyPy
documentation
on
it
just
skims
over
the
functionality
","
rather
than
providing
anything
I
can
try
out
myself
.
If
you
want
some
hand-on
examples
","
PyPy's
Getting
Started
document
has
a
section
titled
""""
Trying
out
the
translator
""""
.
PyPy
translator
is
in
general
","
not
intended
for
more
public
use
.
We
use
it
for
translating
our
own
python
interpreter
(
including
JIT
and
GCs
","
both
written
in
RPython
","
this
restricted
subset
of
Python
)
.
The
idea
is
that
with
good
JIT
and
GC
","
you'll
be
able
to
speedups
even
without
knowing
or
using
PyPy's
translation
toolchain
(
and
more
importantly
","
without
restricting
yourself
to
RPython
)
.
Cheers
","
fijal
Are
you
looking
for
Python
specific
translation
","
or
just
the
general
""""
how
do
you
compile
some
code
to
bytecode
""""
?
If
the
latter
is
your
case
","
check
the
LLVM
tutorial
.
I
especially
find
chapter
two
","
which
teaches
you
to
write
a
compiler
for
your
own
language
","
interesting
.
This
document
seems
to
go
into
quite
a
bit
of
detail
(
and
I
think
a
complete
description
is
out
of
scope
for
a
stackoverflow
answer
)
:
http://codespeak.net/pypy/dist/pypy/doc/translation.html
The
general
idea
of
translating
from
one
language
to
another
isn't
particularly
revolutionary
","
but
it
has
only
recently
been
gaining
popularity
/
applicability
in
""""
real-world
""""
applications
.
GWT
does
this
with
Java
(
generating
Javascript
)
and
there
is
a
library
for
translating
Haskell
into
various
other
languages
as
well
(
called
YHC
)
It
looks
like
something
absolutely
revolutionary
from
simply
reading
the
description
","
As
far
as
I
know
","
PyPy
is
novel
in
the
sense
that
it
is
the
first
system
expressly
designed
for
implementing
languages
.
Other
tools
exist
to
help
with
much
of
the
very
front
end
","
such
as
parser
generators
","
or
for
the
very
back
end
","
such
as
code
generation
","
but
not
much
existed
for
connecting
the
two
.
for
loops
in
MATLAB
used
to
be
slow
","
but
this
is
not
true
anymore
.
So
vectorizing
is
not
always
the
miracle
solution
.
Just
use
the
profiler
","
and
tic
and
toc
functions
to
help
you
identify
possible
bottlenecks
.
If
x
and
y
are
column
vectors
","
you
can
do
:
(
with
row
vectors
","
just
use
x
and
y
)
.
Here
is
an
example
run
:
I
would
recommend
to
join
the
two
arrays
for
the
computation
:
This
will
work
great
if
your
functions
can
work
with
vectors
.
Then
again
","
many
functions
can
even
work
with
matrices
","
so
you
wouldn't
even
need
the
loop
.
Tested
only
in
octave
...
(
no
matlab
license
)
.
Variations
of
arrayfun()
exist
","
check
the
documentation
.
Yields
...
Using
the
zip
function
","
Python
allows
for
loops
to
traverse
multiple
sequences
in
parallel
.
for
(
x
","
y
)
in
"zip(List1, List2)"
:
Does
MATLAB
have
an
equivalent
syntax
?
If
not
","
what
is
the
best
way
to
iterate
over
two
parallel
arrays
at
the
same
time
using
MATLAB
?
If
I'm
not
mistaken
the
zip
function
you
use
in
python
creates
a
pair
of
the
items
found
in
list1
and
list2
.
Basically
it
still
is
a
for
loop
with
the
addition
that
it
will
retrieve
the
data
from
the
two
seperate
lists
for
you
","
instead
that
you
have
to
do
it
yourself
.
So
maybe
your
best
option
is
to
use
a
standard
for
loop
like
this
:
or
whatever
you
have
to
do
with
the
data
.
If
you
really
are
talking
about
parallel
computing
then
you
should
take
a
look
at
the
Parallel
Computing
Toolbox
for
matlab
","
and
more
specifically
at
parfor
A
plugin
for
GSview
for
viewing
encrypted
PDFs
is
here
.
If
this
works
for
you
","
you
may
be
able
to
look
at
the
source
.
If
I
remember
correctly
","
there
is
a
fixed
padding
string
of
32
(
?
)
bytes
to
apply
to
any
password
.
All
passwords
need
to
be
32
bytes
at
the
start
of
computing
the
encryption
key
","
either
by
truncating
or
adding
some
of
those
padding
bytes
.
If
no
user
password
was
set
you
simply
have
to
pad
with
all
32
bytes
of
the
string
","
i.e.
use
the
32
padding
bytes
as
the
starting
point
for
computing
the
encryption
key
.
I
have
to
admit
it's
been
a
while
since
I've
done
this
","
I
do
remember
that
the
encryption
part
of
the
PDF
is
an
absolute
mess
as
it
got
changed
significantly
in
nearly
every
revision
","
requiring
you
to
cope
with
a
lot
of
cases
to
handle
all
PDF's
.
Good
luck
.
Although
the
PDF
specification
is
available
from
Adobe
","
it's
not
exactly
the
simplest
document
to
read
through
.
PDF
allows
documents
to
be
encrypted
so
that
either
a
user
password
and
/
or
an
owner
password
is
required
to
do
various
things
with
the
document
(
display
","
print
","
etc
)
.
A
common
use
is
to
lock
a
PDF
so
that
end
users
can
read
it
without
entering
any
password
","
but
a
password
is
required
to
do
anything
else
.
I'm
trying
to
parse
PDFs
that
are
locked
in
this
way
(
to
get
the
same
privileges
as
you
would
get
opening
them
in
any
reader
)
.
Using
an
empty
string
as
the
user
password
doesn't
work
","
but
it
seems
(
section
3.5.2
of
the
spec
)
that
there
has
to
be
a
user
password
to
create
the
hash
for
the
admin
password
.
What
I
would
like
is
either
an
explanation
of
how
to
do
this
","
or
any
code
that
I
can
read
(
ideally
Python
","
C
","
or
C
+
+
","
but
anything
readable
will
do
)
that
does
this
so
that
I
can
understand
what
I'm
meant
to
be
doing
.
Standalone
code
","
rather
than
reading
through
(
e.g
.
)
the
gsview
source
","
would
be
best
.
xpdf
is
probably
a
good
reference
implementation
for
this
sort
of
problem
.
I
have
successfully
used
them
to
open
encrypted
pdfs
before
.
List
comprehensions
.
I
often
find
myself
filtering
/
mapping
lists
","
and
being
able
to
say
[
"line.replace(""spam"",""eggs"")"
for
line
in
"open(""somefile.txt"")"
if
"line.startswith(""nee"")"
]
is
really
nice
.
Functions
are
first
class
objects
.
They
can
be
passed
as
parameters
to
other
functions
","
defined
inside
other
function
","
and
have
lexical
scope
.
This
makes
it
really
easy
to
say
things
like
people.sort(key=lambda p: p.age)
and
thus
sort
a
bunch
of
people
on
their
age
without
having
to
define
a
custom
comparator
class
or
something
equally
verbose
.
Everything
is
an
object
.
Java
has
basic
types
which
aren't
objects
","
which
is
why
many
classes
in
the
standard
library
define
9
different
versions
of
functions
(
for
boolean
","
byte
","
char
","
double
","
float
","
int
","
long
","
Object
","
short
)
.
Array.sort
is
a
good
example
.
Autoboxing
helps
","
although
it
makes
things
awkward
when
something
turns
out
to
be
null
.
Properties
.
Python
lets
you
create
classes
with
read-only
fields
","
lazily-generated
fields
","
as
well
as
fields
which
are
checked
upon
assignment
to
make
sure
they're
never
0
or
null
or
whatever
you
want
to
guard
against
","
etc.
'
Default
and
keyword
arguments
.
In
Java
if
you
want
a
constructor
that
can
take
up
to
5
optional
arguments
","
you
must
define
6
different
versions
of
that
constructor
.
And
there's
no
way
at
all
to
say
"Student(name=""Eli"", age=25)"
Functions
can
only
return
1
thing
.
In
Python
you
have
tuple
assignment
","
so
you
can
say
spam
","
eggs
=
nee()
but
in
Java
you'd
need
to
either
resort
to
mutable
out
parameters
or
have
a
custom
class
with
2
fields
and
then
have
two
additional
lines
of
code
to
extract
those
fields
.
Built-in
syntax
for
lists
and
dictionaries
.
Operator
Overloading
.
Generally
better
designed
libraries
.
For
example
","
to
parse
an
XML
document
in
Java
","
you
say
Document
doc
=
DocumentBuilderFactory.newInstance()
.
newDocumentBuilder()
.
"parse(""test.xml"")"
;
and
in
Python
you
say
doc
=
"parse(""test.xml"")"
Anyway
","
I
could
go
on
and
on
with
further
examples
","
but
Python
is
just
overall
a
much
more
flexible
and
expressive
language
.
It's
also
dynamically
typed
","
which
I
really
like
","
but
which
comes
with
some
disadvantages
.
Java
has
much
better
performance
than
Python
and
has
way
better
tool
support
.
Sometimes
those
things
matter
a
lot
and
Java
is
the
better
language
than
Python
for
a
task
;
I
continue
to
use
Java
for
some
new
projects
despite
liking
Python
a
lot
more
.
But
as
a
language
I
think
Python
is
superior
for
most
things
I
find
myself
needing
to
accomplish
.
I
think
this
pair
of
articles
by
Philip
J
.
Eby
does
a
great
job
discussing
the
differences
between
the
two
languages
(
mostly
about
philosophy
/
mentality
rather
than
specific
language
features
)
.
Python
is
Not
Java
Java
is
Not
Python
","
either
With
Jython
you
can
have
both
.
It's
only
at
Python
2.2
","
but
still
very
useful
if
you
need
an
embedded
interpreter
that
has
access
to
the
Java
runtime
.
One
key
difference
in
Python
is
significant
whitespace
.
This
puts
a
lot
of
people
off
-
me
too
for
a
long
time
-
but
once
you
get
going
it
seems
natural
and
makes
much
more
sense
than
;
s
everywhere
.
From
a
personal
perspective
","
Python
has
the
following
benefits
over
Java
:
No
Checked
Exceptions
Optional
Arguments
Much
less
boilerplate
and
less
verbose
generally
Other
than
those
","
this
page
on
the
Python
Wiki
is
a
good
place
to
look
with
lots
of
links
to
interesting
articles
.
Besides
the
dynamic
nature
of
Python
(
and
the
syntax
)
","
what
are
some
of
the
major
features
of
the
Python
language
that
Java
doesn't
have
","
and
vice
versa
?
Apart
from
what
Eli
Courtwright
said
:
I
find
iterators
in
Python
more
concise
.
You
can
use
for
i
in
something
","
and
it
works
with
pretty
much
everything
.
Yeah
","
Java
has
gotten
better
since
1.5
","
but
for
example
you
can
iterate
through
a
string
in
python
with
this
same
construct
.
Introspection
:
In
python
you
can
get
at
runtime
information
about
an
object
or
a
module
about
its
symbols
","
methods
","
or
even
its
docstrings
.
You
can
also
instantiate
them
dynamically
.
Java
has
some
of
this
","
but
usually
in
Java
it
takes
half
a
page
of
code
to
get
an
instance
of
a
class
","
whereas
in
Python
it
is
about
3
lines
.
And
as
far
as
I
know
the
docstrings
thing
is
not
available
in
Java
The
open
source
Python
package
","
SciPy
","
has
quite
a
large
set
of
optimization
routines
including
some
for
multivariable
problems
with
constraints
(
which
is
what
fmincon
does
I
believe
)
.
Once
you
have
SciPy
installed
type
the
following
at
the
Python
command
prompt
help(scipy.optimize)
The
resulting
document
is
extensive
and
includes
the
following
which
I
believe
might
be
of
use
to
you
.
Is
your
problem
convex
?
Linear
?
Non-linear
?
I
agree
that
SciPy.optimize
will
probably
do
the
job
","
but
fmincon
is
a
sort
of
bazooka
for
solving
optimization
problems
","
and
you'll
be
better
off
if
you
can
confine
it
to
one
of
the
categories
below
(
in
increasing
level
of
difficulty
to
solve
efficiently
)
Linear
Program
(
LP
)
Quadratic
Program
(
QP
)
Convex
Quadratically-Constrained
Quadratic
Program
(
QCQP
)
Second
Order
Cone
Program
(
SOCP
)
Semidefinite
Program
(
SDP
)
Non-Linear
Convex
Problem
Non-Convex
Problem
There
are
also
combinatoric
problems
such
as
Mixed-Integer
Linear
Programs
(
MILP
)
","
but
you
didn't
mention
any
sort
of
integrality
constraints
","
suffice
to
say
that
they
fall
into
a
different
class
of
problems
.
The
CVXOpt
package
will
be
of
great
use
to
you
if
your
problem
is
convex
.
If
your
problem
is
not
convex
","
you
need
to
choose
between
finding
a
local
solution
or
the
global
solution
.
Many
convex
solvers
'
sort
of
'
work
in
a
non-convex
domain
.
Finding
a
good
approximation
to
the
global
solution
would
require
some
form
Simulated
Annealing
or
Genetic
Algorithm
.
Finding
the
global
solution
will
require
an
enumeration
of
all
local
solutions
or
a
combinatorial
strategy
such
as
Branch
and
Bound
.
GNU
Octave
is
another
MATLAB
clone
that
might
have
what
you
need
.
Is
there
an
open-source
alternative
to
MATLAB's
fmincon
function
for
constrained
linear
optimization
?
I'm
rewriting
a
MATLAB
program
to
use
Python
/
NumPy
/
SciPy
and
this
is
the
only
function
I
haven't
found
an
equivalent
to
.
A
NumPy-based
solution
would
be
ideal
","
but
any
language
will
do
.
Python
optimization
software
:
OpenOpt
http://openopt.org
(
this
one
is
numpy-based
as
you
wish
","
with
automatic
differentiation
by
FuncDesigner
)
Pyomo
https://software.sandia.gov/trac/coopr/wiki/Package/pyomo
CVXOPT
http://abel.ee.ucla.edu/cvxopt/
NLPy
http://nlpy.sourceforge.net/
Have
a
look
at
http://www.aemdesign.com/downloadfsqp.htm.
There
you
will
find
C
code
which
provides
the
same
functionality
as
fmincon
.
(
However
","
using
a
different
algorithm
.
You
can
read
the
manual
if
you
are
interested
in
the
details
.
)
It's
open
source
but
not
under
GPL
.
I
don't
know
if
it's
in
there
","
but
there's
a
python
distribution
called
Enthought
that
might
have
what
you're
looking
for
.
It
was
designed
specifically
for
data
analysis
has
over
60
additional
libraries
.
For
numerical
optimization
in
Python
you
may
take
a
look
at
OpenOpt
solvers
:
http://openopt.org/NLP
http://openopt.org/Problems
There
is
a
program
called
SciLab
that
is
a
MATLAB
clone
.
I
haven't
used
it
at
all
","
but
it
is
open
source
and
might
have
the
function
you
are
looking
for
.
Here
is
the
whois
client
re-implemented
in
Python
:
http://code.activestate.com/recipes/577364-whois-client/
I'm
trying
to
get
a
webservice
up
and
running
that
actually
requires
to
check
whois
databases
.
What
I'm
doing
right
now
is
ugly
and
I'd
like
to
avoid
it
as
much
as
I
can
:
I
call
gwhois
command
and
parse
its
output
.
Ugly
.
I
did
some
search
to
try
to
find
a
pythonic
way
to
do
this
task
.
Generally
I
got
quite
much
nothing
-
this
old
discussion
list
link
has
a
way
to
check
if
domain
exist
.
Quite
not
what
I
was
looking
for
...
But
still
","
it
was
best
anwser
Google
gave
me
-
everything
else
is
just
a
bunch
of
unanwsered
questions
.
Any
of
you
have
succeeded
to
get
some
method
up
and
running
?
I'd
very
much
appreciate
some
tips
","
or
should
I
just
do
it
the
opensource-way
","
sit
down
and
code
something
by
myself
?
:
)
here
is
a
ready-to-use
solution
that
works
for
me
;
written
for
Python
3.1
(
when
backporting
to
Py2.x
","
take
special
care
of
the
bytes
/
Unicode
text
distinctions
)
.
your
single
point
of
access
is
the
method
DRWHO.whois()
","
which
expects
a
domain
name
to
be
passed
in
;
it
will
then
try
to
resolve
the
name
using
the
provider
configured
as
DRWHO.whois_providers
[
'
*
'
]
(
a
more
complete
solution
could
differentiate
providers
according
to
the
top
level
domain
)
.
DRWHO.whois()
will
return
a
dictionary
with
a
single
entry
text
","
which
contains
the
response
text
sent
back
by
the
WHOIS
server
.
Again
","
a
more
complete
solution
would
then
try
and
parse
the
text
(
which
must
be
done
separately
for
each
provider
","
as
there
is
no
standard
format
)
and
return
a
more
structured
format
(
e.g
.
","
set
a
flag
available
which
specifies
whether
or
not
the
domain
looks
available
)
.
have
fun
!
There's
nothing
wrong
with
using
a
command
line
utility
to
do
what
you
want
.
If
you
put
a
nice
wrapper
around
the
service
","
you
can
implement
the
internals
however
you
want
!
For
example
:
Now
","
whether
or
not
you
roll
your
own
using
urllib
","
wrap
around
a
command
line
utility
(
like
you're
doing
)
","
or
import a
third
party
library
and
use
that
(
like
you're
saying
)
","
this
interface
stays
the
same
.
This
approach
is
generally
not
considered
ugly
at
all
-
-
sometimes
command
utilities
do
what
you
want
and
you
should
be
able
to
leverage
them
.
If
speed
ends
up
being
a
bottleneck
","
your
abstraction
makes
the
process
of
switching
to
a
native
Python
implementation
transparent
to
your
client
code
.
Practicality
beats
purity
-
-
that's
what's
Pythonic
.
:
)
I
don't
know
if
gwhois
does
something
special
with
the
server
output
;
however
","
you
can
plainly
connect
to
the
whois
server
on
port
whois
(
43
)
","
send
your
query
","
read
all
the
data
in
the
reply
and
parse
them
.
To
make
life
a
little
easier
","
you
could
use
the
telnetlib.Telnet
class
(
even
if
the
whois
protocol
is
much
simpler
than
the
telnet
protocol
)
instead
of
plain
sockets
.
The
tricky
parts
:
which
whois
server
will
you
ask
?
RIPE
","
ARIN
","
APNIC
","
LACNIC
","
AFRINIC
","
JPNIC
","
VERIO
etc
LACNIC
could
be
a
useful
fallback
","
since
they
tend
to
reply
with
useful
data
to
requests
outside
of
their
domain
.
what
are
the
exact
options
and
arguments
for
each
whois
server
?
some
offer
help
","
others
don't
.
In
general
","
plain
domain
names
work
without
any
special
options
.
Parsing
another
webpage
woulnd't
be
as
bad
(
assuming
their
html
woulnd't
be
very
bad
)
","
but
it
would
actually
tie
me
to
them
-
if
they're
down
","
I'm
down
:
)
Actually
I
found
some
old
project
on
sourceforge
:
rwhois.py
.
What
scares
me
a
bit
is
that
their
last
update
is
from
2003
.
But
","
it
might
seem
as
a
good
place
to
start
reimplementation
of
what
I
do
right
now
...
Well
","
I
felt
obligued
to
post
the
link
to
this
project
anyway
","
just
for
further
reference
.
Another
way
to
do
it
is
to
use
urllib2
module
to
parse
some
other
page's
whois
service
(
many
sites
like
that
exist
)
.
But
that
seems
like
even
more
of
a
hack
that
what
you
do
now
","
and
would
give
you
a
dependency
on
whatever
whois
site
you
chose
","
which
is
bad
.
I
hate
to
say
it
","
but
unless
you
want
to
re-implement
whois
in
your
program
(
which
would
be
re-inventing
the
wheel
)
","
running
whois
on
the
OS
and
parsing
the
output
(
ie
what
you
are
doing
now
)
seems
like
the
right
way
to
do
it
.
Look
at
this
:
http://code.google.com/p/pywhois/
pywhois
-
Python
module
for
retrieving
WHOIS
information
of
domains
Goal
:
-
Create
a
simple
importable
Python
module
which
will
produce
parsed
WHOIS
data
for
a
given
domain
.
-
Able
to
extract
data
for
all
the
popular
TLDs
(
com
","
org
","
net
","
...
)
-
Query
a
WHOIS
server
directly
instead
of
going
through
an
intermediate
web
service
like
many
others
do
.
-
Works
with
Python
2.4
+
and
no
external
dependencies
Example
:
if
it
returns
a
gaierror
you
know
know
it's
not
registered
with
any
DNS
Found
this
question
in
the
process
of
my
own
search
for
a
python
whois
library
.
Don't
know
that
I
agree
with
cdleary's
answer
that
using
a
library
that
wraps
a
command
is
always
the
best
way
to
go
-
but
I
can
see
his
reasons
why
he
said
this
.
Pro
:
cmd-line
whois
handles
all
the
hard
work
(
socket
calls
","
parsing
","
etc
)
Con
:
not
portable
;
module
may
not
work
depending
on
underlying
whois
command
.
Slower
","
since
running
a
command
and
most
likely
shell
in
addition
to
whois
command
.
Affected
if
not
UNIX
(
Windows
)
","
different
UNIX
","
older
UNIX
","
or
older
whois
command
I
am
looking
for
a
whois
module
that
can
handle
whois
IP
lookups
and
I
am
not
interested
in
coding
my
own
whois
client
.
Here
are
the
modules
that
I
(
lightly
)
tried
out
and
more
information
about
it
:
pywhoisapi
:
Home
:
http://code.google.com/p/pywhoisapi/
Design
:
REST
client
accessing
ARIN
whois
REST
service
Pros
:
Able
to
handle
IP
address
lookups
Cons
:
Able
to
pull
information
from
whois
servers
of
other
RIRs
?
BulkWhois
Home
:
http://pypi.python.org/pypi/BulkWhois/0.2.1
Design
:
telnet
client
accessing
whois
telnet
query
interface
from
RIR(?)
Pros
:
Able
to
handle
IP
address
lookups
Cons
:
Able
to
pull
information
from
whois
servers
of
other
RIRs
?
pywhois
:
Home
:
http://code.google.com/p/pywhois/
Design
:
REST
client
accessing
RRID
whois
services
Pros
:
Accessses
many
RRIDs
;
has
python
3.x
branch
Cons
:
does
not
seem
to
handle
IP
address
lookups
python-whois
:
Home
:
http://code.google.com/p/python-whois/
Design
:
wraps
""""
whois
""""
command
Cons
:
does
not
seem
to
handle
IP
address
lookups
whoisclient
-
fork
of
python-whois
Home
:
http://gitorious.org/python-whois
Design
:
wraps
""""
whois
""""
command
Depends
on
:
IPy.py
Cons
:
does
not
seem
to
handle
IP
address
lookups
Update
:
I
ended
up
using
pywhoisapi
for
the
reverse
IP
lookups
that
I
was
doing
To
get
directory
of
executing
script
as
others
have
said
.
You
may
also
want
to
use
:
this
would
print
the
path
of
the
currently
executing
script
I
think
this
is
cleaner
:
and
gets
the
same
information
as
:
Where
[0]
is
the
current
frame
in
the
stack
(
top
of
stack
)
and
[1]
is
for
the
file
name
","
increase
to
go
backwards
in
the
stack
i.e.
would
be
the
file
name
of
the
script
that
called
the
current
frame
.
Also
","
using
[
-
1
]
will
get
you
to
the
bottom
of
the
stack
","
the
original
calling
script
.
The
suggestions
marked
as
best
are
all
true
if
your
script
consists
of
only
one
file
.
If
you
want
to
find
out
the
name
of
the
executable
(
i.e.
the
root
file
passed
to
the
python
interpreter
for
the
current
program
)
from
a
file
that
may
be
imported
as
a
module
","
you
need
to
do
this
(
let's
assume
this
is
in
a
file
named
foo.py
)
:
import inspect
print
inspect.stack()
[
-
1
]
[1]
Because
the
last
thing
(
[
-
1
]
)
on
the
stack
is
the
first
thing
that
went
into
it
(
stacks
are
LIFO
/
FILO
data
structures
)
.
Then
in
file
bar.py
if
you
import foo
it'll
print
bar.py
","
rather
than
foo.py
","
which
would
be
the
value
of
all
of
these
:
__file__
inspect.getfile(inspect.currentframe()
)
inspect.stack()
[0]
[1]
I
used
the
approach
with
__file__
os.path.abspath(__file__)
but
there
is
a
little
trick
","
it
returns
the
.
py
file
when
the
code
is
run
the
first
time
","
next
runs
give
the
name
of
*
.
pyc
file
so
I
stayed
with
:
inspect.getfile(inspect.currentframe()
)
or
sys._getframe()
.
f_code.co_filename
I
wrote
a
function
which
take
into
account
eclipse
debugger
and
unittest
.
It
return
the
folder
of
the
first
script
you
launch
.
You
can
optionally
specify
the
__file__
var
","
but
the
main
thing
is
that
you
don't
have
to
share
this
variable
across
all
your
calling
hierarchy
.
Maybe
you
can
handle
others
stack
particular
cases
I
didn't
see
","
but
for
me
it's
ok
.
if
you
want
just
the
filename
without
.
/
or
.
py
you
can
try
this
file_name
will
print
testscript
you
can
generate
whatever
you
want
by
changing
the
index
inside
[]
I
have
a
script
that
must
work
under
windows
environment
.
This
code
snipped
is
what
I've
finished
with
:
it's
quite
a
hacky
decision
.
But
it
requires
no
external
libraries
and
it's
the
most
important
thing
in
my
case
.
this
will
give
us
the
filename
only
.
i.e.
if
abspath
of
file
is
c:\abcd\abc.py
then
2nd
line
will
print
abc.py
No
need
for
inspect
or
any
other
library
.
This
worked
for
me
when
I
had
to
import a
script
(
from
a
different
directory
then
the
executed
script
)
","
that
used
a
configuration
file
residing
in
the
same
folder
as
the
imported
script
.
It's
not
entirely
clear
what
you
mean
by
""""
the
filepath
of
the
file
that
is
currently
running
within
the
process
""""
.
sys.argv
[0]
usually
contains
the
location
of
the
script
that
was
invoked
by
the
Python
interpreter
.
Check
the
sys
documentation
for
more
details
.
As
@Tim
and
@Pat
Notz
have
pointed
out
","
the
__file__
attribute
provides
access
to
the
file
from
which
the
module
was
loaded
","
if
it
was
loaded
from
a
file
I
think
it's
just
__file__
Sounds
like
you
may
also
want
to
checkout
the
inspect
module
.
The
__file__
attribute
works
for
both
the
file
containing
the
main
execution
code
as
well
as
imported
modules
.
See
https://web.archive.org/web/20090918095828/http://pyref.infogami.com/__file__
p1.py
:
p2.py
:
I
have
scripts
calling
other
script
files
but
I
need
to
get
the
filepath
of
the
file
that
is
currently
running
within
the
process
.
For
example
","
let's
say
I
have
three
files
.
Using
execfile
:
script_1.py
calls
script_2.py
.
In
turn
","
script_2.py
calls
script_3.py
.
How
can
I
get
the
file
name
and
path
of
script_3.py
","
from
code
within
script_3.py
","
without
having
to
pass
that
information
as
arguments
from
script_2.py
?
(
Executing
os.getcwd()
returns
the
original
starting
script's
filepath
not
the
current
file's
.
)
This
should
work
:
You
can
use
inspect.stack()
Here
is
an
experiment
based
on
the
answers
in
this
thread
-
with
Python
2.7.10
on
Windows
.
The
stack-based
ones
are
the
only
ones
that
seem
to
give
reliable
results
.
The
last
two
have
the
shortest
syntax
","
ie
-
Here's
to
these
being
added
to
sys
as
functions
!
Credit
to
@Usagi
and
@pablog
Based
on
the
following
three
files
","
and
running
script1.py
from
its
folder
with
python
script1.py
(
also
tried
execfiles
with
absolute
paths
and
calling
from
a
separate
folder
)
.
C:\testpath\script1.py
:
execfile('script2.py')
C:\testpath\script2.py
:
execfile('lib/script3.py')
C:\testpath\lib\script3.py
:
One
thing
that
has
to
be
considered
when
choosing
session
backend
is
""""
how
often
session
data
is
modified
""""
?
Even
sites
with
moderate
traffic
will
suffer
if
session
data
is
modified
on
each
request
","
making
many
database
trips
to
store
and
retrieve
data
.
In
my
previous
work
we
used
memcache
as
session
backend
exclusively
and
it
worked
really
well
.
Our
administrative
team
put
really
great
effort
in
making
two
special
memcached
instances
stable
as
a
rock
","
but
after
bit
of
twiddling
with
initial
setup
","
we
did
not
have
any
interrupts
of
session
backends
operations
.
The
filesystem
backend
is
only
worth
looking
at
if
you're
not
going
to
use
a
database
for
any
other
part
of
your
system
.
If
you
are
using
a
database
then
the
filesystem
backend
has
nothing
to
recommend
it
.
The
memcache
backend
is
much
quicker
than
the
database
backend
","
but
you
run
the
risk
of
a
session
being
purged
and
some
of
your
session
data
being
lost
.
If
you're
a
really
","
really
high
traffic
website
and
code
carefully
so
you
can
cope
with
losing
a
session
then
use
memcache
.
If
you're
not
using
a
database
use
the
file
system
cache
","
but
the
default
database
backend
is
the
best
","
safest
and
simplest
option
in
almost
all
cases
.
I'm
looking
at
sessions
in
Django
","
and
by
default
they
are
stored
in
the
database
.
What
are
the
benefits
of
filesystem
and
cache
sessions
and
when
should
I
use
them
?
I'm
no
Django
expert
","
so
this
answer
is
about
session
stores
generally
.
Downvote
if
I'm
wrong
.
Performance
and
Scalability
Choice
of
session
store
has
an
effect
on
performance
and
scalability
.
This
should
only
be
a
big
problem
if
you
have
a
very
popular
application
.
Both
database
and
filesystem
session
stores
are
(
usually
)
backed
by
disks
so
you
can
have
a
lot
of
sessions
cheaply
(
because
disks
are
cheap
)
","
but
requests
will
often
have
to
wait
for
the
data
to
be
read
(
because
disks
are
slow
)
.
Memcached
sessions
use
RAM
","
so
will
cost
more
to
support
the
same
number
of
concurrent
sessions
(
because
RAM
is
expensive
)
","
but
may
be
faster
(
because
RAM
is
fast
)
.
Filesystem
sessions
are
tied
to
the
box
where
your
application
is
running
","
so
you
can't
load
balance
between
multiple
application
servers
if
your
site
gets
huge
.
Database
and
memcached
sessions
let
you
have
multiple
application
servers
talking
to
a
shared
session
store
.
Simplicity
Choice
of
session
store
will
also
impact
how
easy
it
is
to
deploy
your
site
.
Changing
away
from
the
default
will
cost
some
complexity
.
Memcached
and
RDBMSs
both
have
their
own
complexities
","
but
your
application
is
probably
going
to
be
using
an
RDBMS
anyway
.
Unless
you
have
a
very
popular
application
","
simplicity
should
be
the
larger
concern
.
Bonus
Another
approach
is
to
store
session
data
in
cookies
(
all
of
it
","
not
just
an
ID
)
.
This
has
the
advantage
that
the
session
store
automatically
scales
with
the
number
of
users
","
but
it
has
disadvantages
too
.
You
(
or
your
framework
)
need
to
be
careful
to
stop
users
forging
session
data
.
You
also
need
to
keep
each
session
small
because
the
whole
thing
will
be
sent
with
every
request
.
If
the
database
have
a
DBA
that
isn't
you
","
you
may
not
be
allowed
to
use
a
database-backed
session
(
it
being
a
front-end
matter
only
)
.
Until
django
supports
easily
merging
data
from
several
databases
","
so
that
you
can
have
frontend-specific
stuff
like
sessions
and
user-messages
(
the
messages
in
django.contrib.auth
are
also
stored
in
the
db
)
in
a
separate
db
","
you
need
to
keep
this
in
mind
.
As
of
Django
1.1
you
can
use
the
cached_db
session
back
end
.
This
stores
the
session
in
the
cache
(
only
use
with
memcached
)
","
and
writes
it
back
to
the
DB
.
If
it
has
fallen
out
of
the
cache
","
it
will
be
read
from
the
DB
.
Although
this
is
slower
than
just
using
memcached
for
storing
the
session
","
it
adds
persistence
to
the
session
.
For
more
information
","
see
:
Django
Docs
:
Using
Cached
Sessions
See
the
wearkref
module
docs
for
more
details
.
You
can
also
use
weakref.proxy
to
create
an
object
that
proxies
o
.
Will
throw
ReferenceError
if
used
when
the
referent
is
no
longer
referenced
.
How
do
you
create
a
weak
reference
to
an
object
in
Python
?
Python
has
pytz
(
http://pytz.sourceforge.net
)
module
which
can
be
used
for
arithmetic
of
'
time
'
objects
.
It
takes
care
of
DST
offsets
as
well
.
The
above
page
has
a
number
of
examples
that
illustrate
the
usage
of
pytz
.
Retrieve
the
times
in
milliseconds
and
then
do
the
subtraction
.
It
seems
that
this
isn't
supported
","
since
there
wouldn't
be
a
good
way
to
deal
with
overflows
in
datetime.time
.
I
know
this
isn't
an
answer
directly
","
but
maybe
someone
with
more
python
experience
than
me
can
take
this
a
little
further
.
For
more
info
","
see
this
:
http://bugs.python.org/issue3250
I
have
2
time
values
which
have
the
type
datetime.time
.
I
want
to
find
their
difference
.
The
obvious
thing
to
do
is
t1
-
t2
","
but
this
doesn't
work
.
It
works
for
objects
of
type
datetime.datetime
but
not
for
datetime.time
.
So
what
is
the
best
way
to
do
this
?
You
could
transform
both
into
timedelta
objects
and
subtract
these
from
each
other
","
which
will
take
care
to
of
the
carry-overs
.
For
example
:
Negative
timedelta
objects
in
Python
get
a
negative
day
field
","
with
the
other
fields
positive
.
You
could
check
beforehand
:
comparison
works
on
both
time
objects
and
timedelta
objects
:
Firstly
","
note
that
a
datetime.time
is
a
time
of
day
","
independent
of
a
given
day
","
and
so
the
different
between
any
two
datetime.time
values
is
going
to
be
less
than
24
hours
.
One
approach
is
to
convert
both
datetime.time
values
into
comparable
values
(
such
as
milliseconds
)
","
and
find
the
difference
.
It's
a
little
lame
","
but
it
works
.
Also
a
little
silly
","
but
you
could
try
picking
an
arbitrary
day
and
embedding
each
time
in
it
","
using
datetime.datetime.combine
","
then
subtracting
:
Environment.TickCount
seems
to
work
well
if
you
need
something
quick
.
int
start
=
Environment.TickCount
...
DoSomething()
int
elapsedtime
=
Environment.TickCount
-
start
Jon
There's
a
text
progress
bar
library
for
python
at
http://pypi.python.org/pypi/progressbar/2.2
that
you
might
find
useful
:
This
library
provides
a
text
mode
progressbar
.
This
is
tipically
used
to
display
the
progress
of
a
long
running
operation
","
providing
a
visual
clue
that
processing
is
underway
.
The
ProgressBar
class
manages
the
progress
","
and
the
format
of
the
line
is
given
by
a
number
of
widgets
.
A
widget
is
an
object
that
may
display
diferently
depending
on
the
state
of
the
progress
.
There
are
three
types
of
widget
:
-
a
string
","
which
always
shows
itself
;
-
a
ProgressBarWidget
","
which
may
return
a
diferent
value
every
time
it's
update
method
is
called
;
and
-
a
ProgressBarWidgetHFill
","
which
is
like
ProgressBarWidget
","
except
it
expands
to
fill
the
remaining
width
of
the
line
.
The
progressbar
module
is
very
easy
to
use
","
yet
very
powerful
.
And
automatically
supports
features
like
auto-resizing
when
available
.
You
might
also
try
:
Using
a
single
carriage
return
at
the
beginning
of
your
string
rather
than
several
backspaces
.
Your
cursor
will
still
blink
","
but
it'll
blink
after
the
percent
sign
rather
than
under
the
first
digit
","
and
with
one
control
character
instead
of
three
you
may
get
less
flicker
.
For
small
files
you
may
need
to
had
this
lines
in
order
to
avoid
crazy
percentages
:
"sys.stdout.write(""\r%2d%%"" % percent)"
sys.stdout.flush()
Cheers
Thats
how
I
did
this
could
help
you
:
https://github.com/mouuff/MouDownloader/blob/master/api/download.py
For
what
it's
worth
","
here's
the
code
I
used
to
get
it
working
:
If
you
use
the
curses
package
","
you
have
much
greater
control
of
the
console
.
It
also
comes
at
a
higher
cost
in
code
complexity
and
is
probably
unnecessary
unless
you
are
developing
a
large
console-based
app
.
For
a
simple
solution
","
you
can
always
put
the
spinning
wheel
at
the
end
of
the
status
messge
(
the
sequence
of
characters
|
","
\
","
-
","
/
which
actually
looks
nice
under
blinking
cursor
.
I
used
this
code
:
Late
to
the
party
","
as
usual
.
Here's
an
implementation
that
supports
reporting
progress
","
like
the
core
urlretrieve
:
I
am
writing
a
little
application
to
download
files
over
http
(
as
","
for
example
","
described
here
)
.
I
also
want
to
include
a
little
download
progress
indicator
showing
the
percentage
of
the
download
progress
.
Here
is
what
I
came
up
with
:
"sys.stdout.write(rem_file + ""..."")"
"urllib.urlretrieve(rem_file, loc_file, reporthook=dlProgress)"
"def dlProgress(count, blockSize, totalSize):"
percent
=
int(count*blockSize*100/totalSize)
"sys.stdout.write(""%2d%%"" % percent)"
"sys.stdout.write(""\b\b\b"")"
sys.stdout.flush()
Output
:
MyFileName
...
9
%
Any
other
ideas
or
recommendations
to
do
this
?
One
thing
that's
somewhat
annoying
is
the
blinking
cursor
in
the
terminal
on
the
first
digit
of
the
percentage
.
Is
there
a
way
to
prevent
this
?
Is
there
a
way
to
hide
the
cursor
?
EDIT
:
Here
a
better
alternative
using
a
global
variable
for
the
filename
in
dlProgress
and
the
'
\
r
'
code
:
global
rem_file
#
global
variable
to
be
used
in
dlProgress
"urllib.urlretrieve(rem_file, loc_file, reporthook=dlProgress)"
"def dlProgress(count, blockSize, totalSize):"
percent
=
int(count*blockSize*100/totalSize)
"sys.stdout.write(""\r"" + rem_file + ""...%d%%"" % percent)"
sys.stdout.flush()
Output
:
MyFileName...9
%
And
the
cursor
shows
up
at
the
END
of
the
line
.
Much
better
.
soup.title.string
actually
returns
a
unicode
string
.
To
convert
that
into
normal
string
","
you
need
to
do
string=string.encode('ascii
'
","
'
ignore
'
)
I'll
always
use
lxml
for
such
tasks
.
You
could
use
beautifulsoup
as
well
.
This
is
probably
overkill
for
such
a
simple
task
","
but
if
you
plan
to
do
more
than
that
","
then
it's
saner
to
start
from
these
tools
(
mechanize
","
BeautifulSoup
)
because
they
are
much
easier
to
use
than
the
alternatives
(
urllib
to
get
content
and
regexen
or
some
other
parser
to
parse
html
)
Links
:
BeautifulSoup
mechanize
The
mechanize
Browser
object
has
a
title()
method
.
So
the
code
from
this
post
can
be
rewritten
as
:
No
need
to
import other
libraries
.
Request
has
this
functionality
in-built
.
Here's
a
simplified
version
of
@Vinko
Vrsalovic's
answer
:
NOTE
:
soup.title
finds
the
first
title
element
anywhere
in
the
html
document
title.string
assumes
it
has
only
one
child
node
","
and
that
child
node
is
a
string
For
beautifulsoup
4.x
","
use
different
import
:
How
can
I
retrieve
the
page
title
of
a
webpage
(
title
html
tag
)
using
Python
?
Using
HTMLParser
:
Using
regular
expressions
Put
the
asterisks
before
the
kwargs
variable
.
This
makes
Python
pass
the
variable
(
which
is
assumed
to
be
a
dictionary
)
as
keyword
arguments
.
Some
experimentation
and
I
figured
this
one
out
:
"def methodA(arg, **kwargs):"
"methodB(""argvalue"", **kwargs)"
Seems
obvious
now
...
Say
I
have
the
following
methods
:
In
methodA
I
wish
to
call
methodB
","
passing
on
the
kwargs
.
However
","
it
seems
that
if
I
define
methodA
as
follows
","
the
second
argument
will
be
passed
on
as
positional
rather
than
named
variable
arguments
.
How
do
I
make
sure
that
the
*
*
kwargs
in
methodA
gets
passed
as
*
*
kwargs
to
methodB
?
As
an
aside
:
When
using
functions
instead
of
methods
","
you
could
also
use
functools.partial
:
The
last
line
will
define
a
function
""""
bar
""""
that
","
when
called
","
will
call
foo
with
the
first
argument
set
to
""""
argvalue
""""
and
all
other
functions
just
passed
on
:
will
call
Unfortunately
that
will
not
work
with
methods
.
Need
I
say
more
?
:
)
Seriously
","
PEP
8
","
'
Blank
lines
'
","
ยง4
is
the
official
way
to
do
it
.
I've
been
really
enjoying
Python
programming
lately
.
I
come
from
a
background
of
a
strong
love
for
C-based
coding
","
where
everything
is
perhaps
more
complicated
than
it
should
be
(
but
puts
hair
on
your
chest
","
at
least
)
.
So
switching
from
C
to
Python
for
more
complex
things
that
don't
require
tons
of
speed
has
been
more
of
a
boon
than
a
bane
in
writing
projects
.
However
","
coming
from
this
land
of
brackets
and
parentheses
and
structs
as
far
as
the
naked
eye
can
see
","
I
come
across
a
small
problem
:
I
find
Python
difficult
to
read
.
For
example
","
the
following
block
of
text
is
hard
for
me
to
decipher
unless
I
stare
at
it
(
which
I
dislike
doing
)
:
The
problem
occurs
at
the
end
of
that
if
block
:
all
the
tabbing
and
then
suddenly
returning
to
a
jarring
block
feels
almost
disturbing
.
As
a
solution
","
I've
started
coding
my
Python
like
this
:
And
this
","
for
some
odd
reason
","
makes
me
more
able
to
read
my
own
code
.
But
I'm
curious
:
has
anyone
else
with
my
strange
problem
found
easier
ways
to
make
their
tabbed-out
code
more
readable
?
I'd
love
to
find
out
if
there's
a
better
way
to
do
this
before
this
becomes
a
huge
habit
for
me
.
You
could
try
increasing
the
indent
size
","
but
in
general
I
would
just
say
","
relax
","
it
will
come
with
time
.
I
don't
think
trying
to
make
Python
look
like
C
is
a
very
good
idea
.
I
would
look
in
to
understanding
more
details
about
Python
syntax
.
Often
times
if
a
piece
of
code
looks
odd
","
there
usually
is
a
better
way
to
write
it
.
For
example
","
in
the
above
example
:
While
it
is
a
small
change
","
it
might
help
the
readability
.
Also
","
in
all
honesty
","
I've
never
used
a
while
loop
","
so
there
is
a
good
change
you
would
end
up
with
a
nice
concise
list
comprehension
or
for
loop
instead
.
;
)
Part
of
learning
a
new
programming
language
is
learning
to
read
code
in
that
language
.
A
crutch
like
this
may
make
it
easier
to
read
your
own
code
","
but
it's
going
to
impede
the
process
of
learning
how
to
read
anyone
else's
Python
code
.
I
really
think
you'd
be
better
off
getting
rid
of
the
end
of
block
comments
and
getting
used
to
normal
Python
.
Rather
than
focusing
on
making
your
existing
structures
more
readable
","
you
should
focus
on
making
more
logical
structures
.
Make
smaller
blocks
","
try
not
to
nest
blocks
excessively
","
make
smaller
functions
","
and
try
to
think
through
your
code
flow
more
.
If
you
come
to
a
point
where
you
can't
quickly
determine
the
structure
of
your
code
","
you
should
probably
consider
refactoring
and
adding
some
comments
.
Code
flow
should
always
be
immediately
apparent
-
-
the
more
you
have
to
think
about
it
","
the
less
maintainable
your
code
becomes
.
I
like
to
put
blank
lines
around
blocks
to
make
control
flow
more
obvious
.
For
example
:
Perhaps
the
best
thing
would
be
to
turn
on
""""
show
whitespace
""""
in
your
editor
.
Then
you
would
have
a
visual
indication
of
how
far
in
each
line
is
tabbed
(
usually
a
bunch
of
dots
)
","
and
it
will
be
more
apparent
when
that
changes
.
Better
still
","
install
the
path.py
module
","
it
wraps
all
the
os.path
functions
and
other
related
functions
into
methods
on
an
object
that
can
be
used
wherever
strings
are
used
:
You
could
use
the
new
Python
3.4
library
pathlib
.
(
You
can
also
get
it
for
Python
2.6
or
2.7
using
pip
install
pathlib
.
)
The
authors
wrote
:
""""
The
aim
of
this
library
is
to
provide
a
simple
hierarchy
of
classes
to
handle
filesystem
paths
and
the
common
operations
users
do
over
them
.
""""
To
get
an
absolute
path
in
Windows
:
Or
on
UNIX
:
Docs
are
here
:
https://docs.python.org/3/library/pathlib.html
Today
you
can
also
use
the
unipath
package
which
was
based
on
path.py
:
http://sluggo.scrapping.cc/python/unipath/
I
would
recommend
using
this
package
as
it
offers
a
clean
interface
to
common
os.path
utilities
.
I
prefer
to
use
glob
here
is
how
to
list
all
file
types
in
your
current
folder
:
here
is
how
to
list
all
(
for
example
)
.
txt
files
in
your
current
folder
:
here
is
how
to
list
all
file
types
in
a
chose
directory
:
hope
this
helped
you
Also
works
if
it
is
already
an
absolute
path
:
Given
a
path
such
as
""""
mydir
/
myfile.txt
""""
","
how
do
I
find
the
absolute
filepath
relative
to
the
current
working
directory
in
Python
?
E.g
.
on
Windows
","
I
might
end
up
with
:
I
don't
think
that
your
results
are
all
that
surprising
-
-
if
anything
it
is
that
Postgres
is
so
fast
.
Does
the
Postgres
query
run
faster
a
second
time
once
it
has
had
a
chance
to
cache
the
data
?
To
be
a
little
fairer
your
test
for
Java
and
Python
should
cover
the
cost
of
acquiring
the
data
in
the
first
place
(
ideally
loading
it
off
disk
)
.
If
this
performance
level
is
a
problem
for
your
application
in
practice
but
you
need
a
RDBMS
for
other
reasons
then
you
could
look
at
memcached
.
You
would
then
have
faster
cached
access
to
raw
data
and
could
do
the
calculations
in
code
.
I
need
a
real
DBA's
opinion
.
Postgres
8.3
takes
200
ms
to
execute
this
query
on
my
Macbook
Pro
while
Java
and
Python
perform
the
same
calculation
in
under
20
ms
(
350
","
000
rows
)
:
Is
this
normal
behaviour
when
using
a
SQL
database
?
The
schema
(
the
table
holds
responses
to
a
survey
)
:
I
wrote
some
tests
in
Java
and
Python
for
context
and
they
crush
SQL
(
except
for
pure
python
)
:
Even
sqlite3
is
competitive
with
Postgres
despite
it
assumping
all
columns
are
strings
(
for
contrast
:
even
using
just
switching
to
numeric
columns
instead
of
integers
in
Postgres
results
in
10x
slowdown
)
Tunings
i've
tried
without
success
include
(
blindly
following
some
web
advice
)
:
So
my
question
is
","
is
my
experience
here
normal
","
and
this
is
what
I
can
expect
when
using
a
SQL
database
?
I
can
understand
that
ACID
must
come
with
costs
","
but
this
is
kind
of
crazy
in
my
opinion
.
I'm
not
asking
for
realtime
game
speed
","
but
since
Java
can
process
millions
of
doubles
in
under
20
ms
","
I
feel
a
bit
jealous
.
Is
there
a
better
way
to
do
simple
OLAP
on
the
cheap
(
both
in
terms
of
money
and
server
complexity
)
?
I've
looked
into
Mondrian
and
Pig
+
Hadoop
but
not
super
excited
about
maintaining
yet
another
server
application
and
not
sure
if
they
would
even
help
.
No
the
Python
code
and
Java
code
do
all
the
work
in
house
so
to
speak
.
I
just
generate
4
arrays
with
350
","
000
random
values
each
","
then
take
the
average
.
I
don't
include
the
generation
in
the
timings
","
only
the
averaging
step
.
The
java
threads
timing
uses
4
threads
(
one
per
array
average
)
","
overkill
but
it's
definitely
the
fastest
.
The
sqlite3
timing
is
driven
by
the
Python
program
and
is
running
from
disk
(
not
:
memory
:
)
I
realize
Postgres
is
doing
much
more
behind
the
scenes
","
but
most
of
that
work
doesn't
matter
to
me
since
this
is
read
only
data
.
The
Postgres
query
doesn't
change
timing
on
subsequent
runs
.
I've
rerun
the
Python
tests
to
include
spooling
it
off
the
disk
.
The
timing
slows
down
considerably
to
nearly
4
secs
.
But
I'm
guessing
that
Python's
file
handling
code
is
pretty
much
in
C
(
though
maybe
not
the
csv
lib
?
)
so
this
indicates
to
me
that
Postgres
isn't
streaming
from
the
disk
either
(
or
that
you
are
correct
and
I
should
bow
down
before
whoever
wrote
their
storage
layer
!
)
Postgres
is
doing
a
lot
more
than
it
looks
like
(
maintaining
data
consistency
for
a
start
!
)
If
the
values
don't
have
to
be
100
%
spot
on
","
or
if
the
table
is
updated
rarely
","
but
you
are
running
this
calculation
often
","
you
might
want
to
look
into
Materialized
Views
to
speed
it
up
.
(
Note
","
I
have
not
used
materialized
views
in
Postgres
","
they
look
at
little
hacky
","
but
might
suite
your
situation
)
.
Materialized
Views
Also
consider
the
overhead
of
actually
connecting
to
the
server
and
the
round
trip
required
to
send
the
request
to
the
server
and
back
.
I'd
consider
200ms
for
something
like
this
to
be
pretty
good
","
A
quick
test
on
my
oracle
server
","
the
same
table
structure
with
about
500k
rows
and
no
indexes
","
takes
about
1
-
1.5
seconds
","
which
is
almost
all
just
oracle
sucking
the
data
off
disk
.
The
real
question
is
","
is
200ms
fast
enough
?
-
-
-
-
-
-
-
-
-
-
-
-
-
-
More
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
I
was
interested
in
solving
this
using
materialized
views
","
since
I've
never
really
played
with
them
.
This
is
in
oracle
.
First
I
created
a
MV
which
refreshes
every
minute
.
While
its
refreshing
","
there
is
no
rows
returned
Once
it
refreshes
","
its
MUCH
faster
than
doing
the
raw
query
If
we
insert
into
the
base
table
","
the
result
is
not
immediately
viewable
view
the
MV
.
But
wait
a
minute
or
so
","
and
the
MV
will
update
behind
the
scenes
","
and
the
result
is
returned
fast
as
you
could
want
.
This
isn't
ideal
.
for
a
start
","
its
not
realtime
","
inserts
/
updates
will
not
be
immediately
visible
.
Also
","
you've
got
a
query
running
to
update
the
MV
whether
you
need
it
or
not
(
this
can
be
tune
to
whatever
time
frame
","
or
on
demand
)
.
But
","
this
does
show
how
much
faster
an
MV
can
make
it
seem
to
the
end
user
","
if
you
can
live
with
values
which
aren't
quite
upto
the
second
accurate
.
I'm
a
MS-SQL
guy
myself
","
and
we'd
use
DBCC
PINTABLE
to
keep
a
table
cached
","
and
SET
STATISTICS
IO
to
see
that
it's
reading
from
cache
","
and
not
disk
.
I
can't
find
anything
on
Postgres
to
mimic
PINTABLE
","
but
pg_buffercache
seems
to
give
details
on
what
is
in
the
cache
-
you
may
want
to
check
that
","
and
see
if
your
table
is
actually
being
cached
.
A
quick
back
of
the
envelope
calculation
makes
me
suspect
that
you're
paging
from
disk
.
Assuming
Postgres
uses
4-byte
integers
","
you
have
(
6
*
4
)
bytes
per
row
","
so
your
table
is
a
minimum
of
(
24
*
350
","
000
)
bytes
~
8.4MB
.
Assuming
40
MB
/
s
sustained
throughput
on
your
HDD
","
you're
looking
at
right
around
200ms
to
read
the
data
(
which
","
as
pointed
out
","
should
be
where
almost
all
of
the
time
is
being
spent
)
.
Unless
I
screwed
up
my
math
somewhere
","
I
don't
see
how
it's
possible
that
you
are
able
to
read
8MB
into
your
Java
app
and
process
it
in
the
times
you're
showing
-
unless
that
file
is
already
cached
by
either
the
drive
or
your
OS
.
I
retested
with
MySQL
specifying
ENGINE
=
MEMORY
and
it
doesn't
change
a
thing
(
still
200
ms
)
.
Sqlite3
using
an
in-memory
db
gives
similar
timings
as
well
(
250
ms
)
.
The
math
here
looks
correct
(
at
least
the
size
","
as
that's
how
big
the
sqlite
db
is
:
-
)
I'm
just
not
buying
the
disk-causes-slowness
argument
as
there
is
every
indication
the
tables
are
in
memory
(
the
postgres
guys
all
warn
against
trying
too
hard
to
pin
tables
to
memory
as
they
swear
the
OS
will
do
it
better
than
the
programmer
)
To
clarify
the
timings
","
the
Java
code
is
not
reading
from
disk
","
making
it
a
totally
unfair
comparison
if
Postgres
is
reading
from
the
disk
and
calculating
a
complicated
query
","
but
that's
really
besides
the
point
","
the
DB
should
be
smart
enough
to
bring
a
small
table
into
memory
and
precompile
a
stored
procedure
IMHO
.
UPDATE
(
in
response
to
the
first
comment
below
)
:
I'm
not
sure
how
I'd
test
the
query
without
using
an
aggregation
function
in
a
way
that
would
be
fair
","
since
if
i
select
all
of
the
rows
it'll
spend
tons
of
time
serializing
and
formatting
everything
.
I'm
not
saying
that
the
slowness
is
due
to
the
aggregation
function
","
it
could
still
be
just
overhead
from
concurrency
","
integrity
","
and
friends
.
I
just
don't
know
how
to
isolate
the
aggregation
as
the
sole
independent
variable
.
You
need
to
increase
postgres
'
caches
to
the
point
where
the
whole
working
set
fits
into
memory
before
you
can
expect
to
see
perfomance
comparable
to
doing
it
in-memory
with
a
program
.
Those
are
very
detailed
answers
","
but
they
mostly
beg
the
question
","
how
do
I
get
these
benefits
without
leaving
Postgres
given
that
the
data
easily
fits
into
memory
","
requires
concurrent
reads
but
no
writes
and
is
queried
with
the
same
query
over
and
over
again
.
Is
it
possible
to
precompile
the
query
and
optimization
plan
?
I
would
have
thought
the
stored
procedure
would
do
this
","
but
it
doesn't
really
help
.
To
avoid
disk
access
it's
necessary
to
cache
the
whole
table
in
memory
","
can
I
force
Postgres
to
do
that
?
I
think
it's
already
doing
this
though
","
since
the
query
executes
in
just
200
ms
after
repeated
runs
.
Can
I
tell
Postgres
that
the
table
is
read
only
","
so
it
can
optimize
any
locking
code
?
I
think
it's
possible
to
estimate
the
query
construction
costs
with
an
empty
table
(
timings
range
from
20-60
ms
)
I
still
can't
see
why
the
Java
/
Python
tests
are
invalid
.
Postgres
just
isn't
doing
that
much
more
work
(
though
I
still
haven't
addressed
the
concurrency
aspect
","
just
the
caching
and
query
construction
)
UPDATE
:
I
don't
think
it's
fair
to
compare
the
SELECTS
as
suggested
by
pulling
350
","
000
through
the
driver
and
serialization
steps
into
Python
to
run
the
aggregation
","
nor
even
to
omit
the
aggregation
as
the
overhead
in
formatting
and
displaying
is
hard
to
separate
from
the
timing
.
If
both
engines
are
operating
on
in
memory
data
","
it
should
be
an
apples
to
apples
comparison
","
I'm
not
sure
how
to
guarantee
that's
already
happening
though
.
I
can't
figure
out
how
to
add
comments
","
maybe
i
don't
have
enough
reputation
?
I
would
say
your
test
scheme
is
not
really
useful
.
To
fulfill
the
db
query
","
the
db
server
goes
through
several
steps
:
parse
the
SQL
work
up
a
query
plan
","
i
.
e
.
decide
on
which
indices
to
use
(
if
any
)
","
optimize
etc.
if
an
index
is
used
","
search
it
for
the
pointers
to
the
actual
data
","
then
go
to
the
appropriate
location
in
the
data
or
if
no
index
is
used
","
scan
the
whole
table
to
determine
which
rows
are
needed
load
the
data
from
disk
into
a
temporary
location
(
hopefully
","
but
not
necessarily
","
memory
)
perform
the
count()
and
avg()
calculations
So
","
creating
an
array
in
Python
and
getting
the
average
basically
skips
all
these
steps
save
the
last
one
.
As
disk
I
/
O
is
among
the
most
expensive
operations
a
program
has
to
perform
","
this
is
a
major
flaw
in
the
test
(
see
also
the
answers
to
this
question
I
asked
here
before
)
.
Even
if
you
read
the
data
from
disk
in
your
other
test
","
the
process
is
completely
different
and
it's
hard
to
tell
how
relevant
the
results
are
.
To
obtain
more
information
about
where
Postgres
spends
its
time
","
I
would
suggest
the
following
tests
:
Compare
the
execution
time
of
your
query
to
a
SELECT
without
the
aggregating
functions
(
i
.
e
.
cut
step
5
)
If
you
find
that
the
aggregation
leads
to
a
significant
slowdown
","
try
if
Python
does
it
faster
","
obtaining
the
raw
data
through
the
plain
SELECT
from
the
comparison
.
To
speed
up
your
query
","
reduce
disk
access
first
.
I
doubt
very
much
that
it's
the
aggregation
that
takes
the
time
.
There's
several
ways
to
do
that
:
Cache
data
(
in
memory
!
)
for
subsequent
access
","
either
via
the
db
engine's
own
capabilities
or
with
tools
like
memcached
Reduce
the
size
of
your
stored
data
Optimize
the
use
of
indices
.
Sometimes
this
can
mean
to
skip
index
use
altogether
(
after
all
","
it's
disk
access
","
too
)
.
For
MySQL
","
I
seem
to
remember
that
it's
recommended
to
skip
indices
if
you
assume
that
the
query
fetches
more
than
10
%
of
all
the
data
in
the
table
.
If
your
query
makes
good
use
of
indices
","
I
know
that
for
MySQL
databases
it
helps
to
put
indices
and
data
on
separate
physical
disks
.
However
","
I
don't
know
whether
that's
applicable
for
Postgres
.
There
also
might
be
more
sophisticated
problems
such
as
swapping
rows
to
disk
if
for
some
reason
the
result
set
can't
be
completely
processed
in
memory
.
But
I
would
leave
that
kind
of
research
until
I
run
into
serious
performance
problems
that
I
can't
find
another
way
to
fix
","
as
it
requires
knowledge
about
a
lot
of
little
under-the-hood
details
in
your
process
.
Update
:
I
just
realized
that
you
seem
to
have
no
use
for
indices
for
the
above
query
and
most
likely
aren't
using
any
","
too
","
so
my
advice
on
indices
probably
wasn't
helpful
.
Sorry
.
Still
","
I'd
say
that
the
aggregation
is
not
the
problem
but
disk
access
is
.
I'll
leave
the
index
stuff
in
","
anyway
","
it
might
still
have
some
use
.
Are
you
using
TCP
to
access
the
Postgres
?
In
that
case
Nagle
is
messing
with
your
timing
.
Thanks
for
the
Oracle
timings
","
that's
the
kind
of
stuff
I'm
looking
for
(
disappointing
though
:
-
)
Materialized
views
are
probably
worth
considering
as
I
think
I
can
precompute
the
most
interesting
forms
of
this
query
for
most
users
.
I
don't
think
query
round
trip
time
should
be
very
high
as
i'm
running
the
the
queries
on
the
same
machine
that
runs
Postgres
","
so
it
can't
add
much
latency
?
I've
also
done
some
checking
into
the
cache
sizes
","
and
it
seems
Postgres
relies
on
the
OS
to
handle
caching
","
they
specifically
mention
BSD
as
the
ideal
OS
for
this
","
so
I
thinking
Mac
OS
ought
to
be
pretty
smart
about
bringing
the
table
into
memory
.
Unless
someone
has
more
specific
params
in
mind
I
think
more
specific
caching
is
out
of
my
control
.
In
the
end
I
can
probably
put
up
with
200
ms
response
times
","
but
knowing
that
7
ms
is
a
possible
target
makes
me
feel
unsatisfied
","
as
even
20-50
ms
times
would
enable
more
users
to
have
more
up
to
date
queries
and
get
rid
of
a
lots
of
caching
and
precomputed
hacks
.
I
just
checked
the
timings
using
MySQL
5
and
they
are
slightly
worse
than
Postgres
.
So
barring
some
major
caching
breakthroughs
","
I
guess
this
is
what
I
can
expect
going
the
relational
db
route
.
I
wish
I
could
up
vote
some
of
your
answers
","
but
I
don't
have
enough
points
yet
.
One
other
thing
that
an
RDBMS
generally
does
for
you
is
to
provide
concurrency
by
protecting
you
from
simultaneous
access
by
another
process
.
This
is
done
by
placing
locks
","
and
there's
some
overhead
from
that
.
If
you're
dealing
with
entirely
static
data
that
never
changes
","
and
especially
if
you're
in
a
basically
""""
single
user
""""
scenario
","
then
using
a
relational
database
doesn't
necessarily
gain
you
much
benefit
.
The
os.statvfs()
function
is
a
better
way
to
get
that
information
for
Unix-like
platforms
(
including
OS
X
)
.
The
Python
documentation
says
""""
Availability
:
Unix
""""
but
it's
worth
checking
whether
it
works
on
Windows
too
in
your
build
of
Python
(
ie
.
the
docs
might
not
be
up
to
date
)
.
Otherwise
","
you
can
use
the
pywin32
library
to
directly
call
the
GetDiskFreeSpaceEx
function
.
I
need
a
way
to
determine
the
space
remaining
on
a
disk
volume
using
python
on
linux
","
Windows
and
OS
X
.
I'm
currently
parsing
the
output
of
the
various
system
calls
(
df
","
dir
)
to
accomplish
this
-
is
there
a
better
way
?
A
good
cross-platform
way
is
using
psutil
:
http://pythonhosted.org/psutil/#disks
(
Note
that
you'll
need
psutil
0.3.0
or
above
)
.
If
you
dont
like
to
add
another
dependency
you
can
for
windows
use
ctypes
to
call
the
win32
function
call
directly
.
Note
that
you
must
pass
a
directory
name
for
GetDiskFreeSpaceEx()
to
work
(
statvfs()
works
on
both
files
and
directories
)
.
You
can
get
a
directory
name
from
a
file
with
os.path.dirname()
.
Also
see
the
documentation
for
os.statvfs()
and
GetDiskFreeSpaceEx
.
From
Python
3.3
you
can
use
"shutil.disk_usage(""/"")"
.
free
from
standard
library
for
both
Windows
and
UNIX
:
)
Below
code
returns
correct
value
on
windows
You
can
use
df
as
a
cross-platform
way
.
It
is
a
part
of
GNU
core
utilities
.
These
are
the
core
utilities
which
are
expected
to
exist
on
every
operating
system
.
However
","
they
are
not
installed
on
Windows
by
default
(
Here
","
GetGnuWin32
comes
in
handy
)
.
df
is
a
command-line
utility
","
therefore
a
wrapper
required
for
scripting
purposes
.
For
example
:
You
could
use
the
wmi
module
for
windows
and
os.statvfs
for
unix
for
window
for
unix
or
linux
I
Don't
know
of
any
cross-platform
way
to
achieve
this
","
but
maybe
a
good
workaround
for
you
would
be
to
write
a
wrapper
class
that
checks
the
operating
system
and
uses
the
best
method
for
each
.
For
Windows
","
there's
the
GetDiskFreeSpaceEx
method
in
the
win32
extensions
.
Install
psutil
using
pip
install
psutil
.
Then
you
can
get
the
amount
of
free
space
in
bytes
using
:
NodeBox
is
awesome
for
raw
graphics
creation
.
I'm
the
one
supporting
CairoPlot
and
I'm
very
proud
it
came
up
here
.
Surely
matplotlib
is
great
","
but
I
believe
CairoPlot
is
better
looking
.
So
","
for
presentations
and
websites
","
it's
a
very
good
choice
.
Today
I
released
version
1.1
.
If
interested
","
check
it
out
at
CairoPlot
v1.1
EDIT
:
After
a
long
and
cold
winter
","
CairoPlot
is
being
developed
again
.
Check
out
the
new
version
on
GitHub
.
Have
you
looked
into
ChartDirector
for
Python
?
I
can't
speak
about
this
one
","
but
I've
used
ChartDirector
for
PHP
and
it's
pretty
good
.
CairoPlot
For
interactive
work
","
Matplotlib
is
the
mature
standard
.
It
provides
an
OO-style
API
as
well
as
a
Matlab-style
interactive
API
.
Chaco
is
a
more
modern
plotting
library
from
the
folks
at
Enthought
.
It
uses
Enthought's
Kiva
vector
drawing
library
and
currently
works
only
with
Wx
and
Qt
with
OpenGL
on
the
way
(
Matplotlib
has
backends
for
Tk
","
Qt
","
Wx
","
Cocoa
","
and
many
image
types
such
as
PDF
","
EPS
","
PNG
","
etc.
)
.
The
main
advantages
of
Chaco
are
its
speed
relative
to
Matplotlib
and
its
integration
with
Enthought's
Traits
API
for
interactive
applications
.
What
are
the
available
libraries
for
creating
pretty
charts
and
graphs
in
a
Python
application
?
You
could
also
consider
google
charts
.
Not
technically
a
python
API
","
but
you
can
use
it
from
python
","
it's
reasonably
fast
to
code
for
","
and
the
results
tend
to
look
nice
.
If
you
happen
to
be
using
your
plots
online
","
then
this
would
be
an
even
better
solution
.
You
can
also
use
pygooglechart
","
which
uses
the
Google
Chart
API
.
This
isn't
something
you'd
always
want
to
use
","
but
if
you
want
a
small
number
of
good
","
simple
","
charts
","
and
are
always
online
","
and
especially
if
you're
displaying
in
a
browser
anyway
","
it's
a
good
choice
.
I
am
a
fan
on
PyOFC2
:
http://btbytes.github.com/pyofc2/
It
just
just
a
package
that
makes
it
easy
to
generate
the
JSON
data
needed
for
Open
Flash
Charts
2
","
which
are
very
beautiful
.
Check
out
the
examples
on
the
link
above
.
You
should
also
consider
PyCha
http://www.lorenzogil.com/projects/pycha/
I
used
pychart
and
thought
it
was
very
straightforward
.
http://home.gna.org/pychart/
It's
all
native
python
and
does
not
have
a
busload
of
dependencies
.
I'm
sure
matplotlib
is
lovely
but
I'd
be
downloading
and
installing
for
days
and
I
just
want
one
measley
bar
chart
!
It
doesn't
seem
to
have
been
updated
in
a
few
years
but
hey
it
works
!
You
didn't
mention
what
output
format
you
need
but
reportlab
is
good
at
creating
charts
both
in
pdf
and
bitmap
(
e.g
.
png
)
format
.
Here
is
a
simple
example
of
a
barchart
in
png
and
pdf
format
:
alt
text
http://i40.tinypic.com/2j677tl.jpg
Note
:
the
image
has
been
converted
to
jpg
by
the
image
host
.
Please
look
at
the
Open
Flash
Chart
embedding
for
WHIFF
http://aaron.oirt.rutgers.edu/myapp/docs/W1100_1600.openFlashCharts
and
the
amCharts
embedding
for
WHIFF
too
http://aaron.oirt.rutgers.edu/myapp/amcharts/doc.
Thanks
.
Chaco
from
enthought
is
another
option
PLplot
is
a
cross-platform
software
package
for
creating
scientific
plots
.
They
aren't
very
pretty
(
eye
catching
)
","
but
they
look
good
enough
.
Have
a
look
at
some
examples
(
both
source
code
and
pictures
)
.
The
PLplot
core
library
can
be
used
to
create
standard
x-y
plots
","
semi-log
plots
","
log-log
plots
","
contour
plots
","
3D
surface
plots
","
mesh
plots
","
bar
charts
and
pie
charts
.
It
runs
on
Windows
(
2000
","
XP
and
Vista
)
","
Linux
","
Mac
OS
X
","
and
other
Unices
.
If
you
like
to
use
gnuplot
for
plotting
","
you
should
consider
Gnuplot.py
.
It
provides
an
object-oriented
interface
to
gnuplot
","
and
also
allows
you
to
pass
commands
directly
to
gnuplot
.
Unfortunately
","
it
is
no
longer
being
actively
developed
.
I
don't
think
that's
possible
.
It
would
be
relatively
easy
to
add
this
functionality
to
the
underlying
C
+
+
wxWidgets
control
","
but
since
you're
using
wxPython
","
you'd
then
have
to
rebuild
that
as
well
which
is
a
tremendous
issue
.
Listing
/
walking
directories
in
Python
is
very
easy
","
so
I
would
recommend
trying
to
""""
roll
your
own
""""
using
one
of
the
simple
tree
controls
(
such
as
TreeCtrl
or
CustomTreeCtrl
)
.
It
should
really
be
quite
easy
to
call
the
directory
listing
code
when
some
directory
is
expanded
and
return
the
result
.
I
am
using
a
wxGenericDirCtrl
","
and
I
would
like
to
know
if
there
is
a
way
to
hide
directories
","
I'd
especially
like
to
hide
siblings
of
parent
nodes
.
For
example
if
my
directory
structure
looks
like
this
:
If
my
currently
selected
directory
is
/
a
/
c
/
d
is
there
any
way
to
hide
b
and
g
","
so
that
the
tree
looks
like
this
in
my
ctrl
:
I'm
currently
working
with
a
directory
structure
that
has
lots
and
lots
directories
that
are
irrelevant
to
most
users
","
so
it
would
be
nice
to
be
able
to
clean
it
up
.
Edit
:
If
it
makes
a
difference
","
I
am
using
wxPython
","
and
so
far
","
I
have
only
tested
my
code
on
linux
using
the
GTK
backend
","
but
I
do
plan
to
make
it
multi-platform
and
using
it
on
Windows
and
Mac
using
the
native
backends
.
Yes
","
take
a
look
at
the
""""
6.4
Packages
""""
section
in
http://docs.python.org/tut/node8.html:
Basically
","
you
can
place
a
bunch
of
files
into
a
directory
and
add
an
__init__.py
file
to
the
directory
.
If
the
directory
is
in
your
PYTHONPATH
or
sys.path
","
you
can
do
""""
import directoryname
""""
to
import everything
in
the
directory
or
""""
import directoryname
.
some_file_in_directory
""""
to
import a
specific
file
that
is
in
the
directory
.
The
__init__.py
files
are
required
to
make
Python
treat
the
directories
as
containing
packages
;
this
is
done
to
prevent
directories
with
a
common
name
","
such
as
""""
string
""""
","
from
unintentionally
hiding
valid
modules
that
occur
later
on
the
module
search
path
.
In
the
simplest
case
","
__init__.py
can
just
be
an
empty
file
","
but
it
can
also
execute
initialization
code
for
the
package
or
set
the
__all__
variable
","
described
later
.
Put
files
in
one
folder
.
Add
__init__.py
file
to
the
folder
.
Do
necessary
imports
in
__init__.py
Replace
multiple
imports
by
one
:
import folder_name
See
Python
Package
Management
I
have
a
file
that
I
want
to
include
in
Python
but
the
included
file
is
fairly
long
and
it'd
be
much
neater
to
be
able
to
split
them
into
several
files
but
then
I
have
to
use
several
include
statements
.
Is
there
some
way
to
group
together
several
files
and
include
them
all
at
once
?
I
would
strongly
consider
using
NumPy
to
do
this
.
You
get
efficient
N-dimensional
arrays
that
you
can
quickly
and
easily
process
.
since
you
are
thinking
in
variables
","
you
might
prefer
a
dictionary
over
a
list
of
lists
:
etc.
Lists
in
python
can
contain
any
type
of
object
-
-
If
I
understand
the
question
correctly
","
will
a
list
of
lists
do
the
job
?
Something
like
this
(
assuming
you
have
a
function
generate_poll_data()
which
creates
your
data
:
Then
","
data
[n]
will
be
the
list
of
data
from
the
(
n-1)th
run
.
Are
you
talking
about
doing
this
?
Would
something
like
this
work
?
In
the
end
","
you
are
left
with
a
list
of
valid
mctest
lists
.
What
I
changed
:
Used
a
list
comprehension
to
build
the
data
instead
of
a
for
loop
Used
random.randint
to
get
random
integers
Used
slices
and
sum
to
calculate
the
average
of
the
first
three
items
(
To
answer
your
actual
question
:
-
)
)
Put
the
results
in
a
list
mcworks
","
instead
of
creating
a
new
variable
for
every
iteration
I
am
writing
a
program
to
simulate
the
actual
polling
data
companies
like
Gallup
or
Rasmussen
publish
daily
:
www.gallup.com
and
www.rassmussenreports.com
I'm
using
a
brute
force
method
","
where
the
computer
generates
some
random
daily
polling
data
and
then
calculates
three
day
averages
to
see
if
the
average
of
the
random
data
matches
pollsters
numbers
.
(
Most
companies
poll
numbers
are
three
day
averages
)
Currently
","
it
works
well
for
one
iteration
","
but
my
goal
is
to
have
it
produce
the
most
common
simulation
that
matches
the
average
polling
data
.
I
could
then
change
the
code
of
anywhere
from
1
to
1000
iterations
.
And
this
is
my
problem
.
At
the
end
of
the
test
I
have
an
array
in
a
single
variable
that
looks
something
like
this
:
The
program
currently
produces
one
array
for
each
correct
simulation
.
I
can
store
each
array
in
a
single
variable
","
but
I
then
have
to
have
a
program
that
could
generate
1
to
1000
variables
depending
on
how
many
iterations
I
requested
!
?
How
do
I
avoid
this
?
I
know
there
is
an
intelligent
way
of
doing
this
that
doesn't
require
the
program
to
generate
variables
to
store
arrays
depending
on
how
many
simulations
I
want
.
Code
testing
for
McCain
:
How
do
I
repeat
without
creating
multiple
mcwork
vars
?
To
provide
some
timing
figures
behind
the
different
approaches
","
consider
the
following
code
.
The
get()
is
my
custom
addition
to
Python's
setobject.c
","
being
just
a
pop()
without
removing
the
element
.
The
output
is
:
This
means
that
the
for
/
break
solution
is
the
fastest
(
sometimes
faster
than
the
custom
get()
solution
)
.
Another
option
is
to
use
a
dictionary
with
values
you
don't
care
about
.
E.g
.
","
You
can
treat
the
keys
as
a
set
except
that
they're
just
an
array
:
A
side
effect
of
this
choice
is
that
your
code
will
be
backwards
compatible
with
older
","
pre-set
versions
of
Python
.
It's
maybe
not
the
best
answer
but
it's
another
option
.
Edit
:
You
can
even
do
something
like
this
to
hide
the
fact
that
you
used
a
dict
instead
of
an
array
or
set
:
Seemingly
the
most
compact
(
6
symbols
)
though
very
slow
way
to
get
a
set
element
(
made
possible
by
PEP
3132
)
:
With
Python
3.5
+
you
can
also
use
this
7-symbol
expression
(
thanks
to
PEP
448
)
:
Both
options
are
roughly
1000
times
slower
on
my
machine
than
for-loop
method
.
Following
@wr
.
post
","
I
get
similar
results
(
for
Python3.5
)
Output
:
However
","
when
changing
the
underlying
set
(
e.g
.
call
to
remove()
)
things
go
badly
for
the
iterable
examples
(
for
","
iter
)
:
Results
in
:
I
use
a
utility
function
I
wrote
.
Its
name
is
somewhat
misleading
because
it
kind
of
implies
it
might
be
a
random
item
or
something
like
that
.
Two
options
that
don't
require
copying
the
whole
set
:
Or
...
But
in
general
","
sets
don't
support
indexing
or
slicing
.
tl;dr
for
first_item
in
muh_set
:
break
remains
the
optimal
approach
in
Python
3.x
.
Curse
you
","
Guido
.
y
u
do
this
Welcome
to
yet
another
set
of
Python
3.x
timings
","
extrapolated
from
wr.'s
excellent
Python
2.x-specific
response
.
Unlike
AChampion's
equally
helpful
Python
3.x-specific
response
","
the
timings
below
also
time
outlier
solutions
suggested
above
â
€
“
including
:
list(s)
[0]
","
John's
novel
sequence-based
solution
.
"random.sample(s, 1)"
","
dF.'s
eclectic
RNG-based
solution
.
Code
Snippets
for
Great
Joy
Turn
on
","
tune
in
","
time
it
:
Quickly
Obsoleted
Timeless
Timings
Behold
!
Ordered
by
fastest
to
slowest
snippets
:
Faceplants
for
the
Whole
Family
Unsurprisingly
","
manual
iteration
remains
at
least
twice
as
fast
as
the
next
fastest
solution
.
Although
the
gap
has
decreased
from
the
Bad
Old
Python
2.x
days
(
in
which
manual
iteration
was
at
least
four
times
as
fast
)
","
it
disappoints
the
PEP
20
zealot
in
me
that
the
most
verbose
solution
is
the
best
.
At
least
converting
a
set
into
a
list
just
to
extract
the
first
element
of
the
set
is
as
horrible
as
expected
.
Thank
Guido
","
may
his
light
continue
to
guide
us
.
Surprisingly
","
the
RNG-based
solution
is
absolutely
horrible
.
List
conversion
is
bad
","
but
random
really
takes
the
awful-sauce
cake
.
So
much
for
the
Random
Number
God
.
I
just
wish
the
amorphous
They
would
PEP
up
a
set.get_first()
method
for
us
already
.
If
you're
reading
this
","
They
:
""""
Please
.
Do
something
.
""""
Since
you
want
a
random
element
","
this
will
also
work
:
The
documentation
doesn't
seem
to
mention
performance
of
random.sample
.
From
a
really
quick
empirical
test
with
a
huge
list
and
a
huge
set
","
it
seems
to
be
constant
time
for
a
list
but
not
for
the
set
.
Also
","
iteration
over
a
set
isn't
random
;
the
order
is
undefined
but
predictable
:
If
randomness
is
important
and
you
need
a
bunch
of
elements
in
constant
time
(
large
sets
)
","
I'd
use
random.sample
and
convert
to
a
list
first
:
Suppose
the
following
:
How
do
I
get
a
value
(
any
value
)
out
of
s
without
doing
s.pop()
?
I
want
to
leave
the
item
in
the
set
until
I
am
sure
I
can
remove
it
-
something
I
can
only
be
sure
of
after
an
asynchronous
call
to
another
host
.
Quick
and
dirty
:
But
do
you
know
of
a
better
way
?
Ideally
in
constant
time
.
Least
code
would
be
:
Obviously
this
would
create
a
new
list
which
contains
each
member
of
the
set
","
so
not
great
if
your
set
is
very
large
.
This
way
fits
in
better
with
the
rest
of
the
language
.
The
convention
in
python
is
that
you
add
__foo__
special
methods
to
objects
to
make
them
have
certain
capabilities
(
rather
than
e.g
.
deriving
from
a
specific
base
class
)
.
For
example
","
an
object
is
callable
if
it
has
a
__call__
method
iterable
if
it
has
an
__iter__
method
","
supports
access
with
[]
if
it
has
__getitem__
and
__setitem__
.
...
One
of
these
special
methods
is
__len__
which
makes
it
have
a
length
accessible
with
len()
.
example
:
Python
being
(
very
)
object
oriented
","
I
don't
understand
why
the
'
len
'
function
isn't
inherited
by
the
object
.
Plus
I
keep
trying
the
wrong
solution
since
it
appears
as
the
logical
one
to
me
Well
","
there
actually
is
a
length
method
","
it
is
just
hidden
:
The
len()
built-in
function
appears
to
be
simply
a
wrapper
for
a
call
to
the
hidden
len()
method
of
the
object
.
Not
sure
why
they
made
the
decision
to
implement
things
this
way
though
.
Guido's
explanation
is
here
:
First
of
all
","
I
chose
len(x)
over
x.len()
for
HCI
reasons
(
def
__len__()
came
much
later
)
.
There
are
two
intertwined
reasons
actually
","
both
HCI
:
(
a
)
For
some
operations
","
prefix
notation
just
reads
better
than
postfix
—
prefix
(
and
infix
!
)
operations
have
a
long
tradition
in
mathematics
which
likes
notations
where
the
visuals
help
the
mathematician
thinking
about
a
problem
.
Compare
the
easy
with
which
we
rewrite
a
formula
like
x*(a+b
)
into
xa
+
xb
to
the
clumsiness
of
doing
the
same
thing
using
a
raw
OO
notation
.
(
b
)
When
I
read
code
that
says
len(x)
I
know
that
it
is
asking
for
the
length
of
something
.
This
tells
me
two
things
:
the
result
is
an
integer
","
and
the
argument
is
some
kind
of
container
.
To
the
contrary
","
when
I
read
x.len()
","
I
have
to
already
know
that
x
is
some
kind
of
container
implementing
an
interface
or
inheriting
from
a
class
that
has
a
standard
len()
.
Witness
the
confusion
we
occasionally
have
when
a
class
that
is
not
implementing
a
mapping
has
a
get()
or
keys()
method
","
or
something
that
isn’t
a
file
has
a
write()
method
.
Saying
the
same
thing
in
another
way
","
I
see
‘
len
‘
as
a
built-in
operation
.
I’d
hate
to
lose
that
.
/
…
/
The
short
answer
:
1
)
backwards
compatibility
and
2
)
there's
not
enough
of
a
difference
for
it
to
really
matter
.
For
a
more
detailed
explanation
","
read
on
.
The
idiomatic
Python
approach
to
such
operations
is
special
methods
which
aren't
intended
to
be
called
directly
.
For
example
","
to
make
x
+
y
work
for
your
own
class
","
you
write
a
__add__
method
.
To
make
sure
that
int(spam)
properly
converts
your
custom
class
","
write
a
__int__
method
.
To
make
sure
that
len(foo)
does
something
sensible
","
write
a
__len__
method
.
This
is
how
things
have
always
been
with
Python
","
and
I
think
it
makes
a
lot
of
sense
for
some
things
.
In
particular
","
this
seems
like
a
sensible
way
to
implement
operator
overloading
.
As
for
the
rest
","
different
languages
disagree
;
in
Ruby
you'd
convert
something
to
an
integer
by
calling
spam.to_i
directly
instead
of
saying
int(spam)
.
You're
right
that
Python
is
an
extremely
object-oriented
language
and
that
having
to
call
an
external
function
on
an
object
to
get
its
length
seems
odd
.
On
the
other
hand
","
len(silly_walks)
isn't
any
more
onerous
than
silly_walks.len()
","
and
Guido
has
said
that
he
actually
prefers
it
(
http://mail.python.org/pipermail/python-3000/2006-November/004643.html
)
.
Maybe
you're
looking
for
__len__
.
If
that
method
exists
","
then
len(a)
calls
it
:
there
is
some
good
info
below
on
why
certain
things
are
functions
and
other
are
methods
.
It
does
indeed
cause
some
inconsistencies
in
the
language
.
http://mail.python.org/pipermail/python-dev/2008-January/076612.html
It
just
isn't
.
You
can
","
however
","
do
:
Adding
a
__len__()
method
to
a
class
is
what
makes
the
len()
magic
work
.
One
big
reason
to
learn
Perl
or
Ruby
is
to
help
you
automate
any
complicated
tasks
that
you
have
to
do
over
and
over
.
Or
if
you
have
to
analyse
contents
of
log
files
and
you
need
more
mungeing
than
available
using
grep
","
sed
","
etc.
Also
using
other
languages
","
e.g
.
Ruby
","
that
don't
have
much
""""
setup
cost
""""
will
let
you
quickly
prototype
ideas
before
implementing
them
in
C
+
+
","
Java
","
etc.
HTH
cheers
","
Rob
Edit
:
I
wrote
this
before
reading
the
update
to
the
original
question
.
See
my
other
answer
for
a
better
answer
to
the
updated
question
.
I
will
leave
this
as
is
as
a
warning
against
being
the
fastest
gun
in
the
west
=
)
Over
a
decade
ago
","
when
I
was
learning
the
ways
of
the
Computer
","
the
Old
Wise
Men
With
Beards
explained
how
C
and
C
+
+
are
the
tools
of
the
industry
.
No
one
used
Pascal
and
only
the
foolhardy
would
risk
their
companies
with
assembler
.
And
of
course
","
no
one
would
even
mention
the
awful
slow
ugly
thing
called
Java
.
It
will
not
be
a
tool
for
serious
business
.
So
.
Um
.
Replace
the
languages
in
the
above
story
and
perhaps
you
can
predict
the
future
.
Perhaps
you
can't
.
Point
is
","
Java
will
not
be
the
Last
Programming
Language
ever
and
also
you
will
most
likely
switch
employers
as
well
.
The
future
is
charging
at
you
24
hours
per
day
.
Be
prepared
.
Learning
new
languages
is
good
for
you
.
Also
","
in
some
cases
it
can
give
you
bragging
rights
for
a
long
time
.
My
first
university
course
was
in
Scheme
.
So
when
people
talk
to
me
about
the
new
language
du
jour
","
my
response
is
something
like
""""
First-class
functions
?
That's
so
last
century
.
""""
And
of
course
","
you
get
more
stuff
done
with
a
high-level
language
.
Testing
.
It's
often
quicker
and
easier
to
test
your
C
#
/
Java
application
by
using
a
dynamic
language
.
You
can
do
exploratory
testing
at
the
interactive
prompt
and
quickly
create
automated
test
scripts
.
A
lot
of
times
some
quick
task
comes
up
that
isn't
part
of
the
main
software
you
are
developing
.
Sometimes
the
task
is
one
off
ie
compare
this
file
to
the
database
and
let
me
know
the
differences
.
It
is
a
lot
easier
to
do
text
parsing
in
Perl
/
Ruby
/
Python
than
it
is
in
Java
or
C
#
(
partially
because
it
is
a
lot
easier
to
use
regular
expressions
)
.
It
will
probably
take
a
lot
less
time
to
parse
the
text
file
using
Perl
/
Ruby
/
Python
(
or
maybe
even
vbscript
cringe
and
then
load
it
into
the
database
than
it
would
to
create
a
Java
/
C
#
program
to
do
it
or
to
do
it
by
hand
.
Also
","
due
to
the
ease
at
which
most
of
the
dynamic
languages
parse
text
","
they
are
great
for
code
generation
.
Sure
your
final
project
must
be
in
C
#
/
Java
/
Transact
SQL
but
instead
of
cutting
and
pasting
100
times
","
finding
errors
","
and
cutting
and
pasting
another
100
times
it
is
often
(
but
not
always
)
easier
just
to
use
a
code
generator
.
A
recent
example
at
work
is
we
needed
to
get
data
from
one
accounting
system
into
our
accounting
system
.
The
system
has
an
import format
","
but
the
old
system
had
a
completely
different
format
(
fixed
width
although
some
things
had
to
be
matched
)
.
The
task
is
not
to
create
a
program
to
migrate
the
data
over
and
over
again
.
It
is
to
shove
the
data
into
our
system
and
then
maintain
it
there
going
forward
.
So
even
though
we
are
a
C
#
and
SQL
Server
shop
","
I
used
Python
to
convert
the
data
into
the
format
that
could
be
imported
by
our
application
.
Ultimately
it
doesn't
matter
that
I
used
python
","
it
matters
that
the
data
is
in
the
system
.
My
boss
was
pretty
impressed
.
Where
I
often
see
the
dynamic
languages
used
for
is
testing
.
It
is
much
easier
to
create
a
Python
/
Perl
/
Ruby
program
to
link
to
a
web
service
and
throw
some
data
against
it
than
it
is
to
create
the
equivalent
Java
program
.
You
can
also
use
python
to
hit
against
command
line
programs
","
generate
a
ton
of
garbage
(
but
still
valid
)
test
data
","
etc..
quite
easily
.
The
other
thing
that
dynamic
languages
are
big
on
is
code
generation
.
Creating
the
C
#
/
C
+
+
/
Java
code
.
Some
examples
follow
:
The
first
code
generation
task
I
often
see
is
people
using
dynamic
languages
to
maintain
constants
in
the
system
.
Instead
of
hand
coding
a
bunch
of
enums
","
a
dynamic
language
can
be
used
to
fairly
easily
parse
a
text
file
and
create
the
Java
/
C
#
code
with
the
enums
.
SQL
is
a
whole
other
ball
game
but
often
you
get
better
performance
by
cut
and
pasting
100
times
instead
of
trying
to
do
a
function
(
due
to
caching
of
execution
plans
or
putting
complicated
logic
in
a
function
causing
you
to
go
row
by
row
instead
of
in
a
set
)
.
In
fact
it
is
quite
useful
to
use
the
table
definition
to
create
certain
stored
procedures
automatically
.
It
is
always
better
to
get
buy
in
for
a
code
generator
.
But
even
if
you
don't
","
is
it
more
fun
to
spend
time
cutting
/
pasting
or
is
it
more
fun
to
create
a
Perl
/
Python
/
Ruby
script
once
and
then
have
that
generate
the
code
?
If
it
takes
you
hours
to
hand
code
something
but
less
time
to
create
a
code
generator
","
then
even
if
you
use
it
once
you
have
saved
time
and
hence
money
.
If
it
takes
you
longer
to
create
a
code
generator
than
it
takes
to
hand
code
once
but
you
know
you
will
have
to
update
the
code
more
than
once
","
it
may
still
make
sense
.
If
it
takes
you
2
hours
to
hand
code
","
4
hours
to
do
the
generator
but
you
know
you'll
have
to
hand
code
equivalent
work
another
5
or
6
times
than
it
is
obviously
better
to
create
the
generator
.
Also
some
things
are
easier
with
dynamic
languages
than
Java
/
C
#
/
C
/
C
+
+
.
In
particular
regular
expressions
come
to
mind
.
If
you
start
using
regular
expressions
in
Perl
and
realize
their
value
","
you
may
suddenly
start
making
use
of
the
Java
regular
expression
library
if
you
haven't
before
.
If
you
have
then
there
may
be
something
else
.
I
will
leave
you
with
one
last
example
of
a
task
that
would
have
been
great
for
a
dynamic
language
.
My
work
mate
had
to
take
a
directory
full
of
files
and
burn
them
to
various
cd's
for
various
customers
.
There
were
a
few
customers
but
a
lot
of
files
and
you
had
to
look
in
them
to
see
what
they
were
.
He
did
this
task
by
hand....A
Java
/
C
#
program
would
have
saved
time
","
but
for
one
time
and
with
all
the
development
overhead
it
isn't
worth
it
.
However
slapping
something
together
in
Perl
/
Python
/
Ruby
probably
would
have
been
worth
it
.
He
spent
several
hours
doing
it
.
It
would
have
taken
less
than
one
to
create
the
Python
script
to
inspect
each
file
","
match
which
customer
it
goes
to
","
and
then
move
the
file
to
the
appropriate
place.....Again
","
not
part
of
the
standard
job
.
But
the
task
came
up
as
a
one
off
.
Is
it
better
to
do
it
yourself
","
spend
the
larger
amount
of
time
to
make
Java
/
C
#
do
the
task
","
or
spend
a
much
smaller
amount
of
time
doing
it
in
Python
/
Perl
/
Ruby
.
If
you
are
using
C
or
C
+
+
the
point
is
even
more
dramatic
due
to
the
extra
concerns
of
programming
in
C
or
C
+
+
(
pointers
","
no
array
bounds
checking
","
etc.
)
.
Personally
I
work
on
a
Java
app
","
but
I
couldn't
get
by
without
perl
for
some
supporting
scripts
.
I've
got
scripts
to
quickly
flip
what
db
I'm
pointing
at
","
scripts
to
run
build
scripts
","
scripts
to
scrape
data
&
compare
stuff
.
Sure
I
could
do
all
that
with
java
","
or
maybe
shell
scripts
(
I've
got
some
of
those
too
)
","
but
who
wants
to
compile
a
class
(
making
sure
the
classpath
is
set
right
etc
)
when
you
just
need
something
quick
and
dirty
.
Knowing
a
scripting
language
can
remove
90
%
of
those
boring
/
repetitive
manual
tasks
.
Don't
tell
your
employer
that
you
want
to
learn
Ruby
.
Tell
him
you
want
to
learn
about
the
state-of-the-art
in
web
framework
technologies
.
it
just
happens
that
the
hottest
ones
are
Django
and
Ruby
on
Rails
.
You
should
also
consider
learning
a
functional
programming
language
like
Scala
.
It
has
many
of
the
advantages
of
Ruby
","
including
a
concise
syntax
","
and
powerful
features
like
closures
.
But
it
compiles
to
Java
class
files
and
and
integrate
seamlessly
into
a
Java
stack
","
which
may
make
it
much
easier
for
your
employer
to
swallow
.
Scala
isn't
dynamically
typed
","
but
its
""""
implicit
conversion
""""
feature
gives
many
","
perhaps
even
all
of
the
benefits
of
dynamic
typing
","
while
retaining
many
of
the
advantages
of
static
typing
.
A
good
hockey
player
plays
where
the
puck
is
.
A
great
hockey
player
plays
where
the
puck
is
going
to
be
.
-
Wayne
Gretzky
Our
industry
is
always
changing
.
No
language
can
be
mainstream
forever
.
To
me
Java
","
C
+
+
","
.
Net
is
where
the
puck
is
right
now
.
And
python
","
ruby
","
perl
is
where
the
puck
is
going
to
be
.
Decide
for
yourself
if
you
wanna
be
good
or
great
!
I
think
the
main
benefits
of
dynamic
languages
can
be
boiled
down
to
Rapid
development
Glue
The
short
design-code-test
cycle
time
makes
dynamic
languages
ideal
for
prototyping
","
tools
","
and
quick
&
dirty
one-off
scripts
.
IMHO
","
the
latter
two
can
make
a
huge
impact
on
a
programmer's
productivity
.
It
amazes
me
how
many
people
trudge
through
things
manually
instead
of
whipping
up
a
tool
to
do
it
for
them
.
I
think
it's
because
they
don't
have
something
like
Perl
in
their
toolbox
.
The
ability
to
interface
with
just
about
anything
(
other
programs
or
languages
","
databases
","
etc.
)
makes
it
easy
to
reuse
existing
work
and
automate
tasks
that
would
otherwise
need
to
be
done
manually
.
Dynamic
languages
are
a
different
way
to
think
and
sometimes
the
practices
you
learn
from
a
dynamic
or
functional
language
can
transfer
to
the
more
statically
typed
languages
but
if
you
never
take
the
time
to
learn
different
languages
","
you'll
never
get
the
benefit
of
having
a
knew
way
to
think
when
you
are
coding
.
Paul
Graham
posted
an
article
several
years
ago
about
why
Python
programmers
made
better
Java
programmers
.
(
http://www.paulgraham.com/pypar.html
)
Basically
","
regardless
of
whether
the
new
language
is
relevant
to
the
company's
current
methodology
","
learning
a
new
language
means
learning
new
ideas
.
Someone
who
is
willing
to
learn
a
language
that
isn't
considered
""""
business
class
""""
means
that
he
is
interested
in
programming
","
beyond
just
earning
a
paycheck
.
To
quote
Paul's
site
:
And
people
don't
learn
Python
because
it
will
get
them
a
job
;
they
learn
it
because
they
genuinely
like
to
program
and
aren't
satisfied
with
the
languages
they
already
know
.
Which
makes
them
exactly
the
kind
of
programmers
companies
should
want
to
hire
.
Hence
what
","
for
lack
of
a
better
name
","
I'll
call
the
Python
paradox
:
if
a
company
chooses
to
write
its
software
in
a
comparatively
esoteric
language
","
they'll
be
able
to
hire
better
programmers
","
because
they'll
attract
only
those
who
cared
enough
to
learn
it
.
And
for
programmers
the
paradox
is
even
more
pronounced
:
the
language
to
learn
","
if
you
want
to
get
a
good
job
","
is
a
language
that
people
don't
learn
merely
to
get
a
job
.
If
an
employer
was
willing
to
pay
for
the
cost
of
learning
a
new
language
","
chances
are
the
people
who
volunteered
to
learn
(
assuming
it
wasn't
a
mandatory
class
)
would
be
the
same
people
to
are
already
on
the
""""
fast
track
""""
.
I
don't
think
anyone
has
mentioned
this
yet
.
Learning
a
new
language
can
be
fun
!
Surely
that's
a
good
enough
reason
to
try
something
new
.
Others
have
already
explained
why
learning
more
languages
makes
you
a
better
programmer
.
As
for
convincing
your
boss
it's
worth
it
","
this
is
probably
just
your
company's
culture
.
Some
places
make
career
and
skill
progress
a
policy
(
move
up
or
out
)
","
some
places
value
it
but
leave
it
up
to
the
employee's
initiative
","
and
some
places
are
very
focused
on
the
bottom
line
.
If
you
have
to
explain
why
learning
a
language
is
a
good
thing
to
your
boss
","
my
advice
would
be
to
stay
at
work
only
as
long
as
necessary
","
then
go
home
and
study
new
things
on
your
own
.
For
after
work
work
","
for
freelance
jobs
...
:
)
and
final
to
be
programming
literate
as
possible
as
...
;
)
They're
useful
for
the
""""
Quick
Hack
""""
that
is
for
plugging
a
gap
in
your
main
language
for
a
quick
(
and
potentially
dirty
)
fix
faster
than
it
would
take
to
develop
the
same
in
your
main
language
.
An
example
:
a
simple
script
in
perl
to
go
through
a
large
text
file
and
replace
all
instances
of
an
email
address
with
another
is
trivial
with
an
amount
of
time
taken
in
the
10
minute
range
.
Hacking
a
console
app
together
to
do
the
same
in
your
main
language
would
take
multiples
of
that
.
You
also
have
the
benefit
that
exposing
yourself
to
additional
languages
broadens
your
abilities
and
learning
to
attack
problems
from
a
different
languages
perspective
can
be
as
valuable
as
the
language
itself
.
Finally
","
scripting
languages
are
very
useful
in
the
realm
of
extension
.
Take
LUA
as
an
example
.
You
can
bolt
a
lua
interpreter
into
your
app
with
very
little
overhead
and
you
now
have
a
way
to
create
rich
scripting
functionality
that
can
be
exposed
to
end
users
or
altered
and
distributed
quickly
without
requiring
a
rebuild
of
the
entire
app
.
This
is
used
to
great
effect
in
many
games
most
notably
World
of
Warcraft
.
Given
the
increasing
focus
to
running
dynamic
languages
(
da-vinci
vm
etc.
)
on
the
JVM
and
the
increasing
number
of
dynamic
languages
that
do
run
on
it
(
JRuby
","
Grrovy
","
Jython
)
I
think
the
usecases
are
just
increasing
.
Some
of
the
scenarios
I
found
really
benifited
are
Prototyping
-
use
RoR
or
Grails
to
build
quick
prototypes
with
advantage
of
being
able
to
runn
it
on
the
standard
app
server
and
(
maybe
)
reuse
existing
services
etc.
Testing
-
right
unit
tests
much
much
faster
in
dynamic
languages
Performance
/
automation
test
scripting
-
some
of
these
tools
are
starting
to
allow
the
use
standard
dynamic
language
of
choice
to
write
the
test
scripts
instead
of
proprietary
script
languages
.
Side
benefit
might
be
to
the
able
to
reuse
some
unit
test
code
you've
already
written
.
If
all
you
have
is
a
hammer
","
every
problem
begins
to
look
like
a
nail
.
There
are
times
when
having
a
screwdriver
or
pair
of
pliers
makes
a
complicated
problem
trivial
.
Nobody
asks
contractors
","
carpenters
","
etc
","
""""
Why
learn
to
use
a
screwdriver
if
i
already
have
a
hammer
?
""""
.
Really
good
contractors
/
carpenters
have
tons
of
tools
and
know
how
to
use
them
well
.
All
programmers
should
be
doing
the
same
thing
","
learning
to
use
new
tools
and
use
them
well
.
But
before
we
use
any
power
tools
","
lets
take
a
moment
to
talk
about
shop
safety
.
Be
sure
to
read
","
understand
","
and
follow
all
the
safety
rules
that
come
with
your
power
tools
.
Doing
so
will
greatly
reduce
the
risk
of
personal
injury
.
And
remember
this
:
there
is
no
more
important
rule
than
to
wear
these
:
safety
glasses
-
-
Norm
Knowing
grep
and
ruby
made
it
possible
to
narrow
down
a
problem
","
and
verify
the
fix
for
","
an
issue
involving
tons
of
java
exceptions
on
some
production
servers
.
Because
I
threw
the
solution
together
in
ruby
","
it
was
done
(
designed
","
implemented
","
tested
","
run
","
bug-fixed
","
re-run
","
enhanced
","
results
analyzed
)
in
an
afternoon
instead
of
a
couple
of
days
.
I
could
have
solved
the
same
problem
using
an
all-java
solution
or
a
C
#
solution
","
but
it
most
likely
would
have
taken
me
longer
.
Having
dynamic
language
expertise
also
sometimes
leads
you
to
simpler
solutions
in
less
dynamic
languages
.
In
ruby
","
perl
or
python
","
you
just
intuitively
reach
for
associative
arrays
(
hashes
","
dictionaries
","
whatever
word
you
want
to
use
)
for
the
smallest
things
","
where
you
might
be
tempted
to
create
a
complex
class
hierarchy
in
a
statically
typed
language
when
the
problem
doesn't
necessarily
demand
it
.
Plus
you
can
plug
in
most
scripting
languages
into
most
runtimes
.
So
it
doesn't
have
to
be
either
/
or
.
Learning
a
new
language
is
a
long-term
process
.
In
a
couple
of
days
you'll
learn
the
basics
","
yes
.
But
!
As
you
probably
know
","
the
real
practical
applicability
of
any
language
is
tied
to
the
standard
library
and
other
available
components
.
Learning
how
to
use
the
efficiently
requires
a
lot
of
hands-on
experience
.
Perhaps
the
only
immediate
short-term
benefit
is
that
developers
learn
to
distinguish
the
nails
that
need
a
Python
/
Perl
/
Ruby
-
hammer
.
And
","
if
they
are
any
good
","
they
can
then
study
some
more
(
online
","
perhaps
!
)
and
become
real
experts
.
The
long-term
benefits
are
easier
to
imagine
:
The
employee
becomes
a
better
developer
.
Better
developer
=
>
better
quality
.
We
are
living
in
a
knowledge
economy
these
days
.
It's
wiser
to
invest
in
those
brains
that
already
work
for
you
.
It
is
easier
to
adapt
when
the
next
big
language
emerges
.
It
is
very
likely
that
the
NBL
will
have
many
of
the
features
present
in
today's
scripting
languages
:
first-class
functions
","
closures
","
streams
/
generators
","
etc.
New
market
possibilities
and
ability
to
respond
more
quickly
.
Even
if
you
are
not
writing
Python
","
other
people
are
.
Your
clients
?
Another
vendor
in
the
project
?
Perhaps
a
critical
component
was
written
in
some
other
language
?
It
will
cost
money
and
time
","
if
you
do
not
have
people
who
can
understand
the
code
and
interface
with
it
.
Recruitment
.
If
your
company
has
a
reputation
of
teaching
new
and
interesting
stuff
to
people
","
it
will
be
easier
to
recruit
the
top
people
.
Everyone
is
doing
Java
/
C
#
/
C
+
+
.
It
is
not
a
very
effective
way
to
differentiate
yourself
in
the
job
market
.
The
""""
real
benefit
""""
that
an
employer
could
see
is
a
better
programmer
who
can
implement
solutions
faster
;
however
","
you
will
not
be
able
to
provide
any
hard
numbers
to
justify
the
expense
and
an
employer
will
most
likely
have
you
work
on
what
makes
money
now
as
opposed
to
having
you
work
on
things
that
make
the
future
better
.
The
only
time
you
can
get
training
on
the
employer's
dime
","
is
when
they
perceive
a
need
for
it
and
it's
cheaper
than
hiring
a
new
person
who
already
has
that
skill-set
.
Learning
something
with
a
flexible
OOP
system
","
like
Lisp
or
Perl
(
see
Moose
)
","
will
allow
you
to
better
expand
and
understand
your
thoughts
on
software
engineering
.
Ideally
","
every
language
has
some
unique
facet
(
whether
it
be
CLOS
or
some
other
technique
)
that
enhances
","
extends
and
grows
your
abilities
as
a
programmer
.
It's
all
about
broadening
your
horizons
as
a
developer
.
If
you
limit
yourself
to
only
strong-typed
languages
","
you
may
not
end
up
the
best
programmer
you
could
.
As
for
tasks
","
Python
/
Lua
/
Ruby
/
Perl
are
great
for
small
simple
tasks
","
like
finding
some
files
and
renaming
them
.
They
also
work
great
when
paired
with
a
framework
(
e.g
.
Rails
","
Django
","
Lua
for
Windows
)
for
developing
simple
apps
quickly
.
Hell
","
37Signals
is
based
on
creating
simple
yet
very
useful
apps
in
Ruby
on
Rails
.
check
out
the
answers
to
this
thead
:
https://stackoverflow.com/questions/76364/what-is-the-single-most-effective-thing-you-did-to-improve-your-programming-ski#84112
Learning
new
languages
is
about
keeping
an
open
mind
and
learning
new
ways
of
doing
things
.
Im
not
sure
if
this
is
what
you
are
looking
for
","
but
we
write
our
main
application
with
Java
at
the
small
company
I
work
for
","
but
have
used
python
to
write
smaller
scripts
quickly
.
Backup
software
","
temporary
scripts
to
manipulate
data
and
push
out
results
.
It
just
seems
easier
sometimes
to
sit
down
with
python
and
write
a
quick
script
than
mess
with
classes
and
stuff
in
java
.
Temp
scripts
that
aren't
going
to
stick
around
don't
need
a
lot
of
design
time
wasted
on
them
.
And
I
am
lazy
","
but
it
is
good
to
just
learn
as
much
as
you
can
of
course
and
see
what
features
exist
in
other
languages
.
Knowing
more
never
hurts
you
in
future
career
changes
:
)
Towards
answering
the
updated
question
","
its
a
chicken
/
egg
problem
.
The
best
way
to
justify
an
expense
is
to
show
how
it
reduces
a
cost
somewhere
else
","
so
you
may
need
to
spend
some
extra
/
personal
time
to
learn
something
first
to
build
some
kind
of
functional
prototype
.
Show
your
boss
a
demo
like
""""
hey
","
i
did
this
thing
","
and
it
saves
me
this
much
time
[
or
better
yet
","
this
much
$$
]
","
imagine
if
everyone
could
use
this
how
much
money
we
would
save
""""
and
then
after
they
agree
","
explain
how
it
is
some
other
technology
and
that
it
is
worth
the
expense
to
get
more
training
","
and
training
for
others
on
how
to
do
it
better
.
I
primarily
program
in
Java
and
C
#
but
use
dynamic
languages
(
ruby
/
perl
)
to
support
smoother
deployment
","
kicking
off
OS
tasks
","
automated
reporting
","
some
log
parsing
","
etc.
After
a
short
time
learning
and
experimenting
with
ruby
or
perl
you
should
be
able
to
write
some
regex
manipulating
scripts
that
can
alter
data
formats
or
grab
information
from
logs
.
An
example
of
a
small
ruby
/
perl
script
that
could
be
written
quickly
would
be
a
script
to
parse
a
very
large
log
file
and
report
out
only
a
few
events
of
interest
in
either
a
human
readable
format
or
a
csv
format
.
Also
","
having
experience
with
a
variety
of
different
programming
languages
should
help
you
think
of
new
ways
to
tackle
problems
in
more
structured
languages
like
Java
","
C
+
+
","
and
C
#
.
Dynamic
languages
are
fantastic
for
prototyping
ideas
.
Often
for
performance
reasons
they
won't
work
for
permanent
solutions
or
products
.
But
","
with
languages
like
Python
","
which
allow
you
to
embed
standard
C
/
C
+
+
/
Java
inside
them
or
visa
versa
","
you
can
speed
up
the
really
critical
bits
but
leave
it
glued
together
with
the
flexibility
of
a
dynamic
language
.
...
and
so
you
get
the
best
of
both
worlds
.
If
you
need
to
justify
this
in
terms
of
why
more
people
should
learn
these
languages
","
just
point
out
much
faster
you
can
develop
the
same
software
and
how
much
more
robust
the
solution
is
(
because
debugging
/
fixing
problems
in
dynamic
languages
is
in
my
experience
","
considerably
easier
!
)
.
I
wonder
why
would
a
C
+
+
","
C
#
","
Java
developer
want
to
learn
a
dynamic
language
?
Assuming
the
company
won't
switch
its
main
development
language
from
C
+
+
/
C
#
/
Java
to
a
dynamic
one
what
use
is
there
for
a
dynamic
language
?
What
helper
tasks
can
be
done
by
the
dynamic
languages
faster
or
better
after
only
a
few
days
of
learning
than
with
the
static
language
that
you
have
been
using
for
several
years
?
Update
After
seeing
the
first
few
responses
it
is
clear
that
there
are
two
issues
.
My
main
interest
would
be
something
that
is
justifiable
to
the
employer
as
an
expense
.
That
is
","
I
am
looking
for
justifications
for
the
employer
to
finance
the
learning
of
a
dynamic
language
.
Aside
from
the
obvious
that
the
employee
will
have
broader
view
","
the
employers
are
usually
looking
for
some
""""
real
""""
benefit
.
Do
you
expect
to
work
for
this
company
forever
?
If
you're
ever
out
on
the
job
market
","
pehaps
some
prospective
employers
will
be
aware
of
the
Python
paradox
.
I
have
found
the
more
that
I
play
with
Ruby
","
the
better
I
understand
C
#
.
1
)
As
you
switch
between
these
languages
that
each
of
them
has
their
own
constructs
and
philosophies
behind
the
problems
that
they
try
to
solve
.
This
will
help
you
when
finding
the
right
tool
for
the
job
or
the
domain
of
a
problem
.
2
)
The
role
of
the
compiler
(
or
interpreter
for
some
languages
)
becomes
more
prominent
.
Why
is
Ruby's
type
system
differ
from
the
.
Net
/
C
#
system
?
What
problems
do
each
of
these
solve
?
You'll
find
yourself
understanding
at
a
lower
level
the
constructs
of
the
compiler
and
its
influence
on
the
language
3
)
Switching
between
Ruby
and
C
#
really
helped
me
to
understand
Design
Patterns
better
.
I
really
suggest
implementing
common
design
patterns
in
a
language
like
C
#
and
then
in
a
language
like
Ruby
.
It
often
helped
me
see
through
some
of
the
compiler
ceremony
to
the
philosophy
of
a
particular
pattern
.
4
)
A
different
community
.
C
#
","
Java
","
Ruby
","
Python
","
etc
all
have
different
communities
that
can
help
engage
your
abilities
.
It
is
a
great
way
to
take
your
craft
to
the
next
level
.
5
)
Last
","
but
not
least
","
because
new
languages
are
fun
:
)
When
I
first
learned
Python
","
I
worked
for
a
Java
shop
.
Occasionally
I'd
have
to
do
serious
text-processing
tasks
which
were
much
easier
to
do
with
quick
Python
scripts
than
Java
programs
.
For
example
","
if
I
had
to
parse
a
complex
CSV
file
and
figure
out
which
of
its
rows
corresponded
to
rows
in
our
Oracle
database
","
this
was
much
easier
to
do
with
Python
than
Java
.
More
than
that
","
I
found
that
learning
Python
made
me
a
much
better
Java
programmer
;
having
learned
many
of
the
same
concepts
in
another
language
I
feel
that
I
understand
those
concepts
much
better
.
And
as
for
what
makes
Python
easier
than
Java
","
you
might
check
out
this
question
:
Java
->
Python
?
Let
me
turn
your
question
on
its
head
by
asking
what
use
it
is
to
an
American
English
speaker
to
learn
another
language
?
The
languages
we
speak
(
and
those
we
program
in
)
inform
the
way
we
think
.
This
can
happen
on
a
fundamental
level
","
such
as
c
+
+
versus
javascript
versus
lisp
","
or
on
an
implementation
level
","
in
which
a
ruby
construct
provides
a
eureka
moment
for
a
solution
in
your
""""
real
job
.
""""
Speaking
of
your
real
job
","
if
the
market
goes
south
and
your
employer
decides
to
""""
right
size
""""
you
","
how
do
you
think
you'll
stack
up
against
a
guy
who
is
flexible
because
he's
written
software
in
tens
of
languages
","
instead
of
your
limited
exposure
?
All
things
being
equal
","
I
think
the
answer
is
clear
.
Finally
","
you
program
for
a
living
because
you
love
programming
...
right
?
Often
","
dynamc
languages
(
especially
python
and
lua
)
are
embedded
in
programs
to
add
a
more
plugin-like
functionality
and
because
they
are
high-level
languages
that
make
it
easy
to
add
certain
behavior
","
where
a
low
/
mid-level
language
is
not
needed
.
Lua
specificially
lacks
all
the
low-level
system
calls
because
it
was
designed
for
easeof-use
to
add
functionality
within
the
program
","
not
as
a
general
programming
language
.
Don't
bother
your
employer
","
spend
~
$40
on
a
book
","
download
some
software
","
and
devote
some
time
each
day
to
read
/
do
exercises
.
In
no
time
you'll
be
trained
:
)
Philosophical
issues
aside
","
I
know
that
I
have
gotten
value
from
writing
quick-and-dirty
Ruby
scripts
to
solve
brute-force
problems
that
Java
was
just
too
big
for
.
Last
year
I
had
three
separate
directory
structures
that
were
all
more-or-less
the
same
","
but
with
lots
of
differences
among
the
files
(
the
client
hadn't
heard
of
version
control
and
I'll
leave
the
rest
to
your
imagination
)
.
It
would
have
taken
a
great
deal
of
overhead
to
write
an
analyzer
in
Java
","
but
in
Ruby
I
had
one
working
in
about
40
minutes
.
I
have
often
found
that
learning
another
language
","
especially
a
dynamically
typed
language
","
can
teach
you
things
about
other
languages
and
make
you
an
overall
better
programmer
.
Learning
ruby
","
for
example
","
will
teach
you
Object
Oriented
programming
in
ways
Java
wont
","
and
vice
versa
.
All
in
all
","
I
believe
that
it
is
better
to
be
a
well
rounded
programmer
than
stuck
in
a
single
language
.
It
makes
you
more
valuable
to
the
companies
/
clients
you
work
for
.
I
use
CherryPy
as
my
web
server
(
which
comes
bundled
with
Turbogears
)
","
and
I
simply
run
multiple
instances
of
the
CherryPy
web
server
on
different
ports
bound
to
localhost
.
Then
I
configure
Apache
with
mod_proxy
and
mod_rewrite
to
transparently
forward
requests
to
the
proper
port
based
on
the
HTTP
request
.
Using
Django
on
apache
with
mod_python
","
I
host
multiple
(
unrelated
)
django
sites
simply
with
the
following
apache
config
:
No
need
for
multiple
apache
instances
or
proxy
servers
.
Using
a
different
PythonInterpreter
directive
for
each
site
(
the
name
you
enter
is
arbitrary
)
keeps
the
namespaces
separate
.
Using
multiple
server
instances
on
local
ports
is
a
good
idea
","
but
you
don't
need
a
full
featured
web
server
to
redirect
HTTP
requests
.
I
would
use
pound
as
a
reverse
proxy
to
do
the
job
.
It
is
small
","
fast
","
simple
and
does
exactly
what
we
need
here
.
WHAT
POUND
IS
:
a
reverse-proxy
:
it
passes
requests
from
client
browsers
to
one
or
more
back-end
servers
.
a
load
balancer
:
it
will
distribute
the
requests
from
the
client
browsers
among
several
back-end
servers
","
while
keeping
session
information
.
an
SSL
wrapper
:
Pound
will
decrypt
HTTPS
requests
from
client
browsers
and
pass
them
as
plain
HTTP
to
the
back-end
servers
.
an
HTTP
/
HTTPS
sanitizer
:
Pound
will
verify
requests
for
correctness
and
accept
only
well-formed
ones
.
a
fail
over-server
:
should
a
back-end
server
fail
","
Pound
will
take
note
of
the
fact
and
stop
passing
requests
to
it
until
it
recovers
.
a
request
redirector
:
requests
may
be
distributed
among
servers
according
to
the
requested
URL
.
Django
has
this
built
in
.
See
the
sites
framework
.
As
a
general
technique
","
include
a
'
host
'
column
in
your
database
schema
attached
to
the
data
you
want
to
be
host-specific
","
then
include
the
Host
HTTP
header
in
the
query
when
you
are
retrieving
data
.
What
are
come
good
(
or
at
least
clever
)
ways
of
running
multiple
sites
from
a
single
","
common
Python
web
framework
(
ie
:
Pylons
","
TurboGears
","
etc
)
?
I
know
you
can
do
redirection
based
on
the
domain
or
path
to
rewrite
the
URI
to
point
at
a
site-specific
location
and
I've
also
seen
some
brutish
""""
if
site
=
=
'
site1
'
/
elseif
/
elseif
/
etc
""""
that
I
would
like
to
avoid
.
As
of
3.3
","
time.clock()
is
deprecated
","
and
it's
suggested
to
use
time.process_time()
or
time.perf_counter()
instead
.
Previously
in
2.7
","
according
to
the
time
module
docs
:
time.clock()
On
Unix
","
return
the
current
processor
time
as
a
floating
point
number
expressed
in
seconds
.
The
precision
","
and
in
fact
the
very
definition
of
the
meaning
of
“
processor
time
”
","
depends
on
that
of
the
C
function
of
the
same
name
","
but
in
any
case
","
this
is
the
function
to
use
for
benchmarking
Python
or
timing
algorithms
.
On
Windows
","
this
function
returns
wall-clock
seconds
elapsed
since
the
first
call
to
this
function
","
as
a
floating
point
number
","
based
on
the
Win32
function
QueryPerformanceCounter()
.
The
resolution
is
typically
better
than
one
microsecond
.
Additionally
","
there
is
the
timeit
module
for
benchmarking
code
snippets
.
To
extend
on
@Hill's
results
","
here's
a
test
using
python
2.7.6
on
Xubuntu
14.04
through
wine
:
(
timeit.default_timer
will
use
time.clock()
because
it
sees
the
OS
as
'
win32
'
)
Note
","
my
laptop
is
slow
and
I'm
currently
hosting
3
servers
while
running
5
resource-hungry
tabs
in
chromium
with
alot
of
extensions
.
So
yes
","
I
have
alot
of
overhead
making
this
look
like
it's
taking
more
time
than
it
should
.
Which
is
better
to
use
for
timing
in
Python
?
time.clock()
or
time.time()
?
Which
one
provides
more
accuracy
?
for
example
:
vs
.
Right
answer
:
They're
both
the
same
length
of
a
fraction
.
But
which
faster
if
subject
is
time
?
A
little
test
case
:
I
am
not
work
an
Swiss
labs
but
I've
tested
.
.
Based
of
this
question
:
time.clock()
is
better
than
time.time()
Edit
:
time.clock()
is
internal
counter
so
can't
use
outside
","
got
limitations
max
32BIT
FLOAT
","
can't
continued
counting
if
not
store
first
/
last
values
.
Can't
merge
another
one
counter
...
On
Unix
time.clock()
measures
the
amount
of
CPU
time
that
has
been
used
by
the
current
process
","
so
it's
no
good
for
measuring
elapsed
time
from
some
point
in
the
past
.
On
Windows
it
will
measure
wall-clock
seconds
elapsed
since
the
first
call
to
the
function
.
On
either
system
time.time()
will
return
seconds
passed
since
the
epoch
.
If
you're
writing
code
that's
meant
only
for
Windows
","
either
will
work
(
though
you'll
use
the
two
differently
-
no
subtraction
is
necessary
for
time.clock()
)
.
If
this
is
going
to
run
on
a
Unix
system
or
you
want
code
that
is
guaranteed
to
be
portable
","
you
will
want
to
use
time.time()
.
Others
have
answered
re
:
time.time()
vs
.
time.clock()
.
However
","
if
you're
timing
the
execution
of
a
block
of
code
for
benchmarking
/
profiling
purposes
","
you
should
take
a
look
at
the
timeit
module
.
Depends
on
what
you
care
about
.
If
you
mean
WALL
TIME
(
as
in
","
the
time
on
the
clock
on
your
wall
)
","
time.clock()
provides
NO
accuracy
because
it
may
manage
CPU
time
.
To
the
best
of
my
understanding
","
time.clock()
has
as
much
precision
as
your
system
will
allow
it
.
The
short
answer
is
:
most
of
the
time
time.clock()
will
be
better
.
However
","
if
you're
timing
some
hardware
(
for
example
some
algorithm
you
put
in
the
GPU
)
","
then
time.clock()
will
get
rid
of
this
time
and
time.time()
is
the
only
solution
left
.
Note
:
whatever
the
method
used
","
the
timing
will
depend
on
factors
you
cannot
control
(
when
will
the
process
switch
","
how
often
","
...
)
","
this
is
worse
with
time.time()
but
exists
also
with
time.clock()
","
so
you
should
never
run
one
timing
test
only
","
but
always
run
a
series
of
test
and
look
at
mean
/
variance
of
the
times
.
Usually
time()
is
more
precise
","
because
operating
systems
do
not
store
the
process
running
time
with
the
precision
they
store
the
system
time
(
ie
","
actual
time
)
I
use
this
code
to
compare
2
methods
.
My
OS
is
windows
8
","
processor
core
i5
","
RAM
4GB
output
:
time()
=
0.0993799996376
clock()
=
0.0993572257367
Use
the
time.time()
is
preferred
.
For
my
own
practice
.
time()
has
better
precision
than
clock()
on
Linux
.
clock()
only
has
precision
less
than
10
ms
.
While
time()
gives
prefect
precision
.
My
test
is
on
CentOS
6.4ï¼Œ
python
2.6
using
clock()
:
Comparing
test
result
between
Ubuntu
Linux
and
Windows
7
.
On
Ubuntu
On
Windows
7
One
thing
to
keep
in
mind
:
Changing
the
system
time
affects
time.time()
but
not
time.clock()
.
I
needed
to
control
some
automatic
tests
executions
.
If
one
step
of
the
test
case
took
more
than
a
given
amount
of
time
","
that
TC
was
aborted
to
go
on
with
the
next
one
.
But
sometimes
a
step
needed
to
change
the
system
time
(
to
check
the
scheduler
module
of
the
application
under
test
)
","
so
after
setting
the
system
time
a
few
hours
in
the
future
","
the
TC
timeout
expired
and
the
test
case
was
aborted
.
I
had
to
switch
from
time.time()
to
time.clock()
to
handle
this
properly
.
Short
answer
:
use
time.clock()
for
timing
in
Python
.
On
*
nix
systems
","
clock()
returns
the
processor
time
as
a
floating
point
number
","
expressed
in
seconds
.
On
Windows
","
it
returns
the
seconds
elapsed
since
the
first
call
to
this
function
","
as
a
floating
point
number
.
time()
returns
the
the
seconds
since
the
epoch
","
in
UTC
","
as
a
floating
point
number
.
There
is
no
guarantee
that
you
will
get
a
better
precision
that
1
second
(
even
though
time()
returns
a
floating
point
number
)
.
Also
note
that
if
the
system
clock
has
been
set
back
between
two
calls
to
this
function
","
the
second
function
call
will
return
a
lower
value
.
The
difference
is
very
platform-specific
.
clock()
is
very
different
on
Windows
than
on
Linux
","
for
example
.
For
the
sort
of
examples
you
describe
","
you
probably
want
the
""""
timeit
""""
module
instead
.
as
python
was
not
meant
to
deal
with
OS-specific
issues
(
it's
supposed
to
be
interpreted
and
cross
platform
)
","
i
would
execute
an
external
command
to
do
so
:
in
unix
the
command
is
ifconfig
if
you
execute
it
as
a
pipe
you
get
the
desired
result
:
You
need
ARP
.
Python's
standard
library
doesn't
include
any
code
for
that
","
so
you
either
need
to
call
an
external
program
(
your
OS
may
have
an
'
arp
'
utility
)
or
you
need
to
build
the
packets
yourself
(
possibly
with
a
tool
like
Scapy
.
You
would
want
to
parse
the
output
of
'
arp
'
","
but
the
kernel
ARP
cache
will
only
contain
those
IP
address(es)
if
those
hosts
have
communicated
with
the
host
where
the
Python
script
is
running
.
ifconfig
can
be
used
to
display
the
MAC
addresses
of
local
interfaces
","
but
not
those
on
the
LAN
.
This
article
","
""""
Send
hand-crafted
Ethernet
Frames
in
Python
(
ARP
for
example
)
""""
","
seems
to
be
exactly
what
you
are
looking
for
.
Mark
Pilgrim
describes
how
to
do
this
on
Windows
for
the
current
machine
with
the
Netbios
module
here
.
You
can
get
the
Netbios
module
as
part
of
the
Win32
package
available
at
python.org
.
Unfortunately
at
the
moment
I
cannot
find
the
docs
on
the
module
.
I
don't
think
there
is
a
built
in
way
to
get
it
from
Python
itself
.
My
question
is
","
how
are
you
getting
the
IP
information
from
your
network
?
To
get
it
from
your
local
machine
you
could
parse
ifconfig
(
unix
)
or
ipconfig
(
windows
)
with
little
difficulty
.
I'd
like
to
search
for
a
given
MAC
address
on
my
network
","
all
from
within
a
Python
script
.
I
already
have
a
map
of
all
the
active
IP
addresses
in
the
network
but
I
cannot
figure
out
how
to
glean
the
MAC
address
.
Any
ideas
?
If
you
want
a
pure
Python
solution
","
you
can
take
a
look
at
Scapy
to
craft
packets
(
you
need
to
send
ARP
request
","
and
inspect
replies
)
.
Or
if
you
don't
mind
invoking
external
program
","
you
can
use
arping
(
on
Un*x
systems
","
I
don't
know
of
a
Windows
equivalent
)
.
Depends
on
your
platform
.
If
you're
using
*
nix
","
you
can
use
the
'
arp
'
command
to
look
up
the
mac
address
for
a
given
IP
(
assuming
IPv4
)
address
.
If
that
doesn't
work
","
you
could
ping
the
address
and
then
look
","
or
if
you
have
access
to
the
raw
network
(
using
BPF
or
some
other
mechanism
)
","
you
could
send
your
own
ARP
packets
(
but
that
is
probably
overkill
)
.
It
seems
that
there
is
not
a
native
way
of
doing
this
with
Python
.
Your
best
bet
would
be
to
parse
the
output
of
""""
ipconfig
/
all
""""
on
Windows
","
or
""""
ifconfig
""""
on
Linux
.
Consider
using
os.popen()
with
some
regexps
.
PyCrash
?
Whether
you
use
SMTP
or
HTTP
to
send
the
data
","
you
need
to
have
a
username
/
password
in
the
application
to
prevent
just
anyone
from
sending
random
data
to
you
.
With
that
in
mind
","
I
suspect
it
would
be
easier
to
use
SMTP
rather
than
HTTP
to
send
the
data
.
I
can't
think
of
a
way
to
do
this
without
including
a
username
and
password
for
the
smtp
server
in
the
application
...
You
only
need
a
username
and
password
for
authenticating
yourself
to
a
smarthost
.
You
don't
need
it
to
send
mail
directly
","
you
need
it
to
send
mail
through
a
relay
","
e.g
.
your
ISP's
mail
server
.
It's
perfectly
possible
to
send
email
without
authentication
-
that's
why
spam
is
so
hard
to
stop
.
Having
said
that
","
some
ISPs
block
outbound
traffic
on
port
25
","
so
the
most
robust
alternative
is
an
HTTP
POST
","
which
is
unlikely
to
be
blocked
by
anything
.
Be
sure
to
pick
a
URL
that
you
won't
feel
restricted
by
later
on
","
or
better
yet
","
have
the
application
periodically
check
for
updates
","
so
if
you
decide
to
change
domains
or
something
","
you
can
push
an
update
in
advance
.
Security
isn't
really
an
issue
.
You
can
fairly
easily
discard
junk
data
","
so
all
that
really
concerns
you
is
whether
or
not
somebody
would
go
to
the
trouble
of
constructing
fake
tracebacks
to
mess
with
you
","
and
that's
a
very
unlikely
situation
.
As
for
the
payload
","
PyCrash
can
help
you
with
that
.
What
would
be
the
best
way
to
implement
a
simple
crash
/
error
reporting
mechanism
?
Details
:
my
app
is
cross-platform
(
mac
/
windows
/
linux
)
and
written
in
Python
","
so
I
just
need
something
that
will
send
me
a
small
amount
of
text
","
e.g
.
just
a
timestamp
and
a
traceback
(
which
I
already
generate
and
show
in
my
error
dialog
)
.
It
would
be
fine
if
it
could
simply
email
it
","
but
I
can't
think
of
a
way
to
do
this
without
including
a
username
and
password
for
the
smtp
server
in
the
application
...
Should
I
implement
a
simple
web
service
on
the
server
side
and
have
my
app
send
it
an
HTTP
request
with
the
info
?
Any
better
ideas
?
Thanks
!
Some
kind
of
simple
web
service
would
suffice
.
You
would
have
to
consider
security
so
not
just
anyone
could
make
requests
to
your
service
.
.
On
a
larger
scale
we
considered
a
JMS
messaging
system
.
Put
a
serialized
object
of
data
containing
the
traceback
/
error
message
into
a
queue
and
consume
it
every
x
minutes
generating
reports
/
alerts
from
that
data
.
The
web
hit
is
the
way
to
go
","
but
make
sure
you
pick
a
good
URL
-
your
app
will
be
hitting
it
for
years
to
come
.
The
web
service
is
the
best
way
","
but
there
are
some
caveats
:
You
should
always
ask
the
user
if
it
is
ok
to
send
error
feedback
information
.
You
should
be
prepared
to
fail
gracefully
if
there
are
network
errors
.
Don't
let
a
failure
to
report
a
crash
impede
recovery
!
You
should
avoid
including
user
identifying
or
sensitive
information
unless
the
user
knows
(
see
#
1
)
and
you
should
either
use
SSL
or
otherwise
protect
it
.
Some
jurisdictions
impose
burdens
on
you
that
you
might
not
want
to
deal
with
","
so
it's
best
to
simply
not
save
such
information
.
Like
any
web
service
","
make
sure
your
service
is
not
exploitable
by
miscreants
.
Jython
and
IronPython
are
useful
if
you
have
an
overriding
need
to
interface
with
existing
libraries
written
in
a
different
platform
","
like
if
you
have
100
","
000
lines
of
Java
and
you
just
want
to
write
a
20-line
Python
script
.
Not
particularly
useful
for
anything
else
","
in
my
opinion
","
because
they
are
perpetually
a
few
versions
behind
CPython
due
to
community
inertia
.
Stackless
is
interesting
because
it
has
support
for
green
threads
","
continuations
","
etc.
Sort
of
an
Erlang-lite
.
PyPy
is
an
experimental
interpreter
/
compiler
that
may
one
day
supplant
CPython
","
but
for
now
is
more
of
a
testbed
for
new
ideas
.
Pros
:
Access
to
the
libraries
available
for
JVM
or
CLR
.
Cons
:
Both
naturally
lag
behind
CPython
in
terms
of
features
.
PyPy
is
a
Python
implementation
written
in
RPython
wich
is
a
Python
subset
.
RPython
can
be
translated
to
run
on
a
VM
or
","
unlike
standard
Python
","
RPython
can
be
statically
compiled
.
I
am
relatively
new
to
Python
","
and
I
have
always
used
the
standard
cpython
(
v2.5
)
implementation
.
I've
been
wondering
about
the
other
implementations
though
","
particularly
Jython
and
IronPython
.
What
makes
them
better
?
What
makes
them
worse
?
What
other
implementations
are
there
?
I
guess
what
I'm
looking
for
is
a
summary
and
list
of
pros
and
cons
for
each
implementation
.
An
additional
benefit
for
Jython
","
at
least
for
some
","
is
it
lacks
the
GIL
(
the
Global
Interpreter
Lock
)
and
uses
Java's
native
threads
.
This
means
that
you
can
run
pure
Python
code
in
parallel
","
something
not
possible
with
the
GIL
.
IronPython
and
Jython
use
the
runtime
environment
for
.
NET
or
Java
and
with
that
comes
Just
In
Time
compilation
and
a
garbage
collector
different
from
the
original
CPython
.
They
might
be
also
faster
than
CPython
thanks
to
the
JIT
","
but
I
don't
know
that
for
sure
.
A
downside
in
using
Jython
or
IronPython
is
that
you
cannot
use
native
C
modules
","
they
can
be
only
used
in
CPython
.
All
of
the
implementations
are
listed
here
:
https://wiki.python.org/moin/PythonImplementations
CPython
is
the
""""
reference
implementation
""""
and
developed
by
Guido
and
the
core
developers
.
ElementTree
1.3
(
unfortunately
not
1.2
which
is
the
one
included
with
Python
)
supports
XPath
like
this
:
Then
you
can
do
lxml.etree
(
which
also
provides
the
ElementTree
interface
)
will
also
work
in
the
same
way
.
You
can
do
this
with
BeautifulSoup
If
you're
doing
work
with
HTML
/
XML
I
would
recommend
you
take
a
look
at
BeautifulSoup
.
It's
similar
to
the
DOM
tree
but
contains
more
functionality
.
I'm
not
really
an
old
hand
at
Python
","
but
here's
an
XPath
solution
using
libxml2
.
With
result
...
Using
a
standard
W3
DOM
such
as
the
stdlib's
minidom
","
or
pxdom
:
I
need
to
get
a
list
of
attribute
values
from
child
elements
in
Python
.
It's
easiest
to
explain
with
an
example
.
Given
some
XML
like
this
:
I
want
to
be
able
to
do
something
like
:
It
looks
like
a
job
for
XPath
but
I'm
open
to
all
recommendations
.
I'd
also
like
to
hear
about
your
favourite
Python
XML
libraries
.
My
preferred
python
xml
library
is
lxml
","
which
wraps
libxml2
.
Xpath
does
seem
the
way
to
go
here
","
so
I'd
write
this
as
something
like
:
I
must
admit
I'm
a
fan
of
xmltramp
due
to
its
ease
of
use
.
Accessing
the
above
becomes
:
In
Python
3.x
","
fetching
a
list
of
attributes
is
a
simple
task
of
using
the
member
items()
Using
the
ElementTree
","
below
snippet
shows
a
way
to
get
the
list
of
attributes
.
NOTE
that
this
example
doesn't
consider
namespaces
","
which
if
present
","
will
need
to
be
accounted
for
.
REFERENCE
:
Element.items()
Returns
the
element
attributes
as
a
sequence
of
(
name
","
value
)
pairs
.
The
attributes
are
returned
in
an
arbitrary
order
.
Python
manual
We
are
mainting
a
web
application
that
is
built
on
Classic
ASP
using
VBScript
as
the
primary
language
.
We
are
in
agreement
that
our
backend
(
framework
if
you
will
)
is
out
dated
and
doesn't
provide
us
with
the
proper
tools
to
move
forward
in
a
quick
manner
.
We
have
pretty
much
embraced
the
current
webMVC
pattern
that
is
all
over
the
place
","
and
cannot
do
it
","
in
a
reasonable
manner
","
with
the
current
technology
.
The
big
missing
features
are
proper
dispatching
and
templating
with
inheritance
","
amongst
others
.
Currently
there
are
two
paths
being
discussed
:
Port
the
existing
application
to
Classic
ASP
using
JScript
","
which
will
allow
us
to
hopefully
go
from
there
to
.
NET
MSJscript
without
too
much
trouble
","
and
eventually
end
up
on
the
.
NET
platform
(
preferably
the
MVC
stuff
will
be
done
by
then
","
ASP.NET
isn't
much
better
than
were
we
are
on
now
","
in
our
opinions
)
.
This
has
been
argued
as
the
safer
path
with
less
risk
than
the
next
option
","
albeit
it
might
take
slightly
longer
.
Completely
rewrite
the
application
using
some
other
technology
","
right
now
the
leader
of
the
pack
is
Python
WSGI
with
a
custom
framework
","
ORM
","
and
a
good
templating
solution
.
There
is
wiggle
room
here
for
even
django
and
other
pre-built
solutions
.
This
method
would
hopefully
be
the
quickest
solution
","
as
we
would
probably
run
a
beta
beside
the
actual
product
","
but
it
does
have
the
potential
for
a
big
waste
of
time
if
we
can't
/
don't
get
it
right
.
This
does
not
mean
that
our
logic
is
gone
","
as
what
we
have
built
over
the
years
is
fairly
stable
","
as
noted
just
difficult
to
deal
with
.
It
is
built
on
SQL
Server
2005
with
heavy
use
of
stored
procedures
and
published
on
IIS
6
","
just
for
a
little
more
background
.
Now
","
the
question
.
Has
anyone
taken
either
of
the
two
paths
above
?
If
so
","
was
it
successful
","
how
could
it
have
been
better
","
etc.
We
aren't
looking
to
deviate
much
from
doing
one
of
those
two
things
","
but
some
suggestions
or
other
solutions
would
potentially
be
helpful
.
Half
a
year
ago
I
took
over
a
large
web
application
(
fortunately
already
in
Python
)
which
had
some
major
architectural
deficiencies
(
templates
and
code
mixed
","
code
duplication
","
you
name
it
...
)
.
My
plan
is
to
eventually
have
the
system
respond
to
WSGI
","
but
I
am
not
there
yet
.
I
found
the
best
way
to
do
it
","
is
in
small
steps
.
Over
the
last
6
month
","
code
reuse
has
gone
up
and
progress
has
accelerated
.
General
principles
which
have
worked
for
me
:
Throw
away
code
which
is
not
used
or
commented
out
Throw
away
all
comments
which
are
not
useful
Define
a
layer
hierarchy
(
models
","
business
logic
","
view
/
controller
logic
","
display
logic
","
etc.
)
of
your
application
.
This
has
not
to
be
very
clear
cut
architecture
but
rather
should
help
you
think
about
the
various
parts
of
your
application
and
help
you
better
categorize
your
code
.
If
something
grossly
violates
this
hierarchy
","
change
the
offending
code
.
Move
the
code
around
","
recode
it
at
another
place
","
etc.
At
the
same
time
adjust
the
rest
of
your
application
to
use
this
code
instead
of
the
old
one
.
Throw
the
old
one
away
if
not
used
anymore
.
Keep
you
APIs
simple
!
Progress
can
be
painstakingly
slow
","
but
should
be
worth
it
.
Don't
try
and
go
2.0
(
more
features
then
currently
exists
or
scheduled
)
instead
build
your
new
platform
with
the
intent
of
resolving
the
current
issues
with
the
code
base
(
maintainability
/
speed
/
wtf
)
and
go
from
there
.
I
agree
with
Michael
Pryor
and
Joel
that
it's
almost
always
a
better
idea
to
continue
evolving
your
existing
code
base
rather
than
re-writing
from
scratch
.
There
are
typically
opportunities
to
just
re-write
or
re-factor
certain
components
for
performance
or
flexibility
.
A
good
place
to
begin
if
you're
considering
the
move
to
Python
is
to
rewrite
your
administrator
interface
in
Django
.
This
will
help
you
get
some
of
the
kinks
worked
out
in
terms
of
getting
Python
up
and
running
with
IIS
(
or
to
migrate
it
to
Apache
)
.
Speaking
of
which
","
I
recommend
isapi-wsgi
.
It's
by
far
the
easiest
way
to
get
up
and
running
with
IIS
.
Use
this
as
an
opportunity
to
remove
unused
features
!
Definitely
go
with
the
new
language
.
Call
it
2.0
.
It
will
be
a
lot
less
work
to
rebuild
the
80
%
of
it
that
you
really
need
.
Start
by
wiping
your
brain
clean
of
the
whole
application
.
Sit
down
with
a
list
of
its
overall
goals
","
then
decide
which
features
are
needed
based
on
which
ones
are
used
.
Then
redesign
it
with
those
features
in
mind
","
and
build
.
(
I
love
to
delete
code
.
)
Whatever
you
do
","
see
if
you
can
manage
to
follow
a
plan
where
you
do
not
have
to
port
the
application
all
in
one
big
bang
.
It
is
tempting
to
throw
it
all
away
and
start
from
scratch
","
but
if
you
can
manage
to
do
it
gradually
the
mistakes
you
do
will
not
cost
so
much
and
cause
so
much
panic
.
I
would
not
recommend
JScript
as
that
is
definitely
the
road
less
traveled
.
ASP.NET
MVC
is
rapidly
maturing
","
and
I
think
that
you
could
begin
a
migration
to
it
","
simultaneously
ramping
up
on
the
ASP.NET
MVC
framework
as
its
finalization
comes
through
.
Another
option
would
be
to
use
something
like
ASP.NET
w
/
Subsonic
or
NHibernate
.
It
works
out
better
than
you'd
believe
.
Recently
I
did
a
large
reverse-engineering
job
on
a
hideous
old
collection
of
C
code
.
Function
by
function
I
reallocated
the
features
that
were
still
relevant
into
classes
","
wrote
unit
tests
for
the
classes
","
and
built
up
what
looked
like
a
replacement
application
.
It
had
some
of
the
original
""""
logic
flow
""""
through
the
classes
","
and
some
classes
were
poorly
designed
[
Mostly
this
was
because
of
a
subset
of
the
global
variables
that
was
too
hard
to
tease
apart
.
]
It
passed
unit
tests
at
the
class
level
and
at
the
overall
application
level
.
The
legacy
source
was
mostly
used
as
a
kind
of
""""
specification
in
C
""""
to
ferret
out
the
really
obscure
business
rules
.
Last
year
","
I
wrote
a
project
plan
for
replacing
30-year
old
COBOL
.
The
customer
was
leaning
toward
Java
.
I
prototyped
the
revised
data
model
in
Python
using
Django
as
part
of
the
planning
effort
.
I
could
demo
the
core
transactions
before
I
was
done
planning
.
Note
:
It
was
quicker
to
build
a
the
model
and
admin
interface
in
Django
than
to
plan
the
project
as
a
whole
.
Because
of
the
""""
we
need
to
use
Java
""""
mentality
","
the
resulting
project
will
be
larger
and
more
expensive
than
finishing
the
Django
demo
.
With
no
real
value
to
balance
that
cost
.
Also
","
I
did
the
same
basic
""""
prototype
in
Django
""""
for
a
VB
desktop
application
that
needed
to
become
a
web
application
.
I
built
the
model
in
Django
","
loaded
legacy
data
","
and
was
up
and
running
in
a
few
weeks
.
I
used
that
working
prototype
to
specify
the
rest
of
the
conversion
effort
.
Note
:
I
had
a
working
Django
implementation
(
model
and
admin
pages
only
)
that
I
used
to
plan
the
rest
of
the
effort
.
The
best
part
about
doing
this
kind
of
prototyping
in
Django
is
that
you
can
mess
around
with
the
model
","
unit
tests
and
admin
pages
until
you
get
it
right
.
Once
the
model's
right
","
you
can
spend
the
rest
of
your
time
fiddling
around
with
the
user
interface
until
everyone's
happy
.
Don't
throw
away
your
code
!
It's
the
single
worst
mistake
you
can
make
(
on
a
large
codebase
)
.
See
Things
You
Should
Never
Do
","
Part
1
.
You've
invested
a
lot
of
effort
into
that
old
code
and
worked
out
many
bugs
.
Throwing
it
away
is
a
classic
developer
mistake
(
and
one
I've
done
many
times
)
.
It
makes
you
feel
""""
better
""""
","
like
a
spring
cleaning
.
But
you
don't
need
to
buy
a
new
apartment
and
all
new
furniture
to
outfit
your
house
.
You
can
work
on
one
room
at
a
time
...
and
maybe
some
things
just
need
a
new
paintjob
.
Hence
","
this
is
where
refactoring
comes
in
.
For
new
functionality
in
your
app
","
write
it
in
C
#
and
call
it
from
your
classic
ASP
.
You'll
be
forced
to
be
modular
when
you
rewrite
this
new
code
.
When
you
have
time
","
refactor
parts
of
your
old
code
into
C
#
as
well
","
and
work
out
the
bugs
as
you
go
.
Eventually
","
you'll
have
replaced
your
app
with
all
new
code
.
You
could
also
write
your
own
compiler
.
We
wrote
one
for
our
classic
ASP
app
a
long
time
ago
to
allow
us
to
output
PHP
.
It's
called
Wasabi
and
I
think
it's
the
reason
Jeff
Atwood
thought
Joel
Spolsky
went
off
his
rocker
.
Actually
","
maybe
we
should
just
ship
it
","
and
then
you
could
use
that
.
It
allowed
us
to
switch
our
entire
codebase
to
.
NET
for
the
next
release
while
only
rewriting
a
very
small
portion
of
our
source
.
It
also
caused
a
bunch
of
people
to
call
us
crazy
","
but
writing
a
compiler
is
not
that
complicated
","
and
it
gave
us
a
lot
of
flexibility
.
Also
","
if
this
is
an
internal
only
app
","
just
leave
it
.
Don't
rewrite
it
-
you
are
the
only
customer
and
if
the
requirement
is
you
need
to
run
it
as
classic
asp
","
you
can
meet
that
requirement
.
Basically
","
something
similar
to
System.Xml.XmlWriter
-
A
streaming
XML
Writer
that
doesn't
incur
much
of
a
memory
overhead
.
So
that
rules
out
xml.dom
and
xml.dom.minidom
.
Suggestions
?
Second
vote
for
ElementTree
(
cElementTree
is
a
C
implementation
that
is
a
little
faster
","
like
cPickle
vs
pickle
)
.
There's
some
short
example
code
here
that
you
can
look
at
to
give
you
an
idea
of
how
it
works
:
http://effbot.org/zone/element-index.htm
(
this
is
Fredrik
Lundh
","
who
wrote
the
module
in
the
first
place
.
It's
so
good
it
got
drafted
into
the
standard
library
with
2.5
:
-
)
)
Some
years
ago
I
used
MarkupWriter
from
4suite
Recently
i
hear
a
lot
about
how
lxml
is
great
","
but
I
don't
have
first-hand
experience
","
and
I
had
some
fun
working
with
gnosis
.
I
think
I
have
your
poison
:
http://sourceforge.net/projects/xmlite
Cheers
xml.etree.cElementTree
","
included
in
the
default
distribution
of
CPython
since
2.5
.
Lightning
fast
for
both
reading
and
writing
XML
.
I
think
you'll
find
XMLGenerator
from
xml.sax.saxutils
is
the
closest
thing
to
what
you
want
.
import time
from
xml.sax.saxutils
import XMLGenerator
from
xml.sax.xmlreader
import AttributesNSImpl
LOG_LEVELS
=
[
'
DEBUG
'
","
'
WARNING
'
","
'
ERROR
'
]
class
xml_logger
:
"def __init__(self, output, encoding):"
""""
""""
""""
Set
up
a
logger
object
","
which
takes
SAX
events
and
outputs
an
XML
log
file
""""
""""
""""
logger
=
"XMLGenerator(output, encoding)"
logger.startDocument()
attrs
=
"AttributesNSImpl({}, {})"
"logger.startElementNS((None, u'log')"
","
u'log
'
","
attrs
)
self._logger
=
logger
self._output
=
output
self._encoding
=
encoding
return
"def write_entry(self, level, msg):"
""""
""""
""""
Write
a
log
entry
to
the
logger
level
-
the
level
of
the
entry
msg
-
the
text
of
the
entry
.
Must
be
a
Unicode
object
""""
""""
""""
#
Note
:
in
a
real
application
","
I
would
use
ISO
8601
for
the
date
#
asctime
used
here
for
simplicity
now
=
time.asctime(time.localtime()
)
attr_vals
=
{
(
None
","
u'date
'
)
:
now
","
(
None
","
u'level
'
)
:
LOG_LEVELS
[level]
","
}
attr_qnames
=
{
(
None
","
u'date
'
)
:
u'date
'
","
(
None
","
u'level
'
)
:
u'level
'
","
}
attrs
=
"AttributesNSImpl(attr_vals, attr_qnames)"
"self._logger.startElementNS((None, u'entry')"
","
u'entry
'
","
attrs
)
self._logger.characters(msg)
"self._logger.endElementNS((None, u'entry')"
","
u'entry
'
)
return
def close(self):
""""
""""
""""
Clean
up
the
logger
object
""""
""""
""""
"self._logger.endElementNS((None, u'log')"
","
u'log
'
)
self._logger.endDocument()
return
if
__name__
=
=
""""
__main__
""""
:
#
Test
it
out
import sys
xl
=
"xml_logger(sys.stdout, 'utf-8')"
"xl.write_entry(2, u""Vanilla log entry"")"
xl.close()
You'll
probably
want
to
look
at
the
rest
of
the
article
I
got
that
from
at
http://www.xml.com/pub/a/2003/03/12/py-xml.html.
I've
always
had
good
results
with
lxml
.
It's
a
pain
to
install
","
as
it's
mostly
a
wrapper
around
libxml2
","
but
lxml.etree
tree
objects
have
a
.
write()
method
that
takes
a
file-like
object
to
stream
to
.
PythonCard
is
really
easy
to
use
.
That's
what
I
would
recommend
.
Here's
their
writeup
:
PythonCard
is
a
GUI
construction
kit
for
building
cross-platform
desktop
applications
on
Windows
","
Mac
OS
X
","
and
Linux
","
using
the
Python
language
.
The
PythonCard
motto
is
""""
Simple
things
should
be
simple
and
complex
things
should
be
possible
.
""""
PythonCard
is
for
you
if
you
want
to
develop
graphical
applications
quickly
and
easily
with
a
minimum
of
effort
and
coding
.
Apple's
HyperCard
is
one
of
our
inspirations
;
simple
","
yet
powerful
.
PythonCard
uses
wxPython
.
If
you
are
already
familiar
with
wxPython
","
just
think
of
PythonCard
as
a
simpler
way
of
doing
wxPython
programs
with
a
whole
lot
of
samples
and
tools
already
in
place
for
you
to
copy
and
subclass
and
tools
to
help
you
build
cross-platform
applications
.
I
prefer
PyGTK
","
because
I
am
a
GNOME
guy
.
Using
PyGTK
feels
very
pythonic
to
me
.
The
code
organization
feels
consistent
","
the
documentation
is
clean
and
thorough
","
and
it's
a
very
easy
toolkit
to
get
used
to
(
except
for
maybe
Treeviews
)
.
WX
has
issues
on
the
Mac
.
I
had
a
look
here
","
as
I
want
to
get
an
event
driven
GUI
API
to
do
some
stuff
in
Python
.
I
have
wx
installed
on
my
mac
as
part
of
MatPlotLib
","
but
it
does
not
work
properly
.
It
wont
take
in
put
from
the
keyboard
.
I
have
installed
this
three
times
on
three
different
Mac
operating
systems
","
and
though
it
worked
the
first
time
","
the
other
two
times
I
had
this
issue
.
This
version
I
am
using
with
Enthought's
distribution
","
so
no
installation
was
necessary
.
When
I
have
installed
it
separately
","
there
were
so
many
dependent
installations
","
that
it
was
a
trial
to
install
.
From
what
I
have
read
here
","
I
will
give
Tkinter
a
go
","
as
this
needs
to
be
simple
and
cross
platform
","
but
I
thought
I
would
just
share
the
above
with
you
.
I
like
the
Mac
OS
for
a
number
of
different
reasons
","
but
python
tools
install
far
easier
on
Windows
(
and
probably
other
Linux
)
.
I
just
thought
I
would
give
a
Mac
perspective
here
.
Simple
question
:
What
Python
GUI
API's
are
out
there
and
what
are
the
advantages
of
any
given
API
?
I'm
not
looking
for
a
religious
war
here
","
I'm
just
wanting
to
get
a
good
handle
on
all
that
is
out
there
in
terms
of
Python
GUI
APIs
.
An
easy
to
use
GUI
creator
for
Python
doesn't
exist
.
That's
amazing
really
considering
small
scripting
languages
like
AutoIt
and
AutoHotkey
have
great
and
very
simple
to
use
GUI
makers
.
Come
on
","
Python
followers
","
can't
you
do
better
?
EasyGUI
is
different
from
other
GUIs
in
that
EasyGUI
is
NOT
event-driven
.
It
allows
you
to
program
in
a
traditional
linear
fashion
","
and
to
put
up
dialogs
for
simple
input
and
output
when
you
need
to
.
If
you
have
not
yet
learned
the
event-driven
paradigm
for
GUI
programming
","
EasyGUI
will
allow
you
to
be
productive
with
very
basic
tasks
immediately
.
Later
","
if
you
wish
to
make
the
transition
to
an
event-driven
GUI
paradigm
","
you
can
do
so
with
a
more
powerful
GUI
package
such
as
anygui
","
PythonCard
","
Tkinter
","
wxPython
","
etc.
EasyGui
Website
There
are
python-specific
gui-api
such
as
kivy
(
successor
or
pymt
)
","
pygui
(
based
on
pyrex
)
","
pyui
and
nufox
","
which
do
not
compare
with
the
more
robust
toolkits
like
wxpython
","
pyqt
","
pygtk
and
tkinter
.
They
are
just
extra
optional
tools
.
The
only
thing
unique
about
them
is
these
are
python-specific
api
","
just
like
there
are
prima
(
perl-specific
api
)
and
shoes
(
ruby-specific
api
)
.
It
helps
us
to
understand
that
when
tk
is
tcl-based
port
of
api
(
and
others
are
c
and
c
+
+
based
)
","
then
these
api
are
specifically
done
for
the
respective
three
scripting
languages
.
Out
of
these
","
kivy
is
the
most
robust
","
whereas
pygui's
coding
is
mentioned
to
be
very
python-like
","
pyui
is
least
robust
but
worth
trying
and
all
of
these
should
be
portable
wherever
python
or
python-based
application
goes
.
Then
there
is
jpype
which
is
a
toolkit
usable
with
jython
and
pydev
","
and
which
is
actually
java's
japi
customized
under
python
/
jython-interface
.
I've
used
Tkinter
and
wxPython
.
Tkinter
is
quite
basic
","
and
doesn't
use
native
widgets
.
This
means
that
Tkinter
applications
will
look
the
same
on
any
platform
–
this
might
sound
appealing
","
but
in
practice
","
it
means
they
look
ugly
on
any
platform
:
-
/
Nevertheless
","
it's
pretty
easy
to
use
.
I
found
Thinking
in
Tkinter
very
helpful
when
I
was
learning
","
because
I'd
never
done
any
GUI
programming
before
.
If
things
like
frames
and
layout
algorithms
and
buttons
and
bindings
are
familiar
to
you
","
though
","
you
can
skip
that
step
.
You
can
augment
Tkinter
with
Tix
(
but
be
warned
","
Tix
doesn't
play
well
with
py2exe
)
.
Also
check
out
Python
Megawidgets
","
which
builds
some
more
advanced
controls
using
the
Tkinter
basics
.
Finally
","
Tkinter
plays
nice
with
the
shell
:
you
can
start
the
interpreter
","
do
things
like
'
import tkinter
'
'
tk
=
tkinter.Tk()
'
etc.
and
build
your
GUI
interactively
(
and
it
will
be
responsive
)
.
(
I
think
this
doesn't
work
if
you
use
IDLE
","
though
)
wxPython
is
much
better
looking
","
and
ships
with
a
much
greater
range
of
controls
.
It's
cross-platform
(
though
it
seems
a
bit
finicky
on
my
Mac
)
and
uses
native
controls
on
each
platform
.
It's
a
bit
confusing
","
though
.
It
also
ships
with
a
demo
application
that
shows
off
most
of
its
features
","
and
provides
a
test-bed
for
you
to
experiment
.
Some
specific
thoughts
on
wxPython
:
There
are
three
(
?
)
different
ways
to
lay
widgets
out
.
Ignore
two
of
them
;
just
use
Sizers
.
And
even
then
","
you
can
do
just
about
any
layout
using
only
BoxSizer
and
GridBagSizer
.
All
wx
widgets
have
IDs
.
You
don't
need
to
care
what
the
IDs
are
","
but
in
the
old
days
(
I
think
)
you
did
need
to
know
","
so
some
old
code
will
be
littered
with
explicit
ID
assignments
.
And
most
demo
code
will
have
-
1
everywhere
as
the
ID
parameter
(
despite
the
fact
that
the
methods
all
have
ID
as
a
keyword
parameter
that
defaults
to
-
1
anyway
)
.
Make
sure
you
get
the
standard
wxWidgets
docs
as
well
as
the
wxPython
Demo
–
you
need
them
both
.
If
you
want
to
use
wxPython
with
py2exe
and
you
want
it
to
look
good
on
Windows
XP
","
you
need
a
bit
of
trickery
in
your
setup.py
.
See
here
I
found
this
link
a
long
time
a
go
:
http://www.awaretek.com/toolkits.html.
It
suggests
a
tookit
based
on
your
criteria
.
For
me
it
suggests
wxPython
all
the
time
.
Anyway
it
gives
you
a
bunch
of
scores
on
the
various
toolkits
.
What
is
right
for
me
may
not
be
right
for
you
.
But
it
gives
you
how
all
the
tookits
scored
according
to
your
criteria
","
so
if
you
don't
like
the
top
toolkit
for
some
reason
you
can
see
which
ones
are
closest
to
your
criteria
.
QT
/
GTK
/
WxWidgets
(
formerly
wxWindows
)
seem
to
be
among
the
most
mature
cross
platform
GUI
toolkits
.
The
only
issue
is
that
none
is
installed
with
the
default
installation
of
Python
","
so
you
may
have
to
compile
the
libraries
.
If
you
want
something
with
no
installation
required
that
just
runs
","
then
go
with
TKInter
because
as
has
been
mentioned
it
is
installed
by
default
with
Python
.
Anyway
my
criteria
were
9
on
Ease
of
Use
","
10
on
maturity
of
documentation
/
widgets
","
10
on
installed
base
","
5
on
gui
code
generators
","
10
on
native
look
and
feel
for
both
windows
/
linux
and
1
and
5
for
the
last
two
","
I'm
not
big
into
Mac
OSX
(
even
with
a
10
here
it
suggests
wxpython
)
.
PyQt
is
excellent
if
you
have
experience
or
interest
in
Qt
.
http://www.riverbankcomputing.co.uk/software/pyqt/intro
Instead
of
posting
a
list
of
your
options
I
will
give
my
humble
opinion
:
I
am
in
love
with
wxPython
.
I
have
used
Qt
in
C
+
+
and
Tk
way
back
in
the
Tcl
days
but
what
really
makes
me
like
wxPython
is
the
demo
that
you
get
with
it
.
In
the
demo
you
can
browse
through
all
the
different
widgets
frames
etc
that
are
part
of
the
framework
see
the
source
code
and
actually
see
how
it
looks
while
it
is
running
.
I
had
some
problems
getting
the
Linux
version
build
and
installed
but
now
that
I
have
it
available
I
use
it
all
the
time
.
I
have
used
wxPython
for
small
data
analysis
applications
and
I
have
written
several
internal
tools
related
to
comparing
test
results
","
merging
source
code
etc.
Most
python
GUI
APIs
will
be
wrappers
around
the
most
common
c
/
c
+
+
GUI
APIs
.
You've
got
a
python
wrapper
for
gtk
","
a
python
wrapper
for
qt
","
a
python
wrapper
for
.
NET
","
etc
etc.
So
really
it
depends
on
what
your
needs
are
.
If
you
are
looking
for
the
easiest
way
to
draw
native-looking
widgets
on
Linux
","
Mac
","
and
Windows
","
then
go
with
wxPython
(
python
wrapper
for
WX
Widgets
)
.
If
cross-platform
isn't
one
of
your
needs
though
","
other
libraries
might
be
more
useful
.
wxPython
","
and
I'm
assuming
PyGTK
also
","
can
use
wxGlade
to
help
you
design
most
UIs
you
will
create
.
That
is
a
big
plus
.
You
don't
have
to
learn
how
to
hand-code
the
GUI
until
you're
ready
.
I
made
several
GUI
programs
straight
from
wxGlade
before
I
was
comfortable
enough
in
how
wxPython
worked
to
take
a
shot
at
hand-coding
.
PyQt
has
a
similar
graphic
layout
device
but
I've
never
had
good
luck
getting
PyQt
to
compile
correctly
.
There
was
also
a
lack
of
tutorials
and
documentation
that
showed
how
to
create
the
final
Python
code
;
many
of
the
documents
I
found
referred
to
the
C
+
+
version
of
Qt
.
Tkinter
is
good
for
quick
and
dirty
programs
but
","
realistically
","
if
you
use
wxGlade
it
may
be
faster
to
make
the
program
with
wxPython
.
At
a
minimum
","
you
can
use
wxGlade
to
show
a
visual
representation
of
the
program
to
a
client
rather
than
take
the
time
to
hand-code
the
""""
dummy
""""
program
.
I
like
wxPython
or
Tk
.
Tk
comes
with
the
standard
Python
distribution
so
you
don't
need
install
anything
else
.
wxPython
(
wxWigets
)
seems
much
more
powerful
and
looks
a
lot
nicer
.
It
also
works
well
cross-platform
(
though
not
perfectly
because
it
uses
different
underlying
graphic
API's
on
diff
system
types
)
I've
been
working
with
wxPython
for
a
few
years
now
and
I
like
it
quite
a
bit
.
The
best
thing
about
wxPython
is
that
the
UI
feels
native
on
the
different
platforms
it
runs
on
(
excellent
on
Windows
and
Linux
though
not
as
good
on
OS
/
X
)
.
The
API
lacks
some
consistency
","
but
you
quickly
get
used
to
it
.
You
can
check
out
Testuff
(
shameless
plug
","
as
it's
my
own
product
)
to
get
a
feeling
of
what
can
be
done
with
wxPython
(
although
I
must
say
","
with
quite
a
bit
of
effort
)
.
Here's
a
good
list
.
The
file
you
create
with
TemporaryFile
or
NamedTemporaryFile
is
automatically
removed
when
it's
closed
","
which
is
why
you
get
an
error
.
If
you
don't
want
this
","
you
can
use
mkstemp
instead
(
see
the
docs
for
tempfile
)
.
I
am
attempting
to
use
the
'
tempfile
'
module
for
manipulating
and
creating
text
files
.
Once
the
file
is
ready
I
want
to
save
it
to
disk
.
I
thought
it
would
be
as
simple
as
using
'
shutil.copy
'
.
However
","
I
get
a
'
permission
denied
'
IOError
:
Is
this
not
intended
when
using
the
'
tempfile
'
library
?
Is
there
a
better
way
to
do
this
?
(
Maybe
I
am
overlooking
something
very
trivial
)
hop
is
right
","
and
dF
.
is
incorrect
on
why
the
error
occurs
.
Since
you
haven't
called
f.close()
yet
","
the
file
is
not
removed
.
The
doc
for
NamedTemporaryFile
says
:
Whether
the
name
can
be
used
to
open
the
file
a
second
time
","
while
the
named
temporary
file
is
still
open
","
varies
across
platforms
(
it
can
be
so
used
on
Unix
;
it
cannot
on
Windows
NT
or
later
)
.
And
for
TemporaryFile
:
Under
Unix
","
the
directory
entry
for
the
file
is
removed
immediately
after
the
file
is
created
.
Other
platforms
do
not
support
this
;
your
code
should
not
rely
on
a
temporary
file
created
using
this
function
having
or
not
having
a
visible
name
in
the
file
system
.
Therefore
","
to
persist
a
temporary
file
(
on
Windows
)
","
you
can
do
the
following
:
The
solution
Hans
Sjunnesson
provided
is
also
off
","
because
copyfileobj
only
copies
from
file-like
object
to
file-like
object
","
not
file
name
:
"shutil.copyfileobj(fsrc, fdst[, length])"
Copy
the
contents
of
the
file-like
object
fsrc
to
the
file-like
object
fdst
.
The
integer
length
","
if
given
","
is
the
buffer
size
.
In
particular
","
a
negative
length
value
means
to
copy
the
data
without
looping
over
the
source
data
in
chunks
;
by
default
the
data
is
read
in
chunks
to
avoid
uncontrolled
memory
consumption
.
Note
that
if
the
current
file
position
of
the
fsrc
object
is
not
0
","
only
the
contents
from
the
current
file
position
to
the
end
of
the
file
will
be
copied
.
Starting
from
python
2.6
you
can
also
use
NamedTemporaryFile
with
the
delete
=
option
set
to
False
.
This
way
the
temporary
file
will
be
accessible
","
even
after
you
close
it
.
Note
that
on
Windows
(
NT
and
later
)
you
cannot
access
the
file
a
second
time
while
it
is
still
open
.
You
have
to
close
it
before
you
can
copy
it
.
This
is
not
true
on
Unix
systems
.
You
could
always
use
shutil.copyfileobj
","
in
your
example
:
If
it's
something
where
you're
going
to
need
tons
of
threads
and
need
better
concurrent
performance
","
check
out
Stackless
Python
.
Otherwise
you
could
just
use
the
SOAP
or
XML-RPC
protocols
.
In
response
to
Ben's
post
","
if
you
don't
want
to
look
over
the
BitTorrent
source
","
you
could
just
look
at
the
article
on
the
BitTorrent
protocol
.
You
could
download
the
source
of
BitTorrent
for
starters
and
see
how
they
did
it
.
http://download.bittorrent.com/dl/
You
could
checkout
pyprocessing
which
will
be
included
in
the
standard
library
as
of
2.6
.
It
allows
you
to
run
tasks
on
multiple
processes
using
an
API
similar
to
threading
.
I
think
you
mean
""""
Networked
Apps
""""
?
Distributed
means
an
app
that
can
split
its
workload
among
multiple
worker
clients
over
the
network
.
You
probably
want
.
Twisted
What
is
the
best
python
framework
to
create
distributed
applications
?
For
example
to
build
a
P2P
app
.
You
probably
want
Twisted
.
There
is
a
P2P
framework
for
Twisted
called
""""
Vertex
""""
.
While
not
actively
maintained
","
it
does
allow
you
to
tunnel
through
NATs
and
make
connections
directly
between
users
in
a
very
abstract
way
;
if
there
were
more
interest
in
this
sort
of
thing
I'm
sure
it
would
be
more
actively
maintained
.
Thanks
Marcel
for
your
question
and
answer
(
I
had
the
same
problem
in
a
different
context
and
encountered
the
same
difficulty
with
file-like
objects
not
really
being
file-like
)
!
Just
as
an
update
:
For
Python
3.0
","
your
code
needs
to
be
modified
slightly
:
Bear
in
mind
that
merely
decompressing
a
ZIP
file
may
result
in
a
security
vulnerability
.
Here's
how
I
did
it
(
grabbing
all
files
ending
in
""""
.
ranks
""""
)
:
I
need
to
read
selected
files
","
matching
on
the
file
name
","
from
a
remote
zip
archive
using
Python
.
I
don't
want
to
save
the
full
zip
to
a
temporary
file
(
it's
not
that
large
","
so
I
can
handle
everything
in
memory
)
.
I've
already
written
the
code
and
it
works
","
and
I'm
answering
this
myself
so
I
can
search
for
it
later
.
But
since
evidence
suggests
that
I'm
one
of
the
dumber
participants
on
Stackoverflow
","
I'm
sure
there's
room
for
improvement
.
This
will
do
the
job
without
downloading
the
entire
zip
file
!
http://pypi.python.org/pypi/pyremotezip
Nope
.
But
you
can
use
short
integers
in
arrays
:
As
long
as
the
value
stays
in
that
array
it
will
be
a
short
integer
.
documentation
for
the
array
module
Thanks
to
Armin
for
pointing
out
the
'
array
'
module
.
I
also
found
the
'
struct
'
module
that
packs
c-style
structs
in
a
string
:
From
the
documentation
(
https://docs.python.org/library/struct.html
)
:
@Armin
:
how
come
?
The
Python
documentation
said
the
minimum
size
for
that
array
of
short
integer
is
2
bytes
and
The
actual
representation
of
values
is
determined
by
the
machine
architecture
(
strictly
speaking
","
by
the
C
implementation
)
.
The
actual
size
can
be
accessed
through
the
itemsize
attribute
.
@Arnav
:
I
suggest
that
your
code
should
check
the
size
of
each
Type
code
and
choose
the
corresponding
2-byte
type
that
is
specific
to
the
underlying
system
.
Python
allocates
integers
automatically
based
on
the
underlying
system
architecture
.
Unfortunately
I
have
a
huge
dataset
which
needs
to
be
fully
loaded
into
memory
.
So
","
is
there
a
way
to
force
Python
to
use
only
2
bytes
for
some
integers
(
equivalent
of
C
+
+
'
short
'
)
?
Armin's
suggestion
of
the
array
module
is
probably
best
.
Two
possible
alternatives
:
You
can
create
an
extension
module
yourself
that
provides
the
data
structure
that
you're
after
.
If
it's
really
just
something
like
a
collection
of
shorts
","
then
that's
pretty
simple
to
do
.
You
can
cheat
and
manipulate
bits
","
so
that
you're
storing
one
number
in
the
lower
half
of
the
Python
int
","
and
another
one
in
the
upper
half
.
You'd
write
some
utility
functions
to
convert
to
/
from
these
within
your
data
structure
.
Ugly
","
but
it
can
be
made
to
work
.
It's
also
worth
realising
that
a
Python
integer
object
is
not
4
bytes
-
there
is
additional
overhead
.
So
if
you
have
a
really
large
number
of
shorts
","
then
you
can
save
more
than
two
bytes
per
number
by
using
a
C
short
in
some
way
(
e.g
.
the
array
module
)
.
I
had
to
keep
a
large
set
of
integers
in
memory
a
while
ago
","
and
a
dictionary
with
integer
keys
and
values
was
too
large
(
I
had
1GB
available
for
the
data
structure
IIRC
)
.
I
switched
to
using
a
IIBTree
(
from
ZODB
)
and
managed
to
fit
it
.
(
The
ints
in
a
IIBTree
are
real
C
ints
","
not
Python
integers
","
and
I
hacked
up
an
automatic
switch
to
a
IOBTree
when
the
number
was
larger
than
32
bits
)
.
Based
on
other
answers
","
here's
a
function
to
read
a
SVG
file
into
a
pygame
image
-
including
correcting
color
channel
order
and
scaling
:
In
a
pyGame
application
","
I
would
like
to
render
resolution-free
GUI
widgets
described
in
SVG
.
What
tool
and
/
or
library
can
I
use
to
reach
this
goal
?
(
I
like
the
OCEMP
GUI
toolkit
but
it
seems
to
be
bitmap
dependent
for
its
rendering
)
You
can
use
Cairo
(
with
PyCairo
)
","
which
has
support
for
rendering
SVGs
.
The
PyGame
webpage
has
a
HOWTO
for
rendering
into
a
buffer
with
a
Cairo
","
and
using
that
buffer
directly
with
PyGame
.
pygamesvg
seems
to
do
what
you
want
(
though
I
haven't
tried
it
)
.
I
realise
this
doesn't
exactly
answer
your
question
","
but
there's
a
library
called
Squirtle
that
will
render
SVG
files
using
either
Pyglet
or
PyOpenGL
.
This
is
a
complete
example
which
combines
hints
by
other
people
here
.
It
should
render
a
file
called
test.svg
from
the
current
directory
.
It
was
tested
on
Ubuntu
10.10
","
python-cairo
1.8.8
","
python-pygame
1.9.1
","
python-rsvg
2.30.0
.
Cairo
cannot
render
SVG
out
of
the
box
.
It
seems
we
have
to
use
librsvg
.
Just
found
those
two
pages
:
Rendering
SVG
with
libRSVG
","
Python
and
c-types
How
to
use
librsvg
from
Python
Something
like
this
should
probably
work
(
render
test.svg
to
test.png
)
:
The
last
comment
crashed
when
I
ran
it
because
svg.render_cairo()
is
expecting
a
cairo
context
and
not
a
cairo
surface
.
I
created
and
tested
the
following
function
and
it
seems
to
run
fine
on
my
system
.
Here
is
a
one
line
Pythonic
version
:
This
code
lists
the
full
path
of
all
files
and
directories
in
the
given
directory
name
.
Below
code
will
list
directories
and
the
files
within
the
dir
FYI
Add
a
filter
of
extension
or
ext
file
import os
You
can
use
For
reference
and
more
os
functions
look
here
:
Python
2
docs
:
https://docs.python.org/2/library/os.html#os.listdir
Python
3
docs
:
https://docs.python.org/3/library/os.html#os.listdir
For
files
in
current
working
directory
without
specifying
a
path
Python
2.7
:
Python
3.x
:
Thanks
to
Stam
Kaly
for
comment
on
python
3.x
Try
this
:
If
figured
I'd
throw
this
in
.
Simple
and
dirty
way
to
do
wildcard
searches
.
If
you
need
globbing
abilities
","
there's
a
module
for
that
as
well
.
For
example
:
will
return
something
like
:
See
the
documentation
here
.
A
recursive
implementation
How
do
I
get
a
list
of
all
files
(
and
directories
)
in
a
given
directory
in
Python
?
Here's
a
helper
function
I
use
quite
often
:
A
nice
one
liner
to
list
only
the
files
recursively
.
I
used
this
in
my
setup.py
package_data
directive
:
I
know
it's
not
the
answer
to
the
question
","
but
may
come
in
handy
I
wrote
a
long
version
","
with
all
the
options
I
might
need
:
http://sam.nipl.net/code/python/find.py
I
guess
it
will
fit
here
too
:
For
Python
2
For
Python
3
For
filter
and
map
","
you
need
wrap
them
with
list()
The
recommendation
now
is
that
you
replace
your
usage
of
map
and
filter
with
generators
expressions
or
list
comprehensions
:
This
is
a
way
to
traverse
every
file
and
directory
in
a
directory
tree
:
A
proper
answer
will
need
more
information
than
that
.
What
are
you
actually
doing
?
How
does
it
fail
?
Are
you
using
the
subprocess
module
?
Are
you
passing
a
list
of
arguments
and
shell=False
(
or
no
shell
argument
)
or
are
you
actually
invoking
the
shell
?
Try
quoting
the
argument
that
contains
the
&
Is
usually
what
has
to
be
done
in
a
Linux
shell
To
answer
my
own
question
:
Quoting
the
actual
command
when
passing
the
parameters
as
a
list
doesn't
work
correctly
(
command
is
first
item
of
list
)
so
to
solve
the
issue
I
turned
the
list
into
a
space
separated
string
and
passed
that
into
subprocess
instead
.
Better
solutions
still
welcomed
.
""""
escaping
the
ampersand
with
^
""""
Are
you
sure
^
is
an
escape
character
to
Windows
?
Shouldn't
you
use
\
?
Make
sure
you
are
using
lists
and
no
shell
expansion
:
I
try
a
situation
as
following
:
This
does
work
.
I'm
currently
having
a
major
issue
with
a
python
script
.
The
script
runs
arbitrary
commands
through
a
handler
to
convert
incorrect
error
reporting
into
correct
error
reporting
.
The
issue
I'm
having
is
getting
the
script
to
work
correctly
on
windows
with
a
command
that
contains
ampersands
in
it's
path
.
I've
attempted
quoting
the
command
","
escaping
the
ampersand
with
^
and
neither
works
.
I'm
now
out
of
ideas
.
Any
suggestions
?
To
clarify
from
current
responses
:
I
am
using
the
subprocess
module
I
am
passing
the
command
line
+
arguments
in
as
a
list
The
issue
is
with
the
path
to
the
command
itself
","
not
any
of
the
arguments
I've
tried
quoting
the
command
.
It
causes
a
[
Error
123
]
The
filename
","
directory
name
","
or
volume
label
syntax
is
incorrect
error
I'm
using
no
shell
argument
(
so
shell=false
)
In
case
it
matters
","
I'm
grabbing
a
pipe
to
stderr
for
processing
it
","
but
ignoring
stdout
and
stdin
It
is
only
for
use
on
Windows
currently
","
and
works
as
expected
in
all
other
cases
that
I've
tested
so
far
.
The
command
that
is
failing
is
:
p
=
"subprocess.Popen(prog, stderr = subprocess.PIPE, bufsize=-1)"
when
the
first
element
of
the
list
'
prog
'
contains
any
ampersands
.
Quoting
this
first
string
does
not
work
.
Use
a
generator
:
The
*
construct
unpacks
into
a
tuple
in
order
to
pass
the
arguments
","
so
there's
no
way
to
use
it
.
Suppose
we
have
an
iterator
(
an
infinite
one
)
that
returns
lists
(
or
finite
iterators
)
","
for
example
one
returned
by
What
is
a
good
Python
idiom
to
get
an
iterator
(
obviously
infinite
)
that
will
return
each
of
the
elements
from
the
first
iterator
","
then
each
from
the
second
one
","
etc.
In
the
example
above
it
would
return
1
","
2
","
3
","
1
","
2
","
3
","
....
The
iterator
is
infinite
","
so
itertools.chain(*infinite)
will
not
work
.
Related
Flattening
a
shallow
list
in
python
Starting
with
Python
2.6
","
you
can
use
itertools.chain.from_iterable
:
You
can
also
do
this
with
a
nested
generator
comprehension
:
The
reason
for
spaces
is
that
tabs
are
optional
.
Spaces
are
the
actual
lowest-common
denominator
in
punctuation
.
Every
decent
text
editor
has
a
""""
replace
tabs
with
spaces
""""
and
many
people
use
this
.
But
not
always
.
While
some
text
editors
might
replace
a
run
of
spaces
with
a
tab
","
this
is
really
rare
.
Bottom
Line
.
You
can't
go
wrong
with
spaces
.
You
might
go
wrong
with
tabs
.
So
don't
use
tabs
and
reduce
the
risk
of
mistakes
.
The
problem
with
tabs
is
that
they
are
invisible
","
and
people
can
never
agree
on
the
width
of
tabs
.
When
you
mix
tabs
and
spaces
","
and
you
set
tabstops
at
something
other
than
Python
(
which
uses
tabstops
every
8
spaces
)
you
will
be
seeing
the
code
in
a
different
layout
than
Python
sees
it
.
And
because
the
layout
determines
blocks
","
you
will
be
seeing
different
logic
.
It
leads
to
subtle
bugs
.
If
you
insist
on
defying
PEP
8
and
using
tabs
-
-
or
worse
","
mixing
tabs
and
spaces
-
-
at
least
always
run
python
with
the
'
-
tt
'
argument
","
which
makes
inconsistent
indentation
(
sometimes
a
tab
","
sometimes
a
space
for
the
same
indentation
level
)
an
error
.
Also
","
if
possible
","
set
your
editor
to
display
tabs
differently
.
But
really
","
the
best
approach
is
not
to
use
tabs
","
period
.
The
universal
problem
with
tabs
is
that
they
can
be
represented
differently
in
different
environment
.
In
a
given
editor
","
a
tab
might
be
8
spaces
or
it
might
be
2
.
In
some
editors
","
you
can
control
this
","
while
in
others
you
can't
.
Another
issue
with
tabs
is
how
they
are
represented
in
printed
output
.
I
believe
most
printers
interpret
a
tab
as
8
spaces
.
With
spaces
","
there
is
no
doubt
.
Everything
will
line
up
as
the
author
intended
.
On
the
discussion
between
Jim
and
Thomas
Wouters
in
the
comments
.
The
issue
was
...
since
the
width
of
tabs
and
spaces
both
can
vary
-
-
and
since
programmers
can't
agree
on
either
width
-
-
why
is
it
that
tabs
bear
the
blame
.
I
agree
with
Jim
on
that
-
-
tabs
are
NOT
evil
in
and
of
themselves
.
But
there
is
a
problem
...
With
spaces
I
can
control
how
""""
MY
OWN
CODE
""""
looks
in
EVERY
editor
in
the
world
.
If
I
use
4
spaces
-
-
then
no
matter
what
editor
you
open
my
code
in
","
it
will
have
the
same
distance
from
the
left
margin
.
With
tabs
I
am
at
the
mercy
of
the
tab-width
setting
for
the
editor
-
-
even
for
MY
OWN
CODE
.
And
I
don't
like
that
.
So
while
it
is
true
that
even
spaces
can't
guarantee
consistency
-
-
they
at
least
afford
you
more
control
over
the
look
of
your
OWN
code
everywhere
-
-
something
that
tabs
can't
.
I
think
it's
NOT
the
consistency
in
the
programmers
writing
the
code
-
-
but
the
consistency
in
editors
showing
that
code
-
-
that
spaces
make
easier
to
achieve
(
and
impose
)
.
The
most
significant
advantage
I
can
tell
of
spaces
over
tabs
is
that
a
lot
of
programmers
and
projects
use
a
set
number
of
columns
for
the
source
code
","
and
if
someone
commits
a
change
with
their
tabstop
set
to
2
spaces
and
the
project
uses
4
spaces
as
the
tabstop
the
long
lines
are
going
to
be
too
long
for
other
people's
editor
window
.
I
agree
that
tabs
are
easier
to
work
with
but
I
think
spaces
are
easier
for
collaboration
","
which
is
important
on
a
large
open
source
project
like
Python
.
You
can
have
your
cake
and
eat
it
to
.
Set
your
editor
to
expand
tabs
into
spaces
automatically
.
(
That
would
be
:
set
expandtab
in
Vim
.
)
The
answer
to
the
question
is
:
PEP-8
wants
to
make
a
recommendation
and
has
decided
that
since
spaces
are
more
popular
it
will
strongly
recommend
spaces
over
tabs
.
Notes
on
PEP-8
PEP-8
says
'
Use
4
spaces
per
indentation
level
.
'
Its
clear
that
this
is
the
standard
recommendation
.
'
For
really
old
code
that
you
don't
want
to
mess
up
","
you
can
continue
to
use
8-space
tabs
.
'
Its
clear
that
there
are
SOME
circumstances
when
tabs
can
be
used
.
'
Never
mix
tabs
and
spaces
.
'
This
is
a
clear
prohibition
of
mixing
-
I
think
we
all
agree
on
this
.
Python
can
detect
this
and
often
chokes
.
Using
the
-
tt
argument
makes
this
an
explicit
error
.
'
The
most
popular
way
of
indenting
Python
is
with
spaces
only
.
The
second-most
popular
way
is
with
tabs
only
.
'
This
clearly
states
that
both
are
used
.
Just
to
be
ultra-clear
:
You
should
still
never
mix
spaces
and
tabs
in
same
file
.
'
For
new
projects
","
spaces-only
are
strongly
recommended
over
tabs
.
'
This
is
a
clear
recommendation
","
and
a
strong
one
","
but
not
a
prohibition
of
tabs
.
I
can't
find
a
good
answer
to
my
own
question
in
PEP-8
.
I
use
tabs
","
which
I
have
used
historically
in
other
languages
.
Python
accepts
source
with
exclusive
use
of
tabs
.
That's
good
enough
for
me
.
I
thought
I
would
have
a
go
at
working
with
spaces
.
In
my
editor
","
I
configured
a
file
type
to
use
spaces
exclusively
and
so
it
inserts
4
spaces
if
I
press
tab
.
If
I
press
tab
too
many
times
","
I
have
to
delete
the
spaces
!
Arrgh
!
Four
times
as
many
deletes
as
tabs
!
My
editor
can't
tell
that
I'm
using
4
spaces
for
indents
(
although
AN
editor
might
be
able
to
do
this
)
and
obviously
insists
on
deleting
the
spaces
one
at
a
time
.
Couldn't
Python
be
told
to
consider
tabs
to
be
n
spaces
when
its
reading
indentations
?
If
we
could
agree
on
4
spaces
per
indentation
and
4
spaces
per
tab
and
allow
Python
to
accept
this
","
then
there
would
be
no
problems
.
We
should
find
win-win
solutions
to
problems
.
I
see
on
Stack
Overflow
and
PEP
8
that
the
recommendation
is
to
use
spaces
only
for
indentation
in
Python
programs
.
I
can
understand
the
need
for
consistent
indentation
and
I
have
felt
that
pain
.
Is
there
an
underlying
reason
for
spaces
to
be
preferred
?
I
would
have
thought
that
tabs
were
far
easier
to
work
with
.
Note
that
the
use
of
tabs
confuses
another
aspect
of
PEP
8
:
Limit
all
lines
to
a
maximum
of
79
characters
.
Let's
say
","
hypothetically
","
that
you
use
a
tab
width
of
2
and
I
use
a
tab
width
of
8
.
You
write
all
your
code
so
your
longest
lines
reach
79
characters
","
then
I
start
to
work
on
your
file
.
Now
I've
got
hard-to-read
code
because
(
as
the
PEP
states
)
:
The
default
wrapping
in
most
tools
disrupts
the
visual
structure
of
the
code
If
we
all
use
4
spaces
","
it's
ALWAYS
the
same
.
Anyone
whose
editor
can
support
an
80
character
width
can
comfortably
read
the
code
.
Note
:
The
80
character
limit
is
a
holy
war
in
and
of
itself
","
so
let's
not
start
that
here
.
Any
non-sucky
editor
should
have
an
option
to
use
spaces
as
if
they
were
tabs
(
both
inserting
and
deleting
)
","
so
that
really
shouldn't
be
a
valid
argument
.
The
answer
was
given
right
there
in
the
PEP
[
ed
:
this
passage
has
been
edited
out
in
2013
]
.
I
quote
:
The
most
popular
way
of
indenting
Python
is
with
spaces
only
.
What
other
underlying
reason
do
you
need
?
To
put
it
less
bluntly
:
Consider
also
the
scope
of
the
PEP
as
stated
in
the
very
first
paragraph
:
This
document
gives
coding
conventions
for
the
Python
code
comprising
the
standard
library
in
the
main
Python
distribution
.
The
intention
is
to
make
all
code
that
goes
in
the
official
python
distribution
consistently
formatted
(
I
hope
we
can
agree
that
this
is
universally
a
Good
Thingâ
„
¢
)
.
Since
the
decision
between
spaces
and
tabs
for
an
individual
programmer
is
a
)
really
a
matter
of
taste
and
b
)
easily
dealt
with
by
technical
means
(
editors
","
conversion
scripts
","
etc.
)
","
there
is
a
clear
way
to
end
all
discussion
:
choose
one
.
Guido
was
the
one
to
choose
.
He
didn't
even
have
to
give
a
reason
","
but
he
still
did
by
referring
to
empirical
data
.
For
all
other
purposes
you
can
either
take
this
PEP
as
a
recommendation
","
or
you
can
ignore
it
-
-
your
choice
","
or
your
team's
","
or
your
team
leaders
.
But
if
I
may
give
you
one
advice
:
don't
mix'em
;
-
)
[
ed
:
Mixing
tabs
and
spaces
is
no
longer
an
option
.
]
Well
well
","
seems
like
everybody
is
strongly
biased
towards
spaces
.
I
use
tabs
exclusively
.
I
know
very
well
why
.
Tabs
are
actually
a
cool
invention
","
that
came
after
spaces
.
It
allows
you
to
indent
without
pushing
space
millions
of
times
or
using
a
fake
tab
(
that
produces
spaces
)
.
I
really
don't
get
why
everybody
is
discriminating
the
use
of
tabs
.
It
is
very
much
like
old
people
discriminating
younger
people
for
choosing
a
newer
more
efficient
technology
and
complaining
that
pulse
dialing
works
on
every
phone
","
not
just
on
these
fancy
new
ones
.
""""
Tone
dialing
doesn't
work
on
every
phone
","
that's
why
it
is
wrong
""""
.
Your
editor
cannot
handle
tabs
properly
?
Well
","
get
a
modern
editor
.
Might
be
darn
time
","
we
are
now
in
the
21st
century
and
the
time
when
an
editor
was
a
high
tech
complicated
piece
of
software
is
long
past
.
We
have
now
tons
and
tons
of
editors
to
choose
from
","
all
of
them
that
support
tabs
just
fine
.
Also
","
you
can
define
how
much
a
tab
should
be
","
a
thing
that
you
cannot
do
with
spaces
.
Cannot
see
tabs
?
What
is
that
for
an
argument
?
Well
","
you
cannot
see
spaces
neither
!
May
I
be
so
bold
to
suggest
to
get
a
better
editor
?
One
of
these
high
tech
ones
","
that
were
released
some
10
years
ago
already
","
that
display
invisible
characters
?
(
sarcasm
off
)
Using
spaces
causes
a
lot
more
deleting
and
formatting
work
.
That
is
why
(
and
all
other
people
that
know
this
and
agree
with
me
)
use
tabs
for
Python
.
Mixing
tabs
and
spaces
is
a
no-no
and
no
argument
about
that
.
That
is
a
mess
and
can
never
work
.
I
personally
don't
agree
with
spaces
over
tabs
.
To
me
","
tabs
are
a
document
layout
character
/
mechanism
while
spaces
are
for
content
or
delineation
between
commands
in
the
case
of
code
.
I
have
to
agree
with
Jim's
comments
that
tabs
aren't
really
the
issue
","
it
is
people
and
how
they
want
to
mix
tabs
and
spaces
.
That
said
","
I've
forced
myself
to
use
spaces
for
the
sake
of
convention
.
I
value
consistency
over
personal
preference
.
Since
python
relies
on
indentation
in
order
to
recognize
program
structure
","
a
clear
way
to
identify
identation
is
required
.
This
is
the
reason
to
pick
either
spaces
or
tabs
.
However
","
python
also
has
a
strong
philosophy
of
only
having
one
way
to
do
things
","
therefore
there
should
be
an
official
recommendation
for
one
way
to
do
indentation
.
Both
spaces
and
tabs
pose
unique
challenges
for
an
editor
to
handle
as
indentation
.
The
handling
of
tabs
themselves
is
not
uniform
across
editors
or
even
user
settings
.
Since
spaces
are
not
configurable
","
they
pose
the
more
logical
choice
as
they
guarantee
that
the
outcome
will
look
everywhere
the
same
.
JWZ
says
it
best
:
When
[
people
are
]
reading
code
","
and
when
they're
done
writing
new
code
","
they
care
about
how
many
screen
columns
by
which
the
code
tends
to
indent
when
a
new
scope
(
or
sexpr
","
or
whatever
)
opens
...
...
My
opinion
is
that
the
best
way
to
solve
the
technical
issues
is
to
mandate
that
the
ASCII
#
9
TAB
character
never
appear
in
disk
files
:
program
your
editor
to
expand
TABs
to
an
appropriate
number
of
spaces
before
writing
the
lines
to
disk
...
...
This
assumes
that
you
never
use
tabs
in
places
where
they
are
actually
significant
","
like
in
string
or
character
constants
","
but
I
never
do
that
:
when
it
matters
that
it
is
a
tab
","
I
always
use
'
\
t
'
instead
.
The
main
problems
with
indentation
occur
when
you
mix
tabs
and
spaces
.
Obviously
this
doesn't
tell
you
which
you
should
choose
","
but
it
is
a
good
reason
to
to
recommend
one
","
even
if
you
pick
it
by
flipping
a
coin
.
However
","
IMHO
there
are
a
few
minor
reasons
to
favour
spaces
over
tabs
:
Different
tools
.
Sometimes
code
gets
displayed
outside
of
a
programmer's
editor
.
Eg
.
posted
to
a
newsgroup
or
forum
.
Spaces
generally
do
better
than
tabs
here
-
everywhere
spaces
would
get
mangled
","
tabs
do
as
well
","
but
not
vice-versa
.
Programmers
see
the
source
differently
.
This
is
deeply
subjective
-
its
either
the
main
benefit
of
tabs
","
or
a
reason
to
avoid
them
depending
on
which
side
you're
on
.
On
the
plus
side
","
developers
can
view
the
source
with
their
preferred
indentation
","
so
a
developer
preferring
2-space
indent
can
work
with
an
8-space
developer
on
the
same
source
and
still
see
it
as
they
like
.
The
downside
is
that
there
are
repercussions
to
this
-
some
people
like
8-space
because
it
gives
very
visible
feedback
that
they're
too
deeply
nested
-
they
may
see
code
checked
in
by
the
2-indenter
constantly
wrapping
in
their
editor
.
Having
every
developer
see
the
code
the
same
way
leads
to
more
consistency
wrt
line
lengths
","
and
other
matters
too
.
Continued
line
indentation
.
Sometimes
you
want
to
indent
a
line
to
indicate
it
is
carried
from
the
previous
one
.
eg
.
If
using
tabs
","
theres
no
way
to
align
this
for
people
using
different
tabstops
in
their
editor
without
mixing
spaces
and
tabs
.
This
effectively
kills
the
above
benefit
.
Obviously
though
","
this
is
a
deeply
religious
issue
","
which
programming
is
plagued
with
.
The
most
important
issue
is
that
we
should
choose
one
-
even
if
thats
not
the
one
you
favour
.
Sometimes
I
think
that
the
biggest
advantage
of
significant
indentation
is
that
at
least
we're
spared
brace
placement
flamewars
.
Also
worth
reading
is
this
article
by
Jamie
Zawinski
on
the
issue
.
I've
always
used
tabs
in
my
code
.
That
said
","
I've
recently
found
a
reason
to
use
spaces
:
When
developing
on
my
Nokia
N900
internet
tablet
","
I
now
had
a
keyboard
without
a
tab
key
.
This
forced
me
to
either
copy
and
paste
tabs
or
re-write
my
code
with
spaces
.
I've
run
into
the
same
problem
with
other
phones
.
Granted
","
this
is
not
a
standard
use
of
Python
","
but
something
to
keep
in
mind
.
Besides
all
the
other
reasons
already
named
(
consistency
","
never
mixing
spaces
and
tabs
etc
)
I
believe
there
are
a
few
more
reasons
for
the
4
spaces
convention
to
note
.
These
only
apply
to
Python
(
and
maybe
other
languages
where
indentation
has
meaning
)
.
Tabs
may
be
nicer
in
other
languages
","
depending
on
individual
preferences
.
If
an
editor
doesn't
show
tabs
(
which
happens
","
depending
on
the
configuration
","
in
quite
a
few
)
","
another
author
might
assume
that
your
code
uses
4
spaces
","
b
/
c
almost
all
of
the
Python
code
being
publicly
available
does
;
if
that
same
editor
happens
to
have
a
tab
width
of
4
","
nasty
things
may
happen
-
at
least
","
that
poor
person
will
lose
time
over
an
indentation
issue
that
would
have
been
very
easy
to
avoid
by
sticking
to
the
convention
.
So
for
me
","
the
number
one
reason
is
to
avoid
bugs
with
consistency
.
Reframing
the
question
of
which
is
better
","
tabs
or
spaces
","
one
should
ask
which
the
advantages
of
tabs
are
;
I've
seen
plenty
posts
praising
tabs
","
but
few
compelling
arguments
for
them
;
good
editors
like
emacs
","
vi(m)
","
kate
","
...
do
proper
indentation
depending
on
the
semantics
of
your
code
-
even
without
tabs
;
the
same
editors
can
easily
be
configured
to
unindent
on
backspace
etc.
Some
people
have
very
strong
preferences
when
it
comes
to
their
freedom
in
deciding
the
look
/
layout
of
code
;
others
value
consistency
over
this
freedom
.
Python
drastically
reduces
this
freedom
by
dictating
that
indentation
is
used
for
blocks
etc.
This
may
be
seen
as
a
bug
or
a
feature
","
but
it
sort
of
comes
with
choosing
Python
.
Personally
","
I
like
this
consistency
-
when
starting
to
code
on
a
new
project
","
at
least
the
layout
is
close
to
what
I'm
used
to
","
so
it's
fairly
easy
to
read
.
Almost
always
.
Using
spaces
for
indentation
allows
""""
layout
tricks
""""
that
may
facilitate
to
comprehend
code
;
some
examples
of
these
are
listed
in
PEP8
;
eg
.
Of
course
","
the
above
can
also
be
written
nicely
as
However
","
the
latter
takes
more
lines
of
code
and
less
lines
are
sometimes
argued
to
be
better
(
b
/
c
you
get
more
on
a
single
screen
)
.
But
if
you
like
alignment
","
spaces
(
preferably
assisted
by
a
good
editor
)
give
you
","
in
a
sense
","
more
freedom
in
Python
than
tabs
.
[
Well
","
I
guess
some
editors
allow
you
to
do
the
same
w
/
tabs
;
)
-
but
with
spaces
","
all
of
them
do
...
]
Coming
back
to
the
same
argument
that
everybody
else
makes
-
PEP
8
dictates
(
ok
","
strongly
recommends
)
spaces
.
If
coming
to
a
project
that
uses
tabs
only
","
of
course
","
you
have
little
choice
.
But
because
of
the
establishment
of
the
PEP
8
conventions
","
almost
all
Python
programmers
are
used
to
this
style
.
This
makes
it
sooooo
much
easier
to
find
a
consensus
on
a
style
that
is
accepted
by
most
programmers
...
and
having
individuals
agree
on
style
might
be
very
hard
otherwise
.
Tools
that
help
enforcing
style
are
usually
aware
of
PEP
8
without
extra
effort
.
That's
not
a
great
reason
","
but
it's
just
nice
to
have
things
work
~
out
of
the
box
.
I'd
like
to
know
do
I
normalize
a
URL
in
python
.
For
example
","
If
I
have
a
url
string
like
:
""""
http://www.example.com/foo
goo
/
bar.html
""""
I
need
a
library
in
python
that
will
transform
the
extra
space
(
or
any
other
non
normalized
character
)
to
a
proper
URL
.
use
urllib.quote
or
urllib.quote_plus
From
the
urllib
documentation
:
"quote(string[, safe])"
Replace
special
characters
in
string
using
the
""""
%
xx
""""
escape
.
Letters
","
digits
","
and
the
characters
""""
_
.
-
""""
are
never
quoted
.
The
optional
safe
parameter
specifies
additional
characters
that
should
not
be
quoted
-
-
its
default
value
is
'
/
'
.
Example
:
quote('/~connolly/')
yields
'
/
%
7econnolly
/
'
.
"quote_plus(string[, safe])"
Like
quote()
","
but
also
replaces
spaces
by
plus
signs
","
as
required
for
quoting
HTML
form
values
.
Plus
signs
in
the
original
string
are
escaped
unless
they
are
included
in
safe
.
It
also
does
not
have
safe
default
to
'
/
'
.
EDIT
:
Using
urllib.quote
or
urllib.quote_plus
on
the
whole
URL
will
mangle
it
","
as
@ΤΖΩΤΖΙΟΥ
points
out
:
@ΤΖΩΤΖΙΟΥ
provides
a
function
that
uses
urlparse.urlparse
and
urlparse.urlunparse
to
parse
the
url
and
only
encode
the
path
.
This
may
be
more
useful
for
you
","
although
if
you're
building
the
URL
from
a
known
protocol
and
host
but
with
a
suspect
path
","
you
could
probably
do
just
as
well
to
avoid
urlparse
and
just
quote
the
suspect
part
of
the
URL
","
concatenating
with
known
safe
parts
.
Just
FYI
","
urlnorm
has
moved
to
github
:
http://gist.github.com/246089
I
encounter
such
an
problem
:
need
to
quote
the
space
only
.
fullurl
=
"quote(fullurl, safe=""%/:=&?~#+!$,;'@()"
*
[]
""""
)
do
help
","
but
it's
too
complicated
.
So
I
used
a
simple
way
:
url
=
"url.replace(' ', '%20')"
","
it's
not
perfect
","
but
it's
the
simplest
way
and
it
works
for
this
situation
.
Because
this
page
is
a
top
result
for
Google
searches
on
the
topic
","
I
think
it's
worth
mentioning
some
work
that
has
been
done
on
URL
normalization
with
Python
that
goes
beyond
urlencoding
space
characters
.
For
example
","
dealing
with
default
ports
","
character
case
","
lack
of
trailing
slashes
","
etc.
When
the
Atom
syndication
format
was
being
developed
","
there
was
some
discussion
on
how
to
normalize
URLs
into
canonical
format
;
this
is
documented
in
the
article
PaceCanonicalIds
on
the
Atom
/
Pie
wiki
.
That
article
provides
some
good
test
cases
.
I
believe
that
one
result
of
this
discussion
was
Mark
Nottingham's
urlnorm.py
library
","
which
I've
used
with
good
results
on
a
couple
projects
.
That
script
doesn't
work
with
the
URL
given
in
this
question
","
however
.
So
a
better
choice
might
be
Sam
Ruby's
version
of
urlnorm.py
","
which
handles
that
URL
","
and
all
of
the
aforementioned
test
cases
from
the
Atom
wiki
.
Have
a
look
at
this
module
:
werkzeug.utils
.
(
now
in
werkzeug.urls
)
The
function
you
are
looking
for
is
called
""""
url_fix
""""
and
works
like
this
:
It's
implemented
in
Werkzeug
as
follows
:
Real
fix
in
Python
2.7
for
that
problem
Right
solution
was
:
For
more
information
see
Issue918368
:
""""
urllib
doesn't
correct
server
returned
urls
""""
This
quotes
only
the
path
component
.
Otherwise
","
you
could
do
:
"urllib.quote(url, safe="":/"")"
Valid
for
Python
3.5
:
example
:
the
output
will
be
http://www.example.com/foo%20goo/bar.html
Font
:
https://docs.python.org/3.5/library/urllib.parse.html?highlight=quote#urllib.parse.quote
How
do
I
get
the
modified
date
/
time
of
a
file
in
Python
?
or
Formated
:
When
you
call
the
object.__repr__()
method
in
Python
you
get
something
like
this
back
:
<
main.Test
object
at
0x2aba1c0cf890
>
Is
there
any
way
to
get
a
hold
of
the
memory
address
if
you
overload
__repr__()
","
other
then
calling
"super(Class, obj)"
.
__repr__()
and
regexing
it
out
?
You
could
reimplement
the
default
repr
this
way
:
There
are
a
few
issues
here
that
aren't
covered
by
any
of
the
other
answers
.
First
","
id
only
returns
:
the
“
identity
”
of
an
object
.
This
is
an
integer
(
or
long
integer
)
which
is
guaranteed
to
be
unique
and
constant
for
this
object
during
its
lifetime
.
Two
objects
with
non-overlapping
lifetimes
may
have
the
same
id()
value
.
In
CPython
","
this
happens
to
be
the
pointer
to
the
PyObject
that
represents
the
object
in
the
interpreter
","
which
is
the
same
thing
that
object.__repr__
displays
.
But
this
is
just
an
implementation
detail
of
CPython
","
not
something
that's
true
of
Python
in
general
.
Jython
doesn't
deal
in
pointers
","
it
deals
in
Java
references
(
which
the
JVM
of
course
probably
represents
as
pointers
","
but
you
can't
see
those—and
wouldn't
want
to
","
because
the
GC
is
allowed
to
move
them
around
)
.
PyPy
lets
different
types
have
different
kinds
of
id
","
but
the
most
general
is
just
an
index
into
a
table
of
objects
you've
called
id
on
","
which
is
obviously
not
going
to
be
a
pointer
.
I'm
not
sure
about
IronPython
","
but
I'd
suspect
it's
more
like
Jython
than
like
CPython
in
this
regard
.
So
","
in
most
Python
implementations
","
there's
no
way
to
get
whatever
showed
up
in
that
repr
","
and
no
use
if
you
did
.
But
what
if
you
only
care
about
CPython
?
That's
a
pretty
common
case
","
after
all
.
Well
","
first
","
you
may
notice
that
id
is
an
integer
;
*
if
you
want
that
0x2aba1c0cf890
string
instead
of
the
number
46978822895760
","
you're
going
to
have
to
format
it
yourself
.
Under
the
covers
","
I
believe
object.__repr__
is
ultimately
using
printf's
%
p
format
","
which
you
don't
have
from
Python
…
but
you
can
always
do
this
:
*
In
3.x
","
it's
an
int
.
In
2.x
","
it's
an
int
if
that's
big
enough
to
hold
a
pointer—which
is
may
not
be
because
of
signed
number
issues
on
some
platforms—and
a
long
otherwise
.
Is
there
anything
you
can
do
with
these
pointers
besides
print
them
out
?
Sure
(
again
","
assuming
you
only
care
about
CPython
)
.
All
of
the
C
API
functions
take
a
pointer
to
a
PyObject
or
a
related
type
.
For
those
related
types
","
you
can
just
call
PyFoo_Check
to
make
sure
it
really
is
a
Foo
object
","
then
cast
with
(
PyFoo
*
)
p
.
So
","
if
you're
writing
a
C
extension
","
the
id
is
exactly
what
you
need
.
What
if
you're
writing
pure
Python
code
?
You
can
call
the
exact
same
functions
with
pythonapi
from
ctypes
.
Finally
","
a
few
of
the
other
answers
have
brought
up
ctypes.addressof
.
That
isn't
relevant
here
.
This
only
works
for
ctypes
objects
like
c_int32
(
and
maybe
a
few
memory-buffer-like
objects
","
like
those
provided
by
numpy
)
.
And
","
even
there
","
it
isn't
giving
you
the
address
of
the
c_int32
value
","
it's
giving
you
the
address
of
the
C-level
int32
that
the
c_int32
wraps
up
.
That
being
said
","
more
often
than
not
","
if
you
really
think
you
need
the
address
of
something
","
you
didn't
want
a
native
Python
object
in
the
first
place
","
you
wanted
a
ctypes
object
.
With
ctypes
","
you
can
achieve
the
same
thing
with
Documentation
:
addressof(C instance)
->
integer
Return
the
address
of
the
C
instance
internal
buffer
Note
that
in
CPython
","
currently
id(a)
=
=
ctypes.addressof(a)
","
but
ctypes.addressof
should
return
the
real
address
for
each
Python
implementation
","
if
ctypes
is
supported
memory
pointers
are
a
valid
notion
.
Edit
:
added
information
about
interpreter-independence
of
ctypes
Just
use
You
can
get
something
suitable
for
that
purpose
with
:
While
it's
true
that
id(object)
gets
the
object's
address
in
the
default
CPython
implementation
","
this
is
generally
useless
...
you
can't
do
anything
with
the
address
from
pure
Python
code
.
The
only
time
you
would
actually
be
able
to
use
the
address
is
from
a
C
extension
library
...
in
which
case
it
is
trivial
to
get
the
object's
address
since
Python
objects
are
always
passed
around
as
C
pointers
.
The
Python
manual
has
this
to
say
about
id()
:
Return
the
`
`
identity
'
'
of
an
object
.
This
is
an
integer
(
or
long
integer
)
which
is
guaranteed
to
be
unique
and
constant
for
this
object
during
its
lifetime
.
Two
objects
with
non-overlapping
lifetimes
may
have
the
same
id()
value
.
(
Implementation
note
:
this
is
the
address
of
the
object
.
)
So
in
CPython
","
this
will
be
the
address
of
the
object
.
No
such
guarantee
for
any
other
Python
interpreter
","
though
.
Note
that
if
you're
writing
a
C
extension
","
you
have
full
access
to
the
internals
of
the
Python
interpreter
","
including
access
to
the
addresses
of
objects
directly
.
Just
in
response
to
Torsten
","
I
wasn't
able
to
call
addressof()
on
a
regular
python
object
.
Furthermore
","
id(a)
!
=
addressof(a)
.
This
is
in
CPython
","
don't
know
about
anything
else
.
I'm
running
Django
1.0
and
I'm
close
to
deploying
my
app
.
As
such
","
I'll
be
changing
the
DEBUG
setting
to
False
.
With
that
being
said
","
I'd
still
like
to
include
the
stacktrace
on
my
500.html
page
when
errors
occur
.
By
doing
so
","
users
can
copy-and-paste
the
errors
and
easily
email
them
to
the
developers
.
Any
thoughts
on
how
best
to
approach
this
issue
?
I
know
this
is
an
old
question
","
but
these
days
I
would
recommend
using
a
service
such
as
Sentry
to
capture
your
errors
.
On
Django
","
the
steps
to
set
this
up
are
incredibly
simple
.
From
the
docs
:
Install
Raven
using
pip
install
raven
Add
'
raven.contrib.django.raven_compat
'
to
your
settings.INSTALLED_APPS
.
Add
RAVEN_CONFIG
=
{
""""
dsn
""""
:
YOUR_SENTRY_DSN
}
to
your
settings
.
Then
","
on
your
500
page
(
defined
in
handler500
)
","
pass
the
request.sentry.id
to
the
template
and
your
users
can
reference
the
specific
error
without
any
of
your
internals
being
exposed
.
If
we
want
to
show
exceptions
which
are
generated
","
on
ur
template(500.html)
then
we
could
write
your
own
500
view
","
grabbing
the
exception
and
passing
it
to
your
500
template
.
Steps
:
#
.
In
views.py
:
}
","
RequestContext(request)
)
)
)
#
.
In
Main
Urls.py
:
#
.
In
Template(500.html)
:
more
about
it
here
:
https://docs.djangoproject.com/en/dev/topics/http/views/#the-500-server-error-view
Automatically
log
your
500s
","
that
way
:
You
know
when
they
occur
.
You
don't
need
to
rely
on
users
sending
you
stacktraces
.
Joel
recommends
even
going
so
far
as
automatically
creating
tickets
in
your
bug
tracker
when
your
application
experiences
a
failure
.
Personally
","
I
create
a
(
private
)
RSS
feed
with
the
stacktraces
","
urls
","
etc.
that
the
developers
can
subscribe
to
.
Showing
stack
traces
to
your
users
on
the
other
hand
could
possibly
leak
information
that
malicious
users
could
use
to
attack
your
site
.
Overly
detailed
error
messages
are
one
of
the
classic
stepping
stones
to
SQL
injection
attacks
.
Edit
(
added
code
sample
to
capture
traceback
)
:
You
can
get
the
exception
information
from
the
sys.exc_info
call
.
While
formatting
the
traceback
for
display
comes
from
the
traceback
module
:
Prints
:
As
@zacherates
says
","
you
really
don't
want
to
display
a
stacktrace
to
your
users
.
The
easiest
approach
to
this
problem
is
what
Django
does
by
default
if
you
have
yourself
and
your
developers
listed
in
the
ADMINS
setting
with
email
addresses
;
it
sends
an
email
to
everyone
in
that
list
with
the
full
stack
trace
(
and
more
)
everytime
there
is
a
500
error
with
DEBUG
=
False
.
You
could
call
sys.exc_info()
in
a
custom
exception
handler
.
But
I
don't
recommend
that
.
Django
can
send
you
emails
for
exceptions
.
I
will
forgo
the
simplest
solutions
using
the
'
random
'
module
since
I
take
it
that's
not
really
what
you
are
after
.
Here's
what
I
think
you
are
looking
for
in
Python
:
To
show
you
how
it
works
:
Python
with
Numeric
Python
:
VoilÃ
!
Sure
you
could
do
something
similar
in
a
functional
programming
style
but
...
why
?
Yesterday
","
I
asked
this
question
and
never
really
got
an
answer
I
was
really
happy
with
.
I
really
would
like
to
know
how
to
generate
a
list
of
N
unique
random
numbers
using
a
functional
language
such
as
Ruby
without
having
to
be
extremely
imperative
in
style
.
Since
I
didn't
see
anything
I
really
liked
","
I've
written
the
solution
I
was
looking
for
in
LINQ
:
Can
you
translate
my
LINQ
to
Ruby
?
Python
?
Any
other
functional
programming
language
?
Note
:
Please
try
not
to
use
too
many
loops
and
conditionals
-
otherwise
the
solution
is
trivial
.
Also
","
I'd
rather
see
a
solution
where
you
don't
have
to
generate
an
array
much
bigger
than
N
so
you
can
then
just
remove
the
duplicates
and
trim
it
down
to
N
.
I
know
I'm
being
picky
","
but
I'd
really
like
to
see
some
elegant
solutions
to
this
problem
.
Thanks
!
Edit
:
Why
all
the
downvotes
?
Originally
my
code
sample
had
the
Distinct()
after
the
Take()
which
","
as
many
pointed
out
","
could
leave
me
with
an
empty
list
.
I've
changed
the
order
in
which
those
methods
are
called
to
reflect
what
I
meant
in
the
first
place
.
Apology
:
I've
been
told
this
post
came
across
as
rather
snobbish
.
I
wasn't
trying
to
imply
that
LINQ
is
better
than
Ruby
/
Python
;
or
that
my
solution
is
much
better
than
everyone
else's
.
My
intent
is
just
to
learn
how
to
do
this
(
with
certain
constraints
)
in
Ruby
.
I'm
sorry
if
I
came
across
as
a
jerk
.
Here's
another
Ruby
solution
:
I
think
","
with
your
LINQ
statement
","
the
Distinct
will
remove
duplicates
after
5
have
already
been
taken
","
so
you
aren't
guaranteed
to
get
5
back
.
Someone
can
correct
me
if
I'm
wrong
","
though
.
EDIT
:
Ok
","
just
for
fun
","
a
shorter
and
faster
one
(
and
still
using
iterators
)
.
Yeah
","
I
know
","
one-liners
should
be
left
to
perl
lovers
","
but
I
think
this
one
is
quite
powerful
isn't
it
?
Old
message
here
:
My
god
","
how
complicated
is
all
that
!
Let's
be
pythonic
:
Enjoy
EDIT
:
As
commentators
noticed
","
this
is
an
exact
translation
of
the
question's
code
.
To
avoid
the
problem
we
got
by
removing
duplicates
after
generating
the
list
","
resulting
in
too
little
data
","
you
can
choose
another
way
:
Here's
a
transliteration
from
your
solution
to
Python
.
First
","
a
generator
that
creates
Random
numbers
.
This
isn't
very
Pythonic
","
but
it's
a
good
match
with
your
sample
code
.
Here's
a
client
loop
that
collects
a
set
of
5
distinct
values
.
This
is
-
-
again
-
-
not
the
most
Pythonic
implementation
.
It's
not
clear
why
you
want
to
use
a
generator
for
random
numbers
-
-
that's
one
of
the
few
things
that's
so
simple
that
a
generator
doesn't
simplify
it
.
A
more
Pythonic
version
might
be
something
like
:
If
the
requirements
are
to
generate
5
values
and
find
distinct
among
those
5
","
then
something
like
I
can't
really
read
your
LINQ
","
but
I
think
you're
trying
to
get
5
random
numbers
up
to
100
and
then
remove
duplicates
.
Here's
a
solution
for
that
:
But
perhaps
you're
actually
looking
for
5
distinct
random
numbers
between
0
and
100
.
In
which
case
:
Now
","
this
one
might
violate
your
sense
of
""""
not
too
many
loops
","
""""
but
presumably
Take
and
Distinct
are
just
hiding
the
looping
from
you
.
It
would
be
easy
enough
to
just
add
methods
to
Enumerable
to
hide
the
while
loop
.
Maybe
this
will
suit
your
needs
and
look
a
bit
more
linqish
:
Here's
another
python
version
","
more
closely
matching
the
structure
of
your
C
#
code
.
There
isn't
a
builtin
for
giving
distinct
results
","
so
I've
added
a
function
to
do
this
.
In
Ruby
1.9
:
In
Ruby
:
Update
:
Here
is
a
slightly
different
way
:
a
=
(
0...100).entries.sort_by{rand
}
[
0...5
]
EDIT
:
and
In
Ruby
1.9
you
can
do
this
:
Hmm
...
How
about
(
Python
)
:
This
should
yield
5
unique
values
in
the
range
0
â
€
”
99
.
The
xrange
object
generates
values
as
requested
so
no
memory
is
used
for
values
that
aren't
sampled
.
Well
","
first
you
rewrite
LINQ
in
Python
.
Then
your
solution
is
a
one-liner
:
)
If
you
put
all
the
simple
methods
above
into
a
module
called
LINQ.py
","
you
can
impress
your
friends
.
(
Disclaimer
:
of
course
","
this
is
not
actually
rewriting
LINQ
in
Python
.
People
have
the
misconception
that
LINQ
is
just
a
bunch
of
trivial
extension
methods
and
some
new
syntax
.
The
really
advanced
part
of
LINQ
","
however
","
is
automatic
SQL
generation
so
that
when
you're
querying
a
database
","
it's
the
database
that
implements
Distinct()
rather
than
the
client
side
.
)
Yeah
","
it's
kind
of
annoying
.
Perhaps
this
will
work
for
your
case
.
EDIT
:
Brian
correctly
pointed
out
that
my
first
attempt
did
not
work
.
Unfortunately
","
this
attempt
is
longer
.
My
solution
would
be
:
I'd
suggest
this
","
as
it
uses
the
least
regex
to
accomplish
your
goal
.
It
is
still
functional
code
","
but
no
worse
then
your
old
Perl
.
Alternatively
","
something
not
using
regular
expressions
at
all
:
Whether
that
is
suitable
depends
on
your
actual
problem
.
Don't
forget
","
regular
expressions
aren't
the
swiss
army
knife
that
they
are
in
Perl
;
Python
has
different
constructs
for
doing
string
manipulation
.
Using
named
groups
and
a
dispatch
table
:
With
a
little
bit
of
introspection
you
can
auto-generate
the
regexp
and
the
dispatch
table
.
For
example
:
To
speed
it
up
","
one
could
turn
all
regexes
into
one
internally
and
create
the
dispatcher
on
the
fly
.
Ideally
","
this
would
be
turned
into
a
class
then
.
Expanding
on
the
solution
by
Pat
Notz
a
bit
","
I
found
it
even
the
more
elegant
to
:
-
name
the
methods
the
same
as
re
provides
(
e.g
.
search()
vs
.
check()
)
and
-
implement
the
necessary
methods
like
group()
on
the
holder
object
itself
:
Example
Instead
of
e.g
.
this
:
One
does
just
this
:
Looks
very
natural
in
the
end
","
does
not
need
too
many
code
changes
when
moving
from
Perl
and
avoids
the
problems
with
global
state
like
some
other
solutions
.
how
about
using
a
dictionary
?
however
","
you
must
ensure
there
are
no
duplicate
match_objects
dictionary
keys
(
mo_foo
","
mo_bar
","
...
)
","
best
by
giving
each
regular
expression
its
own
name
and
naming
the
match_objects
keys
accordingly
","
otherwise
match_objects.setdefault()
method
would
return
existing
match
object
instead
of
creating
new
match
object
by
running
re_xxx.search( text )
.
Here's
the
way
I
solved
this
issue
:
Not
nearly
as
clean
as
the
original
pattern
.
However
","
it
is
simple
","
straightforward
and
doesn't
require
extra
modules
or
that
you
change
the
original
regexs
.
Here
is
a
RegexDispatcher
class
that
dispatches
its
subclass
methods
by
regular
expression
.
Each
dispatchable
method
is
annotated
with
a
regular
expression
e.g
.
In
this
case
","
the
annotation
is
called
'
regex
'
and
its
value
is
the
regular
expression
to
match
on
","
'
\
+
'
","
which
is
the
+
sign
.
These
annotated
methods
are
put
in
subclasses
","
not
in
the
base
class
.
When
the
dispatch(...)
method
is
called
on
a
string
","
the
class
finds
the
method
with
an
annotation
regular
expression
that
matches
the
string
and
calls
it
.
Here
is
the
class
:
To
use
this
class
","
subclass
it
to
create
a
class
with
annotated
methods
.
By
way
of
example
","
here
is
a
simple
RPNCalculator
that
inherits
from
RegexDispatcher
.
The
methods
to
be
dispatched
are
(
of
course
)
the
ones
with
the
'
regex
'
annotation
.
The
parent
dispatch()
method
is
invoked
in
call
.
I
like
this
solution
because
there
are
no
separate
lookup
tables
.
The
regular
expression
to
match
on
is
embedded
in
the
method
to
be
called
as
an
annotation
.
For
me
","
this
is
as
it
should
be
.
It
would
be
nice
if
Python
allowed
more
flexible
annotations
","
because
I
would
rather
put
the
regex
annotation
on
the
method
itself
rather
than
embed
it
in
the
method
parameter
list
.
However
","
this
isn't
possible
at
the
moment
.
For
interest
","
take
a
look
at
the
Wolfram
language
in
which
functions
are
polymorphic
on
arbitrary
patterns
","
not
just
on
argument
types
.
A
function
that
is
polymorphic
on
a
regex
is
a
very
powerful
idea
","
but
we
can't
get
there
cleanly
in
Python
.
The
RegexDispatcher
class
is
the
best
I
could
do
.
I
switched
from
Perl
to
Python
about
a
year
ago
and
haven't
looked
back
.
There
is
only
one
idiom
that
I've
ever
found
I
can
do
more
easily
in
Perl
than
in
Python
:
The
corresponding
Python
code
is
not
so
elegant
since
the
if
statements
keep
getting
nested
:
Does
anyone
have
an
elegant
way
to
reproduce
this
pattern
in
Python
?
I've
seen
anonymous
function
dispatch
tables
used
","
but
those
seem
kind
of
unwieldy
to
me
for
a
small
number
of
regular
expressions
...
With
thanks
to
this
other
SO
question
:
A
minimalist
DataHolder
:
or
as
a
singleton
function
:
(
or
just
first
item
with
site.getsitepackages()
[0]
)
For
Ubuntu
","
...
is
not
correct
.
It
will
point
you
to
/
usr
/
lib
/
pythonX.X
/
dist-packages
This
folder
only
contains
packages
your
operating
system
has
automatically
installed
for
programs
to
run
.
On
ubuntu
","
the
site-packages
folder
that
contains
packages
installed
via
setup_tools\easy_install\pip
will
be
in
/
usr
/
local
/
lib
/
pythonX.X
/
dist-packages
The
second
folder
is
probably
the
more
useful
one
if
the
use
case
is
related
to
installation
or
reading
source
code
.
If
you
do
not
use
Ubuntu
","
you
are
probably
safe
copy-pasting
the
first
code
box
into
the
terminal
.
From
""""
How
to
Install
Django
""""
documentation
(
though
this
is
useful
to
more
than
just
Django
installation
)
-
execute
the
following
from
the
shell
:
Formatted
for
readability
(
rather
than
use
as
a
one-liner
)
","
that
looks
like
the
following
:
All
the
answers
(
or
:
the
same
answer
repeated
over
and
over
)
are
inadequate
.
What
you
want
to
do
is
this
:
The
final
line
shows
you
the
installation
dir
.
Works
on
Ubuntu
","
whereas
the
above
ones
don't
.
Don't
ask
me
about
windows
or
other
dists
","
but
since
it's
the
exact
same
dir
that
easy_install
uses
by
default
","
it's
probably
correct
everywhere
where
easy_install
works
(
so
","
everywhere
","
even
macs
)
.
Have
fun
.
Note
:
original
code
has
many
swearwords
in
it
.
This
should
work
on
all
distributions
in
and
out
of
virtual
environment
due
to
it's
""""
low-tech
""""
nature
.
The
os
module
always
resides
in
the
parent
directory
of
'
site-packages
'
To
change
dir
to
the
site-packages
dir
I
use
the
following
alias
(
on
*
nix
systems
)
:
This
works
for
me
.
It
will
get
you
both
dist-packages
and
site-packages
folders
.
If
the
folder
is
not
on
Python's
path
","
it
won't
be
doing
you
much
good
anyway
.
Output
(
Ubuntu
installation
)
:
Let's
say
you
have
installed
the
package
'
django
'
.
import it
and
type
in
dir(django)
.
It
will
show
you
","
all
the
functions
and
attributes
with
that
module
.
Type
in
the
python
interpreter
-
You
can
do
the
same
thing
if
you
have
installed
mercurial
.
This
is
for
Snow
Leopard
.
But
I
think
it
should
work
in
general
as
well
.
A
side-note
:
The
proposed
solution
(
distutils.sysconfig.get_python_lib()
)
does
not
work
when
there
is
more
than
one
site-packages
directory
(
as
recommended
by
this
article
)
.
It
will
only
return
the
main
site-packages
directory
.
Alas
","
I
have
no
better
solution
either
.
Python
doesn't
seem
to
keep
track
of
site-packages
directories
","
just
the
packages
within
them
.
This
is
what
worked
for
me
:
The
native
system
packages
installed
with
python
installation
can
be
found
at
:
/
usr
/
lib
/
python2.7
/
dist-packages
/
by
using
this
small
code
:
However
","
the
list
of
packages
installed
via
pip
can
be
found
at
:
/
usr
/
local
/
bin
/
Or
one
can
simply
write
the
following
command
to
list
all
paths
where
python
packages
are
.
There
are
two
types
of
site-packages
directories
","
global
and
per
user
.
Global
site-packages
(
""""
dist-packages
""""
)
directories
are
listed
in
sys.path
when
you
run
:
For
a
more
concise
list
run
getsitepackages
from
the
site
module
in
Python
code
:
Note
:
With
virtualenvs
getsitepackages
is
not
available
","
sys.path
from
above
will
list
the
virtualenv's
site-packages
directory
correctly
","
though
.
The
per
user
site-packages
directory
(
PEP
370
)
is
where
Python
installs
your
local
packages
:
If
this
points
to
a
non-existing
directory
check
the
exit
status
of
Python
and
see
python
-
m
site
-
-
help
for
explanations
.
As
others
have
noted
","
distutils.sysconfig
has
the
relevant
settings
:
...
though
the
default
site.py
does
something
a
bit
more
crude
","
paraphrased
below
:
(
it
also
adds
${sys.prefix
}
/
lib
/
site-python
and
adds
both
paths
for
sys.exec_prefix
as
well
","
should
that
constant
be
different
)
.
That
said
","
what's
the
context
?
You
shouldn't
be
messing
with
your
site-packages
directly
;
setuptools
/
distutils
will
work
for
installation
","
and
your
program
may
be
running
in
a
virtualenv
where
your
pythonpath
is
completely
user-local
","
so
it
shouldn't
assume
use
of
the
system
site-packages
directly
either
.
An
additional
note
to
the
get_python_lib
function
mentioned
already
:
on
some
platforms
different
directories
are
used
for
platform
specific
modules
(
eg
:
modules
that
require
compilation
)
.
If
you
pass
plat_specific=True
to
the
function
you
get
the
site
packages
for
platform
specific
packages
.
How
do
I
find
the
location
of
my
site-packages
directory
?
Answer
to
old
question
.
But
use
ipython
for
this
.
This
will
give
the
following
output
about
imaplib
package
-
There
is
also
PyInstaller
:
http://www.pyinstaller.org/
I
want
to
build
an
executable
to
distribute
to
people
without
python
installed
on
their
machines
.
Is
there
an
add-on
to
Eclipse
that
allows
this
?
I
couldn't
find
one
.
If
not
","
do
you
have
a
builder
that
you
recommend
that
would
make
it
easy
to
go
to
my
python
project
directory
created
in
Eclipse
","
and
bundle
it
all
up
?
Thanks
","
Mark
See
these
questions
It's
not
eclipse
","
but
ActiveState's
ActivePython
FAQ
mentions
the
freeze
utility
","
which
sounds
like
it
might
be
close
to
what
you're
asking
for
.
For
Windows
","
there's
the
py2exe
project
.
There's
bbfreeze
","
and
PyInstaller
","
and
py2app
","
also
.
I
have
a
dict
","
that
looks
like
this
:
And
I
need
to
get
it
to
look
like
:
I
should
point
out
that
there
can
and
will
be
multiple
top-level
keys
(
'
foo
'
in
this
case
)
.
I
could
probably
throw
something
together
to
get
what
i
need
","
but
I
was
hoping
that
there
is
a
solution
that's
more
efficient
.
Like
this
:
shutil
has
many
methods
you
can
use
.
One
of
which
is
:
Copy
the
contents
of
the
file
named
src
to
a
file
named
dst
.
The
destination
location
must
be
writable
;
otherwise
","
an
IOError
exception
will
be
raised
.
If
dst
already
exists
","
it
will
be
replaced
.
Special
files
such
as
character
or
block
devices
and
pipes
cannot
be
copied
with
this
function
.
src
and
dst
are
path
names
given
as
strings
.
Look
at
module
shutil
.
It
contains
function
"copyfile(src, dst)"
I
suggest
using
Swati's
answer
","
but
supposing
you
have
a
text
file
and
don't
want
to
use
additional
libraries
in
your
code
just
to
copy
it
","
you
can
use
the
following
one-liner
:
For
large
files
","
what
I
did
was
read
the
file
line
by
line
and
read
each
line
into
an
array
.
Then
","
once
the
array
reached
a
certain
size
","
append
it
to
a
new
file
.
"copy2(src,dst)"
is
often
more
useful
than
"copyfile(src,dst)"
because
:
it
allows
dst
to
be
a
directory
(
instead
of
the
complete
target
filename
)
","
in
which
case
the
basename
of
src
is
used
for
creating
the
new
file
;
it
preserves
the
original
modification
and
access
info
(
mtime
and
atime
)
in
the
file
metadata
(
however
","
this
comes
with
a
slight
overhead
)
.
Here
is
a
short
example
:
How
do
I
copy
a
file
in
Python
?
I
couldn't
find
anything
under
os
.
Directory
and
File
copy
example
-
From
Tim
Golden's
Python
Stuff
:
http://timgolden.me.uk/python/win32_how_do_i/copy-a-file.html
You
could
use
os.system('cp nameoffilegeneratedbyprogram /otherdirectory/')
or
as
I
did
it
","
where
rawfile
is
the
name
that
I
had
generated
inside
the
program
.
This
is
a
Linux
only
solution
You
can
use
one
of
the
copy
functions
from
the
shutil
package
:
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
Function
preserves
supports
accepts
copies
other
permissions
directory
dest
.
file
obj
metadata
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
―
shutil.copy
✔
✔
☐
☐
shutil.copy2
✔
✔
☐
✔
shutil.copyfile
☐
☐
☐
☐
shutil.copyfileobj
☐
☐
✔
☐
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
━
Example
:
Copying
a
file
is
a
relatively
straightforward
operation
as
shown
by
the
examples
below
","
but
you
should
instead
use
the
shutil
stdlib
module
for
that
.
If
you
want
to
copy
by
filename
you
could
do
something
like
this
:
Use
the
shutil
module
.
Copy
the
contents
of
the
file
named
src
to
a
file
named
dst
.
The
destination
location
must
be
writable
;
otherwise
","
an
IOError
exception
will
be
raised
.
If
dst
already
exists
","
it
will
be
replaced
.
Special
files
such
as
character
or
block
devices
and
pipes
cannot
be
copied
with
this
function
.
src
and
dst
are
path
names
given
as
strings
.
Take
a
look
at
filesys
for
all
the
file
and
directory
handling
functions
available
in
standard
Python
modules
.
Additional
information
about
Apache
/
mod_wsgi
and
access
","
authentication
and
authorization
mechanisms
can
be
found
in
:
http://code.google.com/p/modwsgi/wiki/AccessControlMechanisms
The
information
isn't
passed
by
default
because
doing
so
could
leak
password
information
to
applications
which
maybe
shouldn't
get
it
.
I've
got
the
directive
I'd
like
to
know
in
the
/
some
/
script.wsgi
What
user
is
logged
in
.
How
do
I
do
that
?
add
WSGIPassAuthorization
On
:
Then
just
read
environ
[
'
REMOTE_USER
'
]
:
More
information
at
mod_wsgi
documentation
.
This
also
does
the
trick
for
me
:
Here
is
a
demonstration
in
python
shell
:
Getting
the
environment
variable
PWD
didn't
always
work
for
me
so
I
use
the
popen
method
.
Cheers
!
In
python
is
it
possible
to
get
or
set
a
logical
directory
(
as
opposed
to
an
absolute
one
)
.
For
example
if
I
have
:
and
I
have
linked
to
the
same
directory
.
using
os.getcwd
and
os.chdir
will
always
use
the
absolute
path
The
only
way
I
have
found
to
get
around
this
at
all
is
to
launch
'
pwd
'
in
another
process
and
read
the
output
.
However
","
this
only
works
until
you
call
os.chdir
for
the
first
time
.
The
underlying
operational
system
/
shell
reports
real
paths
to
python
.
So
","
there
really
is
no
way
around
it
","
since
os.getcwd()
is
a
wrapped
call
to
C
Library
getcwd()
function
.
There
are
some
workarounds
in
the
spirit
of
the
one
that
you
already
know
which
is
launching
pwd
.
Another
one
would
involve
using
os.environ
[
'
PWD
'
]
.
If
that
environmnent
variable
is
set
you
can
make
some
getcwd
function
that
respects
it
.
The
solution
below
combines
both
:
By
""""
separate
apps
within
Django
""""
do
you
mean
separate
applications
with
a
common
settings
?
That
is
to
say
","
two
applications
within
the
same
Django
site
(
or
project
)
?
If
so
","
the
{
%
url
%
}
tag
will
generate
a
proper
absolute
URL
to
any
of
the
apps
listed
in
the
settings
file
.
If
there
are
separate
Django
servers
with
separate
settings
","
you
have
the
standard
internet
problem
of
URI
design
.
Your
URI's
can
be
consistent
with
only
the
hostname
changing
.
You
never
need
to
provide
the
host
information
","
so
all
of
your
links
are
<
A
HREF
=
""""
/
some
/
path
/
""""
>
.
This
","
generally
","
works
out
the
best
.
You
have
can
someone's
random
laptop
being
a
test
server
;
you
can
get
the
IP
address
using
ifconfig
.
I
am
trying
to
mock-up
an
API
and
am
using
separate
apps
within
Django
to
represent
different
web
services
.
I
would
like
App
A
to
take
in
a
link
that
corresponds
to
App
B
and
parse
the
json
response
.
Is
there
a
way
to
dynamically
construct
the
url
to
App
B
so
that
I
can
test
the
code
in
development
and
not
change
to
much
before
going
into
production
?
The
problem
is
that
I
can't
use
localhost
as
part
of
a
link
.
I
am
currently
using
urllib
","
but
eventually
I
would
like
to
do
something
less
hacky
and
better
fitting
with
the
web
services
REST
paradigm
.
You
could
do
something
like
and
use
other
to
build
the
external
URL
.
Generally
you
code
in
DEBUG
mode
and
deploy
in
non-DEBUG
mode
.
settings.DEBUG
is
a
'
standard
'
Django
thing
.
Many-core
Engine
(
MCE
)
has
been
released
for
Perl
.
MCE
does
quite
well
at
this
","
even
when
reading
directly
from
disk
with
8
workers
(
cold
cache
)
.
Compare
to
wf_mmap
.
MCE
follows
a
bank
queuing
model
when
reading
input
data
.
Look
under
the
images
folder
for
slides
on
it
.
The
source
code
is
hosted
at
http://code.google.com/p/many-core-engine-perl/
The
perl
documentation
can
be
read
at
https://metacpan.org/module/MCE
An
implementation
of
the
Wide
Finder
with
MCE
is
provided
under
examples
/
tbray
/
https://metacpan.org/source/MARIOROY/MCE-1.514/examples/tbray/
Enjoy
MCE
.
The
Perl
implementation
uses
the
mmap
system
call
.
This
.
It
avoids
buffer
copying
and
provides
async
I
/
O
.
I'd
be
very
grateful
if
you
could
compare
the
winning
Oâ€™Rourke's
Perl
solution
to
Lundh's
Python
solution
","
as
I
don't
know
Perl
good
enough
to
understand
what's
going
on
there
.
More
specifically
I'd
like
to
know
what
gave
Perl
version
3x
advantage
:
algorithmic
superiority
","
quality
of
C
extensions
","
other
factors
?
Wide
Finder
:
Results
The
better
regex
implementation
of
perl
is
one
part
of
the
story
.
That
can't
explain
however
why
the
perl
implementation
scales
better
.
The
difference
become
bigger
with
more
processors
.
For
some
reason
the
python
implementation
has
an
issue
there
.
The
Perl
implementation
uses
the
mmap
system
call
.
What
that
call
does
is
establish
a
pointer
which
to
the
process
appears
to
be
a
normal
segment
of
memory
or
buffer
to
the
program
.
It
maps
the
contents
of
a
file
to
a
region
of
memory
.
There
are
performances
advantages
of
doing
this
vs
normal
file
IO
(
read
)
-
one
is
that
there
are
no
user-space
library
calls
necessary
to
get
access
to
the
data
","
another
is
that
there
are
often
less
copy
operations
necessary
(
eg
:
moving
data
between
kernel
and
user
space
)
.
Perl's
strings
and
regular
expressions
are
8-bit
byte
based
(
as
opposed
to
utf16
for
Java
for
example
)
","
so
Perl's
native
'
character
type
'
is
the
same
encoding
of
the
mmapped
file
.
When
the
regular
expression
engine
then
operates
on
the
mmap
backed
variable
","
it
is
directly
accessing
the
file
data
via
the
mamped
memory
region
-
without
going
through
Perl's
IO
functions
","
or
even
libc's
IO
functions
.
The
mmap
is
probably
largely
responsible
for
the
performance
difference
vs
the
Python
version
using
the
normal
Python
IO
libraries
-
which
additionally
introduce
the
overhead
of
looking
for
line
breaks
.
The
Perl
program
also
supports
a
-
J
to
parallelize
the
processing
","
where
the
oepen
""""
-
|
""""
causes
a
fork()
where
the
file
handle
in
the
parent
is
to
the
child's
stdout
.
The
child
processes
serialize
their
results
to
stdout
and
the
parent
de-serializes
them
to
coordinate
and
summarize
the
results
.
Perl
is
heavily
optimized
for
text
processing
.
There
are
so
many
factors
that
it's
hard
to
say
what's
the
exact
difference
.
Text
is
represented
completely
differently
internally
(
utf-8
versus
utf-16
/
utf-32
)
and
the
regular
expression
engines
are
completely
different
too
.
Python's
regular
expression
engine
is
a
custom
one
and
not
as
much
used
as
the
perl
one
.
There
are
very
few
developers
working
on
it
(
I
think
it's
largely
unmaintained
)
in
contrast
to
the
Perl
one
which
is
basically
the
""""
core
of
the
language
""""
.
After
all
Perl
is
the
text
processing
language
.
IMO
python's
regexing
","
esp
.
when
you
try
to
represent
something
like
perl's
/
e
operator
as
in
s
/
whatever
/
somethingelse
/
e
","
becomes
quite
slow
.
So
in
doubt
","
you
may
need
to
stay
with
Perl5
:
-
)
Python
has
a
major
advantage
of
being
available
in
a
production-ready
format
today
.
Python
has
Jython
and
IronPython
","
if
you
need
to
work
closely
with
Java
or
the
.
net
clr
.
Perl
6
has
the
advantages
of
being
based
on
the
same
principles
as
Perl
(
1-5
)
;
If
you
like
Perl
","
you'll
like
Perl
6
for
the
same
reasons
.
(
There's
more
than
one
way
to
do
it
","
etc.
)
Perl
6
also
has
an
advantage
of
being
only
partially
implemented
:
If
you
want
to
hack
on
language
internals
or
help
to
define
the
standard
libraries
","
this
is
a
great
time
to
get
started
in
Perl
6
.
Edit
:
(
2011
)
It's
still
a
great
time
to
hack
on
the
Perl6
internals
","
but
there
is
now
a
much
more
mature
","
usable
Perl6
distribution
","
Rakudo
Star
.
If
you
want
to
use
Perl6
today
","
that's
a
great
choice
.
Perl
6
has
some
great
features
over
python
.
I
think
these
are
all
features
that
Python
3
doesn't
have
:
an
extensible
","
but
optional
type
system
including
multi-method
dispatch
and
type
checking
even
for
builtins
and
""""
operators
""""
with
subset
types
Roles
:
Non-instantiable
classes
for
code
reuse
(
aka
mixins
)
with
some
additional
advantages
over
most
mixins
:
compile-time
method
name
collision
checking
","
the
ability
to
require
","
in
a
role
definition
","
that
certain
methods
are
present
in
the
host
class
","
and
subsequently
to
call
methods
of
the
host
class
in
the
implementation
of
the
role
.
Grammars
(
compositions
of
regexes
that
return
structured
data
)
Powerful
extended
regexes
.
Fancier
than
python's
built-in
regex
capabilities
.
Junctions
and
type
junctions
Metaoperators
The
ability
to
define
circumfix
","
postcircumfix
","
infix
","
prefix
","
and
postfix
operators
.
Theoretically
","
the
ability
to
work
with
the
AST
as
structured
data
.
(
Aka
:
lisp
macros
)
Block
scoping
The
*
term
for
generating
closures
out
of
any
expression
Implicit
concurrency
constructs
(
[]
meta
","
junctions
)
Declarative
concurrency
constructs
(
hyper
for
)
The
MAIN
sub
turns
its
function
signature
into
a
command
line
interface
and
a
command
line
usage
guide
.
Lazy
lists
(
but
you
can
roll
your
own
with
generators
)
And
it
shares
with
Python
the
following
advantages
over
Perl
5
:
The
concept
of
a
""""
class
""""
","
the
class
keyword
","
and
associated
sugar
*
*
:
The
self
keyword
getter
/
setter
sugar
with
the
concept
of
private
data
a
well
defined
","
accessible
meta-object
protocol
great
unicode
support
great
date
/
time
support
built
in
In
general
","
""""
batteries
included
""""
-
lots
of
builtin
functions
non-globally
scoped
variables
by
default
Perl6's
implementation
is
not
100
%
complete
","
or
tuned
enough
to
use
for
certain
values
of
""""
performance
sensitive
""""
applications
.
But
the
80-90
%
of
the
spec
that
is
needed
for
95-99
%
of
use
cases
is
complete
in
Rakudo
","
right
now
*
*
One
glaring
omission
is
any
provision
for
asynchronous
I
/
O
.
That
part
of
the
spec
is
in
flux
.
*
*
Though
the
popular
Moose
project
provides
this
in
Perl
5
.
There
is
no
advantage
to
be
gained
by
switching
from
Perl
to
Python
.
There
is
also
no
advantage
to
be
gained
by
switching
from
Python
to
Perl
.
They
are
both
equally
capable
.
Choose
your
tools
based
on
what
you
know
and
the
problem
you
are
trying
to
solve
rather
than
on
some
sort
of
notion
that
one
is
somehow
inherently
better
than
the
other
.
The
only
real
advantage
is
if
you
are
switching
from
a
language
you
don't
know
to
a
language
you
do
know
","
in
which
case
your
productivity
will
likely
go
up
.
Python
does
not
have
Junctions
.
In
fact
I
think
only
Perl
has
Junctions
so
far
.
:
-
)
Perl
is
generally
better
than
python
for
quick
one
liners
","
especially
involve
text
/
regular
expressions
http://novosial.org/perl/one-liner/
You
have
not
said
why
you
want
to
move
away
from
Perl
*
.
If
my
crystal
ball
is
functioning
today
then
it
is
because
you
do
not
fully
know
the
language
and
so
it
frustrates
you
.
Stick
with
Perl
and
study
the
language
well
.
If
you
do
then
one
day
you
will
be
a
guru
and
know
why
your
question
is
irrelevant
.
Enlightment
comes
to
those
to
seek
it
.
You
called
it
""""
Perl5
""""
but
there
is
no
such
language
.
:
P
Coming
from
a
Perl
5
background
","
what
are
the
advantages
of
moving
to
Perl
6
or
Python
?
Edit
:
If
you
downvoted
this
because
you
think
it's
just
flamebait
","
read
the
answers
below
.
They're
not
raving
arguments
;
they're
well-written
discussions
of
the
pros
and
cons
of
each
language
.
Give
the
Stack
Overflow
community
some
credit
.
Python
has
one
huge
advantage
:
it's
implemented
","
there's
a
rather
stable
compiler
for
it
.
Perl
6
is
a
rather
visionary
language
","
and
not
yet
anywhere
nearly
stable
enough
for
production
.
But
it
has
a
set
of
very
cool
features
","
among
them
:
junctions
","
grammars
(
yes
","
you
can
write
full
parsers
with
Perl
6
""""
regexes
""""
)
","
unicode
handling
at
the
grapheme
level
","
lazy
lists
and
powerful
macros
.
In
your
particular
case
when
you
know
Perl
5
you'll
get
familiar
with
the
Perl
6
syntax
very
quickly
.
For
a
more
comprehensive
list
of
what
cool
features
Perl
6
has
","
see
the
FAQ
.
In
my
opinion
","
Python's
syntax
is
much
cleaner
","
simpler
","
and
consistent
.
You
can
define
nested
data
structures
the
same
everywhere
","
whether
you
plan
to
pass
them
to
a
function
(
or
return
them
from
one
)
or
use
them
directly
.
I
like
Perl
a
lot
","
but
as
soon
as
I
learned
enough
Python
to
""""
get
""""
it
","
I
never
turned
back
.
In
my
experience
","
random
snippets
of
Python
tend
to
be
more
readable
than
random
snippets
of
Perl
.
The
difference
really
comes
down
to
the
culture
around
each
language
","
where
Perl
users
often
appreciate
cleverness
while
Python
users
more
often
prefer
clarity
.
That's
not
to
say
you
can't
have
clear
Perl
or
devious
Python
","
but
those
are
much
less
common
.
Both
are
fine
languages
and
solve
many
of
the
same
problems
.
I
personally
lean
toward
Python
","
if
for
no
other
reason
in
that
it
seems
to
be
gaining
momentum
while
Perl
seems
to
be
losing
users
to
Python
and
Ruby
.
Note
the
abundance
of
weasel
words
in
the
above
.
Honestly
","
it's
really
going
to
come
down
to
personal
preference
.
I
found
an
explanation
here
by
googling
for
""""
windows
ini
""""
""""
default
section
""""
.
Summary
:
whatever
you
put
in
the
[DEFAULT]
section
gets
propagated
to
every
other
section
.
Using
the
example
from
the
linked
website
","
let's
say
I
have
a
config
file
called
test1.ini
:
I
can
read
this
using
ConfigParser
:
But
I
notice
that
lh_server
is
the
same
in
both
sections
;
and
","
indeed
","
I
realise
that
it
will
be
the
same
for
most
hosts
I
might
add
.
So
I
can
do
this
","
as
test2.ini
:
Despite
the
sections
not
having
lh_server
keys
","
I
can
still
access
them
:
Read
the
linked
page
for
a
further
example
of
using
variable
substitution
in
the
DEFAULT
section
to
simplify
the
INI
file
even
more
.
I've
used
ConfigParser
for
quite
a
while
for
simple
configs
.
One
thing
that's
bugged
me
for
a
long
time
is
the
DEFAULT
section
.
I'm
not
really
sure
what's
an
appropriate
use
.
I've
read
the
documentation
","
but
I
would
really
like
to
see
some
clever
examples
of
its
use
and
how
it
affects
other
sections
in
the
file
(
something
that
really
illustrates
the
kind
of
things
that
are
possible
)
.
python's
time
module
seems
a
little
haphazard
.
For
example
","
here
is
a
list
of
methods
in
there
","
from
the
docstring
:
Looking
at
localtime()
and
its
inverse
mktime()
","
why
is
there
no
inverse
for
gmtime()
?
Bonus
questions
:
what
would
you
name
the
method
?
How
would
you
implement
it
?
mktime
documentation
is
a
bit
misleading
here
","
there
is
no
meaning
saying
it's
calculated
as
a
local
time
","
rather
it's
calculating
the
seconds
from
Epoch
according
to
the
supplied
tuple
-
regardless
of
your
computer
locality
.
If
you
do
want
to
do
a
conversion
from
a
utc_tuple
to
local
time
you
can
do
the
following
:
Perhaps
a
more
accurate
question
would
be
how
to
convert
a
utc_tuple
to
a
local_tuple
.
I
would
call
it
gm_tuple_to_local_tuple
(
I
prefer
long
and
descriptive
names
)
:
Validatation
:
Hope
this
helps
","
ilia
.
There
is
actually
an
inverse
function
","
but
for
some
bizarre
reason
","
it's
in
the
calendar
module
:
calendar.timegm()
.
I
listed
the
functions
in
this
answer
.
I'm
only
a
newbie
to
Python
","
but
here's
my
approach
.
There
","
my
proposed
name
for
the
function
too
.
:
-
)
It's
important
to
recalculate
disp
every
time
","
in
case
the
daylight-savings
value
changes
or
the
like
.
(
The
conversion
back
to
tuple
is
required
for
Jython
.
CPython
doesn't
seem
to
require
it
.
)
This
is
super
ick
","
because
time.gmtime
sets
the
DST
flag
to
false
","
always
.
I
hate
the
code
","
though
.
There's
got
to
be
a
better
way
to
do
it
.
And
there
are
probably
some
corner
cases
that
I
haven't
got
","
yet
.
I
always
thought
the
time
and
datetime
modules
were
a
little
incoherent
.
Anyways
","
here's
the
inverse
of
mktime
The
most
obvious
(
and
I
would
argue
most
readable
)
answer
is
to
not
use
a
list
comprehension
or
generator
expression
","
but
rather
a
real
generator
:
It
takes
more
horizontal
space
","
but
it's
much
easier
to
see
what
it
does
at
a
glance
","
and
you
end
up
not
repeating
yourself
.
If
the
calculations
are
already
nicely
bundled
into
functions
","
how
about
using
filter
and
map
?
You
can
use
itertools.imap
if
the
list
is
very
large
.
This
is
exactly
what
generators
are
suited
to
handle
:
This
makes
it
totally
clear
what
is
happening
during
each
stage
of
the
pipeline
.
Explicit
over
implicit
Uses
generators
everywhere
until
the
final
step
","
so
no
large
intermediate
lists
cf
:
'
Generator
Tricks
for
System
Programmers
'
by
David
Beazley
The
Python
list
comprehension
syntax
makes
it
easy
to
filter
values
within
a
comprehension
.
For
example
:
Will
return
a
list
of
the
squares
of
integers
in
mylist
.
However
","
what
if
the
test
involves
some
(
costly
)
computation
and
you
want
to
filter
on
the
result
?
One
option
is
:
This
will
result
in
a
list
of
"non-""false"
""""
expensive(x)
values
","
however
expensive()
is
called
twice
for
each
x
.
Is
there
a
comprehension
syntax
that
allows
you
to
do
this
test
while
only
calling
expensive
once
per
x
?
Came
up
with
my
own
answer
after
a
minute
of
thought
.
It
can
be
done
with
nested
comprehensions
:
I
guess
that
works
","
though
I
find
nested
comprehensions
are
only
marginally
readable
You
could
memoize
expensive(x)
(
and
if
you
are
calling
expensive(x)
frequently
","
you
probably
should
memoize
it
any
way
.
This
page
gives
an
implementation
of
memoize
for
python
:
http://code.activestate.com/recipes/52201/
This
has
the
added
benefit
that
expensive(x)
may
be
run
less
than
N
times
","
since
any
duplicate
entries
will
make
use
of
the
memo
from
the
previous
execution
.
Note
that
this
assumes
expensive(x)
is
a
true
function
","
and
does
not
depend
on
external
state
that
may
change
.
If
expensive(x)
does
depend
on
external
state
","
and
you
can
detect
when
that
state
changes
","
or
you
know
it
wont
change
during
your
list
comprehension
","
then
you
can
reset
the
memos
before
the
comprehension
.
You
could
always
memoize
the
expensive()
function
so
that
calling
it
the
second
time
around
is
merely
a
lookup
for
the
computed
value
of
x
.
Here's
just
one
of
many
implementations
of
memoize
as
a
decorator
.
map()
will
return
a
list
of
the
values
of
each
object
in
mylist
passed
to
expensive()
.
Then
you
can
list-comprehend
that
","
and
discard
unnecessary
values
.
This
is
somewhat
like
a
nested
comprehension
","
but
should
be
faster
(
since
the
python
interpreter
can
optimize
it
fairly
easily
)
.
I
will
have
a
preference
for
:
This
has
the
advantage
to
:
avoid
None
as
the
function
(
will
be
eliminated
in
Python
3
)
:
http://bugs.python.org/issue2186
use
only
iterators
.
There
is
the
plain
old
use
of
a
for
loop
to
append
to
a
list
","
too
:
As
far
as
I
can
tell
","
the
method
described
in
the
question
is
the
only
way
to
pass
build
variables
to
a
Python
script
.
Perhaps
Visual
Studio
2010
has
something
better
?
This
is
a
bit
hacky
","
but
it
could
work
.
Why
not
call
multiple
.
py
scripts
in
a
row
?
Each
scripts
can
pass
in
a
small
subset
of
the
parameters
","
and
the
values
to
a
temp
text
file
.
The
final
script
will
read
and
work
off
of
the
temp
text
file
.
I
agree
that
this
method
is
filled
with
danger
and
WTF's
","
but
sometimes
you
have
to
just
hack
stuff
together
.
You
might
want
to
look
into
PropertySheets
.
These
are
files
containing
Visual
C
+
+
settings
","
including
user
macros
.
The
sheets
can
inherit
from
other
sheets
and
are
attached
to
VC
+
+
projects
using
the
PropertyManager
View
in
Visual
Studio
.
When
you
create
one
of
these
sheets
","
there
is
an
interface
for
creating
user
macros
.
When
you
add
a
macro
using
this
mechanism
","
there
is
a
checkbox
for
setting
the
user
macro
as
an
environment
variable
.
We
use
this
type
of
mechanism
in
our
build
system
to
rapidly
set
up
projects
to
perform
out-of-place
builds
.
Our
various
build
directories
are
all
defined
as
user
macros
.
I
have
not
actually
verified
that
the
environment
variables
are
set
in
an
external
script
called
from
post-build
.
I
tend
to
use
these
macros
as
command
line
arguments
to
my
post-build
scripts
-
but
I
would
expect
accessing
them
as
environment
variables
should
work
for
you
.
[
NOTE
:
This
questions
is
similar
to
but
not
the
same
as
this
one
.
]
Visual
Studio
defines
several
dozen
""""
Macros
""""
which
are
sort
of
simulated
environment
variables
(
completely
unrelated
to
C
+
+
macros
)
which
contain
information
about
the
build
in
progress
.
Examples
:
ConfigurationName
Release
TargetPath
D:\work\foo\win\Release\foo.exe
VCInstallDir
C:\ProgramFiles\Microsoft
Visual
Studio
9.0\VC
\
Here
is
the
complete
set
of
43
built-in
Macros
that
I
see
(
yours
may
differ
depending
on
which
version
of
VS
you
use
and
which
tools
you
have
enabled
)
:
ConfigurationName
IntDir
RootNamespace
TargetFileName
DevEnvDir
OutDir
SafeInputName
TargetFramework
FrameworkDir
ParentName
SafeParentName
TargetName
FrameworkSDKDir
PlatformName
SafeRootNamespace
TargetPath
FrameworkVersion
ProjectDir
SolutionDir
VCInstallDir
FxCopDir
ProjectExt
SolutionExt
VSInstallDir
InputDir
ProjectFileName
SolutionFileName
WebDeployPath
InputExt
ProjectName
SolutionName
WebDeployRoot
InputFileName
ProjectPath
SolutionPath
WindowsSdkDir
InputName
References
TargetDir
WindowsSdkDirIA64
InputPath
RemoteMachine
TargetExt
Of
these
","
only
four
(
FrameworkDir
","
FrameworkSDKDir
","
VCInstallDir
and
VSInstallDir
)
are
set
in
the
environment
used
for
build-events
.
As
Brian
mentions
","
user-defined
Macros
can
be
defined
such
as
to
be
set
in
the
environment
in
which
build
tasks
execute
.
My
problem
is
with
the
built-in
Macros
.
I
use
a
Visual
Studio
Post-Build
Event
to
run
a
python
script
as
part
of
my
build
process
.
I'd
like
to
pass
the
entire
set
of
Macros
(
built-in
and
user-defined
)
to
my
script
in
the
environment
but
I
don't
know
how
.
Within
my
script
I
can
access
regular
environment
variables
(
e.g
.
","
Path
","
SystemRoot
)
but
NOT
these
""""
Macros
""""
.
All
I
can
do
now
is
pass
them
on-by-one
as
named
options
which
I
then
process
within
my
script
.
For
example
","
this
is
what
my
Post-Build
Event
command
line
looks
like
:
postbuild.py
-
-
"t=""$(TargetPath"
)
""""
-
-
"c=""$(ConfigurationName"
)
""""
Besides
being
a
pain
in
the
neck
","
there
is
a
limit
on
the
size
of
Post-Build
Event
command
line
so
I
can't
pass
dozens
Macros
using
this
method
even
if
I
wanted
to
because
the
command
line
is
truncated
.
Does
anyone
know
if
there
is
a
way
to
pass
the
entire
set
of
Macro
names
and
values
to
a
command
that
does
NOT
require
switching
to
MSBuild
(
which
I
believe
is
not
available
for
native
VC
+
+
)
or
some
other
make-like
build
tool
?
Like
so
:
Use
the
datetime.timedelta
class
:
Your
example
could
be
written
as
:
or
Compare
the
difference
to
a
timedelta
that
you
create
:
Alternative
:
Assuming
self.timestamp
is
an
datetime
instance
You
can
subtract
two
datetime
objects
to
find
the
difference
between
them
.
You
can
use
datetime.fromtimestamp
to
parse
a
POSIX
time
stamp
.
I
would
like
to
find
out
if
a
particular
python
datetime
object
is
older
than
X
hours
or
minutes
.
I
am
trying
to
do
something
similar
to
:
This
generates
a
type
error
.
What
is
the
proper
way
to
do
date
time
comparison
in
python
?
I
already
looked
at
WorkingWithTime
which
is
close
but
not
exactly
what
I
want
.
I
assume
I
just
want
the
datetime
object
represented
in
seconds
so
that
I
can
do
a
normal
int
comparison
.
Please
post
lists
of
datetime
best
practices
.
You
can
use
a
combination
of
the
'
days
'
and
'
seconds
'
attributes
of
the
returned
object
to
figure
out
the
answer
","
like
this
:
Use
abs()
in
the
answer
if
you
always
want
a
positive
number
of
seconds
.
To
discover
how
many
seconds
into
the
past
a
timestamp
is
","
you
can
use
it
like
this
:
This
may
not
completely
answer
your
question
but
you
could
also
try
using
the
Elevate
Command
Powertoy
in
order
to
run
the
script
with
elevated
UAC
privileges
.
http://technet.microsoft.com/en-us/magazine/2008.06.elevation.aspx
I
think
if
you
use
it
it
would
look
like
'
elevate
python
yourscript.py
'
The
following
example
builds
on
MARTIN
DE
LA
FUENTE
SAAVEDRA's
excellent
work
and
accepted
answer
.
In
particular
","
two
enumerations
are
introduced
.
The
first
allows
for
easy
specification
of
how
an
elevated
program
is
to
be
opened
","
and
the
second
helps
when
errors
need
to
be
easily
identified
.
Please
note
that
if
you
want
all
command
line
arguments
passed
to
the
new
process
","
sys.argv
[0]
should
probably
be
replaced
with
a
function
call
:
subprocess.list2cmdline(sys.argv)
.
It
took
me
a
little
while
to
get
dguaraglia's
answer
working
","
so
in
the
interest
of
saving
others
time
","
here's
what
I
did
to
implement
this
idea
:
A
variation
on
Jorenko's
work
above
allows
the
elevated
process
to
use
the
same
console
(
but
see
my
comment
below
)
:
It
seems
there's
no
way
to
elevate
the
application
privileges
for
a
while
for
you
to
perform
a
particular
task
.
Windows
needs
to
know
at
the
start
of
the
program
whether
the
application
requires
certain
privileges
","
and
will
ask
the
user
to
confirm
when
the
application
performs
any
tasks
that
need
those
privileges
.
There
are
two
ways
to
do
this
:
Write
a
manifest
file
that
tells
Windows
the
application
might
require
some
privileges
Run
the
application
with
elevated
privileges
from
inside
another
program
This
two
articles
explain
in
much
more
detail
how
this
works
.
What
I'd
do
","
if
you
don't
want
to
write
a
nasty
ctypes
wrapper
for
the
CreateElevatedProcess
API
","
is
use
the
ShellExecuteEx
trick
explained
in
the
Code
Project
article
(
Pywin32
comes
with
a
wrapper
for
ShellExecute
)
.
How
?
Something
like
this
:
When
your
program
starts
","
it
checks
if
it
has
Administrator
privileges
","
if
it
doesn't
it
runs
itself
using
the
ShellExecute
trick
and
exits
immediately
","
if
it
does
","
it
performs
the
task
at
hand
.
As
you
describe
your
program
as
a
""""
script
""""
","
I
suppose
that's
enough
for
your
needs
.
Cheers
.
As
of
2017
","
an
easy
method
to
achieve
this
is
the
following
:
Some
of
the
advantages
here
are
:
No
external
libraries
required
(
nor
Python
for
Windows
extension
)
.
It
only
uses
ctypes
from
standard
library
.
Works
on
both
Python
2
and
Python
3
.
There
is
no
need
to
modify
the
file
resources
nor
creating
a
manifest
file
.
If
you
don't
add
code
below
if
/
else
statement
","
the
code
won't
ever
be
executed
twice
.
You
can
easily
modify
it
to
have
a
special
behavior
if
the
user
reject
the
UAC
prompt
.
You
can
specify
arguments
modifying
the
fourth
parameter
.
You
can
specify
the
display
method
modifying
the
sixth
parameter
.
Documentation
for
the
underlying
ShellExecute
call
is
here
.
Recognizing
this
question
was
asked
years
ago
","
I
think
a
more
elegant
solution
is
offered
on
github
by
frmdstryr
using
his
module
pyminutils
:
Excerpt
:
This
utilizes
the
COM
interface
and
automatically
indicates
that
admin
privileges
are
needed
with
the
familiar
dialog
prompt
that
you
would
see
if
you
were
copying
into
a
directory
where
admin
privileges
are
required
and
also
provides
the
typical
file
progress
dialog
during
the
copy
operation
.
I
want
my
Python
script
to
copy
files
on
Vista
.
When
I
run
it
from
a
normal
cmd.exe
window
","
no
errors
are
generated
","
yet
the
files
are
NOT
copied
.
If
I
run
cmd.exe
""""
as
administator
""""
and
then
run
my
script
","
it
works
fine
.
This
makes
sense
since
User
Account
Control
(
UAC
)
normally
prevents
many
file
system
actions
.
Is
there
a
way
I
can
","
from
within
a
Python
script
","
invoke
a
UAC
elevation
request
(
those
dialogs
that
say
something
like
""""
such
and
such
app
needs
admin
access
","
is
this
OK
?
""""
)
If
that's
not
possible
","
is
there
a
way
my
script
can
at
least
detect
that
it
is
not
elevated
so
it
can
fail
gracefully
?
You
can
make
a
shortcut
somewhere
and
as
the
target
use
:
python
yourscript.py
then
under
properties
and
advanced
select
run
as
administrator
.
When
the
user
executes
the
shortcut
it
will
ask
them
to
elevate
the
application
.
If
your
script
always
requires
an
Administrator's
privileges
then
:
Assuming
your
using
mod_python
:
Or
:
http://www.modpython.org/live/current/doc-html/pyapi-mprequest-mem.html
content_type
String
.
The
content
type
.
Mod_python
maintains
an
internal
flag
(
req._content_type_set
)
to
keep
track
of
whether
content_type
was
set
manually
from
within
Python
.
The
publisher
handler
uses
this
flag
in
the
following
way
:
when
content_type
isn't
explicitly
set
","
it
attempts
to
guess
the
content
type
by
examining
the
first
few
bytes
of
the
output
.
If
you
just
want
to
have
pretty
xml
output
in
a
text
file
","
use
BeautifulSoup
prettify()
on
the
xml
.
How
do
I
display
on
an
html
page
an
xml
?
how
can
I
set
in
python
text
/
html
?
I'm
writing
in
the
html
as
:
I
can't
import WebOb
1.1
with
the
Python
2.7
runtime
","
as
WebOb
imports
io
","
io
imports
_io
","
which
is
blocked
by
the
SDK
.
Is
there
a
way
to
whitelist
_io
?
It
is
obviously
not
supposed
to
be
blacklisted
.
From
context
","
it
sounds
like
you're
trying
to
run
your
app
on
the
dev_appserver
.
The
dev_appserver
does
not
yet
support
the
Python
2.7
runtime
;
for
now
you'll
have
to
do
your
development
and
testing
on
appspot
.
You
can
use
print
formatting
:
where
16
is
the
number
of
digits
you
want
after
the
decimal
point
.
You
can
format
it
as
a
fixed-point
number
.
I
have
a
number
that
prints
out
in
exponential
form
:
How
can
i
make
it
print
in
normal
form
?
I'm
totally
","
completely
new
to
programming
","
so
please
forgive
what
is
probably
a
stupid
question
","
but
I've
been
beating
my
head
on
this
for
the
past
couple
of
days
.
I
have
two
models
","
Photos
and
Thumbnails
.
I'm
trying
to
come
up
with
an
easy
","
dynamic
way
to
get
the
thumbnail
links
for
each
Photo
.
I've
come
up
a
function
that
does
this
(
get_thumbs
)
but
I'd
like
it
to
run
automatically
when
the
model
is
called
(
basically
so
that
I
get
Photo.get_%s_url
%
thumb.name
as
soon
as
the
model
is
available
)
.
Below
is
my
models.py
.
Any
help
or
nudge
in
the
right
direction
(
even
if
it's
just
""""
google
blah
""""
)
would
be
greatly
appreciated
.
Thanks
.
You
want
to
override
the
__init__
method
like
you
did
with
the
save
method
","
and
call
self.get_thumbs()
before
you
call
"super(Photo, self)"
.
"init(*args, **kwargs)"
Alternately
","
you
could
look
at
other
peoples
solution
to
this
problem
sorl.thumbnail
","
django-imagekit
","
or
easy-thumbnails
(
which
is
sort
of
like
a
combination
of
the
two
)
put
the
try
/
except
inside
the
loop
around
the
json.JSONEncoder()
.
encode(item)
:
Why
do
you
have
a
list
of
both
numbers
and
some
other
kind
of
object
?
It
seems
like
you're
trying
to
compensate
for
a
design
flaw
.
As
a
matter
of
fact
","
I
want
it
work
this
way
because
I
want
to
keep
data
that
is
already
encoded
in
JsonedData()
","
then
I
want
json
module
to
give
me
some
way
to
insert
a
'
raw
'
item
data
rather
than
the
defaults
","
so
that
encoded
JsonedData
could
be
reuseable
.
here's
the
code
","
thanks
Use
the
skipkeys
option
for
JSONEncoder()
so
that
it
skips
items
that
it
can't
encode
.
Alternatively
","
create
a
default
method
for
your
JsonedData
object
.
See
the
docs
.
I
read
the
doc
string
of
the
exception
and
It
seems
like
for
mongoalchemy
the
model
defined
doesn't
register
currentPage
as
an
attribute
of
Presentation
document
","
but
in
the
code
you
copy
pasted
the
class
definition
define
the
attribute
.
If
the
class
you
copy
pasted
is
the
class
that
you
defined
in
your
project
","
try
to
delete
.
pyc
files
in
your
project
and
re-run
the
application
.
By
the
way
currentPage
variable
name
does
not
follow
PEP8
Naming
Conventions
.
Here
is
my
class
:
When
I
use
it
from
the
mongo
shell
","
everything
seems
fine
:
db.Presentation.find({id:2})
but
when
I
am
using
MongoAlchemy
","
p
=
query.filter(Presentation.id==2)
.
first()
For
anyone
else
with
a
ExtraValueException
","
you
can
also
see
this
error
if
you
put
an
extra
comma
in
the
Class
definition
.
For
example
if
you
had
had
:
trying
to
use
either
pages
or
tags
would
give
an
ExtraValueException
.
This
I
learnt
to
my
own
pain
.
After
using
cgi.parse_qs()
","
how
to
convert
the
result
(
dictionary
)
back
to
query
string
?
Looking
for
something
similar
to
urllib.urlencode()
.
In
python3
","
slightly
different
:
output
:
'
pram1=foo&param2=bar
'
for
python2
and
python3
compatibility
","
try
this
:
You're
looking
for
something
exactly
like
urllib.urlencode()
!
However
","
when
you
call
parse_qs()
(
distinct
from
parse_qsl()
)
","
the
dictionary
keys
are
the
unique
query
variable
names
and
the
values
are
lists
of
values
for
each
name
.
In
order
to
pass
this
information
into
urllib.urlencode()
","
you
must
""""
flatten
""""
these
lists
.
Here
is
how
you
can
do
it
with
a
list
comprehenshion
of
tuples
:
Maybe
you're
looking
for
something
like
this
:
It
takes
a
dictionary
and
convert
it
to
a
query
string
","
just
like
urlencode
.
It'll
append
a
final
""""
&
""""
to
the
query
string
","
but
return
query
[
:
-
1
]
fixes
that
","
if
it's
an
issue
.
From
the
docs
:
"urllib.urlencode(query[, doseq])"
Convert
a
mapping
object
or
a
sequence
of
two-element
tuples
to
a
“
percent-encoded
”
string
...
a
series
of
key=value
pairs
separated
by
'
&
'
characters
...
A
dict
is
a
mapping
.
I
don't
know
","
though
you
could
try
manually
filtering
the
points
with
I'm
trying
to
plot
some
data
using
pyplot
","
and
then
'
zoom
in
'
by
using
xlim()
the
x
axis
.
However
","
the
new
plot
doesn't
rescale
the
y
axis
when
I
do
this
-
am
I
doing
something
wrong
?
Example
-
in
this
code
","
the
plot
y-axis
range
still
takes
a
maximum
of
20
","
rather
than
10
.
:
If
you
look
at
the
""""
Partition
function
formulas
""""
section
of
the
Partiton
(
number
theory
)
page
on
Wikipedia
","
you'll
see
that
there
isn't
a
simple
way
to
find
the
partition
number
.
Instead
","
your
best
bet
is
probably
:
or
","
slightly
simpler
but
memory
hogging
for
large
numbers
using
your
existing
function
.
Also
note
if
you
want
to
be
able
to
save
the
values
returned
by
P
","
you
should
be
yielding
p
[
:
]
not
p
-
-
you
want
to
make
a
copy
","
not
yield
the
same
list
(
which
you
change
)
over
and
over
.
You
can
see
why
if
you
do
list(P(6)
)
-
-
it
returns
a
list
of
the
same
empty
list
repeated
over
and
over
.
Fore
more
info
about
partitioning
","
see
Partitioning
with
Python
.
I
have
defined
a
recursive
function
that
takes
a
number
","
n
","
and
returns
a
list
of
lists
of
the
numbers
that
sum
to
that
number
(
partitions
)
:
I
was
wondering
how
to
make
the
function
return
the
number
of
partitions
for
number
n
.
For
example
","
P(6)
would
return
10
.
Here's
an
example
in
Java
for
computing
your
desired
result
.
You
need
an
""""
s
""""
after
each
of
those
positional
arguments
.
Trying
to
run
this
line
on
my
computer
results
in
a
string
formatting
error
:
ValueError
:
unsupported
format
character
'
","
'
(
0x2c
)
at
index
36
It
seems
to
be
concerning
the
","
but
I
have
checked
and
all
the
parenthesis
are
properly
nested
(
none
enclosing
an
errant
","
)
What
@imm
said
.
Also
","
you
may
want
to
use
the
built
in
query
formatting
that
is
part
of
MySQLdb
.
I
came
across
this
code
today
from
the
rhelp
listserve
.
Maybe
it'll
help
things
along
:
Now
I
try
it
:
AND
THE
ERROR
:
A
response
that
works
and
works
well
is
found
here
:
http://r.789695.n4.nabble.com/Email-out-of-R-code-td3530671.html
Thank
you
to
nutterb
for
an
answer
from
the
rhelp
list
.
Thank
you
to
all
that
tried
to
assist
me
and
were
patient
with
my
ignorance
of
python
.
Take
a
look
at
the
package
sendmailR
which
can
send
attachments
.
To
make
sendmail
work
with
gmail
on
a
Mac
would
require
some
additional
fiddling
around
","
but
you
can
find
the
instructions
to
do
it
using
a
google
search
.
Pasting
here
for
convenience
the
code
from
one
link
above
I
am
desiring
to
send
an
email
in
R
with
an
attachment
using
gmail
.
I
have
found
that
sendmailR
does
not
work
with
gmail
because
it
requires
authentication
(
I
couldn't
get
it
to
work
with
gmail
so
I
assume
this
to
be
true
unless
someone
tells
me
I'm
wrong
","
in
which
case
I'll
post
the
R
output
and
error
message
for
that
)
.
I
found
a
code
snippet
found
here
(
LINK
)
.
As
the
site
suggests
the
code
is
not
formatted
to
send
attachments
but
I
have
got
it
to
send
an
email
.
I'd
like
to
extend
this
code
to
send
attachments
(
in
an
email
correspondence
the
author
of
this
code
was
unable
to
extend
the
code
to
send
attachments
)
.
I
want
to
send
emails
with
R
using
gmail
.
I
am
a
windows
7
user
with
the
2.14
beta
version
of
R
.
The
code
that
sends
emails
but
not
attachments
:
Note
this
message
is
cross
posted
at
talkstats.com
.
I
did
not
receive
a
reply
there
(
just
members
telling
me
they
wish
they
could
help
)
.
If
I
receive
a
workable
solution
i
will
also
post
it
there
as
well
.
For
working
with
Gmail
using
R
best
working
example
is
available
here
.
It's
basic
is
as
below
:
A
project
in
Google
Developers
Console
to
manage
your
use
of
the
Gmail
API
the
gmailr
R
package
by
Jim
Hester
","
which
wraps
the
Gmail
API
(
development
on
GitHub
)
the
plyr
and
dplyr
packages
for
data
wrangling
(
do
this
with
base
R
if
you
prefer
)
addresses.csv
a
file
containing
email
addresses
","
identified
by
a
key
.
In
our
case
","
student
names
.
marks.csv
a
file
containing
the
variable
bits
of
the
email
you
plan
to
send
","
including
the
same
identifying
key
as
above
.
In
our
case
","
the
homework
marks
.
the
script
send-email-with-r.r
You
could
give
the
new
mailR
package
a
shot
that
allows
SMTP
authorization
:
http://rpremraj.github.io/mailR/
The
following
call
should
then
work
:
27
/
05
/
14
:
Editing
example
below
to
demonstrate
how
attachments
can
be
sent
via
R
:
Option
1
:
For
sendmailR
","
it
seems
that
you're
having
issues
with
the
appending
of
port
25
.
You
should
be
able
to
specify
the
destination
port
via
sendmail_options(smtpPort = 587)
","
prior
to
using
the
sendmail()
command
.
I
am
not
sure
that
this
will
resolve
your
other
issues
","
but
it
should
get
you
the
right
port
.
Option
2
:
If
you'd
like
to
invoke
a
Python
script
","
this
one
looks
most
relevant
.
You
may
find
it
easiest
to
do
token
substitution
","
i.e.
take
a
base
script
","
put
in
strings
that
you'll
simply
find
(
i.e.
the
tokens
)
&
replace
(
i.e.
substitution
with
your
desired
strings
)
","
and
then
execute
the
revised
script
.
For
instance
","
using
the
script
at
the
link
above
(
saved
in
a
local
directory
as
""""
sendmail_base.py
""""
)
:
and
so
on
","
replacing
the
header
","
recipient
","
etc.
","
with
the
text
strings
you'd
like
to
use
","
and
the
same
for
specification
of
the
attachment
file
name
.
Finally
","
you
can
save
the
output
to
a
new
Python
file
and
execute
it
:
Although
this
is
a
very
naive
approach
","
it
is
simple
and
debugging
of
the
script
involves
only
examination
of
whether
your
output
Python
program
is
working
and
whether
or
not
R
is
generating
the
right
output
Python
program
.
This
is
in
contrast
to
debugging
what's
going
on
with
R
passing
objects
to
and
from
various
packages
and
languages
.
You
are
running
Jython
code
inside
of
your
R
environment
","
so
you're
looking
for
a
way
to
send
an
attachment
using
the
Jython
language
","
not
R
.
Since
Jython
is
basically
Python
","
here
is
a
way
to
send
an
email
with
an
attachment
with
Python
:
How
to
send
email
attachments
with
Python
.
You
will
just
have
to
hack
together
that
code
into
yours
.
I
use
the
Tk
library
for
plotting
","
you
can
set
this
up
by
default
in
the
~
/
.
matplotlib
/
matplotlibrc
file
by
writing
:
This
allows
me
to
set
the
window
position
and
dimensions
using
:
As
someone
might
be
wanting
to
position
their
matplotlib
window
on
a
Mac
I
wanted
to
make
a
quick
contribution
.
I
frequently
work
with
and
without
an
external
screen
(
at
work
and
at
home
)
and
wanted
some
way
of
automatically
using
the
external
screen
if
it
is
available
.
Luckily
must
of
the
Mac
operating
system
can
be
interfaced
through
AppKit
.
The
following
snippet
will
return
a
list
of
ScreenInfo
objects
with
position
","
width
and
height
:
I
have
two
questions
regarding
to
the
positioning
of
a
mpl
window
(
using
WXAgg
backend
)
1
-
)
How
to
create
a
maximized
window
","
instead
of
me
clicking
on
window
to
maximize
it
each
time
?
2
-
)
I
have
two
screens
.
Interestingly
","
my
mpl
windows
tend
to
open
on
my
small
screen
.
How
can
I
force
mpl
/
ipython
/
WX
/
X-windows
to
open
mpl
windows
on
my
2nd
and
bigger
monitor
?
Thanks
.
Relative
to
your
first
question
","
you
can
use
Maximize
on
your
figure
manager
(
as
your
figure
manager
is
a
FigureManagerWx
instance
)
or
equivalent
methods
in
case
of
other
backends
:
For
the
second
question
","
I
am
not
sure
(
i
can
not
test
it
)
but
if
the
problem
can
be
solved
by
setting
the
position
of
your
figure
in
a
screen
extended
in
two
monitors
","
then
you
can
use
SetPosition
(
again
for
a
wxAgg
backend
)
:
We
have
a
dummy
Python
module
(
fields.py
)
with
custom
Django
fields
which
loads
real
implementations
based
on
the
database
configured
:
The
reason
for
this
is
that
we
are
using
PostgreSQL
ip4r
extension
which
allows
nice
(
and
fast
)
work
with
fields
containing
IP
values
.
But
we
also
have
dummy
implementations
(
in
Python
code
)
for
other
databases
","
so
that
development
can
be
done
also
in
SQLite
.
So
if
you
are
using
PostgreSQL
those
fields
are
backed
up
with
fields
using
ip4r
indexes
and
if
you
are
using
some
other
database
system
those
fields
are
regular
fields
.
The
problem
is
how
to
use
South
migrations
on
such
fields
.
The
problem
is
that
add_introspection_rules
discovers
fields_postgresql
and
fields_dummy
so
this
leaks
to
migrations
.
This
is
the
problem
later
on
if
you
want
to
apply
a
migration
made
with
SQLite
on
an
installation
with
PostgreSQL
.
How
it
would
be
possible
to
convince
South
that
this
is
all
just
fields
module
and
it
does
not
matter
which
concrete
implementation
migrations
are
run
with
.
Drop
support
for
SQLite
.
You
are
right
","
if
you
aren't
running
your
development
environment
on
a
mobile
phone
","
maintaining
your
very
own
South-based
migration
script
just
in
a
sake
of
not
installing
Postgre
probably
isn't
worth
the
hassle
.
Postgre
installation
versus
maintaining
custom
migration
code
is
a
one-time
action
versus
continuous
and
potentially
time-consuming
procedure
(
since
you
will
potentially
have
to
modify
your
custom
migration
code
in
order
to
be
in
sync
with
newer
South
releases
)
.
I
have
a
lxml
etree
HTMLParser
object
that
I'm
trying
to
build
xpaths
with
to
assert
xpaths
","
attributes
of
the
xpath
and
text
of
that
tag
.
I
ran
into
a
problem
when
the
text
of
the
tag
has
either
single-quotes
(
'
)
or
double-quotes
(
""""
)
and
I've
exhausted
all
my
options
.
Here's
a
sample
object
I
created
Here
is
the
snippet
of
code
and
then
different
variations
of
the
variable
being
read
in
self.text
is
basically
the
expected
text
of
the
tag
","
in
this
case
:
Here
is
my
'
test
'
""""
string
""""
this
fails
when
i
try
to
use
the
xpath
method
of
the
HTMLParser
object
Reason
is
because
the
xpath
that
it
gets
is
this
'
/
html
/
body
/
p
[
starts-with
(
.
","
'
Here
is
my
'
test
'
""""
string
""""
'
)
and
1=1
]
'
How
can
I
properly
escape
the
single
and
double
quotes
from
the
self.text
variable
?
I've
tried
triple
quoting
","
wrapping
self.text
in
repr()
","
or
doing
a
re.sub
or
string.replace
escaping
'
and
""""
with
\
'
and
\
""""
According
to
what
we
can
see
in
Wikipedia
and
w3
school
","
you
should
not
have
'
and
""""
in
nodes
content
","
even
if
only
<
and
&
are
said
to
be
stricly
illegal
.
They
should
be
replaced
by
corresponding
""""
predefined
entity
references
""""
","
that
are
&apos
;
and
&quot
;
.
By
the
way
","
the
Python
parsers
I
use
will
take
care
of
this
transparently
:
when
writing
","
they
are
replaced
;
when
reading
","
they
are
converted
.
After
a
second
reading
of
your
answer
","
I
tested
some
stuff
with
the
'
and
so
on
in
Python
interpreter
.
And
it
will
escape
everything
for
you
!
So
we
can
see
that
Python
escapes
things
correctly
.
Could
you
then
copy-paste
the
error
message
you
get
(
if
any
)
?
there
are
more
options
to
choose
from
","
especially
the
""""
""""
""""
and
'
'
'
might
be
what
you
want
.
For
the
2D
images
you
might
want
to
consider
the
use
of
NCL
","
produced
by
NCAR
(
The
National
Center
for
Atmospheric
Research
)
","
which
is
specifically
designed
for
manipulating
and
plotting
atmospheric
(
meteorological
)
data
.
An
impressive
gallery
of
results
is
available
on
that
site
.
IMO
NCL's
syntax
can
be
very
ugly
at
times
","
and
it
looks
as
if
a
python
wrapper
","
PyNGL
now
exists
(
gallery
)
For
visualization
of
3D
atmospheric
data
","
some
of
my
colleagues
have
used
a
number
of
very
impressive
tools
including
vis5d
and
vapour
.
For
the
ultimate
in
flexibility
","
mayavi2
","
which
is
basically
a
MATLAB
style
ipython
interface
to
Kitware's
VTK
library
is
used
in
a
huge
range
of
scientific
visualizations
.
I
don't
recommend
this
approach
unless
you're
happy
to
'
roll
your
own
'
","
but
it
is
a
very
nice
if
you
need
to
get
a
three
dimensional
results
looking
just
right
.
One-self
answer
:
ccplot
ccplot
is
an
open
source
command-line
application
that
is
capable
of
producing
two-dimensinal
plots
of
profile
","
layer
and
earth
view
data
sets
from
CloudSat
CPR
","
CALIPSO
CALIOP
","
and
Aqua
MODIS
HDF4
and
HDF-EOS2
files
.
Your
question
is
a
rather
vague
.
Are
the
2D
/
3D
visualizations
images
that
you
have
or
are
you
rendering
them
yourself
?
Are
they
contour
plots
or
arrays
of
data
?
There
is
a
wide
variety
of
python
packages
","
but
without
being
more
specific
I
would
suggest
looking
into
this
website
.
I've
used
for
some
of
my
data
and
I've
heard
good
things
about
Basemap
.
It
all
depends
on
what
you
what
to
do
with
what
you've
got
.
Are
there
any
Python
solutions
out
in
the
web
for
satellite
/
radar
/
lidar
/
aircraft
based
2D
/
3D
visualization
of
atmospheric
measurements
.
My
interest
range
vary
from
simply
overlaying
flight
tracks
over
satellite
images
/
data
to
co-located
visualizations
of
lidar
/
radar
/
aircraft
measurements
(
within
themselves
)
and
with
matching
satellite
pixels
.
I
would
be
happy
to
know
if
such
visualization
attempts
(
tools
or
custom
Python
scripts
)
exist
before
I
start
working
on
my
own
solutions
.
Thank
you
.
For
anyone
coming
to
this
question
now
with
similar
needs
there
is
a
tool
called
CIS
which
provides
plotting
and
collocation
of
a
range
of
datasets
including
aircraft
data
","
CALIOP
L2
","
MODIS
L2
and
others
.
It
also
provides
subsetting
and
aggregation
of
the
data
","
in
case
that's
of
use
.
It's
a
command
line
tool
primarily
but
there's
a
Python
API
too
-
see
http://cis.readthedocs.org
for
more
info
.
Full
disclosure
:
I'm
the
lead
dev
on
this
tool
","
and
I'm
happy
to
give
more
specific
answers
to
more
specific
questions
.
It
seems
that
your
goal
is
not
to
decouple
the
database
from
the
MQ
","
but
rather
from
the
workers
.
As
such
","
you
can
create
another
queue
that
receives
completion
notifications
","
and
have
another
single
worker
that
picks
up
the
notifications
and
updates
the
database
appropriately
.
I
am
building
a
web
application
that
allows
a
user
to
upload
an
image
.
When
the
image
is
uploaded
","
it
needs
to
be
resized
to
one
or
more
sizes
","
each
of
which
needs
to
be
sent
to
Amazon
s3
for
storage
.
Metadata
and
urls
for
each
size
of
the
image
are
stored
in
a
single
database
record
on
the
web
server
.
I'm
using
a
message
queue
to
perform
the
resizing
and
uploading
asynchronously
(
as
there
is
potential
for
large
images
and
multiple
resizes
per
request
)
.
When
the
resize
/
upload
task
completes
","
the
database
record
needs
to
be
updated
with
the
url
.
My
problem
is
that
the
worker
executing
the
task
will
not
have
access
to
the
database
.
I
was
thinking
of
firing
off
a
http
callback
from
the
worker
back
to
the
web
application
after
the
task
is
complete
with
the
appropriate
information
for
updating
the
database
record
.
Are
there
any
other
alternatives
or
reasons
I
should
do
this
another
way
?
I'm
using
python
/
pylons
for
the
web
backend
","
mysql
for
the
database
and
celery
/
amqp
for
messaging
.
Thanks
!
Here
","
Paramiko
code
to
execute
in
remote
AWS
EC2
Python
:
I
am
able
to
fire
up
AWS
Ubuntu
EC2
instance
with
boto
.
Have
anyone
tried
to
upload
the
script
to
the
remote
Ubuntu
EC2
(
More
than
1
)
and
execute
the
script
via
SSH
locally
?
The
main
objective
is
to
automate
the
whole
process
using
a
Python
script
written
on
localhost
.
Is
there
an
alternative
way
or
Amazon
api
tools
to
made
this
possible
?
Use
paramiko
API
I'd
recommend
Fabric
","
it's
made
for
this
kind
of
thing
.
I
am
having
the
same
problem
on
Ubuntu
:
installing
the
PyQt
packet
from
source
solved
it
.
I
assume
that
Ninja-IDE
needs
these
packets
to
be
installed
as
a
prerequisite
.
I've
installed
Ninja
IDE
on
two
Windows
machines
","
a
Windows
7
machine
with
only
standard
Python
3
installed
where
it
gives
me
that
error
you
mentioned
.
And
a
Windows
XP
machine
on
which
I
had
pythonxy
which
comes
with
PyQt
in
a
convenient
easy
to
use
installer
thus
it
worked
without
any
further
troubles
.
I
tried
to
configure
the
PyQt
>
=
4.7
on
windows
.
but
i
was
unable
to
configure
it
with
ninja
IDE
source
code
","
when
i
trying
to
run
the
IDE
it
gives
an
error
as
below
:
how
to
run
the
ninja
IDE
source
and
configure
the
PyQt
on
windows
?
You
actually
want
:
That'll
get
you
the
exact
text
produced
by
help(str)
.
I
need
a
str
object
created
out
of
the
text
displayed
in
help(some_object)
.
simply
typecasting
to
str
doesn't
work
.
Whats
the
correct
way
to
do
this
?
How
to
get
wxPython
installed
and
working
in
Win7
?
I
entered
lots
of
details
","
but
there
was
not
enough
indenting
","
then
-
-
-
Oops
!
Your
question
couldn't
be
submitted
because
:
Please
add
some
context
to
explain
the
code
sections
(
or
check
that
you
have
not
incorrectly
formatted
all
of
your
question
as
code
)
.
So
","
extremely
generically
","
how
do
I
get
wxPython
working
without
giving
you
the
file
I
installed
or
the
code
that
I
tried
to
run
?
Go
to
http://wxpython.org/
and
click
the
Download
link
on
the
left
.
Pick
the
appropriate
binary
that
matches
your
version
of
Python
(
make
sure
you
pay
attention
to
the
32
/
64-bit
part
too
)
.
Once
you
have
the
matching
version
","
install
it
.
That's
it
.
That
has
always
worked
for
me
on
Windows
XP
and
7
.
If
I
have
the
following
data
type
:
How
to
I
remove
item
b
so
the
output
will
look
like
:
How
do
I
also
check
if
b
exists
before
deleting
it
?
I
tried
if
'
b
'
in
xxxx
but
this
can't
find
it
I
am
new
to
Python
For
removal
you
can
do
:
or
:
If
you
wanna
create
a
new
list
without
that
value
you
can
use
list
comprehension
:
A
list
comprehension
does
this
nicely
:
You
could
use
a
list
comprehension
like
this
:
However
","
it
looks
like
you're
trying
to
reinvent
a
dictionary
for
some
reason
.
If
it
doesn't
have
to
be
ordered
","
just
use
a
dictionary
:
If
it
does
need
to
be
ordered
","
you
can
use
collections.OrderedDict
rather
than
dict
:
With
a
dictionary
","
if
you
need
it
back
in
the
format
you
have
in
your
question
","
just
call
items()
on
the
dictionary
.
Additionally
","
to
convert
a
list
like
in
your
question
into
some
kind
of
dictionary
","
just
pass
it
into
it
:
Try
to
use
""""
filter
""""
:
In
python
2
it
will
return
filtered
list
","
in
python
3.x
-
iterator
.
Yeah
","
the
basic
idea
here
is
that
the
autoloaded
tables
cannot
be
declared
without
a
valid
engine
","
so
you
need
to
separate
your
initialization
code
from
your
model
code
and
ensure
that
the
models
aren't
imported
until
you
have
setup
the
engine
and
connected
it
to
the
metadata
.
The
link
below
describes
it
better
","
but
it
looks
like
you've
already
taken
the
correct
approach
.
SQLAlchemy
declarative
syntax
with
autoload
(
reflection
)
in
Pylons
I'm
pretty
new
to
Pyramid
","
and
I
can't
figure
out
how
to
use
the
autoload=true
option
in
Pyramid
.
I
used
the
pyramid_routesalchemy
to
create
my
project
using
paster
.
The
problem
is
that
there
is
an
init.py
file
which
uses
the
initialize_sql
(
and
this
function
defines
Base.metadata.bind
=
engine
)
.
In
one
of
my
model
classes
I
would
like
to
use
the
autoload=true
option
(
using
the
declarative
base
)
","
but
I
always
get
the
following
error
:
Actually
Base.metadata.bind
=
engine
is
defined
inside
the
initialize_sql
function
and
I
do
not
realy
know
in
which
order
the
file
are
loaded
","
but
I'm
almost
sure
that
that
init.py
is
loaded
before
the
model
","
and
thus
metadata
was
already
binded
to
the
engine
...
Thus
","
my
question
:
how
can
use
autoload
within
my
classes
without
changing
the
whole
init
and
model
structure
?
If
anyone
has
a
hint
...
Thanks
in
advance
From
SQLAlchemy
0.8
you
can
use
the
DeferredReflection
class
when
creating
your
declarative
base
to
delay
the
autoload
until
an
engine
is
attached
to
your
metadata
:
See
here
:
http://docs.sqlalchemy.org/en/rel_0_8/orm/extensions/declarative.html#using-reflection-with-declarative
i
am
calling
mencoder
from
a
python
script
to
generate
a
movie
from
various
.
png
files
:
mencoder
gives
me
a
4KB
out.avi
and
this
output
:
C:\Windows\system32\cmd.exe
/
c
python
make_video.py
executing
mplayer\mencoder.exe
mf
:
/
/
frames
/
out_*.png
-
mf
type=png:w=800:h=600:fps=10
-
ovc
lavc
-
lavcopts
vcodec=mpeg4:mbd=2:trell
-
oac
copy
-
o
out.avi
MEncoder
Sherpya-SVN-r34118-4.2.5
(
C
)
2000-2011
MPlayer
Team
success
:
format
:
16
data
:
0x0
-
0x0
MF
file
format
detected
.
[mf]
search
expr
:
frames
/
out_*.png
[mf]
number
of
files
:
129
(
516
)
VIDEO
:
[MPNG]
800x600
24bpp
10.000
fps
0.0
kbps
(
0.0
kbyte
/
s
)
[V]
filefmt:16
fourcc:0x474E504D
size:800x600
fps:10.000
ftime:=0.1000
Opening
video
filter
:
[
expand
osd=1
]
Expand
:
-
1
x
-
1
","
-
1
;
-
1
","
osd
:
1
","
aspect
:
0.000000
","
round
:
1
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
Opening
video
decoder
:
[ffmpeg]
FFmpeg's
libavcodec
codec
family
Selected
video
codec
:
[ffpng]
vfm
:
ffmpeg
(
FFmpeg
PNG
)
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
Flushing
video
frames
.
Filters
have
not
been
configured
!
Empty
file
?
Writing
index
...
Writing
header
...
ODML
:
Aspect
information
not
(
yet
?
)
available
or
unspecified
","
not
writing
vprp
header
.
Video
stream
:
nan
kbit
/
s
(
-
2147483648
B
/
s
)
size
:
0
bytes
0.000
secs
0
frames
Please
help
.
I
guess
this
is
an
old
thread
","
but
i
was
having
the
same
issue
so
thought
I'd
share
my
solution
in
case
someone
else
runs
into
this
same
page
.
basically
the
error
comes
in
when
trying
to
run
mencoder
from
outside
your
source
folder
location
.
so
to
get
this
to
work
","
just
CD
into
the
folder
with
your
source
images
and
run
the
command
from
there
.
OR
you
can
modify
your
python
script
and
add
a
line
changing
the
current
path
to
your
source
image
path
just
before
running
mencoder
.
Something
like
:
os.chdir(sourcefolder)
#
where
sourcefolder
is
the
location
of
your
source
images
Cheers
!
-
Kevin
Just
write
an
appropriate
__lt__
method
for
the
objects
in
the
list
so
they
sort
correctly
:
Only
__lt__
is
needed
by
Python
for
sorting
","
though
it's
a
good
idea
to
define
all
of
the
comparisons
or
use
functools.total_ordering
.
You
can
see
that
it
is
working
by
using
two
items
with
the
same
first
value
and
different
second
values
.
The
two
objects
will
swap
places
when
you
heapify
no
matter
what
the
second
values
are
because
lst
[0]
<
lst
[1]
will
always
be
False
.
If
you
need
the
heapify
to
be
stable
","
you
need
a
more
complex
comparison
.
I'm
using
python2.6
.
Is
it
available
in
higher
version
of
python
?
Else
is
there
any
other
way
I
can
maintain
priority
queues
for
list
of
objects
of
non-trivial
classes
?
What
I
need
is
something
like
this
Any
suggestions
?
I
don't
know
if
this
is
better
but
it
is
like
Raymond
Hettinger's
solution
but
the
priority
is
determined
from
the
object
.
Let
this
be
your
object
and
you
want
to
sort
by
the
the
x
attribute
.
Then
have
a
function
which
applies
the
pairing
Then
apply
the
function
to
the
lists
as
input
into
heapq.merge
Which
gave
me
the
following
output
Well
","
this
is
terrible
and
awful
and
you
definitely
shouldn't
do
itâ
€
¦
But
it
looks
like
the
heapq
module
defines
a
cmp_lt
function
","
which
you
could
monkey
patch
if
you
really
wanted
a
custom
compare
function
.
The
traditional
solution
is
to
store
(
priority
","
task
)
tuples
on
the
heap
:
This
works
fine
as
long
as
no
two
tasks
have
the
same
priority
;
otherwise
","
the
tasks
themselves
are
compared
(
which
might
not
work
at
all
in
Python
3
)
.
The
regular
docs
give
guidance
on
how
to
implement
priority
queues
using
heapq
:
http://docs.python.org/library/heapq.html#priority-queue-implementation-notes
I
have
been
trying
to
find
the
location
of
a
module
but
can't
I
have
searched
the
python
folder
but
still
can't
find
it
but
I
know
it's
there
If
it
has
no
__file__
attribute
then
that
means
that
it's
probably
built
into
the
Python
library
itself
(
or
some
other
executable
)
","
and
does
not
exist
independently
on
disk
.
That's
obsolete
.
You
should
replace
invoking
make-messages.py
with
django-admin.py
makemessages
.
Run
this
command
in
project
root
I
run
this
command
:
but
it
has
this
error
:
'
make-messages.py
'
is
not
recognized
as
an
internal
or
external
command
.
I
run
it
in
my
project
root
","
app
roo
and
django
root
","
but
all
of
them
cause
this
error
what
should
I
do
?
You
can
quite
easily
listen
to
your
UDP
port
with
the
standard
socket
module
.
Examples
are
available
.
As
a
first
step
","
your
data
could
go
in
a
simple
Python
list
","
as
lists
are
optimized
for
appending
data
.
Removing
the
first
elements
takes
much
more
time
","
so
you
might
want
to
only
do
this
from
time
to
time
","
and
only
plot
","
in
the
mean
time
","
the
last
1024
(
or
whatever
)
elements
of
the
list
.
Plotting
can
then
conveniently
be
done
with
the
famous
Matplotlib
plotting
library
:
matplotlib.pyplot.plot(data_list)
.
Since
you
want
real
time
","
you
might
find
the
animation
examples
useful
.
If
you
need
to
optimize
the
data
acquisition
speed
","
you
can
have
the
(
also
famous
)
NumPy
array-manipulation
library
directly
interpret
the
data
from
the
stream
as
an
array
of
numbers
(
Matplotlib
can
plot
such
arrays
)
","
with
the
numpy.frombuffer()
function
.
It
is
possible
","
but
not
too
simple
.
You
should
inform
yourself
about
the
API
and
maybe
have
a
look
at
some
implementations
.
If
you
have
done
so
","
you
can
maybe
provide
a
function
which
not
only
gives
you
a
peek
at
the
raw
array
","
but
maybe
even
reassembles
it
into
the
right
order
and
length
(
if
it
is
a
circular
buffer
)
.
This
might
be
very
convenient
as
you
nevertheless
have
to
copy
the
data
.
I
have
a
little
C
program
that's
continuously
acquiring
a
stream
of
data
and
sending
it
via
UDP
","
and
in
real
time
","
to
a
different
computer
.
The
basic
framework
for
what
I
originally
set
out
to
do
has
been
laid
.
In
addition
","
however
","
I'd
like
to
visualize
in
real
time
the
data
that's
being
acquired
.
To
that
end
","
I
was
thinking
of
using
Python
and
its
various
plotting
libraries
.
My
question
is
how
difficult
it
would
be
to
let
Python
have
access
to
what
is
essentially
a
first
in
","
first
out
circular
buffer
of
my
C
program
.
For
concreteness
","
let's
assume
there
are
1024
samples
in
this
buffer
.
Does
the
idea
of
""""
letting
Python
have
a
continuous
peek
at
dynamic
C
array
""""
even
sound
reasonable
/
possible
?
If
not
","
what
sort
of
plotting
options
are
best
suited
to
this
problem
?
Thanks
.
If
the
module
is
specific
to
your
program
","
you
can
put
into
the
same
directory
as
the
main
script
.
From
the
documentation
:
When
a
module
named
spam
is
imported
","
the
interpreter
searches
for
a
file
named
spam.py
in
the
directory
containing
the
input
script
and
then
in
the
list
of
directories
specified
by
the
environment
variable
PYTHONPATH
.
I
would
recommend
that
you
read
the
entire
tutorial
on
modules
.
It's
pretty
short
and
has
a
lot
of
useful
info
.
I
was
wondering
how
do
Python
modules
work
?
Say
I
want
to
make
my
Python
program
some
custom
modules
to
package
with
it
","
where
would
I
put
them
in
relation
to
the
main
.
py
file?(and
how
would
I
load
them
if
not
using
the
usual
way
)
This
python
script
connects
to
MySQL
database
and
MssQL
2008
R2
database
.
MySQL
database
runs
on
Linux
Ubuntu
11.04
.
MssQL
2008
runs
on
Windows
.
The
script
runs
from
Linux
(
Ubuntu
11.04
)
.
pyodbc
works
for
me
.
http://sourceforge.net/projects/pyodb/
The
problem
got
resolved
once
I
called
commit()
as
follows
connMSSQL.commit()
immediately
after
curMSSQL.execute(qryINS)
line
According
to
the
PyCharm
docs
","
PyCharm
has
a
facility
for
this
.
This
is
not
exactly
accessible
by
a
program
as
an
API
.
You
are
probably
better
off
using
XML
Schema
Learner
as
a
separate
program
since
it
is
a
command
line
program
(
subprocess
friendly
!
)
.
Currently
","
there
is
no
module
that
will
run
within
your
python
program
and
do
this
conversion
.
But
I
see
the
problem
of
creating
a
XSD
schema
from
XML
as
a
tooling
problem
.
It's
the
kind
of
functionality
that
I'll
use
once
","
to
get
a
schema
started
but
after
that
I'll
be
maintaining
the
schema
myself
.
From
reading
a
single
XML
file
the
XSD
generator
will
create
a
starting
point
for
a
real
schema
","
it
cannot
infer
all
the
functionality
and
options
offered
by
XSD
.
Basically
","
I
don't
see
the
need
to
have
this
conversion
run
as
a
module
inside
of
my
code
","
generating
new
XSDs
every
time
the
XML
changes
.
After
all
","
it's
the
schema
that
defines
the
XML
not
the
other
way
around
.
As
end-user
pointed
out
you
could
use
xsd.exe
but
you
might
also
want
to
look
at
other
tools
such
as
trang
(
a
bit
old
)
for
Java
and
stylusstudio
(
XML
tool
)
.
I'm
looking
for
a
tool
that
will
play
nicely
with
Python
.
Except
for
my
Python
requirement
","
my
question
is
the
same
as
this
one
:
""""
I
am
looking
for
a
tool
which
will
take
an
XML
instance
document
and
output
a
corresponding
XSD
schema
.
""""
Are
you
looking
for
something
like
pyxsd
?
(
primarily
used
for
validation
against
a
schema
)
Or
maybe
PyXB
?
(
can
generate
classes
based
on
xml
)
Otherwise
","
I
don't
think
there's
a
tool
[yet]
that
will
generate
the
schema
from
within
Python
.
Can
you
do
it
on
demand
using
something
like
xsd.exe
?
Does
it
have
to
be
programmatic
/
repeatable
?
In
addition
","
the
contents
of
__init__.py
becomes
the
contents
of
the
package
when
treated
as
a
module
","
i.e.
the
contents
of
somepackage
/
__init__.py
will
be
found
in
dir(somepackage)
when
you
import somepackage
.
Modules
themselves
can
be
Python
code
","
specially-crafted
C
code
","
or
they
could
be
an
artificial
construct
injected
by
the
executable
that
loads
the
Python
VM
.
Here's
an
explanation
for
why
__init__.py
is
needed
:
The
__init__.py
files
are
required
to
make
Python
treat
the
directories
as
containing
packages
;
this
is
done
to
prevent
directories
with
a
common
name
","
such
as
string
","
from
unintentionally
hiding
valid
modules
that
occur
later
on
the
module
search
path
.
In
the
simplest
case
","
__init__.py
can
just
be
an
empty
file
","
but
it
can
also
execute
initialization
code
for
the
package
or
set
the
__all__
variable
","
described
later
.
As
I've
just
recommended
to
another
poster
","
the
tutorial
on
modules
is
pretty
informative
.
In
python
","
a
directory
containing
one
or
more
modules
sometimes
has
__init__.py
","
so
that
the
directory
can
be
treated
as
a
python
package
","
is
this
correct
?
What
differences
the
__init__
makes
?
(
also
another
Q
","
is
a
python
module
just
a
python
code-file
with
related
and
possibly
independent
(
to
other
files
)
set
of
classes
","
functions
and
variables
?
)
You
can
use
a
StringIO
file
object
instead
of
a
regular
file
as
well
with
both
PIL
Image.open
and
Image.save
There's
also
a
frombuffer
function
I
would
like
to
do
image
conversion
/
rewriting
with
PIL
just
using
RAM
memory
.
I
have
the
image
in
bytes
in
RAM
and
I
would
like
to
convert
it
to
some
other
format
or
possibly
the
same
.
I
know
I
can
do
it
like
saving
it
in
on
the
file
system
with
some
name
","
but
I
would
like
to
do
it
just
using
RAM
without
touching
the
file
system
.
I
haven't
found
any
examples
.
Any
help
would
be
appreciated
!
Thanks
!
For
now
you
can
install
Time
Tracker
plug-in
with
PyCharm
support
from
here
:
https://www.dropbox.com/s/rofzpjly7sv1r9u/TimeTracker_0.4.zip
Sources
available
here
:
https://github.com/a-iv/IntelliJ-Time-Tracker/
I
hope
author
will
publish
update
soon
and
it
will
be
available
on
plugins.intellij.net
I
just
checked
out
http://code.google.com/p/rabbit-eclipse/
for
eclipse
to
do
time
tracking
and
stuff
automatically
.
Does
pycharm
have
something
similar
?
There
are
some
IntelliJ
IDEA
plugins
for
time
tracking
","
but
as
far
as
I
can
see
none
of
them
have
been
updated
for
PyCharm
compatibility
.
You
can
try
to
contact
the
author
of
Time
Tracker
plugin
and
ask
him
to
update
it
for
PyCharm
compatibility
:
http://plugins.intellij.net/plugin/?idea&id=3806
This
is
an
automatic
time
tracker
for
PyCharm
:
https://wakatime.com/pycharm
(
official
site
)
http://plugins.jetbrains.com/plugin/7425?pr=pycharm
(
plugin
page
)
Use
wx.aui.AuiManager.SavePerspective
and
wx.aui.AuiManager.LoadPerspective
to
load
and
save
layout
data
(
assuming
you
have
a
consistent
set
of
panes
when
you
load
as
you
had
when
you
saved
)
For
wxPython
SavePerspective
will
just
return
a
string
which
you
can
store
somewhere
and
then
pass
into
LoadPerspective
.
This
may
not
work
100
%
reliably
-
I've
had
a
number
of
problems
with
it
before
.
Documentation
for
wxWidgets
is
here
:
http://docs.wxwidgets.org/2.8/wx_wxauimanager.html#wxauimanagersaveperspective
http://docs.wxwidgets.org/2.8/wx_wxauimanager.html#wxauimanagerloadperspective
For
a
single
pane
in
a
manager
","
you
can
use
SavePaneInfo
and
LoadPaneInfo
on
the
wx.aui.AuiManager
How
can
i
save
the
position
of
my
wx.aui
panels
in
*
.
ini
file
(
for
examples
)
?
Methods
GetPosition()
and
GetSize()
give
me
only
the
defaultsizes
and
positions
.
I
need
to
wrap
a
simply
fortran90
code
with
f2py
.
The
fortran
module
""""
test.f90
""""
is
and
then
I
wrap
it
with
but
when
I
import it
in
python
it
prompted
me
with
error
saying
Any
ideas
on
how
to
fix
it
?
Thanks
.
In
function
gasdev
you
declare
ran2
as
an
external
function
.
As
you
then
don't
link
in
any
such
function
importing
the
module
will
fail
.
Instead
","
remove
the
declaration
of
ran2
in
gasdev
","
in
which
case
the
ran2
call
uses
the
explicit
interface
to
the
ran2
function
in
the
module
","
and
everything
works
.
I
think
the
problem
is
","
that
you
didn't
tell
Scrapy
to
follow
each
crawled
URL
.
For
my
own
blog
I've
implemented
a
CrawlSpider
that
uses
LinkExtractor-based
Rules
to
extract
all
relevant
links
from
my
blog
pages
:
On
https://www.ask-sheldon.com/build-a-website-crawler-using-scrapy-framework/
I've
described
detailed
how
I've
implemented
a
website
crawler
to
warm
up
my
Wordpress
fullpage
cache
.
See
here
:
http://readthedocs.org/docs/scrapy/en/latest/topics/spiders.html?highlight=allowed_domains#scrapy.spider.BaseSpider.allowed_domains
allowed_domains
An
optional
list
of
strings
containing
domains
that
this
spider
is
allowed
to
crawl
.
Requests
for
URLs
not
belonging
to
the
domain
names
specified
in
this
list
wonâ€™t
be
followed
if
OffsiteMiddleware
is
enabled
.
So
","
as
long
as
you
didn't
activate
the
OffsiteMiddleware
in
your
settings
","
it
doesn't
matter
and
you
can
leave
allowed_domains
completely
out
.
Check
the
settings.py
whether
the
OffsiteMiddleware
is
activated
or
not
.
It
shouldn't
be
activated
if
you
want
to
allow
your
Spider
to
crawl
on
any
domain
.
I'm
using
scrapy
to
extract
data
from
certain
websites.The
problem
is
that
my
spider
can
only
crawl
the
webpage
of
initial
start_urls
","
it
can't
crawl
the
urls
in
the
webpage
.
I
copied
the
same
spider
exactly
:
I
use
the
log.txt
to
test
if
the
parsetext
is
called.However
","
after
I
runned
my
spider
","
there
is
nothing
in
the
log.txt
.
My
guess
would
be
this
line
:
This
isn't
a
domain
like
domain.tld
","
so
it
would
reject
any
links
.
If
you
take
the
example
from
the
documentation
:
allowed_domains
=
[
""""
dmoz.org
""""
]
I
get
two
tables
","
data_table
and
old_data_table
for
example
","
they
contains
exactly
the
same
structure
.
data_table
contains
only
1
month
data
.
say
from
'
2011-10-01
'
to
now
.
old_data_table
contains
the
rest
data
.
Now
if
I
have
to
query
data
between
day
'
2011-09-01
'
to
day
'
2011-10-07
'
","
is
there
a
way
to
query
it
from
both
table
and
combine
results
in
django
?
I
haven't
checked
this
code
","
but
you'll
want
something
like
this
...
Or
you
could
check
the
documentation
on
raw
sql
queries
.
I
am
new
to
python
and
django
and
was
wondering
how
I
would
go
about
making
a
dict
of
lists
.
My
4
lists
are
;
How
would
I
make
that
into
a
dictionary
I
could
then
loop
over
in
a
django
template
such
as
;
Thanks
in
advance
!
Update
:
hmm
I
not
sure
I
posted
my
question
properly
.
In
php
","
I
can
do
the
following
on
an
array
of
fields
:
All
the
form
fields
are
input
text
fields
.
.
How
would
I
do
this
in
Django
?
As
often
happens
","
you're
not
really
asking
about
the
problem
you
actually
need
to
solve
:
you've
done
things
in
a
certain
(
wrong
)
way
","
and
are
asking
about
how
to
get
yourself
out
of
the
problem
you've
got
yourself
into
.
If
I
understand
you
correctly
","
you've
got
a
set
of
fields
-
amount_paid
","
paid_date
","
method
","
comments
-
and
each
field
appears
multiple
times
on
the
form
","
so
you've
got
one
set
of
values
for
each
entry
.
You're
presumably
trying
to
sort
these
into
a
list
of
dicts
for
each
row
.
Well
","
this
is
not
the
right
way
to
go
about
it
in
Django
.
You
should
be
using
formsets
","
which
give
you
one
form
for
each
row
in
a
table
-
ie
","
exactly
what
you
want
to
achieve
.
Python's
dict
syntax
is
very
simple
.
It's
just
key-value
pairs
inside
a
pair
of
curly
braces
","
like
this
:
Following
your
update
","
it
looks
like
you
don't
want
a
dict
at
all
but
zip()
:
You
could
create
:
Also
your
template
code
does
not
make
sense
.
You
are
iterating
over
the
keys
of
the
dict
and
not
using
them
in
the
body
.
EDIT
As
long
as
your
CSV
is
valid
(
for
example
your
kml
values
are
properly
quoted
)
","
it
should
load
fine
.
If
you
believe
Refine
is
the
issue
","
you
can
skip
that
and
just
upload
your
CSV
directly
via
the
fusion
tables
UI
.
I
have
a
CSV
file
with
:
area
ID
area
name
URL
of
a
KML
file
with
the
area
boundaries
:
http://link.to/area_ID.kml
How
can
I
load
in
the
KML
to
a
column
in
the
CSV
in
order
to
upload
it
to
Fusion
Tables
?
I'm
working
in
Python
.
The
KML
is
also
available
as
JSON
if
that
helps
.
I
know
that
I
need
to
add
it
as
a
geometry
column
in
the
Fusion
Table
:
I
just
literally
don't
understand
how
to
get
it
into
the
CSV
file
so
that
I
can
then
upload
it
to
Fusion
Tables
.
Thanks
for
your
help
.
UPDATE
:
I've
tried
to
write
the
KML
as
a
string
to
the
column
in
the
CSV
(
<
Polygon
>
....
<
/
Polygon
>
)
","
but
Fusion
Tables
is
refusing
to
import it
","
with
a
502
error
-
perhaps
because
the
file
is
now
very
large
.
You
should
perhaps
loop
over
i
to
get
the
individual
values
:
range
returns
a
list
.
Or
to
get
the
values
:
They
are
using
pylab's
arange
","
not
simple
python
range
.
And
this
is
the
problem
when
importing
*
:
you
don't
know
which
method
comes
from
where
.
Import
just
pylab
and
reference
to
all
its
methods
/
constants
with
pylab
.
.
You
will
really
love
this
way
of
programming
when
you've
got
several
imports
.
will
work
.
The
error
I
get
is
this
:
I
am
not
sure
how
to
remove
that
error
.
I
type-casted
i
to
int
","
but
its
a
list
","
thus
can't
be
type-casted
.
I
am
following
this
example
.
http://msenux.redwoods.edu/math/python/simple.php
It
sounds
like
you
want
something
like
an
object-relational
mapper
.
I
am
the
primary
author
of
one
Ming
","
but
there
exist
several
others
for
Python
as
well
.
In
Ming
","
you
might
do
the
following
to
set
up
your
mapping
:
Then
you
can
query
for
some
particular
page
via
:
The
various
ODMs
I
know
of
for
MongoDB
in
Python
are
listed
below
.
Ming
MongoKit
MongoEngine
By
default
collection.find
or
collection.findone()
functions
results
in
a
dictionary
types
and
if
you
pass
paramater
as_class=SomeUserClass
than
it
will
try
to
parse
the
result
into
this
class
format
.
but
it
seems
this
class
should
also
be
derived
class
of
dictionary
(
as
it
required
__setitem__
function
to
be
defined
and
i
can
add
keys
in
the
class
)
.
Here
i
want
to
set
the
properties
of
the
class
.
how
can
i
do
achieve
this
?
Also
","
my
collection
class
contains
some
child
classes
as
properties
.
So
how
can
i
set
the
properties
of
child
classes
also
.
I
have
solved
this
by
adding
__setitem__
in
class
.
than
i
do
and
in
my
class
__setitem__
is
like
Solved
:
I
was
missing
mysql_config
-
-
cflags
mysql_config
-
-
libs
while
creating
the
shared
library
_IMysqlConnection.so
g
+
+
-
shared
CMysqlConnection_wrap.o
CMysqlConnection.o
-
o
_CMysqlConnection.so
mysql_config
-
-
cflags
mysql_config
-
-
libs
Above
command
linked
mysql
library
to
the
shared
library
_IMysqlConnection.so
and
it
worked
fine
.
Thanks
Ugo
for
your
help
:
-
)
I
am
trying
to
create
a
swig
package
for
python
that
offers
mysql
connectivity
.
But
when
I
try
to
import the
package
in
python
I
get
the
following
error
:
I
am
creating
the
package
using
the
following
commands
(
probably
this
is
where
I
am
making
a
blunder
)
I
am
able
to
do
mysql
connectivity
when
I
use
this
class
(
IMysqlConnection.cc
)
in
C
+
+
main
.
Any
help
and
guidance
will
me
much
appreciated
.
You
can
use
http://sourceforge.net/projects/mysql-python/
This
does
not
exactly
answer
your
connection
though
.
Is
there
a
comprehensive
reference
list
of
the
generic
setup
import step
names
?
The
names
of
generic
setup
import steps
don't
always
match
the
names
of
their
corresponding
xml
files
for
example
'
types.xml
'
has
an
import step
called
'
typeinfo
'
.
In
the
absence
of
a
list
","
I
would
be
satisfied
with
a
simple
approach
to
finding
out
the
name
of
the
import step
.
For
example
the
import step
name
for
plone.app.registry
which
is
managed
by
the
'
registry.xml
'
file
is
not
obvious
","
I
tried
to
refer
to
it
as
'
registry
'
but
this
fails
","
see
code
below
:
And
the
result
was
:
What
I
ended
up
doing
was
as
follows
:
go
into
the
plone
/
app
/
registry
/
exportimport
/
configure.zcml
file
where
the
name
was
registered
as
:
Turns
out
the
name
of
the
import step
was
registered
as
'
plone.app.registry
'
So
basically
I
had
to
dig
into
the
code
to
find
out
where
the
importStep
was
registered
.
You
should
try
this
:
edit
:
actually
this
will
give
you
the
complete
list
(
I've
tested
it
this
time
)
:
...
and
if
you
want
more
metadata
use
this
:
In
Python
and
Matplotlib
","
it
is
easy
to
either
display
the
plot
as
a
popup
window
or
save
the
plot
as
a
PNG
file
.
How
can
I
instead
save
the
plot
to
a
numpy
array
in
RGB
format
?
This
is
a
handy
trick
for
unit
tests
and
the
like
","
when
you
need
to
do
a
pixel-to-pixel
comparison
with
a
saved
plot
.
One
way
is
to
use
fig.canvas.tostring_rgb
and
then
numpy.fromstring
with
the
approriate
dtype
.
There
are
other
ways
as
well
","
but
this
is
the
one
I
tend
to
use
.
E.g
.
Something
like
this
-
note
'
LSArchitecturePriority
'
:
'
x86_64
'
","
I've
compiled
a
Python
screen
saver
on
my
Mac
OS
X
10.7
using
py2app
0.6.3
","
but
when
I
open
the
screen
saver
in
System
Preferences
i
get
the
following
message
:
You
cannot
use
the
Silly
Balls
screen
saver
on
this
computer
.
I've
read
that
this
message
means
it
needs
to
be
compiled
for
64-bit
.
I'm
running
Python
2.7.1
64-bit
on
a
64-bit
system
.
How
can
I
compile
a
64-bit
app
with
py2app
that
makes
the
screen
saver
example
work
?
I
did
a
little
more
research
and
made
my
question
more
clear
in
Accessing
QTextHtmlImporter
in
PyQt4
That
would
essentially
solve
this
issue
.
I'm
writing
a
blog
editor
using
pyqt4
.
My
issue
is
this
.
There
is
a
wysiwyg
editor
tab
and
a
html
code
editor
tab
.
A
change
done
in
one
should
reflect
in
the
other
.
One
option
is
to
use
the
setHtml
and
toHtml
functions
whenever
text
changes
in
a
qtextedit
.
But
the
issue
with
this
approach
is
that
when
I
use
setHtml
on
a
qtextedit
","
all
the
previous
edit
undo
history
is
lost
.
If
I
try
to
maintain
my
own
history
on
textchanged
signal
","
lots
of
memory
will
be
used
.
Consider
I
enter
a
sentence
","
textchanged
will
be
signaled
for
every
single
character
and
undo
would
be
character
by
character
.
I
tried
sharing
the
same
qtextdocument
between
the
two
qtextedits
but
that
also
failed
as
highlighting
is
linked
to
the
qtextdocument
and
hence
is
applied
to
both
qtextedits
.
Another
option
is
to
do
nothing
when
the
user
works
in
the
wysiwyg
editor
window
.
When
the
user
makes
some
edit
in
the
html
code
editor
","
I'll
wait
till
the
user
finishes
the
edit
and
then
once
done
","
I'll
find
the
diff
of
the
edit
and
somehow
","
make
a
corresponding
edit
change
in
the
wysiwyg
editor's
qtextdocument
.
Is
this
achievable
?
How
can
I
do
such
a
change
in
the
qtextdocument
?
Is
there
a
more
simple
way
that
I
am
not
aware
of
?
Please
help
.
Using
a
Blob
should
work
:
The
answer
you
gave
me
is
great
and
extract
perfectly
the
pixel
information
(
changing
uint8
to
uint16
)
.
However
","
the
values
I
obtain
are
higher
than
the
real
ones
.
There
is
an
offset
and
because
of
the
LOSSY
compression
in
JPEG2000
there
is
a
little
error
of
1
or
2
in
the
value
.
I
don't
like
to
use
external
calls
but
in
this
case
I
found
this
as
a
better
and
faster
solution
:
I
downloaded
Kakadu
(
free
software
for
non
commercial
purposes
)
and
I
use
the
module
kdu_expand
.
os.system('kdu_expand -i image.jp2 -o temp_image.tif')
im=PIL.Image.open('temp_image.tif
'
)
pixels=array(im.getdata()).reshape((im.size
[0]
","
im.size
[1]
)
)
I
convert
the
image
from
JPEG2000
to
TIF
but
it
is
quick
and
the
static
memory
is
not
usually
a
limitation
(
nowadays
)
in
a
computer
.
Then
","
the
PIL
library
perfectly
manages
to
extarct
the
data
.
Note
:
I
tried
the
conversion
straight
with
PythonMagick
but
it
gives
me
the
same
offset
as
before
Note
2
:
I
found
another
interesting
library
in
OpenCV
but
the
result
is
incorrect
pixels_cv2=cv2.imread('image.jp2
'
","
0
)
Note3
:
The
images
I
used
are
satellite
images
codified
with
12
bites
.
Possibly
in
other
type
of
data
the
PythonMagick
behaves
better
.
I
am
working
in
an
application
which
(
among
other
things
)
need
to
read
a
satellite
image
(
with
only
one
band
per
image
)
and
process
the
pixel
data
.
The
format
is
JPEG-2000
and
therefore
I
cannot
use
the
PIL
library
(
which
simplifies
everything
)
.
I
have
found
the
PythonMagick
library
and
I
can
perfectly
read
the
image
and
extract
the
value
of
the
pixel
.
But
only
for
one
pixel
!
Therefore
","
I
need
a
for-loop
to
get
all
the
pixel
values
(
the
images
are
very
large
)
.
I
tried
with
Blob
function
to
get
the
data
but
it
crashes
.
Are
there
any
better
options
?
How
could
I
quickly
get
the
pixel
data
of
a
JPEG2000
image
and
save
it
into
an
array
?
I
downloaded
Kakadu
(
free
software
for
non
commercial
purposes
)
and
I
use
the
module
kdu_expand
.
os.system('kdu_expand -i image.jp2 -o temp_image.tif')
Slight
off-topic
comment
for
those
running
the
Kakadu
(
KDU
)
demo
.
Would
like
to
add
that
if
running
kdu_expand
","
like
above
:
os.system('kdu_expand -i image.jp2 -o temp_image.tif')
and
this
gives
you
an
error
:
Check
the
permissions
of
libkdu_v78R.dylib
and
change
them
to
644
or
666
","
or
something
with
read
/
write
access
.
I'd
like
to
code
autocompletion
in
Linux
terminal
.
The
code
should
work
as
follows
.
It
has
a
list
of
strings
(
e.g
.
""""
hello
","
""""
hi
""""
","
""""
how
are
you
""""
","
""""
goodbye
""""
","
""""
great
""""
","
...
)
.
In
terminal
the
user
will
start
typing
and
when
there
is
some
match
possibility
","
he
gets
the
hint
for
possible
strings
","
from
which
he
can
choose
(
similarly
as
in
vim
editor
or
google
incremental
search
)
.
e.g
.
he
starts
typing
""""
h
""""
and
he
gets
the
hint
"h""ello"
""""
_
""""
i
""""
"_""ow"
are
you
""""
And
better
yet
would
be
if
it
would
complete
words
not
only
from
the
beginning
but
from
arbitrary
part
of
the
string
.
Thank
you
for
advise
.
For
those
(
like
me
)
that
end
up
here
searching
for
autocomplete
in
the
interpreter
:
(
dead
link
was
here
)
This
involves
creating
a
file
.
pythonrc
","
modifying
.
bashrc
and
an
import sys
you
have
to
import every
time
you
launch
the
Python
interpreter
.
I
wonder
if
the
latter
can
be
automated
for
even
more
win
.
Steps
:
Create
a
file
.
pythonrc
in
home
directory
by
this
command
:
vi
.
pythonrc
Enter
this
content
:
Close
the
file
Now
run
echo
""""
export
PYTHONSTARTUP
=
~
/
.
pythonrc
""""
>
>
~
/
.
bashrc
Restart
the
terminal
(
I'm
aware
this
isn't
exactly
what
you're
asking
for
","
but
)
If
you're
happy
with
the
auto-completion
/
suggestions
appearing
on
TAB
(
as
used
in
many
shells
)
","
then
you
can
quickly
get
up
and
running
using
the
readline
module
.
Here's
a
quick
example
based
on
Doug
Hellmann's
PyMOTW
writeup
on
readline
.
This
results
in
the
following
behaviour
(
<
TAB
>
representing
a
the
tab
key
being
pressed
)
:
In
the
last
line
(
HOTAB
entered
)
","
there
is
only
one
possible
match
and
the
whole
sentence
""""
how
are
you
""""
is
auto
completed
.
Check
out
the
linked
articles
for
more
information
on
readline
.
""""
And
better
yet
would
be
if
it
would
complete
words
not
only
from
the
beginning
...
completion
from
arbitrary
part
of
the
string
.
""""
This
can
be
achieved
by
simply
modifying
the
match
criteria
in
the
completer
function
","
ie
.
from
:
to
something
like
:
This
will
give
you
the
following
behaviour
:
Updates
:
using
the
history
buffer
(
as
mentioned
in
comments
)
A
simple
way
to
create
a
pseudo-menu
for
scrolling
/
searching
is
to
load
the
keywords
into
the
history
buffer
.
You
will
then
be
able
to
scroll
through
the
entries
using
the
up
/
down
arrow
keys
as
well
as
use
Ctrl+R
to
perform
a
reverse-search
.
To
try
this
out
","
make
the
following
changes
:
When
you
run
the
script
","
try
typing
Ctrl+r
followed
by
a
.
That
will
return
the
first
match
that
contains
""""
a
""""
.
Enter
Ctrl+r
again
for
the
next
match
.
To
select
an
entry
","
press
ENTER
.
Also
try
using
the
UP
/
DOWN
keys
to
scroll
through
the
keywords
.
I
guess
you
will
need
to
get
a
key
pressed
by
the
user
.
You
can
achieve
it
(
without
pressing
enter
)
with
a
method
like
this
:
Then
","
if
this
key
is
a
tab
key
(
for
example
","
that's
something
you
need
to
implement
)
","
then
display
all
possibilities
to
user
.
If
that's
any
other
key
","
print
it
on
stdout
.
Oh
","
of
course
you
will
need
to
have
getkey()
looped
in
a
while
","
as
long
as
the
user
hits
enter
.
You
can
also
get
a
method
like
raw_input
","
that
will
get
the
whole
word
sign
by
sign
","
or
display
all
the
possibilities
","
when
you
hit
a
tab
.
At
least
that's
the
item
","
you
can
start
with
.
If
you
achieve
any
other
problems
","
than
write
about
them
.
EDIT
1
:
The
get_word
method
can
look
like
this
:
The
issue
I'm
occuring
right
now
is
the
way
to
display
a
sign
","
you
have
just
entered
without
any
enteres
and
spaces
","
what
both
print
a
and
print
a
","
does
.
To
enable
autocomplete
in
a
Python
shell
","
type
this
:
(
thanks
to
http://blog.e-shell.org/221
)
You
need
to
use
equations
7
through
9
in
combination
.
The
only
things
that
are
unknown
in
the
equations
are
the
Lagrange
multipliers
","
the
lambdas
.
Everything
else
depends
on
the
empirical
data
available
","
and
are
thus
just
numbers
.
Given
a
set
of
values
for
the
lambdas
","
you
can
calculate
the
"G(j,r)"
and
the
Jacobian
"J(j,i,r,s)"
.
In
turn
","
if
you
know
the
residuals
and
the
Jacobian
","
you
can
use
Newton's
method
","
given
in
equation
9
","
to
find
the
roots
of
the
system
of
equations
","
i.e.
","
those
values
of
lambda
such
that
"G(j,r)"
=
0
.
Thus
","
you
use
an
initial
guess
at
the
values
for
the
lambdas
to
calculate
the
other
terms
","
then
use
those
terms
to
update
your
guess
.
There's
no
conceptual
challenge
to
working
with
equation
7
and
8
at
all
-
-
just
plug
in
the
values
-
-
but
they
are
adding
up
a
lot
of
numbers
","
so
some
care
is
warranted
.
Equation
9
is
a
little
tricky
","
as
it's
not
written
very
clearly
.
Since
the
paper
describes
a
system
of
equations
","
you'd
generally
expect
to
solve
a
linear
equation
:
where
d_lambda
is
a
vector
of
changes
in
the
guess
","
G
is
a
vector
of
values
for
the
function
","
and
J
is
a
matrix
of
Jacobian
values
.
The
notation
in
the
paper
is
pretty
muddled
","
obscuring
what
should
be
a
simple
expression
.
You
can
get
it
into
a
clearer
form
by
introducing
a
unified
index
a
to
replace
the
pair
of
indices
i
and
s
;
the
authors
mention
just
this
change
in
the
discussion
of
the
method
","
giving
a
formula
for
calculating
the
combined
index
in
the
second
paragraph
on
page
4
.
Overall
","
the
procedure
becomes
(
using
the
unified
index
)
:
Choose
some
lambdas
to
act
as
your
initial
guess
.
Maybe
zeros
","
or
random
numbers
.
Evaluate
G(a)
and
"J(a,b)"
.
Solve
a
system
of
linear
equations
to
get
the
updates
to
your
guess
.
If
the
updates
are
small
compared
to
your
guess
","
stop
.
Otherwise
","
determine
the
new
guess
and
go
back
to
step
2
.
This
looks
quite
feasible
using
Numpy
.
The
paper
talked
about
using
a
parallel
computing
strategy
","
but
that
was
over
ten
years
ago
;
it
seems
like
a
much
smaller
problem
today
.
i
'
ve
been
struggling
for
days
on
this
thing....but
to
no
avail
.
I'm
not
very
good
at
difficult
math
let
alone
this
kind
of
level
of
difficulty
.
I
was
trying
to
implement
the
maximum
entropy
application
for
the
lottery
in
python
for
my
graduation
assignment
","
altough
the
focus
of
the
project
was
to
implement
a
number
of
data
mining
techinques
(
Decision
trees
","
Apriori
","
kmeans
)
something
alreadey
finished
","
i
just
could
not
pass
the
opportunity
to
do
something
more
advanced....but
i
guess
this
is
too
advanced
for
me
.
So
","
my
question
is
how
can
i
solve
the
non
linear
equation
(
8
)
from
the
following
paper
reference1
:
http://eprints.ecs.soton.ac.uk/901/01/paper05.pdf
the
method
is
based
in
the
following
paper
reference2
:
http://www.stanford.edu/~cover/papers/paper91.pdf
any
help
(
theoritical
or
othwerwise
)
will
be
deeply
appreciated
.
thanks
You
can
download
the
KML
file
in
python
using
urllib
.
For
reading
KML
","
you
can
use
a
parser
(
search
for
""""
kml
python
parser
""""
)
.
Google's
new
pyKML
library
is
good
for
this
.
See
e.g
.
pyKML
Examples
Here
is
a
very
simple
example
from
http://packages.python.org/pykml/tutorial.html
Downloading
and
parsing
I'll
leave
to
the
other
answer
.
Here's
how
I
retrieved
the
description
and
coordinates
for
each
placemark
in
a
KML
file
.
The
for
loops
for
the
description
and
coordinates
could
perhaps
be
rewritten
","
but
I
haven't
found
exactly
how
yet
.
I'd
like
to
download
a
KML
file
and
print
a
particular
element
of
it
as
a
string
in
Python
.
Could
anyone
give
me
an
example
of
how
to
do
this
?
Thanks
.
Plotly
lets
you
make
graphs
using
their
online
Python
sandbox
.
Their
gallery
has
some
example
scientific
graphs
with
the
Python
scripts
that
generated
them
:
https://plot.ly/api/Python.
Here's
a
sample
from
the
gallery
:
You
may
want
to
give
details
on
the
types
of
charts
you
want
to
make
.
Simple
graphs
are
easy
with
sage
and
there
are
lots
of
options
as
compared
to
matlab
.
If
you
want
more
of
a
powerpoint
chart
","
or
picture
you
can
insert
into
a
word
doc
","
then
that's
a
little
different
.
If
you
can
get
something
to
create
chart
images
","
then
you
can
hook
it
into
a
python
web
framework
","
such
as
django
or
pylons
.
That
will
allow
you
to
set
up
a
loopback
server
to
host
the
page
on
your
machine
and
view
it
on
your
machine
.
This
is
quite
a
bit
more
complex
though
.
My
suggestion
is
to
break
your
program
down
into
pieces
.
It's
like
building
a
house
out
of
lego
brinks
.
You
have
an
idea
what
you
want
it
to
look
like
","
but
the
details
determine
everything
.
Break
it
down
into
the
smallest
pieces
you
can
","
and
define
larger
pieces
as
groups
of
smaller
pieces
The
house
is
just
several
rooms
.
A
room
is
just
4
walls
","
a
floor
and
a
ceiling
.
A
wall
is
just
several
boards
","
and
a
board
is
2x4
.
Once
you
break
all
the
parts
down
","
then
you'll
know
not
only
what
you
need
to
make
","
but
what
you
need
to
find
for
each
piece
.
You've
got
a
good
start
with
your
list
of
requirements
.
That
defines
what
you
want
your
program
to
do
.
Now
you
need
to
work
backwards
to
define
the
different
parts
.
Don't
get
hung
up
on
how
they
work
","
define
the
way
they
mesh
.
For
a
simple
python
script
to
create
a
web
server
:
see
here
.
Note
the
section
on
dynamic
content
.
By
plugging
that
into
a
""""
black
box
""""
that
produces
your
charts
","
you
suddenly
have
a
simple
working
setup
.
The
charts
section
doesn't
care
how
the
user
gets
them
","
it
just
makes
a
chart
and
passes
it
out
.
The
server
doesn't
care
how
the
chart
is
made
","
it
just
serves
it
up
.
This
section
of
the
sage
manual
has
instructions
for
saving
a
plot
after
you
create
it
.
matplotlib
has
become
a
mature
and
widely
used
graphing
package
.
As
for
your
interaction
with
a
web
browser
","
you
may
have
to
use
another
package
in
conjunction
.
I
suggest
CherryPy
because
it
is
simple
.
If
you
can
do
without
using
a
browser
","
you
can
use
one
of
Python
Plotting
Libraries
.
If
you
insist
on
using
the
browser
","
you
would
be
better
off
using
a
javascript-based
library
for
the
view
.
I
have
used
web2py
web
framework
before
along
with
protovis
.
But
a
simpler
web
framework
like
Bottle
or
CherryPy
can
also
be
used
to
pass
the
data
to
the
view
.
Bottle
has
a
Simple
Template
Engine
(
very
similar
to
what
I
used
in
Web2py
)
.
Developing
in
two
languages
(
one
of
which
is
javascript
)
is
a
bit
of
a
pain
.
You
could
use
something
like
Pyjamas
that
translates
python
into
javascript
but
I
am
not
really
sure
if
this
would
work
out
well
","
and
I
have
no
experience
with
it
.
I'm
looking
for
a
python
library
/
module
that
will
allow
me
to
create
eye
catching
charts
.
The
module
must
have
/
support
the
following
Windows
Support
As
I
work
mainly
in
windows(using Eclipse in Windows)
","
this
is
crucial
","
a
simple
install
.
exe
file
(
or
adequate
install
instructions
a
must
)
Work
on
Python
2.7
I've
got
Python
2.7.2
installed
don't
really
want
to
go
get
some
other
thing
.
Not
require
being
on
the
web
I've
looked
at
googlepychart
","
and
it
looks
like
you
need
to
be
on
the
web
to
make
it
work
","
I'm
not
on
the
world
wide
web
","
actually
behind
a
VERY
restrictive
firewall
.
Output
should
be
viewable
from
HTML
browser
While
I
cannot
get
onto
the
WorldWideWeb
","
I
can
see
localhost
","
it
would
be
great
if
the
chart
result
be
viewable
in
a
browser
.
Good
documentation
","
at
the
very
least
some
samples
on
how
I
can
use
the
library
.
If
there
are
any
suggestions
on
how
I
can
create
a
web
app
using
python
that
simply
display
/
charts
the
data
i
pass
in
","
such
advice
would
be
much
appreciated
Actually
what
you
are
looking
at
is
not
a
proxy
pattern
but
the
builder
pattern
","
and
yes
your
implementation
is
IMHO
is
the
classic
one
(
using
the
Fluent
interface
pattern
)
.
I
don't
know
what
SQLAlchemy
does
","
but
I
would
implement
the
interface
by
having
the
Session.query()
method
return
a
Query
object
with
methods
like
filter()
","
order_by()
","
all()
etc.
Each
of
these
methods
simply
returns
a
new
Query
object
taking
into
account
the
applied
changes
.
This
allows
for
method
chaining
as
in
your
first
example
.
Your
own
code
example
has
numerous
problems
.
One
example
I
find
it
very
interesting
the
way
how
SQLAlchemy
constructing
query
strings
","
eg
:
As
far
as
I
can
see
","
there
applied
some
kind
of
Proxy
Pattern
.
In
my
small
project
I
need
to
make
similar
string
construction
using
OOP
approach
.
So
","
I
tried
to
reconstitute
this
behavior
.
Firstly
","
some
kind
of
object
","
one
of
plenty
similar
objects
:
All
methods
of
this
object
return
self
","
so
I
can
call
them
in
any
order
and
unlimited
number
of
times
.
Secondly
","
proxy
object
","
that
will
call
subject's
methods
if
it's
not
a
perform
method
","
which
calls
object
to
see
the
resulting
string
.
And
finally
:
What
can
you
say
about
this
implementation
?
Is
there
better
ways
to
make
the
desirable
amount
of
classes
that
would
make
such
a
string
cunstructing
with
the
same
syntax
?
PS
:
Sorry
for
my
english
","
it's
not
my
primary
language
.
I
have
a
python
(
2.6.5
64-bit
","
Windows
2008
Server
R2
)
app
that
launches
worker
processes
.
The
parent
process
puts
jobs
in
a
job
queue
","
from
which
workers
pick
them
up
.
Similarly
it
has
a
results
queue
.
Each
worker
performs
its
job
by
querying
a
server
.
CPU
usage
by
the
workers
is
low
.
When
the
number
of
workers
grows
","
CPU
usage
on
the
servers
actually
shrinks
.
The
servers
themselves
are
not
the
bottleneck
","
as
I
can
load
them
up
further
from
other
applications
.
Anyone
else
seen
similar
behavior
?
Is
there
an
issue
with
python
multiprocessing
queues
when
a
large
number
of
processes
are
reading
or
writing
to
the
same
queues
?
Creating
of
new
thead
is
very
expensive
operation
.
One
of
the
simplest
ways
for
controling
a
lot
of
paralell
network
connections
is
to
use
stackless
threads
with
support
of
asyncronical
sockets
.
Python
had
great
support
and
a
bunch
of
libraries
for
that
.
My
favorite
one
is
gevent
","
which
has
a
great
and
comletely
transparent
monkey-patching
utility
.
Not
exactly
sure
what
is
going
on
unless
you
provide
all
the
details
.
However
","
remember
that
the
real
concurrency
is
bounded
by
the
actual
number
of
hardware
threads
.
If
the
number
of
processes
launched
is
much
larger
than
the
actual
number
of
hardware
threads
","
at
some
point
the
context-switching
overhead
will
be
more
than
the
benefit
of
having
more
concurrent
processes
.
Two
different
ideas
for
performance
constraints
:
The
bottleneck
is
the
workers
fighting
each
other
and
the
parent
for
access
to
the
job
queue
.
The
bottleneck
is
connection
rate-limits
(
syn-flood
protection
)
on
the
servers
.
Gathering
more
information
:
Profile
the
amount
of
work
done
:
tasks
completed
per
second
","
use
this
as
your
core
performance
metric
.
Use
packet
capture
to
view
the
network
activity
for
network-level
delays
.
Have
your
workers
document
how
long
they
wait
for
access
to
the
job
queue
.
Possible
improvements
:
Have
your
workers
use
persistent
connections
if
available
/
applicable
(
e.g
.
HTTP
)
.
Split
the
tasks
into
multiple
job
queues
fed
to
pools
of
workers
.
The
generic
attribute
getter
function
getattr
should
work
:
You
could
pass
the
name
of
the
attribute
:
or
a
getter
functor
:
I
prefer
the
latter
approach
as
it
is
more
generic
.
For
example
","
it
easily
allows
for
computed
keys
.
So
I
have
a
working
sorting
algorithm
in
Python
.
(
Its
exact
contents
are
irrelevant
to
this
question
.
)
It
uses
a
list
called
'
people
'
containing
class
instances
","
and
the
function
is
hard-coded
to
sort
that
list
by
a
specific
attribute
","
'
wealth
'
.
Now
","
I'd
like
to
generalize
the
function
so
I
could
sort
by
any
attribute
.
But
this
","
of
course
","
throws
an
error
","
because
it
doesn't
know
to
consider
'
wealth
'
as
a
class
attribute
.
So
","
how
is
this
possible
to
do
?
Create
a
file
called
config.py
and
paste
the
follwing
into
it
.
then
copy
this
code
in
a
file
called
config.xml
After
all
of
the
resources
defined
in
the
config.xml
file
","
simply
call
the
following
from
a
command
prompt
.
Make
sure
you
have
access
to
wsadmin
.
This
will
create
all
your
resources
into
a
profile
called
MyProfile
.
Hope
it
helps
.
I've
a
requirement
for
writing
a
script
in
Python
or
JACL
script
in
which
I
need
to
monitor
and
pull
the
information
from
Admin
Console
of
the
application
running
on
remote
server
.
I
require
to
pull
following
information
:
jdbc
jms
web
container
threads
default
work
manager
threads
.
If
anyone
can
help
me
writing
this
script
","
it'll
be
highly
appreciated
.
Thanks
Yesterday
before
updating
the
Google
App
Engine
SDK
","
I
was
able
to
do
python
manage.py
runserver
without
any
problems
","
but
after
updating
the
SDK
this
morning
","
I'm
getting
these
errors
when
trying
to
do
python
manage.py
runserver
I'm
not
sure
what
the
problem
is
.
Any
ideas
?
Got
it
.
All
I
needed
to
do
was
change
the
port
number
to
a
different
number
.
I
just
chose
8084
:
Because
Python
integers
are
arbitrarily
large
","
you
have
to
mask
the
values
to
limit
conversion
to
the
number
of
bits
you
want
for
your
2s
complement
representation
.
Python
displays
the
simple
case
of
hex(-199703103)
as
a
negative
hex
value
(
-
0xbe73a3f
)
because
the
2s
complement
representation
would
have
an
infinite
number
of
Fs
in
front
of
it
for
an
arbitrary
precision
number
.
The
mask
value
(
2**32-1
=
=
0xFFFFFFFF
)
limits
this
:
I
use
python
2.6
Positive
and
negative
value
are
the
same
?
When
I
use
calc
","
the
value
is
FFFFFFFFF418C5C1
.
Python's
integers
can
grow
arbitrarily
large
.
In
order
to
compute
the
raw
two's-complement
the
way
you
want
it
","
you
would
need
to
specify
the
desired
bit
width
.
Your
example
shows
-
199703103
in
64-bit
two's
complement
","
but
it
just
as
well
could
have
been
32-bit
or
128-bit
","
resulting
in
a
different
number
of
0xf's
at
the
start
.
hex()
doesn't
do
that
.
I
suggest
the
following
as
an
alternative
:
This
prints
out
:
Try
something
like
This
uses
split()
[docs]
and
is
much
more
pythonic
.
For
example
my
file
is
:
I
want
to
get
the
information
in
best
way
possible
.
Remember
that
all
files
are
in
this
format
.
What
changes
are
the
numbers
and
players
EDIT
:
Ok
but
this
is
not
a
homework
.
I've
done
this
a
few
times
but
I
do
not
think
is
the
best
way
.
The
code
works
perfectly
.
But
I
do
not
think
it's
a
good
code
must
have
a
better
way
to
do
this
.
Building
on
the
answer
given
by
@brc
above
:
This
clearly
extracts
the
pieces
you
need
with
a
minimum
of
overhead
.
Of
course
","
you'll
also
need
to
actually
do
something
with
the
values
.
Also
","
I'm
assuming
there
can
be
multiple
""""
game
""""
lines
in
a
file
.
If
that's
not
true
","
you
can
just
check
for
it
in
the
first
line
.
You
can
use
csv.reader
with
a
custom
dialect
for
a
general
solution
with
a
lot
of
the
details
already
sorted
.
Look
at
two
ways
of
structuring
my
functions
:
Will
the
second
option
be
slower
?
I
only
need
to
call
myFunc
from
myFunc2
","
so
'
d
like
to
hide
it
from
my
module
documentation
","
I
could
use
an
underscore
for
that
","
but
I
thought
it
would
be
cleaner
to
have
it
inside
the
function
.
On
the
other
hand
I
might
need
to
call
myFunc2
few
hundred
times
per
second
","
so
""""
redefining
""""
myFunc
when
calling
myFunc2
each
time
might
be
slow
...
is
that
a
good
guess
?
The
local
function
in
the
second
variant
won't
be
compiled
over
and
over
again
-
-
it
is
compiled
once
together
with
the
whole
file
","
and
its
body
is
stored
in
a
code
object
.
The
only
thing
that
happens
during
the
execution
of
the
outer
function
is
that
the
code
object
is
wrapped
in
a
new
function
object
which
is
then
bound
to
the
local
name
myFunc
.
There
might
be
a
difference
between
the
two
variants
if
myFunc()
takes
default
parameters
.
Their
definition
would
be
executed
over
and
over
again
in
the
second
variant
","
resulting
in
a
possible
performance
hit
.
Exaggerated
example
:
With
the
daft
code
above
","
myClass.myFunc2()
will
return
immediately
","
while
myClass2.myFunc2()
takes
a
second
to
execute
.
Using
Python
2.6.5
on
64-bit
Ubuntu
","
there
is
no
discernible
difference
:
Despite
the
other
answers
that
claim
there
is
no
effect
","
I
thought
I
should
check
.
I
found
a
very
definite
advantage
to
defining
the
function
outside
.
10
loops
","
best
of
10
:
4.2
ms
per
loop
10
loops
","
best
of
10
:
5.33
ms
per
loop
1
loops
","
best
of
1
:
438
ms
per
loop
1
loops
","
best
of
1
:
574
ms
per
loop
Dotted
lookup
(
a.k.a
.
attribute
binding
)
always
takes
longer
than
nested
scope
lookups
.
The
former
involves
a
series
dictionary
of
lookups
and
creation
of
a
new
object
(
a
bound
or
unbound
method
)
.
The
latter
uses
cell
variables
and
are
implemented
using
an
array
lookup
.
Processor
from
TEMPLATE_CONTEXT_PROCESSORS
is
not
allowed
to
edit
context
or
I'm
confuse
?
Even
if
i
subclass
Context
and
try
to
pass
it
to
context_instance
it
does
not
gives
me
acces
to
dictionary
.
It
is
appends
it
right
before
render
.
So
how
do
I
analyze
and
edit
contexts
before
it
gets
rendered
?
UPDATE
:
The
only
way
i
found
is
to
subclass
Context
end
overwrite
update
method
.
You
can
just
set
same
variable
name
and
it
will
overwrite
it
","
check
Django
docs
.
You
cannot
read
context
inside
context
processor
.
You
could
probably
use
template
tags
","
but
they're
not
really
meant
for
that
.
A
recompile
of
python2.7
did
the
trick
.
Question
:
How
can
I
get
my
non-system
Python2.7
to
find
the
sqlite3
libraries
?
Details
:
I
run
Ubuntu
10.04
LTS
which
uses
Python2.6
.
I
have
projects
that
require
Python2.7
.
I
use
the
-
-
python=Python2.7
flags
for
virtualenv
to
have
my
project
use
that
.
I
have
the
following
sqlite3
stuff
installed
libsqlite3-0
-
SQLite
3
shared
library
libsqlite3-dev
-
SQLite
3
development
files
sqlite3
-
A
command
line
interface
for
SQLite
3
I
can
import and
use
sqlite3
fine
with
Python2.6
.
But
","
not
so
lucky
with
Python2.7
.
This
is
how
I
got
Python2.7
on
the
machine
tar
xzf
Python-2.7.tgz
cd
Python-2.7
.
/
configure
make
sudo
make
altinstallenter
code
here
I
have
a
system
that
accepts
MP3
uploads
to
the
BlobStore
","
and
now
I
would
like
to
embed
them
in
an
HTML5
player
.
The
trouble
is
:
I
can't
seem
to
get
a
link
directly
to
the
MP3
file
.
I've
used
images.get_serving_url(blob_key)
to
great
effect
","
but
there
doesn't
seem
to
be
an
equivalent
for
audio
files
.
The
documentation
explains
how
to
create
a
handler
that
serves
any
file
from
the
blobstore
","
with
two
slightly
different
approaches
:
BlobstoreDownloadHandler
and
Sample
App
Both
had
the
same
result
for
me
","
which
you
can
see
here
:
http://testgroovebug.appspot.com/v1/audioserve?resource=AMIfv96sdSSVnSIVZXGyeeGDs3ZjbH7dy4mZmeuU_bUPYxxPu9KHrgO-VC4cVxAAYE-MsZmyuBfVRmFHAnlxLxQxhLMpvfBbCRNPjuriSIBCwO5dLTVpo3ncOL7uQI8VBo8KddW19gtUUU_IUr2AN_Er6BZxTwnmvg
This
is
supposed
to
be
a
3
+
minute
song
","
but
usually
only
the
first
few
seconds
come
through
.
Refreshing
the
page
will
randomly
change
the
amount
that
is
streamed
(
sometimes
you
get
lucky
and
10
seconds
play
!
)
.
The
HTTP
header
(
gotten
via
curl
)
looks
like
this
:
Ultimately
","
I
want
to
be
able
to
use
an
embedded
player
of
some
kind
.
However
","
the
following
code
yields
a
big
X
in
firefox
.
Make
sure
that
the
mp3
are
accessible
to
everyone
.
Because
of
licensing
issues
mp3
is
not
available
on
Firefox
audio
tag
","
you
will
need
to
store
your
files
as
ogg
in
order
for
them
to
work
on
Firefox
.
How
about
just
symlinking
them
...
Are
there
any
gotchas
I
should
be
aware
of
when
running
Python
scripts
from
inside
rpm
install
?
Here's
the
gist
of
the
problem
.
We
created
a
custom
RPM
installer
for
deploying
our
Django
app
.
As
part
of
the
installation
process
I
want
to
run
a
Django
management
command
that
collects
all
static
files
and
copies
them
into
a
predefined
location
.
Here's
what
this
looks
like
when
run
manually
from
a
command
line
:
So
to
run
this
as
part
of
the
RPM
install
I
added
the
following
to
the
spec
file
:
The
problem
is
that
when
I
run
this
","
I
can
see
the
task
being
kicked
off
:
But
there
is
no
expected
list
of
files
being
copied
in
the
output
and
the
static
directory
isn't
actually
being
populated
.
So
the
questions
are
:
Is
there
anything
special
about
running
python
scripts
from
RPM
that
I
need
to
be
aware
of
I'm
using
-
vv
option
of
the
rpm
command
to
get
verbose
output
from
the
install
process
","
but
is
there
a
way
to
further
debug
what's
going
on
inside
of
rpm
once
the
python
script
gets
kicked
off
.
Thanks
D
.
I
ended
up
moving
the
scripts
from
%
post
to
%
build
.
That
took
care
of
the
issue
.
You
should
look
at
virtualenv
(
http://pypi.python.org/pypi/virtualenv
)
","
and
then
have
your
rpm
bundle
the
entire
virtual
environment
","
along
with
the
site-packages
directory
","
so
the
eggs
you
need
will
be
available
on
the
deployed
system
","
and
that
the
eggs
will
be
at
the
version
your
script
requires
.
(
In
the
case
above
","
it
sounds
like
the
south
version
may
be
different
from
what
you
expect
)
.
Then
in
the
%
post
section
","
call
the
python
from
your
virtual
environment
instead
of
the
system
python
","
The
missing
egg
problem
will
be
pretty
easy
to
see
","
but
debugging
problems
due
to
differing
egg
versions
can
be
pretty
subtle
.
See
also
:
Deploying
Django
with
virtualenv
inside
a
distribution
package
?
Here
is
a
solution
that
uses
a
metaclass
:
You
could
create
a
metaclass
that
adds
behavior
when
a
class
is
created
","
but
your
use-case
is
simple
enough
to
just
extract
the
dictionary
directly
when
you
need
it
:
once
again
","
a
bit
surprised
not
to
find
an
answer
to
this
one
...
the
real-world
reason
for
my
asking
is
:
I
have
a
dictionary
of
constants
in
a
class
and
I
want
a
dictionary
so
I
can
find
the
constant
if
given
the
number
...
easy
enough
:
...
it
would
be
easy
enough
to
make
this
a
@static
method
of
MySQLConstants
and
call
it
by
going
MySQLConstants.createDic()
","
after
the
class
definition
statement
has
executed
...
but
I
was
just
wondering
whether
there's
any
way
to
get
a
method
","
inevitably
@static
","
to
run
when
a
class
(
NB
I'm
not
talking
about
creating
an
instance
of
this
class
!
)
is
created
...
To
answer
the
question
directly
","
yes
:
Notice
the
rewrite
of
create_dict
","
in
particular
assigning
it
as
a
classmethod
after
calling
it
during
class
creation
.
If
this
is
a
one-shot
use
method
","
I
would
be
inclined
to
go
with
this
:
This
creates
all
the
names
","
and
creates
a
reverse
lookup
dictionary
","
in
one
nice
tidy
package
.
it
was
far
simpler
","
in
fact
","
than
using
a
Metaclass
","
and
no
""""
classmethod
""""
turns
out
to
be
needed
:
but
thanks
to
Ethan
Furman
in
particular
for
showing
that
things
can
be
made
to
happen
during
the
creation
of
a
class
NB
if
you
anticipate
having
multiple
values
for
your
keys
(
this
is
the
case
with
MySQLdb
constants
)
the
best
way
to
handle
this
might
be
as
follows
From
the
documentation
:
I
have
a
field
like
this
in
my
Django
template
:
I
wanna
show
the
first
50
words
of
this
field
.
How
can
I
do
it
?
I've
got
a
fairly
simple
Python
script
:
When
I
run
it
","
I
sometimes
get
this
:
Other
times
","
though
","
the
script
runs
just
fine
.
All
my
other
Python
scripts
also
work
fine
.
This
error
doesn't
really
give
me
enough
context
to
even
know
where
to
look
.
Any
ideas
?
Even
just
a
way
to
get
some
actual
debug
info
would
be
appreciated
.
After
using
Skype4py
on
64bit
Linux
system
I
got
similar
results
(
segfault
)
.
Someone
on
Skype
forums
suggested
to
add
to
the
script
","
to
see
what
is
going
on
","
but
it
didn't
help
much
.
And
the
weird
part
is
that
it
sometimes
works
.
I
tried
starting
the
script
many
times
","
sometimes
it
starts
but
segfaults
at
some
later
point
.
Anyway
","
tested
the
""""
Transport='x11
'
""""
thing
that
Steven
mentioned
","
and
it
seems
like
it
fixes
it
.
I
had
a
similar
problem
with
Skype4Py
and
","
at
least
in
my
case
","
it
turned
out
that
Skype4Py
doesn't
work
with
64-bit
Python
on
the
Mac
.
Might
not
apply
to
you
though
since
your
script
works
sometimes
.
If
you're
using
a
Mac
","
see
the
Ned
Deily's
comment
in
this
question
for
how
to
run
32-bit
Python
:
How
do
I
force
Python
to
be
32-bit
on
Snow
Leopard
and
other
32-bit
/
64-bit
questions
And
just
a
few
hours
after
writing
the
above
","
I
faced
a
similar
intermittent
seg
fault
on
Linux
.
In
this
case
","
the
seg
fault
went
away
when
I
connected
using
X11
instead
of
DBUS
The
plain
Python
stuff
is
exceptionally
unlikely
to
lead
to
seg
fault
.
What
sticks
out
here
is
Skype4Py
.
No
idea
what
that
is
or
where
it
comes
from
","
but
I
bet
it's
the
culprit
.
By
setting
HOST
to
'
my_ip
'
","
you
may
listen
to
a
private
IP
that
only
resolves
to
your
computer
for
your
computer
.
The
best-known
example
is
127.0.0.1
.
Instead
","
pass
an
empty
string
(
HOST
=
'
'
)
to
listen
to
any
requests
coming
in
at
the
specified
port
.
Make
sure
to
use
the
same
port
number
","
i.e.
either
5000
or
8000
","
on
both
machines
.
Also
","
check
whether
a
firewall
between
the
computers
(
or
installed
on
one
of
them
)
prevents
the
connection
.
To
test
whether
your
computer
is
reachable
in
principle
","
run
python
-
m
SimpleHTTPServer
in
a
directory
without
private
information
on
the
server
and
try
to
reach
the
webserver
started
with
that
from
the
client
.
The
code
looks
to
be
correct
","
so
your
problem
lies
elsewhere
(
perhaps
a
firewall
issue
or
dns
problem
for
""""
my_ip
""""
)
.
I
am
using
the
following
code
to
open
a
socket
on
my
computer
.
When
I
go
to
my_ip:5000
on
my
computer
the
program
responds
.
However
","
when
I
use
another
computer
","
nothing
happens
.
I'm
not
sure
if
this
is
an
issue
with
the
firewall
or
not
.
When
I
start
up
the
test
server
in
django
using
manage.py
runserver
my_ip:8000
I
am
able
to
connect
to
the
machine
from
a
different
computer
.
I'm
not
sure
what
is
causing
me
not
to
be
able
to
connect
from
another
computer
using
the
code
above
...
You're
doing
some
very
odd
things
here
.
Let's
go
through
a
few
of
them
.
1
-
name
=
jobs
[j]
[
'
Job_Name
'
]
[0]
It
seems
to
me
that
perhaps
jobs
is
a
container
of
dictionaries
","
which
have
a
key
'
Job_Name
'
","
and
that
key's
value
is
a
list
or
tuple
.
I
think
what
you
might
mean
is
:
name
=
j
[
'
Job_name
'
]
[0]
If
jobs
is
just
a
list
of
strings
","
being
job
names
","
all
you
need
is
this
:
name
=
j
","
which
you
can
just
skip
-
and
just
use
j
when
you
want
to
refer
to
it
.
2
-
"p=re.compile(""(\\w*):?\\w*-?\\d"
*
""""
)
You're
compiling
your
regex
every
time
you
go
through
the
loop
","
where
it
is
exactly
the
same
each
time
","
and
you
use
it
exactly
once
.
Do
this
compilation
before
the
loop
begins
.
3
-
You
don't
need
your
else
clause
at
all
.
And
if
you
want
""""
else
if
""""
in
Python
","
you
use
""""
elif
""""
.
4
-
Your
regex
could
be
simplified
by
using
a
raw
string
to
eliminate
all
the
double
escapes
:
"p=re.compile(r""(\w*):?\w*-?\d"
*
""""
)
5
-
Your
whole
regex
may
be
unnecessary
.
If
you're
doing
what
I
think
you're
doing
","
you
just
want
what
appears
before
the
first
colon
","
if
there
is
one
.
(
Unless
you
really
mean
group(1)
","
which
I
think
you
don't
.
)
Which
means
you
could
skip
the
regex
(
since
you're
matching
with
in
anyhow
)
","
and
just
use
something
like
name
=
name.split(':')
[0]
The
match()
won't
""""
interfere
""""
with
an
else-clause
.
The
most
likely
cause
here
is
that
the
name==job
condition
is
failing
for
one
of
your
test
cases
.
A
judicious
use
of
print-statements
will
likely
will
show
which
string
is
skipped
.
Or
you
an
use
pdb
.
You're
changing
what
name
is
.
That
probably
makes
this
false
:
Which
makes
this
statement
not
execute
:
I
suggest
taking
a
look
at
what
name
is
after
the
name.p.match(name)
.
group(1)
statement
.
Is
there
any
way
that
regular
expression
match
inside
a
if
block
would
interfere
with
the
code
inside
an
else
block
?
In
the
following
code
","
I
have
100
test
strings
that
are
being
inputed
as
job_name
that
should
be
appended
to
my
del_job_ids
array
.
Only
99
of
them
are
being
appended
.
If
I
comment
out
the
line
name=p.match(name).group(1
)
","
the
one
case
that
doesn't
get
appended
before
now
get
appended
.
I
can
also
append
that
job
by
uncommmenting
the
code
in
the
else
block
(
the
string
passes
the
conditon
for
getting
appended
)
.
The
difference
between
the
strings
is
that
99
of
them
have
colon
","
dashes
","
or
both
and
the
one
other
string
has
neither
.
I
tested
the
regex
multiple
times
.
The
string
without
dashes
or
colons
should
go
the
else
clause
and
use
the
'
default
'
name
defined
at
the
top
of
the
for
loop
.
This
part
doesn't
make
sense
to
me
.
If
'
jobs
'
is
an
iterable
","
then
the
first
time
through
the
loop
","
'
j
'
is
set
to
'
jobs
[0]
'
","
the
next
time
it's
set
to
'
jobs
[1]
'
","
etc.
So
using
'
j
'
to
index
into
'
jobs
'
is
crazy
talk
.
You're
saying
""""
give
me
the
next
element
from
the
iterable
'
jobs
'
","
then
use
that
element
as
an
index
into
'
jobs
'
.
You
can
accumulate
the
keys
in
the
order
of
appearance
and
can
build-up
the
key
frequencies
in
all
in
one
pass
.
Take
advantage
of
sort
stability
to
make
a
sort
by
decreasing
frequency
","
then
by
order
of
appearance
:
In
step
1
","
you're
iterating
over
a
query
object
.
This
results
in
one
fetch
RPC
per
20
objects
returned
","
which
is
inefficient
","
time-wise
.
Instead
","
call
fetch(n)
on
the
Query
object
","
where
n
is
the
maximum
number
of
results
to
return
-
this
does
only
a
single
RPC
.
It
also
has
the
benefit
of
limiting
the
number
of
results
to
search
-
right
now
","
if
I
search
for
'
I
'
","
your
app
will
get
stuck
processing
nearly
every
record
in
step
1
.
It's
also
completely
unnecessary
to
convert
the
keys
into
strings
-
you
can
add
keys
to
a
set
just
fine
.
For
what
it's
worth
","
though
","
I
personally
find
'
or
'
searches
to
be
particularly
useless
.
I
realise
you'll
rank
items
that
match
all
the
terms
first
","
but
those
will
inevitably
be
followed
by
piles
and
piles
of
irrelevant
results
.
You
could
do
an
'
and
'
search
simply
by
doing
one
query
with
an
equality
filter
for
each
search
term
.
In
lieu
of
full-text
search
on
GAE
I'm
using
the
solution
below
to
return
a
resultset
that's
sorted
","
first
by
keyword
relevance
","
and
secondly
by
date
(
though
the
second
sorting
could
be
anything
really
)
.
It
feels
a
bit
bulky
and
I'm
concerned
about
performance
at
scale
so
I'm
looking
for
optimization
suggestions
or
a
different
approach
altogether
.
The
secondary
sorting
is
important
to
my
use
case
","
since
a
given
search
will
likely
have
multple
results
of
the
same
relevance
(
as
measured
by
the
number
of
keyword
matches
)
","
but
preserving
the
original
query
ordering
adds
a
lot
of
complexity
right
now
.
Any
ideas
?
Step
1
:
Get
a
list
of
keys
that
match
each
search
term
Step
2
:
Group
the
list
by
frequency
while
maintaining
the
original
order
I
was
wondering
how
I
can
make
a
custom
QDialog
message
box
with
my
own
buttons
","
similar
to
the
code
below
.
So
far
I
have
this
code
","
which
works
pretty
well
.
The
problem
with
this
code
is
that
it
launches
from
a
full
screen
application
","
and
it
steals
the
focus
of
it
(
the
main
taskbar
on
the
top
appears
along
with
the
QDialog
object
)
.
I
want
this
to
work
seamlessly
with
my
fullscreen
application
in
the
background
","
meaning
no
taskbar
at
the
top
should
appear
when
I
click
on
a
button
to
show
this
message
box
.
I'm
working
in
Ubuntu
11.10
with
PyQt4
and
Python
2.7.2
.
I
think
the
issue
you're
having
is
that
you
aren't
giving
your
msgBox
a
parent
.
This
makes
Qt
treat
it
as
a
top-level
window
.
Try
changing
your
instantiation
of
your
message
box
to
look
like
this
:
This
isn't
simple
.
But
the
good
news
is
that
it's
very
well
understood
.
Start
here
:
http://en.wikipedia.org/wiki/Job_scheduler
Then
read
this
:
http://en.wikipedia.org/wiki/List_of_job_scheduler_software
You
probably
want
something
like
http://en.wikipedia.org/wiki/Simple_Linux_Utility_for_Resource_Management
Assuming
all
the
scripts
are
python
the
easiest
solution
would
be
to
write
functions
that
imports(and thus runs)
the
other
scripts
.
point
to
these
functions
with
either
the
multiprocessing
or
threading
module
.
Now
that
you
have
each
script
running
in
its
own
process
or
thread
","
you
can
use
condition
objects
and
the
.
wait()
.
notify()
methods
to
have
them
stop
and
wait
on
each
other
.
All
the
documentation
to
implement
such
a
thing
can
be
found
here
.
http://docs.python.org/library/threading.html
I
am
looking
for
a
mechanism
to
sync
between
scripts
(
bash
or
perl
or
Python
)
My
requirement
is
as
follows
Scripts
a
","
b
and
c
will
start
(
not
necessarily
at
the
same
time
)
.
Once
a
","
b
","
or
c
hits
a
certain
point
in
the
script
","
it
should
stop
for
other
scripts
to
hit
the
same
point
.
For
the
sake
of
discussion
lets
call
these
as
syncpoints
.
Once
the
syncpoint
is
hit
","
by
all
scripts
","
all
scripts
can
proceed
to
next
syncpoint
or
end
of
script
Here
is
an
example
Script
a
Script
2
I
have
a
function
in
a
superclass
that
returns
a
new
version
of
itself
.
I
have
a
subclass
of
this
super
that
inherits
the
particular
function
","
but
would
rather
it
return
a
new
version
of
the
subclass
.
How
do
I
code
it
so
that
when
the
function
call
is
from
the
parent
","
it
returns
a
version
of
the
parent
","
but
when
it
is
called
from
the
child
","
it
returns
a
new
version
of
the
child
?
If
new
does
not
depend
on
self
","
use
a
classmethod
:
Or
","
if
new
does
depend
on
self
","
use
type(self)
to
determine
self's
class
:
I'm
using
networkx(library for python to deal with graphs)
.
I
basically
have
nodes
with
various
edges
but
want
to
see
what
a
path
would
look
like
if
it
used
the
nodes
that
were
the
most
connected
.
I
can
use
this
command
to
see
the
number
of
connections
:
and
I
can
get
the
number
of
edges
","
but
I'm
not
sure
how
to
apply
this
to
list
as
a
path
.
For
example
","
I
can
add
this
number
as
an
attribute
but
I
don't
think
attributes
are
taken
into
consideration
when
finding
a
path
and
because
I'm
adding
this
after
the
edges
are
connected
","
I
cannot
add
the
weights
to
the
edges
themselves
.
The
other
problem
is
the
higher
the
score
the
more
I
want
the
path
to
be
followed
but
with
edges
I
think
it
follows
the
lowest
weighted
edge
.
I'm
wondering
what
approach
do
other
people
take
to
find
paths
based
on
certain
characteristics
of
the
node
?
If
someone
knows
how
to
do
this
for
networkx
","
great
!
but
I
think
networkx
has
many
features
so
if
I
can
get
the
theory
or
general
approach
I'm
sure
I
can
find
a
way
to
do
it
in
python
.
UPDATE
:
Sorry
I
might
be
explaining
it
wrong
.
I
understand
I
can
add
attributes
to
nodes
","
but
I'm
not
sure
how
to
make
path
decisions
based
on
those
attributes
.
So
in
my
case
","
based
on
certain
conditions
I
am
adding
edges
between
nodes
.
Each
group
of
nodes
represents
a
different
"day(day1data.., day2data.., day3data..)"
","
so
I'm
connecting
a
few
nodes
from
day1
to
nodes
on
day2
only
if
certain
rules
are
matched
.
Once
I
have
the
edges
connected
","
I
want
those
ones
to
be
considered
more
heavily
when
choosing
a
path
.
So
I
added
an
attribute
'
weight
'
to
each
node
of
the
current
day
which
is
basically
the
total
number
of
edges
connecting
that
node
.
My
problem
is
","
the
weight
attribute
is
not
used
in
any
of
the
path
decision
making
because
its
an
attribute
I
created
and
labeled
myself(I could create a label named 'abc'='hello world' and it would apply that attribute to the node)
.
How
can
I
get
this
weight
to
be
considered
when
creating
the
path(the edges are already created so I don't think I can go back and recreate them)
?
From
the
NetworkX
Tutorial
It
looks
like
weights
can
be
added
after
the
fact
.
You
can
certainly
add
weights
to
edges
in
NetworkX
.
In
fact
","
you
can
set
arbitrary
data
for
edges
","
since
it
is
basically
a
dict
.
Furthermore
you
can
change
the
parameters
of
edges
(
or
nodes
)
after
you
added
them
.
And
you
can
of
course
calculate
path
according
to
weights
(
or
any
other
attribute
specified
in
the
weight
parameter
)
.
For
your
case
","
deciding
edge
weights
might
be
tricky
.
Bear
in
mind
that
weighted
shortest
path
is
calculated
usually
with
Djikstra's
Algorithm
and
it
favors
smaller
weights
.
It
also
requires
positive
weights
.
One
possible
solution
would
be
assigning
a
weight
of
1
/
"max(k_i,k_j)"
to
edge
(
i
","
j
)
where
k_i
","
k_j
is
the
degree
of
nodes
i
and
j
.
The
correct
way
to
compute
shortest
paths
over
transition
probabilities
is
to
transform
the
edge
weights
to
represent
surprisal
:
that
is
","
the
negative
log
of
the
probability
.
This
results
in
weights
that
are
positive
","
and
any
given
shortest
path
is
then
interpreted
as
minimizing
surprisal
.
And
since
Dijkstra's
algorithm
sums
up
weights
","
it
does
so
in
log
space
","
which
means
it
really
is
multiplying
probabilities
.
To
recover
the
joint
probability
of
observing
any
given
shortest
path
","
then
","
you
just
take
the
exponential
of
the
negative
surprisal
.
Instead
of
this
:
do
this
:
I
have
an
excel
sheet
with
a
bunch
of
info
regarding
the
stops
a
delivery
truck
makes
throughout
the
day
.
I
can
successfully
extract
the
information
I
need
with
xlrd
.
This
is
the
code
I
am
using
:
Printing
odList
give
me
this
output
where
fields
are
:
I
used
list
indices
to
grab
the
coordinates
and
query
gmaps
with
:
Unfortunately
","
gmaps
does
not
understand
[
35.779999
","
-
78.115784
]
","
[
36.075812
","
-
78.256766
]
","
gmaps
does
understand
(
35.779999
","
-
78.115784
)
","
(
36.075812
","
-
78.256766
)
.
Any
ideas
on
how
to
get
the
query
to
send
(
)
instead
of
[]
?
`
Try
using
an
tuple
(
var1
","
var2
)
or
(
var
","
)
instead
of
a
list
"[var1, var2]"
","
or
convert
the
lists
to
tuples
before
sending
printing
tuple([var])
.
I
am
trying
to
get
the
rasterized
line
data
from
a
pylab
plot
function
.
My
code
is
like
so
:
I
want
to
get
the
x
and
y
line
data
from
plotinfo
.
When
I
use
""""
type(plotinfo)
","
""""
plotinfo
is
a
list
","
but
when
using
""""
print
plotinfo
","
""""
it
is
a
2dlist
object
.
These
are
the
original
(
x
","
y
)
data
points
:
Here
we
(
linearly
)
interpolate
to
find
additional
points
.
You
can
increase
the
argument
to
path.interpolated
to
find
more
interpolated
points
between
the
original
points
.
Twisted
will
make
it
easy
to
get
up
and
running
right
away
while
making
it
possible
for
you
to
intercept
","
delay
","
or
block
requests
:
http://twistedmatrix.com/documents/current/api/twisted.web.proxy.Proxy.html
What
are
some
useful
methods
or
libraries
that
can
be
used
to
track
IP
request
from
a
personal
computer
.
Ideally
I
would
like
the
option
to
block
or
pause
a
specific
outgoing
request
before
/
after
some
checks
are
carried
out
.
I've
seen
Twisted
","
but
I'm
not
sure
if
its
exactly
what
I'm
looking
for
just
yet
","
or
if
there
exist
simpler
methods
for
doing
this
.
I'm
not
looking
for
a
standalone
application
as
there
are
other
features
that
will
be
build
around
this
for
a
specific
purpose
.
Language
:
Preferably
in
Python
","
but
C
/
C
+
+
are
possible
options
as
well
.
OS
:
The
current
target
is
Linux
(
ubuntu
)
.
However
cross-platform
options
would
be
best
.
This
prints
ENODEV
:
No
such
device
.
I
am
using
the
ctypes
module
to
do
some
ptrace
system
calls
on
Linux
","
which
actually
works
pretty
well
.
But
if
I
get
an
error
I
wanna
provide
some
useful
information
.
Therefore
I
do
an
get_errno()
function
call
which
returns
the
value
of
errno
","
but
I
didn't
found
any
function
or
something
else
which
interprets
the
errno
value
and
gives
me
an
associated
error
message
.
Am
I
missing
something
?
Is
there
a
ctypes
based
solution
?
Here
is
my
setup
:
Example
:
How
about
these
:
BadLinksPlugin
:
This
plugin
logs
bad
local
links
found
in
wiki
content
.
It's
a
quite
new
one
","
just
deals
with
dangling
links
","
but
any
bad
links
as
I
see
from
source
code
.
This
is
at
least
one
building
block
to
your
solution
request
.
VisitCounterMacro
:
Macro
displays
how
many
times
was
wiki
page
displayed
.
This
is
a
rather
old
one
.
You'll
get
just
the
statistic
per
page
while
an
administrative
view
is
missing
","
but
this
could
be
built
rather
easily
","
i.e.
like
a
custom
PageIndex
.
Hi
guys
:
Is
there
a
way
to
improve
trac
wiki
quality
using
a
plugin
that
deals
with
artifacts
like
for
obsolete
pages
","
or
pages
that
refer
to
code
which
doesn't
exist
anymore
","
pages
that
are
unlinked
","
or
pages
which
have
a
low
update-rate
?
I
think
there
might
be
several
heuristics
which
could
be
used
to
prevent
wiki-rot
:
Number
of
recent
edits
Number
of
recent
views
Wether
or
not
a
page
links
to
a
source
file
Wether
or
not
a
wiki
page's
last
update
is
<
or
>
the
source
files
it
links
to
Wether
entire
directories
in
the
wiki
have
been
used
/
edited
/
ignored
over
the
last
""""
n
""""
days
etc.
etc.
etc.
If
nothing
else
","
just
these
metrics
alone
would
be
useful
for
each
page
and
each
directory
from
an
administrative
standpoint
.
I
don't
know
of
an
existing
plugin
that
does
this
","
but
everything
you
mentioned
certainly
sounds
do-able
in
one
way
or
another
.
You
can
use
the
trac-admin
CLI
command
to
get
a
list
of
wiki
pages
and
to
dump
the
contents
of
a
particular
wiki
page
(
as
plain
text
)
to
a
file
or
stdout
.
Using
this
","
you
can
write
a
script
that
reads
in
all
of
the
wiki
pages
","
parses
the
content
for
links
","
and
generates
a
graph
of
which
pages
link
to
what
.
This
should
pinpoint
""""
orphans
""""
(
pages
that
aren't
linked
to
)
","
pages
that
link
to
source
files
","
and
pages
that
link
to
external
resources
.
Running
external
links
through
something
like
wget
can
help
you
identify
broken
links
.
To
access
last-edited
dates
","
you'll
want
to
query
Trac's
database
.
The
query
you'll
need
will
depend
on
the
particular
database
type
that
you're
using
.
For
playing
with
the
database
in
a
(
relatively
)
safe
and
easy
manner
","
I
find
the
WikiTableMacro
and
TracSql
plugins
quite
useful
.
The
hardest
feature
in
your
question
to
implement
would
be
the
one
regarding
page
views
.
I
don't
think
that
Trac
keeps
track
of
page
views
","
you'll
probably
have
to
parse
your
web
server's
log
for
that
sort
of
information
.
My
main
goal
is
to
check
an
FTP
server
at
anytime
for
a
new
file
hits
and
then
generate
a
.
txt
file
with
only
the
new
files
copied
there
.
If
there
are
no
new
files
then
it
returns
nothing
.
Here
is
what
I
have
so
far
.
I
have
started
by
copying
the
files
from
the
server
into
oldlist.txt
","
then
connecting
to
the
FTP
site
and
comparing
data
from
newlist.txt
and
oldlist.txt
and
the
differences
I
want
in
Temporary
FTP
file
changes.txt
.
Each
time
I
connect
I
will
change
newlist.txt
and
make
it
the
oldlist.txt
so
that
I
can
compare
the
next
time
I
connect
.
Is
there
a
better
way
to
do
this
?
My
lists
seem
to
never
change
data
each
time
.
Sorry
if
this
is
confusing
thanks
.
Your
implementation
of
this
scheme
is
reasonable
.
I
would
not
choose
this
scheme
to
implement
automated
FTP
messaging
","
if
that
is
what
you're
doing
.
There
are
two
weaknesses
of
this
approach
:
It
does
not
support
filenames
that
repeat
.
Any
filename
that
occurs
in
the
""""
old
""""
history
will
not
be
detected
as
a
new
file
.
Maybe
this
is
a
problem
for
you
","
maybe
not
.
But
even
if
filenames
are
guaranteed
unique
now
","
that
may
not
always
be
true
.
It
does
not
tell
you
whether
a
new
file
is
ready
to
be
consumed
or
not
.
It
is
possible
that
a
new
file
will
be
processed
while
it
is
still
being
uploaded
.
Some
people
apply
a
""""
no
change
in
size
for
X
seconds
""""
rule
","
but
that
just
increases
delay
and
still
leaves
a
vulnerability
to
severed
connections
.
One
scheme
that
is
similar
but
does
not
have
either
of
these
two
problems
is
to
actually
store
a
file
on
the
server
with
a
reserved
name
","
or
in
a
separate
place
","
and
use
its
timestamp
(
preferably
the
modification
time
of
the
file
itself
)
to
decide
which
files
can
be
safely
processed
.
This
""""
semaphore
""""
file
is
updated
to
the
current
time
as
the
last
step
in
uploading
a
file
.
All
files
with
a
modification
time
older
than
the
semaphore
timestamp
can
be
processed
.
Once
processed
","
all
files
must
be
deleted
out
of
the
upload
folder
so
they
won't
be
processed
twice
.
I
have
seen
this
scheme
work
well
in
an
automated
production
data
flow
.
It's
a
little
easier
/
faster
to
convert
the
lists
to
a
set
and
not
worry
about
sorting
.
Also
","
instead
of
saving
the
list
to
a
file
as
raw
text
","
you
could
use
the
shelve
module
to
make
a
persistent
store
that
is
conveniently
accessible
like
a
regular
Python
dict
.
Otherwise
","
your
code
has
the
virtues
of
being
simple
and
straight-forward
.
Here's
a
worked
out
example
:
Congratulations
","
you've
discovered
the
motivating
use
case
for
Python's
double-underscore
name
mangling
:
-
)
For
the
details
and
a
worked-out
example
see
:
http://docs.python.org/tutorial/classes.html#private-variables
and
at
http://docs.python.org/reference/expressions.html#atom-identifiers
.
Here's
how
to
use
it
for
your
example
:
The
use
case
is
described
as
a
way
of
implementing
the
Open-Closed
Principle
in
""""
The
Art
of
Subclassing
""""
found
at
http://www.youtube.com/watch?v=yrboy25WKGo&noredirect=1
.
The
same
can
be
achieved
by
making
fnX
and
printFnX
both
classmethods
.
I
would
like
a
method
in
a
base
class
to
call
another
method
in
the
same
class
instead
of
the
overriding
method
in
an
inherited
class
.
I
would
like
the
following
code
to
print
out
Class
B
:
6
Class
A
:
9
Can
this
be
done
?
Is
there
a
way
I
can
filter
a
directory
by
using
the
absolute
path
to
it
?
This
doesn't
seem
to
work
when
trying
to
filter
the
""""
Common
""""
Directory
located
under
""""
aDir
""""
.
If
I
do
this
:
It
works
","
but
every
directory
called
Common
will
be
filtered
in
that
""""
tree
""""
","
which
is
not
what
I
want
.
Any
suggestions
?
Thanks
.
You'll
want
to
make
your
own
ignore
function
","
which
checks
the
current
directory
being
processed
and
returns
a
list
containing
'
Common
'
only
if
the
dir
is
'
/
Full
/
Path
/
To
/
aDir
'
.
You
can
make
your
own
ignore
function
:
Or
","
if
you
want
to
be
able
to
call
copytree
with
a
relative
path
:
From
the
docs
:
If
ignore
is
given
","
it
must
be
a
callable
that
will
receive
as
its
arguments
the
directory
being
visited
by
copytree()
","
and
a
list
of
its
contents
","
as
returned
by
os.listdir()
.
Since
copytree()
is
called
recursively
","
the
ignore
callable
will
be
called
once
for
each
directory
that
is
copied
.
The
callable
must
return
a
sequence
of
directory
and
file
names
relative
to
the
current
directory
(
i.e.
a
subset
of
the
items
in
its
second
argument
)
;
these
names
will
then
be
ignored
in
the
copy
process
.
ignore_patterns()
can
be
used
to
create
such
a
callable
that
ignores
names
based
on
glob-style
patterns
.
The
API
for
shutil.ignore_patterns()
doesn't
support
absolute
paths
","
but
it
is
trivially
easy
to
roll
your
own
variant
.
As
a
starting
point
","
look
at
the
source
code
for
*
ignore_patterns
*
:
You
can
see
that
it
returns
a
function
that
accepts
a
path
and
list
of
names
","
and
it
returns
a
set
of
names
to
ignore
.
To
support
your
use
case
","
create
you
own
similar
function
that
uses
takes
advantage
of
path
argument
.
Pass
your
function
to
the
ignore
parameter
in
the
call
to
copytree()
.
Alternatively
","
don't
use
shutil
as-is
.
The
source
code
is
short
and
sweet
","
so
it
isn't
hard
to
cut
","
paste
","
and
customize
.
Use
quit()
in
this
context
.
break
expects
to
be
inside
a
loop
","
and
return
expects
to
be
inside
a
function
.
To
break
a
loop
","
use
break
instead
of
return
.
Or
put
the
loop
or
control
construct
into
a
function
","
only
functions
can
return
values
.
The
return
statement
only
makes
sense
inside
functions
:
As
per
the
documentation
on
the
return
statement
","
return
may
only
occur
syntactically
nested
in
a
function
definition
.
The
same
is
true
for
yield
.
When
running
the
following
code
(
in
Python
2.7.1
on
a
mac
with
Mac
OS
X
10.7
)
I
get
the
following
error
I've
carefully
checked
for
errant
tabs
and
/
or
spaces
.
I
can
confirm
that
the
code
fails
with
the
above
error
when
I
use
the
recommended
4
spaces
of
indentation
.
This
behavior
also
happens
when
the
return
is
placed
inside
of
other
control
statements
(
e.g
.
if
","
for
","
etc.
)
.
Any
help
would
be
appreciated
.
Thanks
!
What's
the
best
way
to
convert
numpy's
recarray
to
a
normal
array
?
i
could
do
a
.
tolist()
first
and
then
do
an
array()
again
","
but
that
seems
somewhat
inefficient
.
.
Example
:
By
""""
normal
array
""""
I
take
it
you
mean
a
NumPy
array
of
homogeneous
dtype
.
Given
a
recarray
","
such
as
:
we
must
first
make
each
column
have
the
same
dtype
.
We
can
then
convert
it
to
a
""""
normal
array
""""
by
viewing
the
data
by
the
same
dtype
:
astype
returns
a
new
numpy
array
.
So
the
above
requires
additional
memory
in
an
amount
proportional
to
the
size
of
a
.
Each
row
of
a
requires
4+8+4=16
bytes
","
while
a.astype(...)
requires
8*3=24
bytes
.
Calling
view
requires
no
new
memory
","
since
view
just
changes
how
the
underlying
data
is
interpreted
.
a.tolist()
returns
a
new
Python
list
.
Each
Python
number
is
an
object
which
requires
more
bytes
than
its
equivalent
representation
in
a
numpy
array
.
So
a.tolist()
requires
more
memory
than
a.astype(...)
.
Calling
a.astype(...)
.
view(...)
is
also
faster
than
np.array(a.tolist()
)
:
Try
setting
the
button
style
on
the
toolbar
that
the
actions
are
being
added
to
:
I
am
learning
PyQt
by
playing
with
examples
.
In
this
case
","
I'm
playing
with
the
webbrowser
example
that
is
located
at
\
Python26\Lib\site-packages\PyQt4\examples\activeqt\webbrowser
.
The
demo
does
something
really
odd
if
you
add
one
line
to
set
the
icon
text
property
of
a
QAction
.
Here's
a
code
sample
of
the
change
I
tried
:
One
time
I
tried
it
","
and
the
entire
toolbar
went
blank
.
I
can't
reproduce
that
","
now
I
have
no
effect
from
that
one
line
change
.
What
I'm
trying
to
figure
out
is
what
to
do
to
the
QAction
so
that
the
toolbar
has
text
on
the
button
beside
the
image
","
or
can
it
be
done
at
all
like
this
?
Is
there
some
other
way
to
make
the
toolbar
have
some
text
plus
an
icon
?
Variations
of
this
question
have
been
asked
before
.
The
short
answer
is
""""
do
a
return
""""
break
""""
in
the
code
executed
by
your
binding
.
I
gave
a
much
longer
answer
here
:
python
gui
events
out
of
order
I'm
using
a
Tkinter
Text
box
in
my
Python
(
using
2.7.2
)
script
as
an
entry-type
box
-
-
when
enter
is
pressed
","
it
copies
the
contents
into
a
different
text
box
and
then
deletes
it
out
of
the
entry
one
.
I've
bound
the
necessary
event
to
the
Text
box
when
the
Enter
key
is
pressed
.
The
only
problem
I
have
is
that
whenever
I
hit
the
Enter
key
","
it
seems
to
execute
my
event
and
then
the
widgets
""""
default
""""
binding
:
adding
a
newline
.
I'm
not
sure
of
a
way
to
either
delete
the
newline
after
it
is
added
","
or
simply
get
rid
of
the
widgets
default
binding
.
Thanks
a
lot
!
Returning
'
break
'
at
the
end
of
your
event
handler
will
interrupt
event
propagation
.
I
tried
a
similar
approach
myself
","
and
couldn't
get
it
working
.
I'm
not
sure
about
the
LWPCookieJar
","
but
you
can
get
persistent
cookie
support
with
pywebkitgtk
""""
natively
""""
-
check
out
my
answer
to
python
webkit
webview
remember
cookies
?
I
am
trying
to
get
urllib2
to
work
with
PyWebKitGtk
to
support
cookies
.
I
think
it's
mostly
working
","
but
cookies
aren't
working
between
sessions
.
The
cookies.txt
file
is
saved
","
and
it
does
look
like
it
uses
the
cookies
in
the
requests
(
examined
in
Wireshark
)
","
but
the
data
I
am
seeing
loaded
into
the
browser
window
doesn't
appear
to
have
been
using
the
cookies
.
After
I
log
in
","
shut
down
the
app
","
then
restart
it
","
my
login
session
is
gone
.
My
code
Note
","
this
is
some
what
pseudo
but
the
code
will
probably
work
to
and
extent
of
99
%
:
)
I'd
try
going
with
a
simple
code
as
possible
:
(
I'm
not
sure
if
cj.save(...)
discards
cookies
between
sessions
so
i've
used
pickle
for
the
most
part
","
and
for
other
things
i
need
to
store
""""
as
is
""""
between
sessions
)
Secondly
","
are
you
SURE
that
the
cookie
you
are
getting
is
not
just
a
session
cookie
that's
supposed
to
end
after
a
certain
period
of
time
or
when
you
close
the
connection
?
You
know
","
not
one
of
those
""""
remember
me
""""
cookies
?
Try
setting
up
your
own
""""
webserver
""""
in
Python
:
See
what
your
output
is
in
actual
terms
of
HTTP
data
?
It's
always
nice
to
have
the
""""
before
""""
data
and
the
""""
after
""""
data
.
.
this
way
you'll
know
why
it
isn't
stored
/
loaded
.
Lets
say
i
have
2
lists
:
How
do
i
check
if
first
element
in
a
can
found
in
first
element
of
b
?
If
you
only
need
to
know
if
it's
in
the
string
or
not
:
However
","
the
above
has
the
problem
that
both
of
these
will
return
true
:
So
if
you
care
about
words
vs
.
simple
presence
","
as
long
as
your
delimiters
are
consistent
:
If
you
need
to
know
which
word
position
:
If
you
need
to
know
the
position
in
the
string
:
If
you
want
to
know
if
each
element
from
a
is
in
the
corresponding
element
of
b
(
ignores
extra
entries
on
either
list
)
:
Just
that
:
maybe
you
want
to
check
all
of
them
:
or
You
can
simply
do
:
which
will
return
a
True
if
the
first
element
of
a
can
be
found
in
the
first
element
of
b
","
otherwise
it
will
return
False
.
Python
is
actually
very
robust
.
You
can
just
do
.
In
my
Python
script
","
I
want
to
prevent
certain
stdlib
modules
","
such
as
os
and
sys
","
from
being
imported
.
How
would
I
accomplish
this
?
Don't
import them
.
More
generally
","
don't
execute
untrusted
code
inside
your
module
.
eval()
looks
spiffy
but
it's
almost
certainly
not
your
friend
.
If
you're
intent
on
sandboxing
external
code
","
look
at
the
SandboxedPython
article
on
the
Python
wiki
.
Until
you've
read
(
and
understood
)
everything
there
","
please
don't
try
it
.
Taking
you
very
literally
","
and
if
you
just
mean
""""
to
stub
them
out
so
that
they
won't
be
loaded
by
a
straight
import
""""
","
not
""""
make
them
unloadable
by
untrusted
code
""""
","
then
:
Of
course
","
there
is
no
module
system
so
you
might
have
meant
sys
","
in
which
case
you're
in
trouble
.
If
you're
trying
to
keep
untrusted
code
from
being
able
to
do
Bad
Things
","
then
take
a
look
at
http://wiki.python.org/moin/SandboxedPython
and
realise
that
you're
after
something
not
immediately
feasible
.
Yes
","
you
can
use
Python
from
the
command
line
.
python
-
c
<
stuff
>
will
run
<
stuff
>
as
Python
code
.
Example
:
There
isn't
a
direct
equivalent
to
the
-
p
option
for
Perl
(
the
automatic
input
/
output
line-by-line
processing
)
","
but
that's
mostly
because
Python
doesn't
use
the
same
concept
of
$_
and
whatnot
that
Perl
does
-
in
Python
","
all
input
and
output
is
done
manually
(
via
raw_input()
/
input()
","
and
print
/
print()
)
.
For
your
particular
example
:
(
Obviously
somewhat
more
unwieldy
.
It's
probably
better
to
just
write
the
script
to
do
it
in
actual
Python
.
)
You
can
use
:
You
can
in
theory
","
but
Python
doesn't
have
anywhere
near
as
much
regex
magic
that
Perl
does
","
so
the
resulting
command
will
be
much
more
unwieldy
","
especially
as
you
can't
use
regular
expressions
without
importing
re
(
and
you'll
probably
need
sys
for
sys.stdin
too
)
.
The
Python
equivalent
of
your
colleague's
Perl
one-liner
is
approximately
:
You
have
a
problem
which
can
be
solved
several
ways
.
I
think
you
should
consider
using
regular
expression
(
what
perl
is
doing
in
your
example
)
directly
from
Python
.
Regular
expressions
are
in
the
re
module
.
An
example
would
be
:
(
I
would
prefer
using
$
instead
of
'
\
n
'
for
line
endings
","
because
line
endings
are
different
between
operational
systems
and
file
encodings
)
If
you
want
to
call
bash
commands
from
inside
Python
","
you
could
use
:
Where
command
is
the
bash
command
.
I
use
it
all
the
time
","
because
some
operations
are
better
to
perform
in
bash
than
in
Python
.
Finally
","
if
you
want
to
extract
the
numbers
with
grep
","
use
the
-
o
option
","
which
prints
only
the
matched
part
.
You
can
use
python
to
execute
code
directly
from
your
bash
command
line
","
by
using
python
-
c
","
or
you
can
process
input
piped
to
stdin
using
sys.stdin
","
see
here
.
Perl
(
or
sed
)
is
more
convenient
.
However
it
is
possible
","
if
ugly
:
I
need
to
pick
some
numbers
out
of
some
text
files
.
I
can
pick
out
the
lines
I
need
with
grep
","
but
didn't
know
how
to
extract
the
numbers
from
the
lines
.
A
colleague
showed
me
how
to
do
this
from
bash
with
perl
:
However
","
I
usually
code
in
Python
","
not
Perl
.
So
my
question
is
","
could
I
have
used
Python
in
the
same
way
?
I.e
.
","
could
I
have
piped
something
from
bash
to
Python
and
then
gotten
the
result
straight
to
stdout
?
...
if
that
makes
sense
.
Or
is
Perl
just
more
convenient
in
this
case
?
There
are
a
lot
more
features
overall
","
but
I
think
one
major
reason
people
use
nose
/
djano_nose
is
that
it
allows
you
to
very
easily
do
code
coverage
.
I've
been
seeing
and
reading
about
a
lot
of
people
using
nose
to
run
their
Django
tests
.
I
haven't
been
able
to
figure
out
the
added
benefits
of
using
Nose
to
run
my
Django
tests
.
If
someone
could
fill
me
in
on
what
nose
is
and
how
it
adds
more
to
a
Django
project
","
it
would
be
helpful
.
I
haven't
been
able
to
find
a
good
document
/
article
outlining
these
points
.
Thank
you
I
was
curious
about
this
too
and
it
seems
that
the
main
advantage
of
django-nose
using
the
python
nose
library
is
""""
Test
Discovery
""""
.
In
addition
","
from
http://readthedocs.org/docs/nose/en/latest/testing.html
you
can
also
write
simple
test
functions
","
as
well
as
test
classes
that
are
not
subclasses
of
unittest.TestCase
.
nose
also
supplies
a
number
of
helpful
functions
for
writing
timed
tests
","
testing
for
exceptions
","
and
other
common
use
cases
.
See
Writing
tests
and
Testing
tools
for
more
.
From
what
I
understand
from
other
python
developers
on
freenode
irc
","
Trial
test
runner
on
Twisted
Framework
have
these
similar
features
like
nose
.
I
am
still
not
entirely
convinced
about
using
django-nose
for
django
development
but
am
giving
a
shot
and
report
back
if
I
find
out
more
!
I
don't
see
any
reason
why
you
wouldn't
be
able
to
do
the
following
at
the
end
of
your
first
program
:
As
per
what
you
describe
","
your
text
variable
should
contain
all
the
text
","
and
your
regexp
should
then
be
able
to
filter
out
the
necessary
parts
from
that
.
Both
programs
are
reading
the
same
XML
file
.
First
program
copies
all
data
between
<
text
>
<
/
text
>
tags
.
And
second
program
copies
limited
data
from
<
text
>
<
/
text
>
tags
.
I
want
to
only
limited
data
.
So
is
it
possible
to
use
this
statement
in
first
program
:
First
Program
Second
Program
UPDATE
:
please
help
me
.
Is
there
any
other
alternative
?
thousands
of
threads
can
kill
the
server
(
correct
me
if
I'm
wrong
","
but
is
it
due
to
GIL
?
)
For
one
thing
","
GIL
has
nothing
to
do
with
no
.
of
threads
.
If
you're
are
doing
IO
within
these
threads
","
you
could
have
hundreds
of
thousands
of
these
threads
without
any
problem
from
GIL
or
otherwise
.
GIL
comes
into
play
when
you
have
CPU
intensive
tasks
.
See
this
very
informative
talk
from
David
Beazly
to
know
more
about
GIL
.
I've
just
begun
learning
sockets
with
Python
.
So
I've
written
some
examples
of
chat
servers
and
clients
.
Most
of
what
I've
seen
on
the
internet
seems
to
use
threading
module
for
(
asynchronous
)
handling
of
clients
'
connections
to
server
.
I
do
understand
that
for
scalable
server
you
need
to
use
some
additional
tricks
","
because
thousands
of
threads
can
kill
the
server
(
correct
me
if
I'm
wrong
","
but
is
it
due
to
GIL
?
)
","
but
that's
not
my
concern
at
the
moment
.
The
strange
thing
is
that
I've
found
somewhere
in
Python
documentation
that
creating
subprocesses
is
the
right
way
(
unfortunelty
I've
lost
the
reference
","
sorry
:
(
)
for
handling
sockets
.
So
the
question
is
:
to
use
threading
or
multiprocessing
?
Or
is
there
even
better
solution
?
Please
","
give
me
the
answer
and
explain
the
difference
to
me
.
By
the
way
:
I
do
know
that
there
are
things
like
Twisted
which
are
well-written
.
I'm
not
looking
for
pre-made
scalable
server
","
I
am
instead
trying
to
understand
how
to
write
one
that
can
be
scaled
or
will
deal
with
at
least
10k
clients
.
EDIT
:
The
operating
system
is
Linux
.
Facebook
needed
a
scalable
server
so
they
wrote
Tornado
(
which
uses
async
)
.
Twisted
is
also
famously
scalable
(
it
also
uses
async
)
.
Gunicorn
is
also
a
top
performer
(
it
uses
multiple
processes
)
.
None
of
the
fast
","
scalable
tools
that
I
know
about
uses
threading
.
An
easy
way
to
experiment
with
the
different
approaches
is
to
start
with
the
SocketServer
module
in
the
standard
library
:
http://docs.python.org/library/socketserver.html
.
It
lets
you
easily
switch
approaches
by
alternately
inheriting
from
either
ThreadingMixin
or
ForkingMixin
.
Also
","
if
you're
interested
in
learning
about
the
async
approach
","
the
easiest
way
to
build
your
understanding
is
to
read
a
blog
post
discussing
the
implementation
of
Tornado
:
http://golubenco.org/2009/09/19/understanding-the-code-inside-tornado-the-asynchronous-web-server-powering-friendfeed/
Good
luck
and
happy
computing
:
-
)
Django
can
handle
the
unique
filename
correctly
.
The
duplicate
file
name
will
be
renamed
automatically
.
If
you
want
to
set
the
filename
manually
","
just
define
upload_to
function
as
DrTyrsa
said
.
This
question
may
help
you
.
You
need
to
define
upload_to
function
.
I
am
creating
a
site
where
users
can
upload
images
.
I
need
to
make
sure
that
each
filename
has
a
unique
name
to
prevent
the
files
from
overwriting
each
other
.
I
will
generate
the
unique
name
.
But
how
do
I
change
the
filename
before
saving
the
file
?
I
see
that
there
are
ways
to
change
the
folder
that
it
is
saved
to
","
but
that's
not
quite
what
I'm
after
.
In
my
code
I
do
:
What
I
need
is
for
the
actual
name
of
the
saved
file
to
be
new_name
.
Too
much
work
.
Note
that
this
will
fail
for
numbers
less
than
1
.
You
can
do
this
I'm
trying
to
get
the
user
to
input
a
birth
date
and
then
add
the
individual
ints
in
those
numbers
.
Also
","
if
a
sum
of
any
of
these
digits
is
greater
than
or
equal
to
10
","
the
loop
repeats
and
the
process
runs
again
for
the
value
.
Here's
my
code
so
far
This
works
however
I
think
it
would
be
better
done
as
a
loop
.
And
if
there's
some
way
I
won't
have
to
use
something
like
sumYear2
that
would
be
great
.
Note
","
I
don't
think
I
can
use
the
sum()
function
.
Thanks
guys
for
the
help
.
I'm
having
an
issue
though
.
I'm
not
sure
why
this
code
isn't
being
evaluated
when
I
provide
the
month
as
02
and
the
day
as
30
@Ignacio
Vazquez-Abrams's
answer
provides
the
formula
.
But
if
there
were
none
then
your
code
as
a
loop
without
using
sumYear2
could
look
like
:
If
you're
not
allowed
to
use
sum
(
a
homework
)
then
:
For
the
second
question
assuming
Python
3
:
If
you
are
not
allowed
to
use
try
/
except
then
:
Where
get_int()
:
And
number_of_days_in_month()
:
How
do
I
find
the
program
associated
with
a
given
file
type
using
Python
under
Windows
?
For
example
","
given
http
I'd
like
python
to
determine
the
default
browser
.
I
can
do
this
from
a
cmd
prompt
by
running
'
ftype
http'.
Windows
gives
me
the
full
path
to
the
browser
.
I've
tried
:
which
results
in
:
WindowsError
:
[
Error
2
]
The
system
cannot
find
the
file
specified
which
launches
a
cmd
window
","
but
doesn't
do
anything
else
Try
this
:
You're
missing
the
'
/
c
'
option
that
configures
cmd
to
run
a
command
and
then
exit
:
How
can
I
get
the
HTML
source
in
a
variable
using
the
Selenium
module
with
Python
?
I
wanted
to
do
something
like
this
:
How
can
I
do
this
?
I
don't
know
how
to
access
the
HTML
source
.
By
using
the
page
source
you
will
get
the
whole
HTML
code
.
So
first
decide
the
block
of
code
or
tag
in
which
you
require
to
retrieve
the
data
or
to
click
the
element
.
.
You
can
find
the
elements
by
name
","
XPath
","
id
","
link
and
CSS
path
.
With
Selenium2Library
you
can
use
get_source()
To
simply
download
the
HTM
code
of
a
page
you
can
use
this
:
If
the
source
it's
some
kind
of
XML
file
you
can
use
this
other
co
encode
and
/
or
replace
some
part
of
the
code
:
I'd
recommend
getting
the
source
with
urllib
and
","
if
you're
going
to
parse
","
use
something
like
Beautiful
Soup
.
To
answer
your
question
about
getting
the
URL
to
use
for
urllib
","
just
execute
this
JavaScript
code
:
You
need
to
call
the
page_source
property
.
See
below
.
%
has
higher
precedence
than
","
.
But
now
you'll
get
a
different
error
since
you
have
more
values
than
placeholders
.
You're
using
the
%
operator
incorrectly
.
You
might
be
looking
for
something
like
this
:
Notice
that
the
substitution
variables
x
","
y
","
z
are
all
in
parentheses
","
which
means
they
are
a
single
tuple
passed
to
the
%
operator
.
In
your
code
","
notice
how
you
have
four
parameters
to
the
write()
function
(
I've
put
each
parameter
on
a
separate
line
to
make
it
easier
to
see
)
:
I
would
like
to
write
this
to
a
file
"f.write(""add unit at-wc 0 0 0 %s"" % x ,y, z, ""0.000 0.000 0.000 "")"
but
when
I
do
that
I
get
an
error
saying
function
takes
exactly
1
argument
(
4
given
)
.
You
implemented
a
naive
matrix
multiplication
algorithm
","
which
scipy.weave
compiles
to
fast
machine
code
.
However
","
there
are
non-obvious
","
more
CPU
cache
efficient
algorithms
for
matrix
multiplication
(
which
usually
split
the
matrix
into
blocks
and
deal
with
those
)
","
and
additional
speed
can
be
gained
with
CPU-specific
optimizations
.
Numpy
by
default
uses
an
optimized
BLAS
library
for
this
operation
","
if
you
have
one
installed
.
These
libraries
will
likely
be
fast
compared
to
anything
you
can
code
up
yourself
without
doing
an
amount
of
research
.
I
am
a
Python
novice
who
is
trying
to
learn
a
bit
about
this
fantastic
programming
language
.
I
have
tried
using
scipy.weave.inline
to
speed
up
some
computation
.
Just
to
learn
a
bit
","
I
tried
to
implement
a
matrix
multiplication
using
scipy.weave.inline
.
I
have
not
included
any
error
handling
-
just
trying
it
out
to
better
understand
it
.
The
code
is
as
follows
:
When
I
compare
with
numpy.dot
","
I
experience
that
the
weave.inline
version
takes
roughly
50x
the
time
as
numpy.dot
.
I
know
that
numpy
is
very
fast
when
it
can
be
applied
.
The
difference
is
even
seen
for
large
matrices
such
as
size
1000
x
1000
.
I
have
checked
both
numpy.dot
and
scipy.weave.inline
and
both
appear
to
use
one
core
100
%
when
computing
.
Numpy.dot
delivers
10.0
GFlops
compared
to
the
theoretical
11.6
GFlops
of
my
laptop
(
double
precision
)
.
In
single
precision
I
measure
the
double
performance
as
expected
.
But
the
scipy.weave.inline
is
way
behind
.
1
/
50
times
this
performance
for
scipy.weave.inline
.
Is
this
difference
to
be
expected
?
Or
what
am
I
doing
wrong
?
you
can
do
:
but
I
think
it
just
makes
things
less
readable
.
However
","
before
summing
you
are
actually
building
the
list
of
multiplication
sum([...])
.
This
is
not
needed
","
simply
do
:
I
don't
see
any
problem
with
using
indexes
here
:
If
you
really
want
to
avoid
them
","
you
can
do
:
But
","
to
be
honest
I
would
prefer
your
commented
out
version
.
It
is
clear
","
readable
and
more
explicit
.
And
you
don't
really
gain
much
by
writing
it
as
above
just
for
three
variables
.
Is
there
a
function
I
can
use
that
acts
like
sum()
","
but
only
for
multiplication
?
Built-in
?
No
.
But
you
can
get
that
functionality
rather
simply
with
the
following
:
Let's
say
I
have
a
What's
everyone's
take
on
line
12
?
I've
heard
that
explicit
list
index
references
should
be
avoided
.
Is
there
a
function
I
can
use
that
acts
like
sum()
","
but
only
for
multiplication
?
Thanks
for
the
help
everyone
.
Alternatively
you
can
use
'
np.prod
'
can
take
both
lists
and
tuple
as
a
parameter
.
It
returns
the
product
you
want
.
I
did
make
a
very
simple
definition
of
product
;
helpful
for
""""
calculating
the
product
of
a
tuple
""""
Might
be
a
more
elegant
way
to
do
it
but
this
seems
to
work
OK
.
Presumably
it
would
work
on
a
list
just
as
well
.
I
have
a
script
which
reads
the
input
and
than
lists
it
","
however
i
want
it
to
convert
upper
case
letters
to
lower
case
","
how
can
i
do
that
?
this
is
what
i
got
To
convert
a
string
to
lower
case
in
Python
","
use
something
like
this
.
I
found
this
in
the
first
result
after
searching
for
""""
python
upper
to
lower
case
""""
.
str.lower()
converts
all
cased
characters
to
lowercase
.
You
can
find
more
methods
and
functions
related
to
Python
strings
in
section
5.6.1
.
String
Methods
of
the
documentation
.
The
following
code
gives
me
an
error
.
This
is
the
error
.
Traceback
(
most
recent
call
last
)
:
File
""""
/
Volumes
/
CHROME
USB
/
STORAGE
/
TKinker
GUI
/
easygui
inputbox.py
""""
","
line
2
","
in
result
=
"easygui.enterbox(message=""Enter your name"", title=""Name query"", argDefaultText=""Ian Ozsvald"")"
TypeError
:
enterbox()
got
an
unexpected
keyword
argument
'
message
'
I'm
running
Python
2.5
on
OS
X
Snow
Leopard
.
easygui.enterbox
has
no
argument
message
","
nor
argDefaultText
.
What
you
want
is
:
The
signature
of
the
enterbox
function
can
be
found
here
:
http://easygui.sourceforge.net/download/version0.95/pydoc/easygui.html#-enterbox
alternate
link
http://www.ferg.org/easygui/easygui.html#-enterbox
Assuming
that
on
any
line
you
only
want
to
rearrange
the
first
22
words
:
You
can
use
sequence
slicing
to
select
only
the
part
you
want
to
rearrange
:
Use
array
slices
:
Python
2.4.x
I
have
a
series
of
lines
that
I'm
reading
in
-
rearranging
and
then
making
each
line
into
a
list
which
is
nested
inside
another
list
.
I've
got
the
following
.
In
other
words
-
if
the
rec
being
passed
is
.
then
the
list
for
this
line
would
be
.
(
after
rearranging
)
.
(
this
list
would
get
appended
to
another
list
-
nested
lists
)
.
Which
it
does
fantastically
well
.
However
what
I'd
like
to
do
is
take
the
last
6
and
group
them
together
like
this
.
And
if
the
line
being
passed
does
not
have
28
elements
but
say
only
25
-
than
it
just
groups
whatever
beyond
the
22nd
element
-
(
the
text
is
not
the
same
line
after
line
)
.
i.e
the
above
example
has
28
elements
","
this
one
has
25
.
Lorem
ipsum
dolor
sit
amet
consectetur
adipiscing
elit
In
vitae
neque
nec
magna
tristique
ornare
Cras
faucibus
risus
eu
odio
pharetra
interdum
Nunc
dui
mi
and
the
resulting
list
would
be
:
Hope
that's
clear
-
any
ideas
?
Thank
you
.
If
you
supply
the
optional
second
argument
to
str.split
","
you
can
control
how
many
splits
are
performed
:
yields
while
yields
I've
a
dictionary
(
dict
type
)
which
I
would
store
in
a
gtk.ListStore
.
Is
there
a
way
to
do
this
?
(
Which
column
type
is
good
for
that
?
)
or
do
I
have
to
add
as
many
columns
in
the
gtk.ListStore
as
there
are
elements
in
the
dictionary
?
Thanks
You
might
want
to
look
at
the
DictionaryGrid
quickly
widget
.
See
here
for
it's
usage
.
The
DictionaryGrid
might
exactly
do
what
you
need
or
you
can
copy
parts
of
it
.
I
have
a
ttk.Treeview
widget
with
some
rows
of
data
.
How
do
I
set
the
focus
to
and
select
(
highlight
)
a
specified
item
?
does
nothing
complains
that
:
Item
0
not
found
","
although
the
widget
is
plainly
populated
with
more
than
zero
items
.
Trying
item
1
does
no
better
.
EDIT
:
to
select
an
item
","
find
its
id
","
then
use
tree.selection_set(id)
.
Neither
tree.focus(id)
nor
tree.focus_set(id)
appears
to
do
anything
.
Come
across
this
question
when
I'm
looking
to
solve
the
exact
same
problem
.
Found
out
this
:
tree.selection_set(item)
highlights
the
item
tree.focus(item)
or
tree.focus_set(item)
selects
the
item
Note
:
I
haven't
worked
with
python
.
Looking
at
this
link
","
the
focus
method
with
optional
parameter
item
","
should
highlight
the
node
.
If
not
","
look
at
selectmode
option
&
set
it
to
""""
browse
""""
.
This
StackOverflow
link
might
be
of
use
:
Practicing
BDD
with
python
Here's
a
link
to
the
mentioned
doctest
.
I
am
developing
a
very
simple
web
application
in
python
using
the
bottle
microframework
and
I
was
wondering
if
there
are
frameworks
that
can
rigorously
aid
in
testing
the
application
.
One
solution
might
be
using
a
dictionary
:
And
the
file.txt
:
So
","
if
you
want
to
see
the
variable
value
you
just
do
:
A
simple
way
to
do
it
is
to
load
the
content
of
your
file
and
then
use
execfile(file_content)
","
then
you
will
be
able
to
make
print
x
and
it
will
print
the
content
of
x
.
I
have
a
text
file
","
xyz.txt
that
has
variables
","
and
I
want
to
use
those
variables
","
I
know
how
to
read
them
but
do
not
know
how
to
use
it
and
call
them
when
I
need
it
.
For
example
","
If
in
the
text
file
there
is
x
=
123
I
would
like
to
be
able
to
call
on
that
variable
for
use
","
so
if
i
said
print
x
it
would
give
me
123
.
text
file
:
Later
on
It
will
have
more
data
.
You
could
call
pack_forget()
to
hide
widgets
","
and
(
later
)
pack
to
show
them
again
:
Tkinter
is
singled-threaded
","
and
mainloop
runs
the
main
event
loop
.
Therefore
you
shouldn't
call
mainloop
twice
.
I'm
trying
to
make
my
first
GUI
program
.
The
problem
is
","
that
I
can't
figure
out
how
to
make
a
main
menu
","
which
would
switch
to
one
of
the
programs
after
clicking
a
button
.
I
know
that
I'm
doing
it
wrong
.
I
just
have
no
idea
how
to
do
it
","
So
i
gave
it
a
try
of
how
could
it
be
.
Please
help
me
.
I
have
this
simple
structure
:
...
and
I
am
going
nuts
trying
to
isolate
each
number
in
the
lists
(
eg
.
0
","
148
","
149
)
if
I
do
any
kind
of
loop
I
get
:
What
do
I
need
to
do
?
Thanks
!
BTW
Python
2.6
Next
time
","
please
show
the
code
that
fails
.
I'm
going
to
guess
you
do
something
like
:
The
problem
is
that
if
you
iterate
over
o
","
you
get
the
keys
not
the
values
.
What
you
want
is
:
Items
returns
a
list
of
the
keys
and
values
as
tuples
and
you
can
iterate
over
that
.
The
for-loop
iterates
over
keys
which
you
can
then
use
to
lookup
the
corresponding
sequences
:
I
have
some
ancient
code
(
5
years
old
)
and
how
I
used
to
access
real_fft()
method
was
this
:
I
guess
the
FFT
module
came
with
NumPy
.
Now
","
years
later
I
installed
the
NumPy
1.6.1
with
And
all
I
see
in
the
docs
"http://www.scipy.org/Numpy_Functions_by_Category,"
are
these
functions
:
fft()
fftfreq()
fftshift()
ifft()
It
is
strange
","
because
in
this
numpy
docs
","
real_fft()
is
there
:
http://numpy.sourceforge.net/numdoc/HTML/numdoc.htm#pgfId-304711
It
looks
like
NumPy
underwent
some
reorganisation
in
recent
years
.
http://www.scipy.org/Numpy_Example_List_With_Doc#fft
FFT
is
now
numpy.fft
","
and
real_fft()
seems
to
be
renamed
into
rfft()
I've
got
a
dozen
programs
that
can
accept
input
via
stdin
or
an
option
","
and
I'd
like
to
implement
the
same
features
in
a
similar
way
for
the
output
.
The
optparse
code
looks
like
this
:
The
rest
of
the
applicable
code
looks
like
this
:
This
code
works
fine
and
I
like
its
simplicity
-
but
I
haven't
been
able
to
find
a
reference
to
anyone
using
a
default
value
of
'
-
'
for
output
to
indicate
stdout
.
Is
this
a
good
consistent
solution
or
am
I
overlooking
something
better
or
more
expected
?
If
you
can
use
argparse
(
i.e.
Python
2.7
+
)
","
it
has
built-in
support
for
what
you
want
:
straight
from
argparse
doc
The
FileType
factory
creates
objects
that
can
be
passed
to
the
type
argument
of
ArgumentParser.add_argument()
.
Arguments
that
have
FileType
objects
as
their
type
will
open
command-line
arguments
[
…
]
FileType
objects
understand
the
pseudo-argument
'
-
'
and
automatically
convert
this
into
sys.stdin
for
readable
FileType
objects
and
sys.stdout
for
writable
FileType
objects
.
So
my
advice
is
to
simply
use
Then
you
can
do
To
read
from
file
and
output
to
output
","
or
to
read
""""
Ni
!
""""
and
output
to
output
","
or
…
And
it's
good
","
since
using
-
for
the
relevant
stream
is
a
convention
that
a
lot
of
programs
use
.
If
you
want
to
point
it
out
","
add
it
to
the
help
strings
.
For
reference
","
the
usage
message
will
be
For
input
files
you
could
use
fileinput
module
.
It
follows
common
convention
for
input
files
:
if
no
files
given
or
filename
is
'
-
'
it
reads
stdin
","
otherwise
it
reads
from
files
given
at
a
command-line
.
There
is
no
need
in
-
f
and
-
-
file
options
.
If
your
program
always
requires
an
input
file
then
it
is
not
an
option
.
-
o
and
-
-
output
is
used
to
specify
the
output
file
name
in
various
programs
.
optparse
argparse
argparse
module
allows
you
to
specify
explicitly
files
as
arguments
:
Note
:
I've
added
-
-
inplace
option
in
the
second
example
:
That
is
indeed
CSS
.
You're
getting
a
document
like
this
:
You
need
to
remove
all
style
tags
before
parsing
out
the
text
.
I
am
using
lxml
to
convert
html
into
txt
.
I
almost
get
to
where
I
wanted
with
parsing
","
converting
and
some
parts
of
the
cleanup
(
tabs
","
spaces
","
empty
lines
)
functions
ready
and
a
program
up
and
running
.
However
","
after
I
tried
my
code
with
about
a
hundred
htmls
(
all
from
different
sites
)
","
I
noticed
some
exceptions
","
i.e.
lines
like
:
I
assume
these
are
CSS
?
or
other
web
programming
things
.
But
I
am
totally
unfamiliar
with
these
.
Questions
:
What
are
these
lines
?
And
any
suggestions
for
how
to
get
ride
of
these
lines
?
Edit
:
Here
is
how
I
did
the
parts
before
this
question
for
reference
for
anyone
who
drops
into
this
post
in
the
future
(
new
to
python
","
a
lot
of
things
here
can
be
improved
","
but
it
works
ok
for
me
)
:
Im
trying
to
make
a
regular
expression
in
python
that
allows
me
to
find
a
word
within
a
string
""""
n
""""
times
For
example
","
if
i
wanted
to
find
a
expression
that
could
match
if
the
word
""""
cat
""""
is
exactly
two
times
.
How
i
would
do
that
?
It
should
accept
""""
The
blue
cat
talks
to
the
red
cat
in
the
tree
""""
.
Because
it
has
""""
cat
""""
exactly
two
times
.
But
it
should
not
accept
""""
The
cat
is
big
""""
.
Because
it
has
""""
cat
""""
only
once
And
it
should
not
accept
either
""""
the
dog
is
yellow
""""
.
For
similar
reasons
Thanks
a
lot
EDIT
Hey
guys
Sorry
for
complicating
the
problem
too
much
","
but
i
forgot
to
mention
one
thing
.
If
i
wanted
to
find
""""
cat
""""
exactly
two
times
","
""""
The
catcat
runs
""""
would
also
match
findall
+
len
seems
like
one
solution
.
If
you
wish
to
use
a
single
regular
expression
to
ensure
a
string
contains
exactly
2
instances
of
the
word
""""
cat
""""
","
(
no
more
","
no
less
","
and
not
""""
catastrophic
""""
or
""""
catcat
""""
)
","
then
the
following
tested
script
will
do
the
trick
:
However
","
if
you
do
wish
to
match
the
cat
in
""""
catastrophic
""""
and
""""
catcat
""""
","
then
remove
all
the
\
b
word
boundary
anchors
from
the
regex
.
Don't
use
regular
expressions
just
because
they
are
there
.
As
Vincent
points
out
","
that
assumes
all
words
are
seperated
by
whitespace
.
Is
probably
a
better
options
.
Although
whether
that
is
neccesary
depends
on
details
not
provided
in
your
post
.
EDIT
If
you
don't
even
care
about
word
boundaries
","
there
is
even
less
reason
to
be
using
a
regular
expression
.
How
about
this
:
The
{
2
}
means
“
repeat
2
times
”
.
Use
{
7
}
for
7
repetitions
.
The
\
b
is
a
word
boundary
;
in
this
case
the
cat
in
""""
blue
cat
talks
""""
would
match
","
but
""""
verification
""""
wouldn't
.
And
the
.
*
will
match
any
string
.
You
might
want
to
go
over
the
re
documentation
.
Just
build
a
regex
with
multiple
instance
of
'
cat
'
separated
by
a
group
that
consumes
other
characters
:
I
am
trying
to
follow
an
example
from
an
online
tutorial
regarding
basic
client-server
socket
programming
using
standard
Python
libraries
(
version
2.7
)
","
but
I
cannot
get
the
example
to
work
under
Windows
(
Vista
)
.
It
works
fine
in
Ubuntu
11.10
","
so
I
know
that
the
following
code
at
least
works
in
a
UNIX-based
environment
:
The
program
stops
responding
as
soon
as
the
socket
calls
accept()
in
the
serve()
function
","
and
none
of
the
requested
data
is
received
as
far
as
I
can
tell
.
What
am
I
overlooking
regarding
Windows
'
handling
of
sockets
?
It
looks
like
you
may
need
to
have
it
running
in
2
different
processes
or
split
the
client
and
server
into
2
different
threads
.
I
got
this
to
work
on
my
Win7
box
just
now
:
client.py
server.py
:
Results
:
C:\Users\jon\Desktop>python.exe
client.py
String
to
send
:
""""
Hello
!
""""
Separate
Console
C:\Users\jon\Desktop>python
server.py
Hello
!
More
good
info
on
the
Winsock
Programmerâ€™s
FAQ
The
script
your
using
hangs
at
sock
","
addr
=
listen_socket.accept()
(
until
a
timeout
is
reached
if
one
is
set
with
sock.settimeout(x)
)
.
If
using
sock.setblocking(0)
","
the
exception
socket.error
:
[
Errno
10035
]
A
non-blocking
socket
operation
could
not
be
completed
immediately
is
thrown
at
the
accept()
line
and
this
is
what
happens
with
a
timeout
:
Results
in
:
I'd
like
to
get
the
hostname
in
Google
App
Engine
when
running
dev_appserver.py
","
because
GAE
is
making
a
request
of
an
external
server
and
that
server
needs
to
know
where
to
send
its
response
.
Unfortunately
","
the
canonical
Python
way
doesn't
seem
to
work
","
being
:
The
socket
library
doesn't
work
in
GAE
.
Also
","
Are
both
localhost
.
How
could
one
get
the
IP
that
an
external
server
would
connect
to
GAE
?
Thanks
for
reading
the
socket
package
might
be
disabled
in
the
GAE
sandbox
.
Have
you
tried
using
the
request
object
to
determine
this
?
http://docs.webob.org/en/latest/reference.html#id1
might
be
the
most
reliable
way
.
I
am
developing
a
Python
model
that
will
support
graphing
if
the
correct
modules
are
installed
.
I
would
like
the
source
code
to
be
the
same
if
possible
","
IE
","
if
the
graphing
model
can't
load
","
graphing
would
be
ignored
from
the
menu
logic
.
How
can
I
accomplish
this
?
Yes
.
You
can
wrap
an
import statement
in
a
try-except
block
.
It
is
commonly
used
for
backwards-compatability
cruft
.
For
instance
","
by
importing
a
fall-back
module
as
the
desired
module
.
That
way
the
rest
of
the
code
can
be
oblivious
to
which
module
is
actually
in
use
.
Instead
of
a
flag
as
suggested
by
@Raymond
Hettinger
you
could
set
to
None
the
actual
name
that
provides
optional
capabilities
:
Attempt
an
import and
set
a
flag
if
fails
.
Then
use
the
flag
to
determine
whether
to
offer
graphic
output
:
I'm
parsing
an
XML
file
with
some
coordinates
in
Python
to
write
a
transformed
output
file
.
The
problem
is
that
some
coordinates
are
-
0.00
and
I'm
having
some
problems
parsing
them
in
another
system
.
I
would
need
them
to
be
0.00
instead
of
-
0.00
.
How
could
I
achieve
such
thing
?
This
is
what
I'm
doing
so
far
:
You
can
use
the
fact
that
the
negative
zero
compares
equal
to
the
positive
zero
:
This
turns
-
0
.
to
0
.
","
and
leaves
every
other
number
intact
.
You
don't
need
abs()
.
Maybe
you
could
split
the
string
before
parsing
it
as
a
number
?
Just
remove
the
""""
-
""""
from
the
input
.
If
the
value
is
equal
to
zero
(
either
positive
or
negative
)
","
take
the
absolute
value
:
not
enough
reputation
to
comment
yet
apparently
","
but
tim
stone's
answer
is
correct
.
suppose
we
have
a
function
like
this
:
now
in
order
to
perform
type
inference
","
there
has
to
be
at
least
one
call
to
blah
","
or
it
becomes
impossible
to
know
the
types
of
the
arguments
at
compile-time
.
for
a
stand-alone
program
","
this
not
a
problem
","
since
everything
that
has
to
be
compiled
for
it
to
run
is
called
indirectly
from
somewhere
.
.
for
an
extension
module
","
calls
can
come
from
the
'
outside
'
","
so
sometimes
we
have
to
add
a
'
fake
'
call
to
a
function
for
type
inference
to
become
possible
.
.
hence
the
'
if
False
'
.
in
the
shedskin
example
set
there
are
a
few
programs
that
are
compiled
as
extension
modules
","
in
order
to
be
combined
with
for
example
pygame
or
multiprocessing
.
While
browsing
some
code
","
I
came
across
this
line
:
I
understand
that
Shedskin
is
a
kind
of
Python
->
C
+
+
compiler
","
but
I
can't
understand
that
line
.
Shouldn't
if
False
:
never
execute
?
What's
going
on
here
?
For
context
:
This
is
the
whole
block
:
More
context
is
on
Google
Code
(
scroll
down
all
the
way
)
.
It
will
never
get
executed
.
It's
one
way
to
temporarily
disable
part
of
the
code
.
It
won't
execute
","
because
it
isn't
supposed
to
.
The
if
False
:
is
there
to
intentionally
prevent
the
next
line
from
executing
","
because
that
code's
only
purpose
is
seemingly
to
help
Shed
Skin
infer
type
information
about
the
argument
to
the
AStar()
function
.
You
can
see
another
example
of
this
in
httplib:
You
are
correct
in
assuming
that
this
will
never
evaluate
to
true
.
This
is
sometimes
done
when
the
programmer
has
a
lot
of
debugging
code
but
does
not
want
to
remove
the
debugging
code
in
a
release
","
so
they
just
put
if
False
:
above
it
all
.
Theoretically
","
it
could
get
executed
:
But
typically
this
will
be
used
to
temporarily
disable
a
code
path
.
I
am
trying
to
write
a
small
python
script
to
rename
a
bunch
of
filenames
by
searching
and
replacing
.
For
example
:
Original
filename
:
MyMusic.Songname.Artist-mp3.iTunes.mp3
Intendet
Result
:
Songname.Artist.mp3
what
i've
got
so
far
is
:
(
got
it
from
this
site
as
far
as
i
can
remember
)
Anyway
","
this
will
only
get
rid
of
the
String
at
the
beginning
","
but
not
of
those
in
the
filename
.
Also
I
would
like
to
maybe
use
a
seperate
file
(
eg
badwords.txt
)
containing
all
the
strings
that
should
be
searched
for
and
replaced
","
so
that
i
can
update
them
without
having
to
edit
the
whole
code
.
I
have
been
searching
for
quite
some
time
now
but
havent
found
anything
.
Would
appreciate
any
help
!
Thank
you
!
Note
that
it
is
possible
for
some
files
to
be
overwritten
(
and
thus
lost
)
if
two
names
get
reduced
to
the
same
shortened
name
after
badwords
have
been
removed
.
A
set
of
new
fnames
could
be
kept
and
checked
before
calling
os.rename
to
prevent
losing
data
through
name
collisions
.
fnmatch.translate
takes
shell-style
patterns
and
returns
the
equivalent
regular
expression
.
It
is
used
above
to
convert
badwords
(
e.g
.
'
.
iTunes
'
)
into
regular
expressions
(
e.g
.
r'\.iTunes
'
)
.
Your
badwords
list
seems
to
indicate
you
want
to
ignore
case
.
You
could
ignore
case
by
adding
'
(
?
i
)
'
to
the
beginning
of
pat
:
Here's
another
variant
that
works
both
in
Python
2.x
and
3.x
:
Another
way
of
making
Rock
","
Paper
","
Scissors
but
without
looping
is
this
...
If
that
is
any
help
.
This
code
might
be
a
good
reference
for
you
.
:
)
Good
Luck
!
Note
that
this
is
Py2.x
code
I
answered
your
question
separately
","
but
just
for
fun
here's
a
little
working
Rock
","
Paper
","
Scissors
game
to
look
at
.
This
one
is
for
Python
2.x
and
probably
won't
work
in
Python
3
","
but
it
might
be
helpful
for
you
or
somebody
in
the
future
searching
for
this
.
A
couple
quick
notes
from
quickly
skimming
the
code
:
In
get_score()
you
could
add
an
else
clause
to
handle
any
ties
that
happen
and
you
wouldn't
have
to
check
for
it
explicitly
in
play_game()
Move
the
import random
to
the
top
of
the
file
.
imports
are
generally
always
found
at
the
top
of
the
file
.
Also
","
there's
no
need
to
re-import
every
time
you
want
a
random
number
.
Not
sure
if
this
is
a
typo
","
cause
play
seems
to
always
hold
an
integer
","
but
you
have
play
=
=
True
and
play
=
=
False
inside
play_game()
.
If
you
want
to
make
play
contain
either
True
or
False
","
you
need
to
be
using
a
single
equals
sign
","
eg
","
play
=
True
.
But
this
doesn't
seem
to
make
sense
because
you're
comparing
play
to
computer
as
if
they're
integers
.
Also
","
what
are
you
trying
to
accomplish
with
the
score
variable
in
the
get_score()
method
?
Ah
","
if
you
made
the
get_score()
method
return
something
so
you
know
who
won
the
match
it
would
be
helpful
.
You
can't
access
computer_wins
or
player_wins
inside
the
get_score()
method
because
they
were
defined
inside
main()
.
A
simple
way
to
do
this
is
return
an
int
from
get_score()
.
here
is
a
rather
C-style
way
of
handling
it
(
returning
-
1
/
0
/
1
)
.
something
like
(
pseudo
code
)
:
Yet
another
way
","
adding
Lizard
and
Spock
Output
:
Here
is
another
way
to
do
it
:
There's
so
much
wrong
with
this
","
it's
hard
to
know
where
to
start
(
but
don't
get
discouraged
)
...
First
of
all
","
it
looks
like
(
mostly
from
your
use
of
input
vs
.
raw_input
and
your
parens
with
your
print
statements
)
you're
using
Python
3
","
which
already
is
going
to
limit
the
amount
of
help
you
get
.
Most
people
are
still
using
Python
2.6
or
2.7
.
But
with
that
out
of
the
way
...
The
main
remaining
issues
addressing
your
question
are
:
First
:
you're
using
strings
for
player
input
(
e.g
.
'
1
'
","
'
2
'
","
'
3
'
)
","
and
numbers
for
computer
choice
(
e.g
.
1
","
2
","
3
)
.
So
you
need
to
compare
them
as
such
.
In
other
words
","
instead
of
:
You
would
need
to
say
:
Second
:
you're
trying
to
reference
one
function's
variables
in
another
one
","
and
that
won't
work
.
If
you
want
your
computer_wins
","
etc.
variables
to
be
global
","
you
need
to
initialize
them
at
the
global
scope
","
e.g
.
right
after
your
""""
#
constants
""""
are
declared
and
before
you
get
into
main
.
Then
in
any
function
that
uses
them
","
you
must
say
e.g
.
global
computer_wins
to
indicate
they
are
global
and
not
local
.
Once
you
get
these
issues
addressed
","
it
should
work
a
bit
better
","
but
you'll
still
need
to
do
a
lot
of
cleanup
and
keep
working
on
it
!
Keep
at
it
","
and
soon
it
will
be
natural
for
you
.
I
am
trying
to
write
a
Python
program
and
I
am
having
a
hard
time
getting
my
score
.
I
have
written
it
as
a
value
returning
function
and
every
time
I
run
the
program
it
seems
to
skip
the
step
where
it
retrieves
the
score
unless
I
include
an
else
statement
which
it
will
automatcially
jump
the
the
else
statement
.
I
will
attach
the
full
code
below
.
Thank
you
very
much
for
any
help
","
I'm
greatful
!
This
is
also
my
first
time
posting
in
this
forum
I
apologize
if
I
screw
something
up
.
Unfortunately
Satchmo
uses
SORL-Thumbnail
version
3.2.5
","
those
docs
are
for
11
.
My
guess
is
they
didn't
support
formats
in
version
3.x
.
I
haven't
had
any
luck
finding
docs
for
it
but
you
can
probably
dig
through
the
code
to
be
sure
.
If
I
remember
correctly
you
can
swap
out
the
library
for
version
11
","
but
you'll
have
to
go
through
all
the
templates
that
use
the
template
tag
and
add
the
{
%
endthumbnail
%
}
tag
after
each
time
it's
used
(
the
endtag
wasn't
used
back
in
version
3.x
)
.
can
someone
please
give
me
an
example
of
how
to
change
the
SORL-thumbnail
format
in
the
django
template
tag
.
I've
read
the
documentation
here
:
http://thumbnail.sorl.net/template.html#thumbnail
and
have
tried
various
ways
of
implimenting
to
no
avail
.
I
get
errors
similar
to
:
'
thumbnail
'
tag
received
a
bad
argument
:
'
format
'
My
code
works
fine
without
the
""""
"format=""png"
""""
""""
part
","
it
just
makes
a
jpg
thumbnail
.
However
","
I
want
a
png
thumbnail
.
{
%
thumbnail
product.main_image.picture
84x84
"format=""png"
""""
as
image
%
}
Also
","
adding
THUMBNAIL_FORMAT
=
""""
PNG
""""
to
my
settings.py
did
nothing
Thanks
","
UPDATE
:
HERE's
HOW
I
FIXED
THE
PROBLEM
:
So
Issac
and
zachwood
were
right
on
.
This
was
a
version
dependent
thing
.
I
resolved
my
problem
this
way
:
1
.
)
upgrade
to
newest
sorl
1.1
)
syncdb
2
.
)
in
settings
","
changed
THUMBNAIL_DEBUG
=
True
3
.
)
added
closing
tag
so
the
templates
looked
like
:
It
worked
!
4
.
)
Python
doesn't
support
adding
functions
together
which
both
of
your
attempts
tried
to
do
.
Instead
","
you
need
to
create
a
new
function
","
such
as
with
lambda
which
calls
the
original
functions
.
Lets
say
we
have
two
defined
function
objects
and
now
I
want
to
have
a
function
that
calls
and
adds
the
result
of
these
two
functions
.
What
I
thought
would
work
is
:
Which
I
thought
would
give
me
an
answer
of
111
(
10*10
+
10+1
)
But
that
just
gave
me
an
error
TypeError
:
unsupported
operand
type(s)
for
+
:
'
function
'
and
'
function
'
So
I
tried
:
But
still
to
no
avail
...
However
","
if
I
do
it
pops
out
with
100
","
so
it
seems
to
evaluate
the
function
on
return
","
But
I
can't
figure
out
how
to
get
it
to
evaluate
the
functions
first
and
then
operate
on
them
.
Any
help
would
be
greatly
appreciated
!
EDIT
Got
it
!
Your
original
idea
will
work
if
you
instead
call
these
functions
and
then
add
their
return
values
","
rather
than
trying
to
add
them
themselves
;
This
is
because
f
and
g
are
actually
LambdaTypes
","
and
the
(
)
operator
calls
them
","
allowing
the
+
operator
to
add
their
return
values
.
When
you
use
the
+
operator
on
them
directly
","
the
+
operator
doesn't
know
how
to
add
two
LambdaTypes
.
EDIT
To
add
a
little
more
;
the
reason
doesn't
work
is
because
you
are
","
again
","
trying
to
add
together
two
function
objects
.
However
","
your
example
of
WILL
work
","
because
this
will
simply
return
another
function
object
","
which
is
then
called
with
an
argument
of
value
10
in
your
statement
addFuncs
takes
arbitrarily
many
functions
as
input
and
returns
a
function
:
yields
I
am
trying
to
horizontally
flip
an
image
(
from
left
to
right
)
on
Python
using
pypng
","
I
have
written
the
following
codes
but
it
does
not
seem
to
work
","
anybody
have
any
idea
what
I'm
doing
wrong
here
?
The
inner
loop
logic
is
wrong
but
particularly
","
this
line
:
You
are
changing
the
image
variable
in-place
","
but
you
seemed
to
forget
the
row
variable
r
.
So
right
now
","
you
are
changing
rows
.
And
your
negative
slicing
is
a
bit
off
.
For
c=0
","
you'll
get
image
[
-
3:0
]
and
this
is
not
a
valid
slice
and
it
will
return
[]
.
But
judging
from
your
code
you
don't
mean
to
change
image
in-place
","
you
rather
want
to
create
new_image
.
What
you
should
be
doing
is
inserting
slices
at
the
end
of
new_row
:
By
the
way
","
you
can
also
change
the
image
in-place
","
but
be
careful
.
As
you
are
passing
a
list
","
you
should
copy
it
before
changing
so
that
original
is
unchanged
.
Here
is
that
version
:
This
can
also
be
done
with
a
list
comprehension
in
single
line
","
but
it
will
be
less
readable
.
If
you
like
","
here
is
how
you
can
do
it
:
new_row.append(image[r][c])
should
be
outside
of
the
if
.
Also
","
you're
flipping
the
image
horizontally
...
twice
.
Make
your
for
loop
use
"range(0,cols/2,3)"
.
(
That
may
also
eliminate
the
need
for
that
if
.
)
You're
also
modifying
the
original
image
in-place
;
are
you
sure
you
want
to
do
that
?
Seems
a
simpler
solution
might
be
to
loop
through
each
row
in
reverse
","
appending
to
a
row
for
the
new
image
.
I'm
not
experienced
with
strings
","
but
the
version
I
have
(
2.21.51.20110605
)
has
an
8-bit
encoding
option
(
-
eS
)
that
would
work
with
utf-8
text
.
It
must
have
to
cast
a
wide
net
looking
for
'
text
'
delimited
by
non-printable
characters
(
value
<
32
)
.
I'd
expect
a
lot
of
noise
.
A
test
on
a
random
executable
showed
the
-
eS
(
8-bit
)
result
was
5
times
bigger
than
-
es
(
7-bit
)
.
I've
noticed
gnu-binutils-strings
can
printout
utf-16
content
in
a
file
-
is
it
possible
for
the
program
to
print
out
utf-8
strings
?
if
so
","
which
arguments
are
appropriate
?
i'm
working
in
a
python
environment
using
subprocess
and
would
like
to
work
with
the
output
from
gnu-binutils-strings
that
a
subprocess.Popen
call
would
generate
through
a
pipe
.
I
often
find
myself
wanting
to
use
a
3rd
party
python
module
in
my
own
project
","
but
I
know
that
I
will
also
need
to
make
changes
to
the
3rd
party
module
that
I
want
to
push
upstream
.
What
is
the
best
practice
of
file
layout
/
installation
to
achieve
this
?
Most
python
modules
are
laid
out
with
root
dir
containing
a
""""
setup.py
""""
to
compile
/
install
the
module
.
The
problem
is
","
every
time
I
make
changes
to
the
module
source
I
need
to
re-run
the
full
install
step
in
order
to
use
those
changes
in
my
project
.
For
large
modules
","
like
scipy
this
can
take
some
time
.
Alternatively
","
I
can
hack
on
the
installed
version
of
the
python
module
","
but
then
I
have
to
manually
move
those
changes
back
to
the
source
version
of
the
module
in
order
to
generate
patches
etc.
I
know
about
virtualenv
and
PYTHONPATH
but
they
are
ways
of
installing
a
module
to
a
different
location
.
So
far
","
I
have
manually
created
symlinks
","
but
that
is
messy
.
If
the
3rd
party
project
is
using
setuptools
or
distribute
","
you
can
do
python
setup.py
develop
instead
of
install
.
This
will
create
the
appropriate
sym-links
in
the
site-packages
dir
for
you
.
Is
there
a
simple
method
to
pull
content
between
a
regex
?
Assume
I
have
the
following
sample
text
My
regex
is
:
This
will
obviously
return
the
entire
[
SOME
MORE
TEXT
]
"value=""ssss"
""""
","
however
I
only
want
ssss
to
be
returned
since
that's
what
I'm
looking
for
I
can
obviously
define
a
parser
function
but
I
feel
as
if
python
provides
some
simple
pythonic
way
to
do
such
a
task
Your
original
regex
is
too
greedy
:
r
'
.
*
\
]
'
won't
stop
at
the
first
'
]
'
and
the
second
'
.
*
'
won't
stop
at
'
""""
'
.
To
stop
at
c
you
could
use
[
^
c
]
or
'
.
*
?
'
:
Example
This
is
what
capturing
groups
are
designed
to
do
.
The
?
:
inside
the
old
groups
(
the
parentheses
)
means
that
the
group
is
now
a
non-capturing
group
;
that
is
","
it
won't
be
accessible
as
a
group
in
the
result
.
I
converted
them
to
keep
the
output
simpler
","
but
you
can
leave
them
as
capturing
groups
if
you
prefer
(
but
then
you
have
to
use
matches.group(2)
instead
","
since
the
first
quote
would
be
the
first
captured
group
)
.
I'm
running
my
unit
tests
using
nose
.
I
have
.
ini
files
such
as
production.ini
","
development.ini
","
local.ini
.
Finally
","
I
have
a
test.ini
file
which
looks
like
:
In
my
test
class
I
want
to
setup
the
database
as
I
would
in
my
app
server
code
.
Something
like
:
How
does
nose
pass
'
settings
'
to
my
test
code
?
I've
been
reading
the
following
link
for
some
guidance
","
but
I
haven't
been
able
to
connect
all
the
dots
.
http://farmdev.com/projects/fixture/using-fixture-with-pylons.html
Thanks
much
!
You
will
need
to
parse
the
settings
from
the
INI
file
yourself
.
Pylons
used
to
do
this
automatically
for
you
by
just
hard-coding
a
load
for
""""
test.ini
""""
.
The
two
options
you
have
are
1
)
just
load
the
INI
settings
via
settings
=
paste.deploy.appconfig('test.ini')
or
2
)
loading
the
actual
WSGI
app
yourself
","
like
if
you
wanted
to
use
it
via
WebTest
app
=
pyramid.paster.get_app('test.ini')
which
would
parse
the
INI
file
and
return
an
actual
WSGI
app
.
Unfortunately
that
route
doesn't
give
you
access
to
the
INI
file
directly
","
it
automatically
just
passes
the
settings
to
your
app's
startup
function
"main(global_conf, **settings)"
.
You
may
also
find
the
Pyramid
docs
on
functional
tests
useful
.
So
Here
is
my
problem
.
I
want
to
check
if
a
site
has
had
any
recent
comments
done
on
it
.
Usually
wordpress
sites
have
their
comment
feed
in
this
pattern
www.site.com
/
comments
/
feed
Now
the
problem
is
each
wp
site
can
be
in
many
formats
of
rss
or
atom
with
different
fields
.
I
am
currently
working
on
the
logic
on
how
I
can
tell
if
there
has
been
a
new
update
.
Any
ideas
Reference
sites
with
different
formats
in
Rss
and
Atom
feeds
:
cheers
","
vishal
Very
simple
solution
I
can
think
of
is
md5
digest
.
Open
an
URL
using
urllib
and
fetch
the
lines
and
convert
to
string
.
Create
a
md5
digest
of
the
values
and
store
it
in
a
file
.
Next
time
do
the
same
and
check
the
result
from
the
file
.
You
need
to
use
urllib
and
hashlib
modules
.
You
might
consider
web2py
.
The
framework
itself
is
very
easy
to
set
up
","
learn
","
and
use
","
and
it
provides
an
easy
way
to
distribute
your
app
as
a
binary
.
The
user
would
simply
unzip
it
and
click
run
","
and
it
will
run
as
a
standalone
app
in
the
browser
on
the
user's
machine
.
It
even
includes
its
own
Python
interpreter
","
so
the
user
doesn't
have
to
have
Python
installed
(
very
helpful
on
Windows
","
which
does
not
typically
have
Python
installed
)
.
The
built-in
Rocket
server
will
be
more
than
adequate
for
running
locally
(
some
people
even
use
it
in
production
)
.
You
can
also
use
Rocket
with
other
frameworks
.
If
you
need
help
","
ask
on
the
mailing
list
.
I'm
considering
several
options
for
the
user
interface
of
an
application
.
I
want
this
application
to
be
multi-platform
(
Windows
","
Linux
","
OSX
)
","
so
one
of
the
options
I
considered
was
to
develop
it
as
a
web
application
","
but
run
it
on
a
local
server
to
still
have
access
to
administrator
privileges
(
which
are
required
)
.
simply
","
a
web
interface
to
my
program
.
I
want
to
develop
in
python
for
convenience
reasons
.
Does
using
Pylons
for
this
job
is
recommended
and
if
so
","
what
is
the
best
way
to
run
it
in
this
setup
?
Have
a
look
at
the
micro
webdevelopment
framework
Flask
.
From
the
docs
:
Just
save
it
as
hello.py
or
something
similar
and
run
it
with
your
Python
interpreter
.
$
easy_install
Flask
$
python
hello.py
*
Running
on
http://127.0.0.1:5000/
I
am
trying
to
get
Python
to
run
to
use
with
Blender
.
I
have
64
bit
Vista
SP2
.
2.6.7
Python
.
When
I
start
python
the
command
prompt
tells
me
this
So
","
I
opened
pypm-script.py
This
is
very
frustrating
","
because
have
no
idea
how
to
read
code
or
how
to
use
Python
!
I
hope
this
is
easily
fixable
.
The
file
pypm-script.py
(
and
pkg_resources
)
come
from
ActivePython
.
You
probably
must
have
installed
a
different
Python
on
top
of
ActivePython
.
I
can
think
of
two
ways
to
fix
this
problem
:
Uninstall
Python
","
remove
C:\Python26
and
install
ActivePython
2.6
(
or
2.7
)
","
or
Remove
C:\Python26\Scripts\pypm
*
and
C:\Python26\Lib\site-packages\pypm
*
I'm
new
to
python
","
and
I
have
the
following
problem
:
I
am
trying
to
minimize
a
python
function
that
has
a
numpy
array
as
one
of
its
arguments
.
When
I
use
scipy.optimize.fmin
","
it
turns
my
array
into
a
list
(
which
results
in
the
function
failing
to
evaluate
)
.
Is
there
an
optimization
function
that
does
accept
numpy
arrays
as
function
arguments
?
Thanks
in
advance
!
-
MB
Edit
:
Here
is
an
example
of
what
I'm
talking
about
","
courtesy
of
@EOL
:
Here
is
an
example
using
optimize.fmin
which
comes
from
the
scipy
tutorial
:
Does
this
help
?
If
not
","
can
you
modify
this
example
to
show
what
is
turning
into
a
list
?
There
is
nothing
wrong
with
your
current
approach
.
time_diff
is
written
to
once
only
and
then
all
future
accesses
are
reads
.
It
effect
it
is
a
module
wide
constant
.
You
run
into
problems
with
shared
global
state
when
you
have
multiple
threads
accessing
an
object
and
at
least
one
of
the
threads
is
writing
.
That's
not
happening
here
and
you
have
nothing
to
be
concerned
about
.
You
can
wrap
a
function
in
a
closure
like
this
:
Or
you
can
use
a
partial
from
stdlib
like
this
:
I'm
trying
to
increment
all
timestamps
(
of
the
form
'
HH:MM:SS
'
)
in
a
text
file
by
a
number
of
seconds
specified
by
a
command-line
parameter
to
my
program
.
Here's
a
simplified
version
of
my
effort
so
far
:
This
works
fine
:
the
result
of
running
this
is
01:28:05
which
is
just
what
I
want
.
However
","
I've
heard
that
I
should
use
global
variables
as
less
as
possible
.
So
I
was
wondering
if
there's
a
simple
way
to
pass
time_diff
as
an
argument
to
replace_time
instead
of
using
a
global
variable
.
I
tried
the
obvious
","
but
it
failed
:
with
this
error
:
NameError
:
name
'
matchobj
'
is
not
defined
","
so
I
can't
pass
matchobj
directly
.
I've
looked
at
the
standard
re
page
and
standard
re
howto
","
but
can't
find
the
information
I
need
over
there
.
How
can
I
avoid
using
a
global
variable
here
?
Can
I
somehow
pass
an
extra
argument
to
the
replace_time
function
?
Thanks
in
advance
.
Your
documentation
reference
is
for
Python
3.0.1
.
There
is
no
good
reason
using
Python
3.0
.
You
should
be
using
3.2
or
2.7
.
What
exactly
are
you
using
?
Suggestion
:
(
1
)
change
bytes
to
raw_bytes
to
avoid
confusion
with
the
bytes
built-in
(
2
)
check
for
raw_bytes
=
=
bytes_back
in
your
test
script
(
3
)
while
your
test
should
work
with
quoted-printable
","
it
is
very
inefficient
for
binary
data
;
use
base64
instead
.
Update
:
Base64
encoding
produces
4
output
bytes
for
every
3
input
bytes
.
Your
base64
code
doesn't
work
with
56-byte
chunks
because
56
is
not
an
integral
multiple
of
3
;
each
chunk
is
padded
out
to
a
multiple
of
3
.
Then
you
join
the
chunks
and
attempt
to
decode
","
which
is
guaranteed
not
to
work
.
Your
chunking
loop
would
be
much
better
written
as
:
In
any
case
","
chunking
is
rather
slow
and
pointless
;
just
do
b2a_base64(raw_bytes)
I
want
to
convert
a
binary
file
(
such
as
a
jpg
","
mp3
","
etc
)
to
web-safe
text
and
then
back
into
binary
data
.
I've
researched
a
few
modules
and
I
think
I'm
really
close
but
I
keep
getting
data
corruption
.
After
looking
at
the
documentation
for
binascii
I
came
up
with
this
:
When
I
try
to
open
the
converted.jpg
I
get
data
corruption
:
-
/
I
also
tried
using
b2a_base64
with
57-long
blocks
of
binary
data
.
I
took
each
block
","
converted
to
a
string
","
concatenated
them
all
together
","
and
then
converted
back
in
a2b_base64
and
got
corruption
again
.
Can
anyone
help
?
I'm
not
super
knowledgeable
on
all
the
intricacies
of
bytes
and
file
formats
.
I'm
using
Python
on
Windows
if
that
makes
a
difference
with
the
\
r\n
stuff
@PMC's
answer
copied
from
the
question
:
Here's
what
works
:
Thanks
for
the
help
guys
.
I'm
not
sure
why
this
would
fail
with
[
0:56
]
instead
of
[
0:57
]
but
I'll
leave
that
as
an
exercise
for
the
reader
:
P
Your
code
looks
quite
complicated
.
Try
this
:
Lines
tagged
with
#
1
and
#
2
represent
alternatives
to
each
other
.
#
1
seems
most
straightforward
to
me
-
just
make
it
one
string
","
process
it
and
convert
it
back
.
You
should
use
base64
encoding
instead
of
quoted
printable
.
Use
b2a_base64()
and
a2b_base64()
.
Quoted
printable
is
much
bigger
for
binary
data
like
pictures
.
In
this
encoding
each
binary
(
non
alphanumeric
character
)
code
is
changed
into
=
HEX
.
It
can
be
used
for
texts
that
consist
mainly
of
alphanumeric
like
email
subjects
.
Base64
is
much
better
for
mainly
binary
data
.
It
takes
6
bites
of
first
byte
","
then
last
2
bits
of
1st
byte
and
4
bites
from
2nd
byte
.
etc.
It
can
be
recognized
by
=
padding
at
the
end
of
the
encoded
text
(
sometimes
other
character
is
used
)
.
As
an
example
I
took
.
jpeg
of
271
700
bytes
.
In
qp
it
is
627
857
b
while
in
base64
it
is
362
269
bytes
.
Size
of
qp
is
dependent
of
data
type
:
text
which
is
letters
only
do
not
change
.
Size
of
base64
is
orig_size
*
8
/
6
.
Answering
my
own
question
here
","
but
if
anyone
knows
better
feel
free
to
answer
too
.
Some
of
it
seems
quite
fragile
(
eg
.
version
numbers
in
paths
)
","
so
comment
or
edit
if
you
know
a
better
way
.
1
.
Finding
the
files
Firstly
","
I
use
this
code
to
actually
find
the
root
of
the
GTK
runtime
.
This
is
very
specific
to
how
you
install
the
runtime
","
though
","
and
could
probably
be
improved
with
a
number
of
checks
for
common
locations
:
2
.
What
files
to
include
This
depends
on
(
a
)
how
much
of
a
concern
size
is
","
and
(
b
)
the
context
of
your
application's
deployment
.
By
that
I
mean
","
are
you
deploying
it
to
the
whole
wide
world
where
anyone
can
have
an
arbitrary
locale
setting
","
or
is
it
just
for
internal
corporate
use
where
you
don't
need
translated
stock
strings
?
If
you
want
Windows
theming
","
you'll
need
to
include
:
If
you
want
the
Tango
icons
:
There
is
also
localisation
data
(
which
I
omit
","
but
you
might
not
want
to
)
:
3
.
Piecing
it
together
Firstly
","
here's
a
function
that
walks
the
filesystem
tree
at
a
given
point
and
produces
output
suitable
for
the
data_files
option
.
So
now
you
can
call
setup()
like
so
:
I'm
using
Python
2.6
and
PyGTK
2.22.6
from
the
all-in-one
installer
on
Windows
XP
","
trying
to
build
a
single-file
executable
(
via
py2exe
)
for
my
app
.
My
problem
is
that
when
I
run
my
app
as
a
script
(
ie
.
not
built
into
an
.
exe
file
","
just
as
a
loose
collection
of
.
py
files
)
","
it
uses
the
native-looking
Windows
theme
","
but
when
I
run
the
built
exe
I
see
the
default
GTK
theme
.
I
know
that
this
problem
can
be
fixed
by
copying
a
bunch
of
files
into
the
dist
directory
created
by
py2exe
","
but
everything
I've
read
involves
manually
copying
the
data
","
whereas
I
want
this
to
be
an
automatic
part
of
the
build
process
.
Furthermore
","
everything
on
the
topic
(
including
the
FAQ
)
is
out
of
date
-
PyGTK
now
keeps
its
files
in
C:\Python2x\Lib\site-packages\gtk-2.0\runtime
\
...
","
and
just
copying
the
lib
and
etc
directories
doesn't
fix
the
problem
.
My
questions
are
:
I'd
like
to
be
able
to
programmatically
find
the
GTK
runtime
data
in
setup.py
rather
than
hard
coding
paths
.
How
do
I
do
this
?
What
are
the
minimal
resources
I
need
to
include
?
Update
:
I
may
have
almost
answered
#
2
by
trial-and-error
.
For
the
""""
wimp
""""
(
ie
.
MS
Windows
)
theme
to
work
","
I
need
the
files
from
:
...
without
the
runtime
prefix
","
but
otherwise
with
the
same
directory
structure
","
sitting
directly
in
the
dist
directory
produced
by
py2exe
.
But
where
does
the
2.10.0
come
from
","
given
that
gtk.gtk_version
is
(
2
","
22
","
0
)
?
Hey
guys
I've
got
a
problem
.
I'm
trying
to
send
variable
x
that
is
found
in
script
a
to
script
b
and
then
execute
script
b
with
that
variable
.
Example
:
Script
a
Script
b
Any
ideas
on
how
I
could
do
this
?
Thanks
Use
sys.argv
.
Which
gives
you
a
list
of
the
items
passed
on
the
command
line
Script
b
sys.argv
will
be
a
list
that
contains
[
'
.
/
scriptB.py
'
","
'
10
'
]
in
this
case
.
I'm
parsing
XML
in
python
by
ElementTree
I
wish
to
parse
all
the
'
xml
'
files
in
a
given
directory
.
The
user
should
enter
only
the
directory
name
and
I
should
be
able
to
loop
through
all
the
files
in
directory
and
parse
them
one
by
one
.
Can
someone
tell
me
the
approach
.
I'm
using
Linux
.
Just
create
a
loop
over
os.listdir()
:
Sort
b
based
on
items
'
index
in
a
","
with
all
items
not
in
a
at
the
end
.
If
you
want
to
make
list2
identical
to
list1
","
you
don't
need
to
mess
with
order
or
re-arrange
anything
","
just
replace
list2
with
a
copy
of
list1
:
list()
takes
any
iterable
and
produces
a
new
list
from
it
","
so
we
can
use
this
to
copy
list1
","
thus
creating
two
lists
that
are
exactly
the
same
.
It
might
also
be
possible
to
just
do
list2
=
list1
","
but
do
note
that
this
will
cause
any
changes
to
either
to
affect
the
other
(
as
they
point
to
the
same
object
)
","
so
this
is
probably
not
what
you
want
.
If
list2
is
referenced
elsewhere
","
and
thus
needs
to
remain
the
same
object
","
it's
possible
to
replace
every
value
in
the
list
using
list2
[
:
]
=
list1
.
In
general
","
you
probably
want
the
first
solution
.
or
may
be
just
check
everything
is
perfect
then..copy
list
a
for
b
The
easiest
way
to
do
this
would
be
to
use
zip
to
combine
the
elements
of
the
two
lists
into
tuples
:
sorted
will
compare
the
tuples
by
their
first
element
(
the
element
from
a
)
first
;
zip(*...)
will
""""
unzip
""""
the
sorted
list
.
I
have
two
lists
in
python
.
What
i
need
is
to
order
b
according
to
a
.
Is
there
any
methods
to
do
it
so
simply
without
any
loops
?
I'm
posting
this
without
further
comments
","
for
learning
purposes
(
in
the
real
life
please
do
use
a
library
)
.
Note
that
there's
no
error
checking
(
a
homework
for
you
!
)
Feel
free
to
ask
if
there's
something
you
don't
understand
.
im
trying
to
parse
lines
in
the
form
:
Where
OP
is
a
symbol
for
a
logical
gate
(
AND
","
OR
","
NOT
)
and
something
is
the
thing
i
want
to
evaluate
.
The
output
im
looking
for
is
something
like
:
Where
a
condition
itself
can
be
a
dict
/
list
pair
itself
(
nested
conditions
)
.
So
far
i
tried
something
like
:
I
tried
other
ways
aswell
but
i
just
can't
wrap
my
head
around
this
whole
recursion
thing
so
im
wondering
if
i
could
get
some
pointers
here
","
i
looked
around
the
web
and
i
found
some
stuff
about
recursive
descent
parsing
but
the
tutorials
were
all
trying
to
do
something
more
complicated
than
i
needed
.
PS
:
i
realize
i
could
do
this
with
existing
python
libraries
but
what
would
i
learn
by
doing
that
eh
?
You
are
basically
reinventing
the
indexing
scheme
of
a
multidimensional
array
.
It
is
relatively
easy
to
code
","
but
you
can
use
the
two
functions
unravel_index
and
ravel_multi_index
to
your
advantage
here
.
If
your
grid
is
of
M
rows
and
N
columns
","
to
get
the
idx
and
idy
of
a
single
item
you
could
do
:
This
also
works
if
","
instead
of
a
single
index
","
you
provide
an
array
of
indices
:
So
if
cells
has
the
indices
of
several
cells
you
want
to
find
neighbors
to
:
You
can
get
their
neighbors
as
:
Or
","
if
you
prefer
it
like
that
:
The
nicest
thing
about
going
this
way
is
that
ravel_multi_index
has
a
mode
keyword
argument
you
can
use
to
handle
items
on
the
edges
of
your
lattice
","
see
the
docs
.
Let's
suppose
I
have
a
set
of
2D
coordinates
that
represent
the
centers
of
cells
of
a
2D
regular
mesh
.
I
would
like
to
find
","
for
each
cell
in
the
grid
","
the
two
closest
neighbors
in
each
direction
.
The
problem
is
quite
straightforward
if
one
assigns
to
each
cell
and
index
defined
as
follows
:
idx_cell
=
idx+N*idy
where
N
is
the
total
number
of
cells
in
the
grid
","
idx=x
/
dx
and
idy=y
/
dx
","
with
x
and
y
being
the
x-coordinate
and
the
y-coordinate
of
a
cell
and
dx
its
size
.
For
example
","
the
neighboring
cells
for
a
cell
with
idx_cell=5
are
the
cells
with
idx_cell
equal
to
4
","
6
(
for
the
x-axis
)
and
5+N
","
5-N
(
for
the
y-axis
)
.
The
problem
that
I
have
is
that
my
implementation
of
the
algorithm
is
quite
slow
for
large
(
N>1e6
)
data
sets
.
For
instance
","
to
get
the
neighbors
of
the
x-axis
I
do
[
x
[
(
idx_cell==idx_cell
[i]
-
1)|(idx_cell==idx_cell
[i]
+
1
)
]
for
i
in
cells
]
Do
you
think
there's
a
fastest
way
to
implement
this
algorithm
?
You'll
have
to
reference
the
XML
DOM
documentation
and
grit
your
teeth
.
To
get
the
first
<
server
>
element
","
then
its
<
name
>
:
I
am
using
python
minidom
to
parse
a
xml
","
but
not
able
to
get
it
working
for
below
xml
.
I
want
to
select
the
first
server
tag
and
want
the
value
of
name
tag
","
in
this
case
""""
Server1
""""
In
python
3
I
have
a
line
asking
for
input
that
will
then
look
in
an
imported
dictionary
and
then
list
all
their
inputs
that
appear
in
the
dictionary
.
My
problem
is
when
I
run
the
code
and
put
in
the
input
it
will
only
return
the
last
word
I
input
.
For
example
the
dictionary
contains
(
AIR
","
AMA
)
and
if
I
input
(
AIR
","
AMA
)
it
will
only
return
AMA
.
Any
information
to
resolve
this
would
be
very
helpful
!
The
dictionary
:
The
Code
:
Ok
","
the
code
seems
somewhat
confused
.
Here's
a
simpler
version
that
seems
to
do
what
you
want
:
And
here's
an
example
of
use
:
The
code
sample
you
provided
actually
does
what
was
expected
","
if
you
gave
it
space
separated
quote
names
.
Hope
this
helps
.
cr.dictfetchall()
will
give
you
all
the
matching
records
in
the
form
of
*
*
list
of
dictionary
*
*
containing
key
","
value
.
cr.dictfetchone()
works
same
way
as
cr.dictfetchall()
except
it
returns
only
single
record
.
cr.fetchall()
will
give
you
all
the
matching
records
in
the
form
of
list
of
tupple
.
cr.fetchone()
works
same
way
as
cr.fetchall()
except
it
returns
only
single
record
.
In
your
given
query
","
if
you
use
:
cr.dictfetchall()
will
give
you
[
{
'
reg_no
'
:
123
}
","
{
'
reg_no
'
:
543
}
","
]
.
cr.dictfetchone()
will
give
you
{
'
reg_no
'
:
123
}
.
cr.fetchall()
will
give
you
'
[
(
123
)
","
(
543
)
]
'
.
cr.fetchone()
will
give
you
'
(
123
)
'
.
What
is
the
difference
between
these
fetching
.
?
please
give
me
a
examples
for
reference
site
to
get
clear
idea.still
i'm
confuse
with
it
cr
is
the
current
row
","
from
the
database
cursor
(
OPENERP
7
)
ex
:
Use
itertools.groupby
with
key
as
operator.itemgetter('user')
to
group
the
elements
and
then
iterate
through
the
groupby
object
and
determine
the
count
based
on
length
of
the
group
Here
is
probably
a
better
(
more
simple
)
way
of
doing
it
:
The
count
is
no
longer
a
key
but
you
can
get
the
count
like
so
I
have
list
of
logs
in
python
like
Now
i
need
to
find
the
activity
count
on
the
basis
of
users
and
at
the
end
return
something
like
i
tried
traversing
all
logs
and
find
but
that
was
quite
messy
can
someone
suggest
me
better
solution
?
Try
this
:
To
get
the
output
that
you
want
:
Or
if
you
want
only
one
loop
:
Having
setup
the
settings.py
file
to
recognize
an
installed
app
of
'
blog
'
","
I
still
get
this
error
No
module
named
blog
But
the
models
saves
in
'
blog
'
folder
are
installed
on
the
sqlite3
db
using
syncdb
.
Any
idea
why
this
happens
and
how
to
fix
this
error
?
You
might
want
to
check
INSTALLED_APPS
section
in
you
settings.py
file
.
If
'
blog
'
","
is
not
added
you
should
consider
adding
.
After
this
you
should
proceed
to
python
manage.py
syncdb
.
Last
step
check
your
urls.py
and
see
if
all
necessary
URL
mapping
exists
.
[
N.B
.
I'm
quite
new
to
Python
&
Xpath
]
I
was
wanting
to
perform
an
Xpath
query
and
use
a
variable
in
the
expression
to
cycle
through
all
the
elements
using
an
index
.
For
example
","
rather
than
having
the
specific
position
of
[1]
","
[2]
","
etc
","
I'd
like
to
be
able
to
place
a
variable
i
in
this
expression
:
I'm
unsure
whether
it's
even
possible
","
but
I
guess
it
can't
hurt
to
ask
!
To
clarify
","
this
is
why
I
would
like
to
do
this
:
I'm
trying
to
process
all
the
elements
of
a_movie
[1]
","
then
at
the
end
of
the
function
","
I
want
to
process
all
the
elements
of
a_movie
[2]
","
and
so
on
.
You
want
to
loop
over
the
/
movies
/
a_movie
tags
instead
","
but
only
those
that
have
a
studio
child
:
If
you
wanted
to
just
print
out
the
child
elements
of
the
a_movie
element
as
XML
","
I'd
just
loop
over
the
element
(
looping
over
the
child
elements
)
and
use
ElementTree.tostring()
on
each
:
I
know
this
is
a
bit
of
a
zombie
question
","
but
I
couldnt
find
a
relevant
newer
one
.
So
I'll
post
this
in
case
it
helps
someone
.
here's
an
algorithm
I've
used
.
It
relies
on
the
following
assumptions
:
its
a
square
/
rectangle
:
4
vertices
","
edges
at
90
degrees
to
each
other
vertices
are
already
in
sequence
(
either
clockwise
or
anti-clockwise
)
its
rotated
by
R
such
that
-
45
<
R
<
+
45
degrees
Note
:
it
uses
numpy
and
the
scikit-image
coordinate
convention
:
(
Y
","
X
)
with
(
0
","
0
)
in
the
top
left
corner
Make
a
vector
from
point
1
to
point
2
.
Make
a
vector
from
point
1
to
point
3
.
Make
a
vector
from
point
1
to
point
4
.
Calculate
the
z
component
of
the
cross
product
of
the
vectors
1->3
and
1->2
.
Calculate
the
z
component
of
the
cross
product
of
the
vectors
1->3
and
1->4
.
If
those
two
z's
have
different
signs
(
one
is
negative
and
the
other
is
positive
)
","
you
have
your
points
in
order
.
If
they
aren't
in
order
","
repeat
all
of
the
above
","
first
swapping
points
3
and
2
.
And
if
that's
not
sufficient
","
then
swap
points
3
and
4
in
the
original
list
/
array
of
points
.
I'm
trying
to
write
a
function
in
python(2.7)
.
This
function
will
take
a
list
of
8
values
which
represents
the
coordinates
of
vertices
.
(
Form
of
the
input
is
"[Ax,Ay,Bx,By,Cx,Cy,Dx,Dy]"
)
The
function
will
determine
whether
these
vertices
are
given
in
order
to
form
a
valid
rectangle
.
If
not
","
it
will
put
them
in
order
and
return
a
sorted
list
of
these
vertices
.
The
starting
point
or
the
direction(whether it is clockwise or anti-clockwise)
is
not
important
.
Let
me
illusrate
what
I
want
:
If
the
given
input
forms
the
2nd
or
3rd
shape
in
the
link
below
;
the
function
will
convert
it
to
the
first
one
.
http://i.stack.imgur.com/IsRqr.png
Which
algorithm
can
I
use
to
do
this
?
I
have
used
the
way
Alexey
suggested
and
wrote
my
code
.
It
can
take
some
optimizations
but
it
is
not
neccessary
for
me
atm
.
For
linux
and
Window
:
Just
modify
1
line
","
and
you
can
change
it
.
cwp.py
in
C:\Users
\
[
your
computer
name
]
\
Anaconda2
.
os.chdir(documents_folder)
at
the
end
of
the
file
.
Change
it
to
"os.chdir(""your expected working folder"")"
for
example
:
"os.chdir(""D:/Jupyter_folder"")"
It
worked
.
Update
:
When
it
comes
to
MacOS
","
I
couldnt
find
the
cwp.py
.
Here
is
what
I
found
:
Open
terminal
on
your
Macbook
","
run
'
jupyter
notebook
-
-
generate-config
'
.
It
will
create
a
config
file
at
\
Users\catbuilts\.jupyter\jupyter_notebook_config
.
Open
the
config
file
","
then
change
this
line
#
c.NotebookApp.notebook_dir
=
'
'
(
search
for
the
word
'
_dir
'
)
to
c.NotebookApp.notebook_dir
=
'
your
path
'
and
remember
un-comment
this
line
too
.
For
example
","
mine
is
'
/
Users
/
catbuilts
/
Jupyter_Projects
/
'
Besides
@Matt's
approach
","
one
way
to
change
the
default
directory
to
use
for
notebooks
permanently
is
to
change
the
config
files
.
Firstly
in
the
cmdline
","
type
:
to
initialize
a
profile
with
the
default
configuration
file
.
Secondly
","
in
file
ipython_notebook_config.py
","
uncomment
and
edit
this
line
:
changing
D:\\Documents\\Desktop
to
whatever
path
you
like
.
This
works
for
me
;
)
UPDATE
:
There
is
no
c.NotebookManager.notebook_dir
anymore
.
Now
","
the
line
to
uncomment
and
config
is
this
one
:
c.NotebookApp.notebook_dir
=
'
Z:\\username_example\folder_that_you_whant
'
To
do
the
same
trick
described
below
for
Windows
in
OS
X
","
create
this
shell
script
Call
it
ipython-notebook.command
and
make
it
executable
.
Put
it
in
the
directory
you
want
to
work
in
","
then
double-click
it
.
On
MiniConda2
/
Anaconda2
under
Windows
to
change
Jupyter
or
iPython
working
directory
","
you
can
modify
this
file
:
and
add
your
project
folder
location
:
development_folder
=
'
C:\Users\USERNAME\Development
'
Which
is
My
Username
\
Development
in
my
case
.
also
change
:
os.chdir(documents_folder)
to
os.chdir(development_folder)
Execute
by
using
your
regular
Jupiter
Notebook
shortcuts
.
When
I
open
an
Jupyter
notebook
(
formerly
IPython
)
it
defaults
to
C:\Users\USERNAME
.
How
can
I
change
this
so
to
another
location
?
Thanks
.
In
iPython
Notebook
on
Windows
","
this
worked
for
me
:
Simply
follow
the
guide
on
the
official
site
","
also
copied
below
.
For
the
first
step
","
instead
of
copying
the
launcher
","
you
can
just
go
to
start
menu
and
right
click
to
open
the
location
.
Copy
the
Jupyter
Notebook
launcher
from
the
menu
to
the
desktop
.
Right
click
on
the
new
launcher
and
change
the
“
Start
in
”
field
by
pasting
the
full
path
of
the
folder
which
will
contain
all
the
notebooks
.
Double-click
on
the
Jupyter
Notebook
desktop
launcher
(
icon
shows
[IPy]
)
to
start
the
Jupyter
Notebook
App
","
which
will
open
in
a
new
browser
window
(
or
tab
)
.
Note
also
that
a
secondary
terminal
window
(
used
only
for
error
logging
and
for
shut
down
)
will
be
also
opened
.
If
only
the
terminal
starts
","
try
opening
this
address
with
your
browser
:
http://localhost:8888/.
Upper
Solution
may
not
work
for
you
if
you
have
installed
latest
version
of
Python
in
Windows
.
I
have
installed
Python
3.6.0
:
:
Anaconda
4.3.0
(
64-bit
)
and
I
wanted
to
change
the
working
directory
of
iPython
Notebook
called
Jupyter
and
this
is
how
it
worked
for
me
.
Step-1
:
Open
your
CMD
and
type
following
command
.
Step1
:
CMD
Step-2
:
It
has
now
generated
a
file
in
your
.
jupyter
folder
.
For
me
","
it's
C:\Users\Admin.jupyter
.
There
you
will
find
a
file
called
jupyter_notebook_config.py
.
Right
click
and
edit
it
.
Add
the
following
line
and
set
path
of
your
working
directory
.
Set
your
own
working
directory
in
place
of
""""
I:\STUDY\Y2-Trimester-1\Modern
Data
Science
""""
We
are
done
.
Now
you
can
try
restarting
your
Jupyter
Notebook
.
Hope
this
is
useful
to
you
.
Thanks
In
command
line
before
typing
""""
jupyter
notebook
""""
navigate
to
the
desired
folder
.
In
my
case
my
all
python
files
are
in
""""
D:\Python
""""
.
Then
type
the
command
""""
jupyter
notebook
""""
and
there
you
have
it
.
You
have
changed
your
working
directory
.
A
simpler
modification
to
the
Windows
Trick
above
-
without
the
need
to
hard-code
the
directory
.
A
)
Create
a
batch
file
with
the
following
contents
:
(
Note
:
A
batch
file
is
a
simple
text
file
containing
commands
that
can
be
run
in
the
cmd
window
.
It
must
have
a
'
.
bat
'
extension
","
therefore
...
you'll
need
to
disable
the
folder
setting
which
hides
extensions
of
known
types
)
B
)
Copy
and
paste
the
batch
file
to
any
folder
you
want
to
start
a
notebook
server
in
.
(
Make
sure
it's
a
folder
that
you
have
permission
to
edit
.
""""
C
:
\
""""
is
not
a
good
choice
.
)
C
)
Double-click
on
the
batch
file
in
Windows
Explorer
.
The
notebook
server
should
start
as
it
normally
does
.
When
launched
from
the
command
line
","
the
IPython
Notebook
will
use
your
current
working
directory
.
I
took
advantage
of
this
and
created
context
menu
entries
to
open
it
directly
from
Windows
Explorer
.
No
need
for
shortcuts
or
batch
scripts
!
I
was
inspired
by
the
registry-based
'
Git
GUI
Here
/
Git
Bash
Here
'
entries
created
by
Git
for
Windows
.
This
page
(
archived
version
linked
)
was
helpful
in
locating
the
correct
keys
.
This
first
pair
is
for
the
context
menu
presented
with
nothing
selected
(
e.g
.
the
directory
background
)
.
The
notebook
will
open
with
the
current
directory
as
it's
working
directory
.
This
pair
is
for
the
context
menu
presented
when
clicking
on
a
folder
.
The
notebook
will
open
with
the
selected
folder
as
it's
working
directory
.
Pay
attention
to
%
v
vs
%
1
arguments
or
it
won't
work
.
Don't
forget
the
quotes
either
.
On
my
platform
the
full
path
to
IPython
Notebook
is
C:\WinPython-32bit-2.7.6.4\IPython
Notebook.exe
but
this
value
will
obviously
dependent
on
your
installation
.
Edit
:
AFAICT
the
full
path
is
required
even
if
the
executable
is
on
the
system
path
.
jupyter
notebook
-
-
help-all
could
be
of
help
:
You
can
of
course
set
it
in
your
profiles
if
needed
","
you
might
need
to
escape
backslash
in
Windows
.
You
can
also
use
AutoHotKey
with
a
simple
script
to
open
a
Jupyter
Notebook
server
in
a
default
directory
(
CTRL+I
)
or
a
path
highlighted
in
explorer
(
or
elsewhere
with
CTRL+SHIFT+I
)
.
For
Mac
OS
X
with
blanks
in
target
directory
(
follow
up
to
@pheon
)
.
Add
extra
pair
of
double
quotes
around
$
(
...
)
in
line
2
thus
.
See
:
https://stackoverflow.com/a/1308838
(
Sean
Bright
)
A
neat
trick
for
those
using
IPython
in
windows
is
that
you
can
make
an
ipython
icon
in
each
of
your
project
directories
designed
to
open
with
the
notebook
pointing
at
that
chosen
project
.
This
helps
keep
things
separate
.
For
example
if
you
have
a
new
project
in
C:\fake\example\directory
Copy
an
ipython
notebook
icon
to
the
directory
or
create
a
new
link
to
the
windows
""""
cmd
""""
shell
.
Then
right
click
on
the
icon
and
""""
Edit
Properties
""""
Set
the
shortcut
properties
to
:
(
Note
the
added
slash
at
the
end
of
""""
start
in
""""
)
This
runs
windows
command
line
","
changes
to
your
working
directory
","
and
runs
the
ipython
notebook
pointed
at
that
directory
.
Drop
one
of
these
in
each
project
folder
and
you'll
have
ipython
notebook
groups
kept
nice
and
separate
while
still
just
a
doubleclick
away
.
UPDATE
:
IPython
has
removed
support
for
the
command
line
inlining
of
pylab
so
the
fix
for
that
with
this
trick
is
to
just
eliminate
""""
-
-
pylab
inline
""""
if
you
have
a
newer
IPython
version
(
or
just
don't
want
pylab
obviously
)
.
UPDATE
FOR
JUPYTER
NOTEBOOK
~
version
4.1.1
On
my
test
machines
and
as
reported
in
comments
below
","
the
newest
jupyter
build
appears
to
check
the
start
directory
and
launch
with
that
as
the
working
directory
.
This
means
that
the
working
directory
override
is
not
needed
.
Thus
your
shortcut
can
be
as
simple
as
:
If
jupyter
notebook
is
not
in
your
PATH
you
just
need
to
add
the
full
directory
reference
in
front
of
the
command
.
If
that
doesn't
work
please
try
working
from
the
earlier
version
.
Very
conveniently
","
now
""""
Start
in
:
""""
can
be
empty
in
my
tests
with
4.1.1
and
later
.
Perhaps
they
read
this
entry
on
SO
and
liked
it
","
so
long
upvotes
","
nobody
needs
this
anymore
:
)
Before
runing
ipython
:
Change
directory
to
your
preferred
directory
Run
ipython
After
runing
ipython
:
Use
%
cd
/
Enter
/
your
/
prefered
/
path
/
here
/
Use
%
pwd
to
check
your
current
directory
If
you
are
using
ipython
in
windows
","
then
follow
the
steps
:
navigate
to
ipython
notebook
in
programs
and
right
click
on
it
and
go
to
properties
.
In
shortcut
Tab
","
change
the
'
Start
in
'
directory
to
your
desired
directory
.
Restart
the
kernal
.
just
change
to
the
preferred
directory
in
CMD
","
so
if
you
are
in
just
change
the
path
like
this
the
CMD
cursor
then
will
move
to
this
folder
next
you
can
call
jupyter
As
MrFancypants
mentioned
in
the
comments
","
if
you
are
using
Jupyter
(
which
you
should
","
since
it
currently
supersedes
the
older
IPython
Notebook
project
)
","
things
are
a
little
different
.
For
one
","
there
are
no
profiles
any
more
.
After
installing
Jupyter
","
first
check
your
~
/
.
jupyter
folder
to
see
its
content
.
If
no
config
files
were
migrated
from
the
default
IPython
profile
(
as
they
weren't
in
my
case
)
","
create
a
new
one
for
Jupyter
Notebook
:
This
generates
~
/
.
jupyter
/
jupyter_notebook_config.py
file
with
some
helpfully
commented
possible
options
.
To
set
the
default
directory
add
:
As
I
switch
between
Linux
and
OS
X
","
I
wanted
to
use
a
path
relative
to
my
home
folder
(
as
they
differ
â
€
“
/
Users
/
username
and
/
home
/
username
)
","
so
I
set
something
like
:
Now
","
whenever
I
run
jupyter
notebook
","
it
opens
my
desired
notebook
folder
.
I
also
version
the
whole
~
/
.
jupyter
folder
in
my
dotfiles
repository
that
I
deploy
to
every
new
work
machine
.
As
an
aside
","
you
can
still
use
the
-
-
notebook-dir
command
line
option
","
so
maybe
a
simple
alias
would
suit
your
needs
better
.
I'll
add
to
the
long
list
of
answers
here
.
If
you
are
on
Windows
/
using
Anaconda3
","
I
accomplished
this
by
going
to
the
file
/
Scripts
/
ipython-script.py
","
and
just
added
the
lines
before
the
line
Usually
$
ipython
notebook
will
launch
the
notebooks
and
kernels
at
he
current
working
directory
of
the
terminal
.
But
if
you
want
to
specify
the
launch
directory
","
you
can
use
-
-
notebook-dir
option
as
follows
:
$
ipython
notebook
-
-
notebook-dir
=
/
path
/
to
/
specific
/
directory
If
you
are
using
ipython
in
linux
","
then
follow
the
steps
:
!
cd
/
directory_name
/
You
can
try
all
the
commands
which
work
in
you
linux
terminal
.
!
vi
file_name.py
Just
specify
the
exclamation(!)
symbol
before
your
linux
commands
.
I
have
a
very
effective
method
to
save
the
notebooks
in
a
desired
location
in
windows
.
One-off
activity
:
Make
sure
the
path
of
jupyter-notebook.exe
is
saved
under
environment
variable
.
Open
your
desired
directory
either
from
windows
explorer
or
by
cd
from
command
prompt
From
the
windows
explorer
on
your
desired
folder
","
select
the
address
bar(in a way that the path label is fully selected)
and
type
jupyter-notebook.exe
voila
!
!
the
notebook
opens
from
the
desired
folder
and
any
new
notebook
will
be
saved
in
this
location
.
According
to
official
Jupyter
Notebook
Documentation
Change
%
USERPROFILE
%
to
your
folder
path
Documentation
Link
Locate
your
ipython
binary
.
If
you
have
used
anaconda
to
install
ipython-notebook
on
a
mac
","
chances
are
it
will
be
in
the
/
Users
/
[name]
/
anaconda
/
bin
/
directory
in
that
directory
","
instead
of
launching
your
notebook
as
add
a
-
-
notebook-dir=<unicode
>
option
.
I
use
a
bashscript
in
my
ipython
bin
directory
to
launch
my
notebooks
:
Note
-
the
path
to
the
notebook
dir
is
relative
to
the
ipython
bin
directory
.
I
have
both
32
and
64
bit
python
and
ipython
using
WinPython
","
I
wanted
both
32
and
64
bit
versions
to
point
to
the
same
working
directory
for
ipython
notebook
.
I
followed
the
above
suggestions
here
I
was
still
unable
to
get
my
setup
working
.
Here's
what
I
did
-
in
case
anyone
needs
it
:
It
looks
like
Ipython
notebook
was
using
the
configuration
from
C:\pythonPath\winpythonPath\settings\.ipython\profile_default
Even
though
ipython
locate
returns
C:\users\Username\.ipython
As
a
result
","
modifying
the
ipython_notebook_config.py
file
did
nothing
to
change
my
working
directory
.
Additionally
ipython
profile_create
was
not
creating
the
needed
python
files
in
C:\pythonPath\winpythonPath\settings\.ipython\profile_default
I'm
sure
there's
a
better
way
","
but
to
resolve
this
quickly
","
I
copied
the
edited
python
files
from
C:\users\Username\.ipython\profile_default
to
C:\pythonPath\winpythonPath\settings\.ipython\profile_default
Now
(
finally
)
ipython
notebook
64
bit
runs
and
provides
me
the
correct
working
directory
Note
on
Windows
I'm
having
no
issue
with
the
following
syntax
:
This
question
keeps
coming
up
when
I
search
for
ipython
change
pwd
even
though
I
am
not
interested
in
a
notebook
","
but
a
terminal
or
qtconsole
.
Not
finding
a
relevant
config
entry
I
tried
:
This
is
the
base
level
shell
class
;
there
are
Terminal
and
Console
(
and
probably
notebook
)
entries
that
could
further
customize
the
action
.
From
the
docs
it
looks
like
import statements
are
most
common
in
the
entry
","
but
it
appears
that
many
magic
commands
work
as
well
.
I've
a
problem
:
I
created
a
small
Python
script
to
read
data
from
an
Omron
PLC
memory
on
LAN
.
Delphi
program
run
batch
file
that
run
Python
script
periodically
(
every
6
seconds
)
.
This
script
runs
on
2
Win
7
PCs
and
1
Win
XP
PC
.
My
problem
is
:
no
data
transfer
between
Win
XP
PC
and
PLC
for
a
random
period
of
time
(
around
1
minute
","
sometimes
more
)
","
but
Win
7
PCs
have
no
problem
to
communicate
with
same
PLC
.
I
use
UDP
protocol
.
LAN
seems
""""
fall
to
sleep
""""
.
This
is
LOG
File
:
(
08:41:13
->
08:42:30
.
Expectation
08:41:13
->
08:41:19
","
08:41:25
","
08:41:31
","
...
)
What
could
be
the
problem
?
This
could
be
due
to
network
congestion
in
your
LAN
.
If
it
is
just
happening
for
a
little
while
like
1
minute
.
I
try
to
understand
how
to
handle
1D
array
(
vector
in
linear
algebra
)
with
numpy
.
In
the
following
example
","
I
generate
two
numpy.array
a
and
b
:
For
me
","
a
and
b
have
the
same
shape
according
linear
algebra
definition
:
1
row
","
3
columns
","
but
not
for
numpy
.
Now
","
the
numpy
dot
product
:
I
have
three
different
output
.
What's
the
difference
between
"dot(a,a)"
and
"dot(b,a)"
?
Why
"dot(b,b)"
doesn't
work
?
I
also
have
some
differencies
with
those
dot
products
:
Notice
you
are
not
only
working
with
1D
arrays
:
So
","
b
is
a
2D
array
.
You
also
see
this
in
the
output
of
b.shape
:
(
1
","
3
)
indicates
two
dimensions
as
(
3
","
)
is
one
dimension
.
The
behaviour
of
np.dot
is
different
for
1D
and
2D
arrays
(
from
the
docs
)
:
For
2-D
arrays
it
is
equivalent
to
matrix
multiplication
","
and
for
1-D
arrays
to
inner
product
of
vectors
That
is
the
reason
you
get
different
results
","
because
you
are
mixing
1D
and
2D
arrays
.
Since
b
is
a
2D
array
","
"np.dot(b, b)"
tries
a
matrix
multiplication
on
two
1x3
matrices
","
which
fails
.
With
1D
arrays
","
np.dot
does
a
inner
product
of
the
vectors
:
With
2D
arrays
","
it
is
a
matrix
multiplication
(
so
1x3
x
3x1
=
1x1
","
or
3x1
x
1x3
=
3x3
)
:
I've
been
working
on
a
event
based
AJAX
application
that
stores
recurring
events
in
the
a
table
in
the
following
format
(
Django
models
)
:
The
rec_type
stores
data
in
the
following
format
:
For
example
:
This
works
fine
","
and
allows
many
events
to
be
transmitted
concisely
","
but
I
now
have
the
requirement
to
extract
all
events
that
occur
during
a
given
range
.
For
example
on
a
specific
date
","
week
or
month
and
I
am
a
bit
lost
as
to
how
best
to
approach
.
In
particular
","
I
am
stuck
with
how
to
check
if
an
event
with
a
given
recurrence
pattern
is
eligible
to
be
in
the
results
.
What
is
the
best
approach
here
?
Personally
","
I'd
store
an
rrule
object
from
python-dateutil
(
http://labix.org/python-dateutil
)
rather
than
inventing
your
own
recurrence
format
.
Then
you
can
just
define
some
methods
that
use
rrule
.
"between(after, before)"
to
generate
instances
of
your
event
object
for
a
given
range
.
One
catch
though
","
dateutil's
rrule
object
doesn't
pickle
correctly
","
so
you
should
define
your
own
mechanism
of
serialising
the
object
to
the
database
.
I've
generally
gone
with
a
JSON
representation
of
the
keyword
arguments
for
instantiating
the
rrule
.
The
annoying
edge
case
is
that
if
you
want
to
store
stuff
like
'
2nd
Monday
of
the
month
'
","
you
have
to
do
additional
work
with
MO(2)
","
because
the
value
it
returns
isn't
useful
.
It's
hard
to
explain
","
but
you'll
see
the
problem
when
you
try
it
.
I'm
not
aware
of
any
efficient
way
to
find
all
eligible
events
within
a
range
though
","
you'll
have
to
load
in
all
the
Event
models
that
potentially
overlap
with
the
range
.
So
you'll
always
be
loading
in
potentially
more
data
than
you'll
eventually
use
.
Just
make
sure
relatively
smart
about
it
to
reduce
the
burden
.
Short
of
someone
adding
recurrence
handling
to
databases
themselves
","
I'm
not
aware
of
any
way
to
improve
this
.
The
problem
is
the
name
of
your
script
file
:
netrc.py
which
is
the
same
as
the
module
name
.
Rename
it
.
Providing
filename
to
netrc.netrc()
causes
only
to
use
the
specific
netrc
file
in
place
of
the
default
~
\
.
netrc
.
I
am
trying
to
run
a
piece
of
code
using
netrc
lib
in
python
.
I
got
examples
from
the
Internet
but
they
have
all
failed
at
the
first
line
.
Change
it
to
Also
this
is
wrong
:
it
should
be
:
I'm
learning
python
and
I
found
myself
lost
trying
to
create
a
an
if
statement
that
should
be
true
if
the
user
input
y
or
yes
.
The
problem
is
that
it
becomes
true
only
if
I
type
y
but
not
for
yes
.
If
I
remove
the
parenthesis
it
becomes
always
false
.
Ok
","
I
can
do
a
workaround
like
but
I
was
wondering
if
there
is
any
trick
that
don't
force
me
to
write
so
many
times
tha
variable
.
Thank
you
in
advance
.
This
section
of
the
reference
manual
documents
the
order
","
but
claims
it
is
different
than
what
you
are
seeing
:
http://docs.python.org/2/reference/expressions.html#evaluation-order
In
the
following
lines
","
expressions
will
be
evaluated
in
the
arithmetic
order
of
their
suffixes
:
And
there
is
an
open
bug
against
the
implementation
about
it
:
Evaluation
order
of
dictionary
display
is
different
from
reference
manual
.
It
turns
out
that
it's
not
consistent
across
implementations
.
Gives
:
but
prints
val
","
key
","
i.e.
the
value
is
evaluated
first
.
Is
this
behaviour
consistent
across
python
versions
and
implementations
?
documented
anywhere
?
(
http://docs.python.org/2/reference/expressions.html#dictionary-displays
says
nothing
)
That
is
a
three
dim
array
","
you
can
create
it
by
:
Hi
im
sure
there
is
a
much
more
efficient
way
of
coding
this
","
im
primarily
a
java
programmer
but
trying
to
help
some
one
out
with
some
basic
python
.
I
need
to
have
a
2D
array
which
contains
a
set
of
random
co-ordinates
eg
(
2
","
10
)
.
My
initial
thought
was
to
create
the
array
then
fill
it
will
smaller
arrays
containing
the
two
co-ordinates
.
This
produces
the
error
ValueError
:
setting
an
array
element
with
a
sequence
.
So
whats
the
best
way
to
go
about
solving
this
problem
?
Sorry
for
this
hacked
together
code
i've
never
really
had
to
use
python
before
!
@HYRY
gives
the
correct
answer
if
I
understand
what
you
are
trying
to
do
.
.
The
reason
you
get
a
Value
error
is
because
you
are
assigning
a
sequence
","
or
python
list
(
tmp
)
to
a
single
float
(
a
[i]
[j]
)
.
@HalCanary
shows
you
how
to
do
fix
your
code
to
get
around
this
error
","
but
HYRY's
code
should
be
optimal
for
python
computing
.
Running
%
edit
?
will
give
you
the
help
for
the
%
edit
magic
function
.
You
need
to
set
c.TerminalInteractiveShell.editor
","
which
is
in
your
ipython_config.py
.
I'm
not
quite
sure
where
this
is
located
in
Windows
;
on
OS
X
and
Linux
","
it
is
in
~
/
.
ipython
.
You'll
want
to
set
the
variable
to
be
the
full
path
of
the
editor
you
want
.
Alternatively
","
you
can
create
an
environment
variable
EDITOR
in
Windows
itself
","
and
set
that
equal
to
the
full
path
of
the
editor
you
want
.
iPython
should
use
that
.
I
am
using
IPython
notebook
and
I
want
to
edit
programs
in
an
external
editor
.
How
do
I
get
the
%
edit
file_name.py
to
open
an
editor
such
as
Notepad
+
+
.
I'm
using
Windows
7
and
8
(
and
10TP
)
and
Python
3.4.2
.
I
started
with
ipython
locate
to
tell
me
where
ipython
thought
config
files
suggested
elsewhere
should
be
.
When
I
saw
it
was
different
I
read
around
and
came
up
with
the
following
:
On
my
system
","
the
ipython
locate
gave
me
c:\users\osmith\.ipython
","
not
the
_ipython
you'll
see
mentioned
in
the
YouTube
videos
done
with
Windows
XP
","
Look
in
the
directory
ipython
locate
specifies
for
a
profile
directory
;
if
you
aren't
actively
doing
anything
with
ipython
profiles
","
it
should
be
.
ipython\profile_default
","
if
you
are
using
profiles
","
then
I
leave
it
to
you
to
s
/
profile_default
/
${YOUR_PROFILE_NAME
}
/
g
Check
the
profile_default
directory
for
a
ipython_config.py
file
","
if
it's
not
there
","
tell
IPython
to
initialize
itself
:
ipython
profile
create
Open
the
config
file
in
a
text
editor
","
If
you
are
the
kind
of
person
who
hasn't
messed
around
with
their
console
overly
much
and
installs
things
in
standard
places
","
you
can
skip
straight
to
this
step
by
typing
:
ipython
profile
create
followed
by
start
notepad
.
ipython\profile_default\ipython_config.py
.
Search
for
the
string
c.TerminalInteractiveShell.editor
","
The
comment
above
this
indicates
you
can
also
use
the
EDITOR
environment
variable
","
but
hard
coding
file
paths
never
hurt
anyone
so
lets
do
eet
:
Copy
the
line
and
remove
the
leading
hash
and
spaces
from
the
copy
.
Replace
the
text
between
the
apostrophes
(
'
notepad
'
)
with
the
path
of
our
desired
editor
","
e.g
.
c.TerminalInteractiveShell.editor
=
'
c
:
/
program
files
(
x86
)
/
noddyeditor
/
noddy.exe
'
There
is
a
catch
here
","
though
;
some
modern
editors
get
a
bit
fancy
and
automatically
and
","
when
invoked
like
this
","
detach
from
the
console
.
Notepad
+
+
and
Sublime
Text
","
for
example
.
Sublime
accepts
a
""""
-
-
wait
""""
option
","
which
works
some
of
the
time
;
this
tells
the
command
invocation
to
hang
around
until
you
close
the
file
","
for
some
definition
of
until
and
some
other
definition
of
close
.
However
","
the
following
setting
will
work
most
of
the
time
for
sublime
text
:
(
assuming
c:\program
files
\
is
where
your
sublime
text
3
directory
is
)
To
allow
the
user
to
interact
with
the
desktop
in
real-time
","
you
need
to
run
the
application
in
the
users
web
browser
.
Interaction
with
a
webserver
would
just
be
too
slow
to
do
anything
meaningful
.
I
do
not
know
about
any
way
to
execute
Python
in
a
web
browser
","
so
I
would
rule
it
out
.
Some
of
your
options
for
client-sided
code
execution
are
:
Javascript
(
the
recent
addition
of
Canvas
and
WebSocket
made
it
suitable
for
this
kind
of
problem
)
Java
Applets
(
felt
out
of
favor
recently
due
to
security
problems
)
ActiveX
(
IE
-
and
Windows
only
","
very
rarely
used
in
a
public
context
nowadays
)
Flash
(
a
popular
but
dying
technology
)
I
did
a
pretty
fair
bit
of
scouring
","
yet
could
not
find
anything
useful
which
answers
my
questions
.
Either
that
or
I
am
asking
the
wrong
questions
.
I
am
trying
to
make
a
web
application
which
gives
a
user
a
graphical
view
of
the
server
desktop
.
I
have
understood
that
somewhere
in
here
X
engine
has
to
be
invoked
and
I
have
also
understood
that
this
is
not
something
that
php
can
accomplish
primarily
because
its
a
language
which
processes
before
sending
requests
","
please
correct
me
if
I
am
wrong
in
this
regard
.
You
may
say
that
what
I
am
trying
to
accomplish
is
something
akin
to
what
teamviewer
does
only
on
the
web
.
My
dilemma
is
whether
I
should
be
using
python
or
java
for
this
task
","
both
would
be
pretty
apt
for
the
task
","
but
which
one
would
be
better
?
Please
give
your
suggestions
The
short
answer
is
","
you
don't
.
It
is
not
possible
to
use
those
host
functions
from
the
OpenSSL
library
in
GPU
code
.
You
would
need
to
port
the
entire
library
to
the
GPU
first
.
There
","
however
","
are
CUDA
accelerated
hashing
codes
available
","
if
you
care
to
search
for
them
.
Famously
","
a
few
years
ago
Daniel
Bernstein
and
a
group
of
Netherlands
researchers
wrote
a
fast
CUDA
accelerated
SHA1
hasher
to
win
an
EngineYard
sponsored
hash
breaking
competition
.
I've
a
fully
functional
python
code
who
generates
some
hashes
and
uses
cryptography
.
It
is
an
implementation
of
MS-CHAPV2
GenerateNTResponse()
method
so
I
use
SHA1
","
MD4
and
DES
.
Since
I
need
to
deal
with
almost
10000000
hashes
","
I
was
looking
for
a
way
to
speed
it
up
by
using
CUDA
.
The
pure
python
version
takes
something
like
2
hours
to
generate
about
the
90
%
of
all
the
hashes
.
Now
the
problem
is
:
can
I
reuse
the
python
code
and
adapt
it
to
PyCuda
?
I've
not
found
any
kind
of
information
about
it
so
","
following
some
of
the
examples
","
I'm
trying
to
do
it
in
C
using
PyCuda's
SourceModule
.
But
here
comes
another
problem
:
So
","
as
far
as
I
know
","
only
functions
declared
with
__device__
can
be
used
in
SourceModule
.
But
how
can
I
do
it
with
openssl
/
sha.h
functions
?
Here's
the
simple
testing
code
that
I'm
using
to
calculate
SHA1
with
PyCuda
There
are
many
ways
to
import a
python
file
:
Don't
just
hastily
pick
the
first
import strategy
that
works
for
you
or
else
you'll
have
to
rewrite
the
codebase
later
on
when
you
find
it
doesn't
meet
your
needs
.
I
start
out
explaining
the
the
easiest
console
example
#
1
","
then
move
toward
the
most
professional
and
robust
program
example
#
5
Example
1
","
Import
a
python
module
with
python
interpreter
:
Put
this
in
/
home
/
el
/
foo
/
fox.py
:
Get
into
the
python
interpreter
:
You
invoked
the
python
function
what_does_the_fox_say()
from
within
the
file
fox
through
the
python
interpreter
.
Option
2
","
Use
execfile
in
a
script
to
execute
the
other
python
file
in
place
:
Put
this
in
/
home
/
el
/
foo2
/
mylib.py
:
Put
this
in
/
home
/
el
/
foo2
/
main.py
:
run
the
file
:
The
function
moobar
was
imported
from
mylib.py
and
made
available
in
main.py
Option
3
","
Use
from
...
import
...
functionality
:
Put
this
in
/
home
/
el
/
foo3
/
chekov.py
:
Put
this
in
/
home
/
el
/
foo3
/
main.py
:
Run
it
like
this
:
If
you
defined
other
functions
in
chekov.py
","
they
would
not
be
available
unless
you
import
*
Option
4
","
Import
riaa.py
if
it's
in
a
different
file
location
from
where
it
is
imported
Put
this
in
/
home
/
el
/
foo4
/
bittorrent
/
riaa.py
:
Put
this
in
/
home
/
el
/
foo4
/
main.py
:
Run
it
:
That
imports
everything
in
the
foreign
file
from
a
different
directory
.
Option
5
","
Import
files
in
python
with
the
bare
import command
:
Make
a
new
directory
/
home
/
el
/
foo5
/
Make
a
new
directory
/
home
/
el
/
foo5
/
herp
Make
an
empty
file
named
__init__.py
under
herp
:
Make
a
new
directory
/
home
/
el
/
foo5
/
herp
/
derp
Under
derp
","
make
another
__init__.py
file
:
Under
/
home
/
el
/
foo5
/
herp
/
derp
make
a
new
file
called
yolo.py
Put
this
in
there
:
The
moment
of
truth
","
Make
the
new
file
/
home
/
el
/
foo5
/
main.py
","
put
this
in
there
;
Run
it
:
The
empty
__init__.py
file
communicates
to
the
python
interpreter
that
the
developer
intends
this
directory
to
be
an
importable
package
.
If
you
want
to
see
my
post
on
how
to
include
ALL
.
py
files
under
a
directory
see
here
:
https://stackoverflow.com/a/20753073/445131
Bonus
protip
","
whether
you
are
using
Mac
","
Linux
or
Windows
","
you
need
to
be
using
python's
idle
editor
as
described
here
.
It
will
unlock
your
python
world
.
http://www.youtube.com/watch?v=DkW5CSZ_VII
Tweaking
PYTHONPATH
is
generally
not
a
very
good
idea
.
A
better
way
is
to
make
your
current
directory
behave
like
a
module
","
by
adding
a
file
named
__init__.py
","
which
can
be
empty
.
Then
the
python
interpretter
allows
you
to
import files
from
that
directory
.
If
you
are
working
in
the
same
directory
","
that
is
","
b.py
is
in
the
same
folder
as
a.py
","
I
am
unable
to
reproduce
this
problem
(
and
do
not
know
why
this
problem
occurs
)
","
but
it
would
be
helpful
if
you
post
what
os.getcwd()
returns
for
b.py
.
If
that's
not
the
case
","
add
this
on
top
of
b.py
OR
if
they
are
in
the
same
path
","
How
do
I
import Python
files
during
execution
?
I
have
created
3
file
a.py
","
b.py
and
c.py
in
a
path
C:\Users\qksr\Desktop\Samples
The
files
contain
the
code
as
shown
below
:
a.py
b.py
c.py
When
I
execute
the
code
b.py
the
following
error
is
thrown
:
I
believe
my
working
directory
is
default
and
all
the
file
a
","
b
","
c
is
just
created
by
me
in
the
samples
folder
.
How
do
I
import python
files
in
Python
?
By
default
","
Python
won't
import modules
from
the
current
working
directory
.
There's
2
(
maybe
more
)
solutions
for
this
:
which
tells
python
to
look
for
modules
to
import in
.
","
or
:
which
modifies
the
import path
on
runtime
","
adding
the
directory
of
the
'
current
'
file
.
Referring
to
:
I
would
like
to
know
how
to
import a
file
which
is
created
in
any
path
outside
the
default
path
?
My
script
fails
tests
when
working
with
SQLite
base
(
via
sqlalchemy
)
created
in
:
memory
:
and
passes
tests
when
working
with
a
base
created
with
physical
file
.
The
script
is
multi-threaded
.
I
know
it
is
not
the
best
idea
to
use
SQLite
with
multiple
threads
(
locking
","
etc.
)
","
but
I
use
SQLite
to
test
script's
DB
interface
only
.
When
I
use
:
memory
:
","
the
script
dies
complaining
that
there
is
no
table
:
The
testing
procedure
(
with
nose
)
is
as
follows
:
So
in
the
setup
I
do
create
the
database
and
the
columns
.
I
can
see
it
also
in
logs
(
echo=True
)
:
My
guess
was
that
when
I
create
the
base
in
thread
A
","
and
then
I
use
it
in
thread
B
","
then
B
starts
to
operate
on
the
base
before
it
was
actually
created
.
But
","
I
added
time.sleep(3)
after
create_all
and
it
didn't
work
.
And
","
as
mentioned
before
","
it
works
when
I
use
a
file
instead
of
:
memory
:
","
even
if
the
file
is
placed
on
virtual
partition
(
so
","
in
fact
in
memory
)
.
Any
idea
why
is
that
?
You
cannot
create
multiple
connections
to
an
in-memory
database
.
Instead
","
a
new
connection
to
:
memory
:
creates
a
new
database
.
From
the
SQLite
documentation
:
Every
:
memory
:
database
is
distinct
from
every
other
.
So
","
opening
two
database
connections
each
with
the
filename
""""
:
memory
:
""""
will
create
two
independent
in-memory
databases
.
This
is
different
from
an
on-disk
database
","
where
creating
multiple
connections
with
the
same
connection
string
means
you
are
connecting
to
one
database
.
You
are
creating
a
new
connection
for
the
thread
","
thus
creating
a
new
database
that
does
not
have
the
tables
created
.
I
am
trying
to
save
few
variables
in
a
file
using
tkFileDialog
save
option
.
and
in
later
part
i
have
to
retrieve
the
same
variables
.
i
am
not
getting
any
idea
how
to
do
that
.
Thank
you
You
can
use
pickle
:
Now
load
:
I
was
having
a
similar
problem
.
I
resolved
the
issue
this
way
:
Hope
this
helps
.
:
)
The
problem
is
in
this
line
:
The
backward
slash
has
a
special
meaning
.
For
example
","
the
character
""""
\
t
""""
in
a
normal
string
is
the
tab
character
:
That's
why
in
your
error
message
:
You
see
a
double
slash
in
front
of
the
R
-
-
\
R
doesn't
have
any
special
meaning
","
and
so
it
knew
you
meant
one
""""
real
""""
slash
:
but
\
t
does
have
a
special
meaning
.
To
solve
this
problem
you
can
either
use
a
""""
raw
string
""""
","
and
add
""""
r
""""
before
the
string
literal
:
or
","
you
can
simply
use
forward
slashes
instead
-
-
which
Windows
supports
-
-
and
avoid
all
the
trouble
:
Either
way
should
work
.
I
have
trouble
loading
excel
files
into
a
dataframe
using
ExcelFile()
.
I
have
imported
pandas
","
xlrd
and
openpyxl
.
I
am
using
spyder
for
interactive
data
analysis
.
I'm
new
to
pandas
and
python
","
so
I
would
appriciate
an
answer
that
is
understandable
for
a
beginner
.
Could
someone
help
me
?
I
have
2
tables
TBL1
and
TBL2
.
TBL1
has
2
columns
id
","
nSql
.
TBL2
has
3
columns
date
","
custId
","
userId
.
I
have
17
rows
in
TBL1
with
id
1
to
17
.
Each
nSql
has
a
SQL
query
in
it
.
For
example
nSql
for
id
=
=
1
is
:
""""
select
date
","
pId
as
custId
","
tId
as
userId
from
TBL3
""""
id
=
=
2
is
:
""""
select
date
","
qId
as
custId
","
rId
as
userId
from
TBL4
""""
...
nSql
result
is
always
same
3
columns
.
Below
query
runs
and
puts
data
into
the
table
TBL2
.
If
there
is
already
data
in
TBL2
for
that
day
","
I
want
the
query
to
replace
the
data
with
new
data
.
If
there
is
not
data
in
TBL2
","
I
want
to
put
data
in
normal
way
.
For
example
","
if
I
run
the
query
in
the
morning
and
if
I
want
to
run
it
again
in
evening
","
I
want
new
data
to
replace
old
data
for
that
day
","
since
data
will
be
inserted
into
TBL2
everyday
.
It
is
also
precaution
that
if
the
data
already
exists
(
if
run
by
coworker
)
","
I
do
not
want
duplicate
data
for
that
day
.
How
can
I
do
it
?
Thank
you
.
(
I
am
new
to
python
","
I
would
appreciate
if
someone
could
explain
in
steps
and
show
in
the
code
)
Timestamp
(
using
datetime
)
all
data
inserted
into
the
table
.
Before
inserting
","
delete
from
table
where
the
datetime's
day
is
today
.
For
MySQL
","
you
can
use
function
to_days()
with
day
to
get
which
day
a
datetime
is
on
:
https://dev.mysql.com/doc/refman/5.5/en/date-and-time-functions.html#function_to-days
When
inserting
new
rows
","
now()
will
let
you
use
the
datetime
value
corresponding
to
the
current
time
:
https://dev.mysql.com/doc/refman/5.5/en/date-and-time-functions.html#function_now
you
said
a
long
line
is
divided
in
2
lines
or
more
","
right
?
you
could
count
lines
this
way
where
string
is
the
text
you
are
dealing
with
","
and
MaxLen
is
the
maximum
number
of
characters
that
the
textbox
can
show
in
a
line
but
this
just
doesn't
solve
the
problem
if
you
don't
know
how
to
get
MaxLen
","
and
i
actually
don't
...
I
count
lines
in
python
for
a
text
like
this
:
The
same
text
goes
into
a
textBox
control
.
I
would
like
to
scroll
to
the
last
line
but
the
line
count
does
not
help
","
because
the
textBox
wraps
long
lines
making
them
2
or
more
lines
.
I
can
go
to
the
last
line
by
scrolling
to
-
1
","
but
then
I
cannot
scroll
up
anymore
.
I
need
to
know
the
(
actual
)
max
line
count
so
I
can
scroll
to
any
position
i
want
.
A
little
late
but
just
in-case
someone
else
could
use
this
","
here
is
my
solution
.
This
example
will
dynamically
size
the
height
of
the
curses
textpad
widget
.
I'm
trying
to
use
tastypie
filtering
but
when
I
try
to
get
a
resource
through
filtering
I
receive
a
404
.
code
I'm
using
urlopen
to
access
the
resource
:
How
do
I
make
it
so
I
don't
get
a
404
when
trying
to
get
a
resource
through
filtering
with
Tastypie
?
If
there
are
no
results
in
a
list
that
you
query
on
","
TastyPie
would
just
send
back
Json
with
zero
elements
(
something
like
the
following
)
:
So
it
seems
that
if
you're
getting
a
404
","
you
don't
have
something
set
up
correctly
.
The
following
things
could
resolve
your
issue
:
Make
sure
you
have
?
format=json
appended
to
your
url
before
the
&
Make
sure
you've
registered
the
APIResource
Make
sure
you've
set
up
the
appropriate
urls.py
if
anything
is
different
.
I
have
the
following
function
that
takes
3
pieces
of
information
(
name
","
age
","
hometown
)
for
3
people
and
saves
it
in
a
txt
file
.
I
am
now
trying
to
create
a
function
that
will
read
the
text
file
and
print
a
persons
name
if
their
hometown
is
'
Oxford
'
.
So
far
I
have
the
following
just
to
read
the
text
from
the
file
but
im
not
sure
how
to
skip
back
a
line
and
print
the
name
if
the
town
is
Oxford
.
Thanks
for
any
help
!
I
used
the
following
for
anyone
that
is
interested
:
I
am
trying
to
use
Form
Wizard
but
I
can't
figure
out
where
to
set
the
choices
for
the
fields
.
I
see
an
empty
form
that
looks
like
the
admin
panel
for
adding
an
object
.
I
would
like
to
be
able
to
pass
a
question
to
the
form
and
have
the
question
field
filled
out
and
not
editable
and
preferable
not
submitted
.
If
I
do
the
function
get_form_list
has
no
length
Quiz_id
is
unknown
.
so
now
I
am
trying
to
pass
quiz_id
to
the
view
function
and
generate
the
list
of
question
forms
to
be
used
in
the
form
wizard
urls.py
views.py
I
am
getting
the
error
message
Update
based
on
Rohan's
answer
:
With
this
code
I
am
getting
an
error
Here
is
my
models.py
You
should
call
it
using
the
class
instead
of
object
.
So
change
your
call
to
Update
:
The
wizard
view
takes
form
class
list
as
parameters
not
the
form
instance
.
You
are
creating
form
instances
in
question_forms
and
passing
it
to
view
.
If
you
want
to
pass
instance
for
the
form
in
each
step
","
you
can
pass
instance_dict
.
Something
like
...
Use
the
enumerate()
function
to
generate
the
index
along
with
the
elements
of
the
sequence
you
are
looping
over
:
I
am
writing
a
simple
Python
for
loop
to
prnt
the
current
character
in
a
string
.
However
","
I
could
not
get
the
index
of
the
character
.
Here
is
what
I
have
","
does
anyone
know
a
good
way
to
get
the
current
index
of
the
character
in
the
loop
?
Do
you
want
to
iterate
over
characters
or
words
?
For
words
","
you'll
have
to
split
the
words
first
","
such
as
This
prints
the
index
of
the
word
.
For
the
absolute
character
position
you'd
need
something
like
You'd
read
the
lines
into
memory
","
into
a
list
and
then
index
into
that
list
:
I
have
a
list
which
contains
a
series
of
numbers
.
This
list
of
numbers
corresponds
to
a
line
in
a
.
dat
file
.
How
can
I
use
the
list
say
"[0,1,2,3,4,5,6,9,4]"
and
then
print
out
the
line
in
the
.
dat
file
which
each
number
corresponds
to
.
I
figured
out
how
to
make
it
work
.
From
this
:
To
this
:
I
am
working
on
inserting
a
local
image
into
a
table
on
my
mysql
server
.
I
can
insert
data
but
when
I
download
it
","
it
isn't
a
valid
jpg
.
Here
is
a
sample
like
what
I
am
using
.
Am
I
doing
something
wrong
when
I
format
the
data
?
The
code
completes
as
expected
but
the
jpg
is
no
good
.
I'm
playing
around
with
IPython
and
remote
control
:
There
is
a
behavior
difference
between
and
I
realy
can't
understand
what
append
?
Why
is
that
?
what
do
assignement
do
?
Assignment
itself
does
nothing
.
rc
here
is
your
client
.
Doing
rc
[0]
","
or
any
sort
of
indexing
","
generates
and
returns
a
DirectView
object
that's
a
view
with
whatever
engines
you
specify
in
[]
.
This
is
a
shorthand
for
generating
the
views
:
it's
not
actually
just
getting
a
specific
object
.
Thus
","
those
views
aren't
unique
.
The
best
way
to
explain
it
","
I
think
","
is
with
an
example
.
Say
you
have
2
engines
.
You
want
to
run
some
tasks
on
only
engine
one
","
and
want
the
tasks
to
block
.
You
want
to
run
others
on
only
engine
one
","
but
don't
want
them
to
block
.
You
want
to
run
yet
more
on
engines
1
and
2
","
and
don't
want
them
to
block
.
Then
you
could
do
:
Then
","
you
can
use
these
to
run
tasks
in
whatever
way
you'd
like
","
eg
There's
no
actual
magic
being
used
here
.
When
you
run
rc
[0]
twice
","
it
generates
two
views
.
The
second
view
is
not
the
same
as
the
first
.
When
you
assign
rc
[0]
to
a
variable
","
and
then
use
that
variable
","
you're
working
with
one
view
","
and
not
creating
a
new
one
.
iPython
","
like
Numpy
and
Scipy
","
has
quite
a
few
shorthand
notations
that
don't
necessarily
fit
Python's
idioms
perfectly
.
This
is
especially
the
case
with
[]
and
getitem
.
A
purer
Python
way
of
writing
this
could
would
be
to
use
the
much
more
unwieldy
rc.direct_view(1)
","
and
so
on
","
which
would
make
clear
that
this
wasn't
just
getting
an
item
","
and
was
actually
creating
a
view
.
It
is
in
most
of
the
situations
easy
to
implement
copy
constructors
(
or
overloaded
assignment
operator
)
in
C
+
+
since
there
is
a
concept
of
pointers
.
However
","
I'm
quite
confused
about
how
to
implement
shallow
and
deep
copy
in
Python
.
I
know
that
there
are
special
commands
in
one
of
the
libraries
but
they
don't
work
on
classes
that
you
have
written
by
yourself
.
So
what
are
the
common
ways
to
implement
?
P.S
.
Showing
process
on
some
basic
data
structures
(
linked
list
or
tree
)
will
be
appreciated
.
EDIT
:
Thanks
","
they
worked
","
it
was
my
mistake
in
syntax
.
I
am
very
interested
in
overwriting
these
functions
with
__copy__()
and
__deep_copy()
__
.
For
example
.
how
can
I
make
a
deep
copy
without
knowing
which
type
of
information
is
in
a
data
structure
?
The
python
copy
module
can
reuse
the
pickle
module
interface
for
letting
classes
customize
copy
behaviour
.
The
default
for
instances
of
custom
classes
is
to
create
a
new
","
empty
class
","
swap
out
the
__class__
attribute
","
then
for
shallow
copies
","
just
update
the
__dict__
on
the
copy
with
the
values
from
the
original
.
A
deep
copy
recurses
over
the
__dict__
instead
.
Otherwise
","
you
specify
a
__getstate__()
method
to
return
internal
state
.
This
can
be
any
structure
that
your
class
__setstate__()
can
accept
again
.
You
can
also
specify
the
__copy__()
and
/
or
__deepcopy__()
methods
to
control
just
copy
behaviour
.
These
methods
are
expected
to
do
all
the
copying
themselves
","
the
__deepcopy__()
method
is
passed
a
memo
mapping
to
pass
on
to
recursive
deepcopy()
calls
.
An
example
could
be
:
This
example
defines
custom
copy
hooks
to
prevent
self.spam
being
copied
too
","
as
a
new
instance
will
calculate
it
anew
.
I
am
new
to
Python
and
am
trying
to
grasp
existing
scripts
.
This
example
is
given
from
pdfrw
as
follows
:
http://code.google.com/p/pdfrw/wiki/ExampleTools
""""
A
printer
with
a
fancy
printer
and
/
or
a
full-up
copy
of
Acrobat
can
easily
turn
your
small
PDF
into
a
little
booklet
(
for
example
","
print
4
letter-sized
pages
on
a
single
11
""""
x
17
""""
)
.
But
that
assumes
several
things
","
including
that
the
personnel
know
how
to
operate
the
hardware
and
software
.
booklet.py
lets
you
turn
your
PDF
into
a
preformatted
booklet
","
to
give
them
fewer
chances
to
mess
it
up
:
""""
As
best
I
can
tell
the
steps
are
:
the
function
starts
looping
through
the
pages
a
variable
""""
shift_right
""""
is
declared
that
does
?
?
?
(
I
think
it
is
for
the
metadata
?
)
a
variable
""""
stuff
""""
is
declared
that
is
useful
only
for
metadata
(
I
think
)
x
is
incremented
by
the
third
index
of
the
BBox
list
(
what
is
this
list
","
when
was
it
established
?
)
y
is
assigned
to
the
larger
value
between
y
and
the
third
index
of
the
Bbox
list
(
again
","
what
does
this
list
refer
to
?
)
I
think
my
core
confusion
is
regarding
those
two
lines
with
BBox
","
and
how
they
interact
to
create
a
11x17
booklet
PDF
.
Thank
you
for
anyone
who
can
clear
this
up
.
Bbox
is
typically
shorthand
for
""""
bounding
box
""""
:
in
this
case
","
the
rectangle
defining
the
dimensions
of
your
page
.
In
this
case
","
it
looks
like
it
is
an
array
of
4
numbers
","
the
first
two
of
which
are
the
top-left
corner
of
the
page
at
(
0
","
0
)
","
the
third
is
the
x-coordingate
of
the
right
side
of
the
page
(
which
","
because
the
left
side
is
at
0
","
is
also
the
width
)
","
and
the
fourth
is
the
bottom
/
height
of
the
page
.
I
wrote
this
simple
function
""""
processing_flush
""""
in
order
to
print
a
sequence
of
points
(
given
by
index
)
to
test
if
my
software
is
processing
my
data
and
eventually
the
speed
.
The
total
size
of
my
data
is
unknown
.
The
problem
that
i
cannot
fix
is
when
all
points
are
printed
the
first
time
(
ex
:
Processing
....
if
index
is
equal
to
5
)
the
cursor
doesn't
start
from
zero
.
Before
you
overwrite
the
same
line
again
you
need
to
clear
at
least
the
positions
where
the
dots
are
with
spaces
.
The
code
above
may
lead
to
some
brief
flicker
.
In
your
specific
case
it
is
sufficient
to
clear
the
line
when
n
%
index
becomes
0
:
Or
even
better
always
write
index-1
characters
:
Edit
1
:
Or
if
you
prefer
to
have
the
cursor
always
after
the
last
dot
:
Edit
2
:
Or
if
you
prefer
to
have
the
cursor
always
at
the
beginning
of
the
line
:
The
reason
is
that
your
shell
remembers
the
remaining
characters
of
the
previous
line
if
you
overwrite
just
the
first
part
of
it
.
This
might
be
more
easily
accomplished
using
the
Google
Maps
API
and
JavaScript
on
the
client
side
.
Your
server-side
component
sending
the
coordinates
of
taxis
could
certainly
be
done
in
Python
using
Django
","
or
whichever
framework
you
choose
","
but
for
ease
of
mapping
and
dynamic
data
generation
in
the
browser
JavaScript
/
HTML5
is
the
way
to
go
.
I
am
working
on
a
taxi
dataset
.
I
want
to
plot
them
on
country
map
.
also
","
after
plotting
the
trajectories
of
the
taxis
","
i
want
the
user
to
choose
a
point
on
the
map
and
request
him
to
enter
an
epsilon
radius
.
After
he
enters
a
value
","
a
circle
should
be
drawn
for
the
user-selected
point
and
points
of
other
taxis
within
that
circle
should
be
engulfed
.
I
should
show
the
user
how
many
taxis
are
participating
in
that
epsilon
radius
.
so
what
sorta
APIs
or
packages
should
i
use
?
Does
basemap
in
python
provide
with
this
kind
of
dynamism
.
Or
is
there
any
other
tool
i
can
use
?
Or
is
there
a
way
","
where
there
are
ready
maps
available
and
i
can
just
plot
my
trajectories
on
them
and
include
them
in
my
web-based
GUI
and
provide
the
user
to
enter
epsilon
radius
and
do
the
same
stuff
as
i
mentioned
above
.
how
to
go
ahead
?
please
suggest
.
Found
the
issue
myself
!
The
error
is
in
the
os.rename
syntax
.
by
doing
:
The
output
of
"os.path.join(imageFolder, currentLanguage)"
is
IMAGES\REF_IMAGES\-FR
and
not
IMAGES\REF_IMAGES-FR
as
I
expected
The
syntax
I
use
now
is
:
Thanks
to
Mayk
who
lead
me
on
it
actually
.
Test
with
this
:
I'm
using
a
base
folder
REF_IMAGES
where
I
keep
reference
bitmaps
for
a
specified
language
.
I
have
other
several
languages
folder
like
REF_IMAGES-EN
","
REF_IMAGES-NL
etc.
I
want
to
create
a
script
which
will
rename
the
base
folder
to
its
language
ie
.
REF_IMAGES-xx
and
after
rename
the
selected
language
folder
to
the
base
folder
name
.
To
do
this
I
have
a
created
a
txt
file
which
keeps
track
of
the
original
language
in
the
folder
.
My
code
is
the
following
:
However
I
get
the
following
error
while
trying
to
rename
image
folder
to
its
original
name
:
I
have
tried
several
several
Unlocker
programs
but
non
of
them
work
.
I
suppose
it
is
the
script
blocking
it
itself
but
I
can't
figure
out
what
.
Does
anyone
know
?
Regards
Let's
say
you
have
the
following
dict
:
Your
sql
query
could
be
constructed
as
following
:
Now
you
have
to
construct
the
fields
string
and
for
the
values
you
do
something
similar
","
using
the
already
created
fields
lists
.
(
This
could
be
done
in
another
way
if
your
dictionary
was
an
OrderedDict
)
.
Finally
you
can
compose
the
string
.
This
is
a
very
rough
method
","
but
very
understandable
if
you
are
new
to
this
topic
.
Most
of
frameworks
allow
for
much
refined
methods
to
achieve
this
.
Especially
as
to
SQL
injections
.
Print
it
","
play
around
","
understand
the
concepts
and
with
time
you'll
probably
use
other
ways
.
EDIT
A
more
succinct
way
would
be
:
You
can
concatenate
the
keys
and
the
values
of
the
dictionary
with
join()
and
map()
:
I'm
not
so
familiar
with
SQL
.
I
have
a
table
with
over
100
columns
.
What
is
the
best
way
to
insert
data
from
a
dictionary
into
the
corresponding
column
of
my
table
(
key
and
column
names
are
identical
)
?
Example
:
My
SQL
Table
Column1
Column2
Column3
Column4
Column5
My
data
which
I
get
from
a
dictionary
:
First
Data
set
Column2:apple
","
Column4:tree
Second
Data
set
Column1:banana
","
Column2:blue
","
Column3:green
","
Column4:house
","
Column5:red
So
if
the
key
of
the
dictionary
is
identical
to
the
column
name
of
the
table
","
the
data
should
be
inserted
.
I
know
how
to
do
this
for
a
single
query
but
how
can
I
do
this
(
clever
)
for
a
large
number
of
queries
?
Thanks
!
Edit
:
What
I
would
do
at
a
single
level
and
some
clarifications
:
1
)
Create
a
SQL
Table
with
all
120
columns
but
without
any
data
.
2
)
I
have
about
500
word
files
which
contain
a
part
of
the
data
which
should
go
into
the
database
(
about
5-10
from
the
120
columns
)
.
3
)
So
I
have
to
read
them
","
parse
them
and
if
the
key
of
the
data
in
the
dictionary
match
any
of
the
120
column
names
","
the
value
should
be
inserted
.
3
)
Single
mode
:
sql
=
INSERT
INTO
Column1
valuexy
This
is
of
course
very
stupid
for
500
files
and
120
columns
.
I
hope
this
was
more
clear
.
How
do
I
change
the
mode
stdin
is
opened
in
?
Specifically
","
we're
piping
CSV
files
to
the
python
script
to
clean
up
the
data
","
but
with
vertical
tabs
in
the
data
it
seems
to
need
to
be
in
universal-newlines
mode
.
The
problem
data
seems
to
be
some
\
x0b
characters
in
the
input
stream
.
As
printed
by
python
","
after
opening
one
of
the
files
with
'
rU
'
According
to
PEP
278
universal-newlines
is
on
by
default
.
Universal
newline
support
is
enabled
by
default
","
but
can
be
disabled
during
the
configure
of
Python
.
Do
you
have
any
sample
data
?
Your
problem
is
that
the
CSV
file
you
are
reading
uses
CR
(
\
r
)
newlines
exclusively
;
it
has
nothing
to
do
with
the
vertical
tabs
.
Python
2.x
opens
stdin
without
universal
line
support
(
so
that
binary
files
work
correctly
)
.
As
a
workaround
","
you
can
try
this
","
assuming
your
input
is
relatively
small
:
Please
help
.
Besides
","
there
is
one
more
way
to
get
the
average
of
list
is
","
you
can
use
this
one
also
.
Parse
the
IP
into
an
int
","
and
use
bitwise
operators
to
get
it
.
Another
way
would
be
to
use
a
library
like
ipaddr-py
.
I'd
personally
prefer
the
library
.
Use
Regex
:
Using
a
regular
expression
would
be
simple
in
this
situation
.
Using
the
re.match
method
in
conjunction
would
yield
your
result
.
Regular
expressions
can
be
cumbersome
.
You
can
also
use
the
split()
function
I
would
use
regular
expressions
.
You
can
split
all
the
octets
into
a
list
using
r
'
.
'
and
then
recombine
them
in
whatever
order
you
like
.
You
could
write
a
more
complicated
re
to
do
it
in
one
stroke
","
but
i
think
that'd
be
a
bit
harder
.
I
am
writing
a
Python
script
that
will
take
an
IP
address
in
CIDR
format
(
ie
.
1.1.1.0
/
32
or
10.0.0.1
/
24
)
and
split
the
IP
into
three
parts
:
the
first
three
octets
(
1.1.1
)
","
the
last
octet
(
0
)
and
the
Subnet
mask
(
32
)
.
The
IP
will
be
of
variable
length
so
I
don't
know
if
I
can
use
some
sort
of
string
character
counter
.
Any
ideas
?
Thanks
Use
Portable
Python
-
it
is
a
version
of
python
modified
to
do
exactly
what
you
want
.
I
think
it
certainly
sounds
like
a
viable
option
.
As
far
as
I
know
","
the
only
""""
interaction
""""
the
official
Python
installer
has
with
Windows
is
to
add
registry
keys
associating
.
py
and
.
pyw
files
with
the
proper
executables
and
possibly
modifying
the
PATH
variable
.
As
long
as
the
user
has
the
correct
.
dll
files
to
which
the
.
exe's
are
linked
","
you
could
just
zip
up
c:\Python33
or
whichever
version
you're
using
and
distribute
that
with
your
application
.
Before
you
do
that
","
though
","
clone
the
directory
and
go
through
c:\clonedPython\libs\site-packages
and
get
rid
of
any
modules
that
aren't
required
for
your
application
.
Don't
delete
any
necessary
dependencies
!
Portable
Python
is
a
possibility
","
but
there
may
be
some
issues
with
certain
modules
not
working
properly
","
and
it's
not
available
yet
for
Python
3.3
(
3.2.1
is
the
latest
version
","
as
well
as
2.7.3
)
","
so
if
you
have
version-dependent
syntax
that
might
not
be
the
best
choice
.
I'd
like
to
""""
install
""""
a
version
of
Python
locally
","
which
doesn't
touch
anything
on
the
system
(
Windows
in
this
case
)
except
the
directory
I
extract
it
into
.
I
would
run
it
by
specifying
that
particular
python.exe
.
This
is
for
the
end-user
.
Essentially
","
I
want
to
be
able
to
extract
a
Python
into
a
directory
and
start
using
it
immediately
","
without
requiring
the
user
to
even
know
that
my
program
is
using
Python
.
I'm
looking
into
py2exe
and
PyInstaller
as
well
","
but
I'd
like
to
know
if
this
option
is
viable
.
When
you
install
python
on
Windows
","
select
""""
for
current
user
only
""""
rather
than
""""
for
all
users
of
this
system
""""
when
you're
asked
.
And
select
the
installation
target
to
some
custom
directory
","
e.g
.
D:\mypython
\
This
kind
of
installation
will
package
all
necessary
binaries
and
DLL
files
(
e.g
.
msvcr90.dll
)
to
this
specified
dir
","
with
which
you
can
deploy
easily
to
another
system
(
with
same
CPU-bit
and
operating
system
)
.
(
I
got
this
solution
from
a
Chinese
website
http://www.oschina.net/question/23734_13481
-
comment
1
)
When
you
call
pyglet.app.run()
","
you
enter
the
pyglet
loop
and
it
doesn't
come
back
until
the
pyglet
window
is
closed
","
so
your
m.menu()
is
called
only
when
the
pyglet
loop
ends
.
If
you
remove
the
pyglet.app.run
line
from
Main
and
call
it
like
this
:
It
works
.
I
have
a
main
object
with
a
Pyglet
window
as
an
attribute
.
Pylget's
window
class
has
a
method
called
push
handlers
","
which
lets
me
push
methods
to
the
event
stack
.
The
following
code
works
:
The
above
code
will
spawn
a
new
window
at
the
default
size
and
attach
the
on_mouse_press()
and
on_draw
event
handlers
to
it
.
That
works
well
and
good
-
however
","
trying
to
call
on
the
push_handlers()
method
in
other
classes
doesn't
seem
to
work
.
The
above
code
spawns
a
new
window
","
but
it
doesn't
attach
the
menu
class's
handlers
.
Is
there
a
reason
for
this
","
or
a
workaround
I
can
use
?
Thanks
!
There
is
no
moveToThread()
for
processes
since
a
process
lives
in
its
own
memory
space
","
so
it
cannot
see
MainWindow
or
any
of
it
members
.
The
application
that
you
start
using
QProcess
should
be
able
to
execute
as
stand-alone
application
.
To
run
spectra
in
another
QProcess
","
you
will
need
to
make
spectra
a
separate
executable
module
instead
of
MainWindow
member
as
it
is
now
.
EDIT
:
You
need
to
define
the
self-contained
module
that
is
least
dependent
on
the
MainWindow
-
it
can
be
spectra
process
only
","
or
spectra
process
with
tab
.
You
can
pass
info
to
process
either
on
construction
","
or
through
standard
input
","
and
retrieve
data
from
process
through
standard
output
.
Key
idea
when
selecting
lawyer
to
put
process
in
is
to
minimize
communication
and
dependency
between
the
process
and
MainWindow
.
You
may
think
of
a
process
as
simple
C
program
:
int
"main(int argc,char* argv[])"
;
you
can
pass
arguments
on
startup
","
get
additional
input
from
MainWindow
through
cin
/
stdin
if
necessary
","
output
some
results
to
MainWindow
through
cout
/
stdout
/
stderr
(
QProcess
have
interface
for
that
)
.
I
have
a
program
which
uses
QThreads
and
Signals
/
Slots
to
communicate
to
a
GUI
.
It
has
the
simplified
form
shown
below
.
However
","
I
would
like
to
change
this
to
a
QProcess
so
that
I
can
take
advantage
of
multicore
processing
.
Is
there
a
simple
way
to
do
this
?
If
I
simply
change
QThread
to
QProcess
","
there
is
no
moveToThread()
function
for
processes
.
I
have
been
trying
several
different
approaches
to
multicore
processing
","
such
as
Pipes()
and
Queues()
in
the
multiprocessing
module
","
but
I
can't
get
anything
to
work
very
well
.
So
","
I
figured
it
would
be
easier
to
use
QProcess
since
I
am
already
in
Qtland
.
Here
is
the
simplified
code
I
have
for
QThreads
","
signals
and
slots
.
I
suggest
you
do
your
loops
to
match
the
indices
of
the
result
in
the
result
array
rather
than
the
parameters
in
your
computational
function
.
I
have
a
question
.
I
am
currently
moving
over
from
matlab
to
python
and
whilst
I
found
it
easy
to
adapt
to
numpy
and
scipy
some
parts
are
still
a
bit
enigmatic
;
)
What
I
want
to
do
is
calculate
data
in
two
nested
loops
and
put
(
reshape
)
it
into
an
(
m
","
n
)
array
to
make
a
contour
plot
.
So
I
thought
about
the
following
:
but
now
I
am
not
entirely
sure
how
I
should
put
this
into
the
right
shape
(
in
Matlab
I
would
just
index
the
loops
with
i=1:1:10
and
so
on
and
pull
the
actual
value
of
i
out
of
a
vector
...
)
.
I
could
append
the
results
res.append(res)
and
reshape
it
with
"res.reshape((m,n)"
)
.
But
there
might
be
a
more
elegant
way
of
doing
so
?
Every
help
is
very
welcome
!
best
wishes
","
Chris
This
is
one
possible
solution
that
doesn't
need
the
explict
index
to
store
the
values
For
cases
in
which
you
need
the
explict
index
using
enumerate(arange(...)
)
is
both
compact
and
readable
.
I'm
not
sure
this
is
the
most
efficient
way
to
build
large
matrices
with
numpy
however
(
if
you
need
some
speed
then
you
should
decompose
the
function
in
multiple
simple
operations
each
one
performed
explicitly
with
numpy
)
.
You
will
need
to
have
a
date
passed
to
your
template
from
the
view
.
Then
to
get
the
current
month
you
need
to
do
This
will
give
you
the
month
name
i.e
January
using
template
tags
and
filters
To
get
it
in
Spanish
","
enable
Spanish
by
enabling
internationalization
and
creating
localization
strings
for
Spanish
.
You
can
even
use
the
humanize
template
tag
which
has
localizations
available
in
Spanin
Hi
i
am
using
a
variable
`
in
my
template
file
which
gives
me
a
current
month
value
in
english
{
{
currentMonth
}
}
how
can
i
convert
this
into
spanish
language
I
think
you're
misunderstanding
the
way
celery
works
.
You
can't
just
replace
its
broker
with
a
MySQL
table
of
your
own
design
-
well
","
not
without
making
substantial
changes
to
its
source
code
.
The
broker
is
an
internal
part
of
celery
which
it
uses
to
keep
track
of
its
tasks
","
using
its
own
internal
format
","
so
there's
no
inherent
advantage
to
using
a
MySQL
broker
solely
because
that's
the
way
in
which
you
currently
store
the
information
necessary
to
perform
your
tasks
.
You
can
still
use
celery
if
you
like
","
but
you'd
have
to
write
the
code
necessary
to
translate
your
user
activity
table
into
celery
tasks
.
However
","
I'd
recommend
experimenting
with
celery
first
to
...
get
a
basic
understanding
of
how
to
use
it
determine
if
it's
a
good
solution
for
what
you're
trying
to
achieve
Start
off
with
the
celery
tutorial
","
and
see
how
it
goes
from
there
.
Task
at
hand
:
There
is
a
MySQL
table
","
where
a
user
activity
is
pushed
as
a
row
.
That
activity
needs
to
be
processed
later
.
Processing
without
Celery
:
Use
a
script
which
picks
up
N
records
to
process
","
processes
them
and
then
updates
the
status
as
PROCESSED
for
processed
records
.
How
can
the
same
be
achieved
using
Celery
","
I
would
like
to
use
the
same
functionality
to
mark
tasks
instead
of
using
a
broker
?
i.e.
task
add
->
take
10
oldest
rows
added
to
the
mysql
table
with
status
UNPROCESSED
","
which
has
the
task
related
data
as
well
.
on
task
finish
->
mark
the
rows
as
PROCESSED
.
How
can
celery
be
told
that
instead
of
pushing
the
task
onto
the
broker
","
it
has
to
retrieve
tasks
from
the
MySQL
table
?
I
am
a
beginner
at
Celery
","
hence
not
aware
of
all
its
functionalities
.
Using
MySQL
as
broker
is
not
recommended
","
but
I
would
like
to
know
the
feasibility
.
I'm
currently
using
the
staticfiles_urlpatterns
to
serve
static
files
in
development
","
as
recommended
","
but
I'm
getting
404's
on
the
static
files
for
a
particular
app
I'm
using
and
not
sure
why
...
turning
on
directory
indexes
for
all
the
static
files
would
be
cool
if
its
possible
.
I
noticed
the
'
show_indexes
'
option
for
serving
some
types
of
static
files
.
Is
it
possible
to
write
my
debug-only
url
conf
in
a
way
that
enables
this
across
the
board
for
my
static
files
in
development
?
Something
like
this
should
work
(
update
to
use
STATIC_ROOT
)
:
In
the
following
code
","
everything
defaults
over
to
the
Member's
index
controller
.
How
can
I
setup
add_view
to
point
to
particular
methods
within
Members
?
This
way
I
can
have
a
single
Members
button
with
functionality
related
to
it
in
the
dropdown
.
views
:
admin
module
:
Not
sure
how
it'll
help
","
as
you're
creating
3
instances
of
same
class
.
While
you
can
keep
implementation
highly
coupled
","
it
won't
be
as
different
as
having
3
different
classes
.
Anyway
","
add_view
will
always
point
menu
item
to
index
method
.
However
","
self._default_view
contains
default
view
method
name
(
index
for
your
Members
class
)
.
Just
to
give
an
idea
how
it
can
look
like
:
%
d
stands
for
""""
decimal
""""
.
!
is
there
with
the
rest
of
the
string
.
%
d
means
that
a
decimal
number
will
be
outputted
there
.
Synonim
for
%
d
is
also
%
i
.
same
goes
for
instance
with
%
s
and
strings
It
is
used
for
string
formatting
.
The
random
number
will
be
converted
to
an
integer
(
since
%
d
is
being
used
)
","
then
it
will
be
converted
to
a
string
and
inserted
into
the
given
string
.
For
example
:
There
are
several
other
letters
that
correspond
to
different
types
.
They
can
be
found
here
(
in
the
second
table
of
that
section
)
.
These
are
format
specifiers
.
The
%
d
specifier
refers
specifically
to
a
decimal
(
base
10
)
integer
.
The
%
s
specifier
refers
to
a
Python
string
.
I
was
looking
at
this
python
tutorial
and
in
the
generator
section
they
had
something
like
this
:
What
I
want
to
know
is
exactly
what
does
that
mean
and
is
there
any
other
letters
because
I've
seen
%
s
before
.
I've
googled
everything
I
could
think
of
and
nothing
gave
me
an
answer
.
I'm
writing
a
Python
script
that
needs
to
tail
-
f
a
logfile
.
The
operating
system
is
RHEL
","
running
Linux
2.6.18
.
The
normal
approach
I
believe
is
to
use
an
infinite
loop
with
sleep
","
to
continually
poll
the
file
.
However
","
since
we're
on
Linux
","
I'm
thinking
I
can
also
use
something
like
pyinotify
(
https://github.com/seb-m/pyinotify
)
or
Watchdog
(
https://github.com/gorakhargosh/watchdog
)
instead
?
What
are
the
pros
/
cons
of
the
this
?
I've
heard
that
using
sleep()
","
you
can
miss
events
","
if
the
file
is
growing
quickly
-
is
that
possible
?
I
thought
GNU
tail
uses
sleep
as
well
anyhow
?
Cheers
","
Victor
The
cleanest
solution
would
be
inotify
in
many
ways
-
this
is
more
or
less
exactly
what
it's
intended
for
","
after
all
.
If
the
log
file
was
changing
extremely
rapidly
then
you
could
potentially
risk
being
woken
up
almost
constantly
","
which
wouldn't
necessarily
be
particularly
efficient
-
however
","
you
could
always
mitigate
this
by
adding
a
short
delay
of
your
own
after
the
inotify
filehandle
returns
an
event
.
In
practice
I
doubt
this
would
be
an
issue
on
most
systems
","
but
I
thought
it
worth
mentioning
in
case
your
system
is
very
tight
on
CPU
resources
.
I
can't
see
how
the
sleep()
approach
would
miss
file
updates
except
in
cases
where
the
file
is
truncated
or
rotated
(
i.e.
renamed
and
another
file
of
the
same
name
created
)
.
These
are
tricky
cases
to
handle
however
you
do
things
","
and
you
can
use
tricks
like
periodically
re-opening
the
file
by
name
to
check
for
rotation
.
Read
the
tail
man
page
because
it
handles
many
such
cases
","
and
they're
going
to
be
quite
common
for
log
files
in
particular
(
log
rotation
being
widely
considered
to
be
good
practice
)
.
The
downside
of
sleep()
is
of
course
that
you'd
end
up
batching
up
your
reads
with
delays
in
between
","
and
also
that
you
have
the
overhead
of
constantly
waking
up
and
polling
the
file
even
when
it's
not
changing
.
If
you
did
this
","
say
","
once
per
second
","
however
","
the
overhead
probably
isn't
noticeable
on
most
systems
.
I'd
say
inotify
is
the
best
choice
unless
you
want
to
remain
compatible
","
in
which
case
the
simple
fallback
using
sleep()
is
still
quite
reasonable
.
EDIT
:
I
just
realised
I
forgot
to
mention
-
an
easy
way
to
check
for
a
file
being
renamed
is
to
perform
an
os.fstat(fd.fileno()
)
on
your
open
filehandle
and
a
os.stat()
on
the
filename
you
opened
and
compare
the
results
.
If
the
os.stat()
fails
then
the
error
will
tell
you
if
the
file's
been
deleted
","
and
if
not
then
comparing
the
st_ino
(
the
inode
number
)
fields
will
tell
you
if
the
file's
been
deleted
and
then
replaced
with
a
new
one
of
the
same
name
.
Detecting
truncation
is
harder
-
effectively
your
read
pointer
remains
at
the
same
offset
in
the
file
and
reading
will
return
nothing
until
the
file
content
size
gets
back
to
where
you
were
-
then
the
file
will
read
from
that
point
as
normal
.
If
you
call
os.stat()
frequently
you
could
check
for
the
file
size
going
backwards
-
alternatively
you
could
use
fd.tell()
to
record
your
current
position
in
the
file
and
then
perform
an
explicit
seek
to
the
end
of
the
file
and
call
fd.tell()
again
.
If
the
value
is
lower
","
then
the
file's
been
truncated
under
you
.
This
is
a
safe
operation
as
long
as
you
keep
the
original
file
position
around
because
you
can
always
seek
back
to
it
after
the
check
.
Alternatively
if
you're
using
inotify
anyway
","
you
could
just
watch
the
parent
directory
for
changes
.
Note
that
files
can
be
truncated
to
non-zero
sizes
","
but
I
doubt
that's
likely
to
happen
to
a
log
file
-
the
common
cases
will
be
being
deleted
and
replaced
","
or
truncated
to
zero
.
Also
","
I
don't
know
how
you'd
detect
the
case
that
the
file
was
truncated
and
then
immediately
filled
back
up
to
beyond
your
current
position
","
except
by
remembering
the
most
recent
N
characters
and
comparing
them
","
but
that's
a
pretty
grotty
thing
to
do
.
I
think
inotify
will
just
tell
you
the
file
has
been
modified
in
that
case
.
What
makes
you
think
your
only
have
one
connection
to
redis
?
Actually
my
little
test
shows
that
your
server
is
indeed
opening
lots
of
connections
to
redis
.
To
make
the
test
more
clear
","
I
modified
your
print
statement
a
bit
:
Then
run
this
script
to
make
some
requests
:
And
here's
what
I
got
:
It
can
be
seen
that
at
peek
time
you
have
10
greenlets
running
simultaneously
","
waiting
for
sockets
.
Your
code
looks
perfectly
fine
to
me
.
Why
'
the
performance
is
bad
'
is
another
story
.
It
could
be
your
sorted
set
of
'
online
'
is
tooo
large
.
Or
more
likely
you
are
using
a
blocking
client
to
test
the
server
","
in
which
case
you'll
see
only
one
connection
to
redis
.
Have
a
look
at
http://gehrcke.de/2013/01/highly-concurrent-connections-to-redis-with-gevent-and-redis-py/
I'm
not
100
%
is
your
monkey-patching
is
doing
the
trick
but
I'd
replace
it
with
:
You
could
also
go
and
create
your
own
pool
with
gevent
supported
connection
to
redis
...
I'm
using
gevent
to
build
a
server
which
do
some
redis
stuff
and
return
the
result
to
client
.
But
the
performance
is
bad
.
After
some
research
I
found
that
there
is
only
one
connection
to
redis
.
It
looks
like
there
is
only
one
greenlet
spawned
.
Here
is
my
program
:
Is
there
something
wrong
with
my
program
?
Why
there
is
only
one
connection
to
redis
?
For
the
template
part
everything
looks
absolutely
ok
and
there
should
not
be
problem
if
what
you
do
is
what
you
showed
.
Is
Your
list
is
dict()
or
actually
list()
?
Because
your
problem
is
here
:
This
syntax
will
work
only
if
List
is
dictionary
.
In
case
of
list
you
should
write
:
Here
is
what
i
want
tmpl1.jinja
tmpl2.jinja
tmpl3.jinja
Basically
i
have
a
user
block
that
exists
across
site
with
only
the
action(one or more link but with quiet a few html like image etc)
changing
.
What
can
i
do
.
Thanks
AppEngine
can
now
write
to
a
local
""""
ephemeral
""""
disk
storage
when
using
Managed-VM
which
is
not
supported
when
using
the
sandbox
method
as
specified
on
this
documentation
:
https://cloud.google.com/appengine/docs/managed-vms/tutorial/step3
I
have
seen
a
number
of
questions
relating
to
writing
files
&
creating
new
directories
using
Python
and
GAE
","
but
a
number
of
them
conclude
(
not
only
on
SO
)
by
saying
that
Python
cannot
write
files
or
create
new
directories
.
Yet
these
commands
exist
and
plenty
of
other
people
seem
to
be
writing
files
and
opening
directories
no
problem
.
I'm
trying
to
write
to
.
txt
files
and
create
folders
and
getting
the
following
errors
:
Case
#
1
:
produces
""""
IOError
:
[
Errno
30
]
Read-only
file
system
:
'
aardvark.txt
'
""""
.
But
i've
checked
and
it's
def-o
not
a
read
only
file
.
Case
#
2
:
produces
""""
OSError
:
[
Errno
38
]
Function
not
implemented
:
'
C:\project\folder
'
""""
What
am
i
missing
?
Appengine
does
not
support
any
write
operations
to
the
filesystem
(
amongst
other
restrictions
)
.
The
BlobStore
does
have
a
file
like
api
","
but
you
cannot
rewrite
/
append
to
existing
blob
store
entities
.
The
dev
server
also
presents
these
restrictions
to
emulate
production
environment
.
You
should
probably
have
a
read
of
the
some
of
the
docs
about
appengine
.
The
overview
doc
https://developers.google.com/appengine/docs/python/overview
explicitly
states
you
can't
write
.
I
am
facing
a
strange
error
while
executing
a
python
code
.
The
following
code
is
a
small
snippet
of
the
python
code
I
am
executing
:
While
executing
samplecode.py
I
am
facing
an
error
showing
the
following
:
My
logging.py
which
contains
the
code
that
needs
to
be
imported
while
execution
.
The
following
is
the
code
:
we
can
see
that
the
class
Dynamic
is
created
yet
the
import error
is
thrown
.
The
strangest
thing
is
I
did
few
examples
of
importing
files
and
it
worked
well
.
I
have
tried
hard
but
still
cannot
figure
it
out
.
I
would
like
to
know
why
this
error
was
thrown
and
why
suddenly
for
this
and
not
in
previous
samples
?
Python
already
has
a
built-in
logging
module
","
which
is
being
located
before
yours
(
you're
appending
your
folder
to
the
end
of
the
path
)
.
Rename
your
logging.py
file
to
something
else
.
You're
reading
in
2048
byte
at
once
","
which
put
the
reading
cursor
in
the
middle
of
a
line
.
In
the
next
read
","
the
rest
of
that
line
is
discard
because
it
doesn't
start
with
a
tag
.
Instead
of
rolling
your
own
parser
","
consider
using
iterparse
.
An
even
faster
version
of
iterparse
is
included
with
lxml
Here's
an
example
I
am
trying
to
parse
a
large
xml
file
and
print
the
tags
to
an
output
file
.
I
am
using
minidom
","
my
code
is
working
fine
for
30Mb
files
but
for
larger
ones
it
is
getting
memory
error
.
So
I
used
bufferred
reading
the
on
file
but
now
I
am
unable
to
get
the
desired
output
.
XML
File
Desired
Output
Sony
","
Burger
","
Apple
Samsung
","
Pizza
","
HTC
Bravia
","
Pasta
","
BlackBerry
When
reading
with
buffer
its
giving
me
an
output
saying
:
-
Sony
","
Burger
","
Apple
Samsung
","
Piz
Bravia
","
Pasta
","
BlackBerry
I
tried
with
seek()
but
still
I
was
unable
to
get
the
desired
output
.
Thanks
for
your
support
and
i
have
finally
written
my
code
and
its
working
great
here
it
is
I
have
a
dataframe
and
I
grouped
it
by
two
keys
"df.groupby(['key1',key2'])"
.
For
each
key2
entry
","
how
do
I
display
the
its
percent
of
key1
values
?
call
groupby
twice
for
""""
k1
""""
and
(
""""
k1
""""
","
""""
k2
""""
)
","
and
then
do
div
:
output
:
Here's
an
alternative
method
using
one
groupby
statement
.
Group
by
k1
","
select
column
k2
and
apply
a
lambda
function
.
The
lambda
gets
frequency
counts
for
each
level
of
k2
within
k1
and
then
we
divide
by
the
count
of
k1
:
Performance
:
HYRY's
method
:
My
method
:
In
order
to
ensure
db
consistency
","
I
would
like
to
batch
set
the
type
of
the
last
column
of
every
table
to
TINYINT(1)
UNSIGNED
NOT
NULL
.
I
found
out
how
to
loop
through
tables
and
target
the
last
column
","
change
its
type
and
setting
the
NOT
NULL
flag
but
I
can't
find
how
to
set
the
UNSIGNED
flag
.
I
tried
both
:
but
I
get
a
TypeError
:
flag
is
read-only
.
I
also
tried
setting
the
dataType
property
of
the
column
to
a
reference
to
the
dataType
property
of
a
column
having
the
UNSIGNED
flag
(
defined
through
GUI
)
.
Finally
I
tried
:
but
it
returns
0
and
doesn't
change
anything
(
it
returns
1
if
I
remove
UNSIGNED
so
I
think
it
doesn't
work
with
flags
)
.
Is
there
any
way
to
change
column
flags
(
ie
:
UNSIGNED
","
ZEROFILL
)
using
a
Python
script
in
MySQL
Workbench
?
You
need
to
use
grt.root.wb.doc.physicalModels
[0]
.
catalog.schemata
[0]
.
tables
[1]
.
columns
[7]
.
flags.append('UNSIGNED')
I
know
its
been
2
years
since
the
question
was
asked
This
provides
a
basic
understanding
on
how
to
do
this
TO
add
is
whit
column.isNoNull=1
I've
pointed
setup.py
towards
/
lib
where
my
libz.so
is
.
When
I
then
run
setup.py
install
","
I
get
the
ZLIB
support
confirmed
:
But
then
","
after
install
","
I
still
get
the
error
:
I've
fixed
this
by
downloading
the
zlib
source
","
building
it
and
pointing
the
Image
build
to
that
.
goal
Understanding
how
a
radiobutton
in
a
Tkinter
menu
works
code
I
have
a
radio
button
inside
the
options
menu
as
so
:
v
=
BooleanVar()
v.set(True)
"options.add_radiobutton(label=""change pop up"", command =togglePopUp,variable=v,onvalue=True,offvalue=False)"
togglePopUp
is
a
function
that
changes
the
value
of
variable
v
from
True
to
False
or
vice
versa
.
Main
window
is
already
opened
and
this
menu
will
be
added
later
to
the
window
.
This
is
just
the
fragment
of
code
that
is
related
to
the
radiobutton
.
Question
Now
my
question
is
when
I
press
the
radiobutton
(
after
running
the
code
)
will
the
value
of
the
variable
be
changed
or
will
the
function
togglePopUp
be
called
?
If
the
function
will
be
called
then
what
will
happen
to
the
status
of
the
radiobutton
?
will
the
status
of
the
radiobutton
be
updated
instantly
or
will
there
be
a
delay
?
research
I
read
about
the
radiobutton
and
the
Boolean
variable
from
the
Tkinter
book
at
effbot.org
.
But
I
was
not
convinced
about
how
it
worked
.
I
tried
a
program
but
I
am
not
getting
the
output
that
I
essentially
want
.
So
I
decided
to
understand
how
it
works
at
a
deeper
level
.
specs
python
2.7
Tkinter
8.5
Linux
Mint
14
Both
actions
will
occur
.
When
you
click
on
a
radiobutton
","
first
the
variable
will
change
its
value
","
and
after
that
the
event
handler
passed
as
command
option
is
called
if
present
.
Also
your
example
would
not
work
","
since
add_radiobutton
doesn't
allow
the
onvalue
and
offvalue
options
-
only
value
.
For
a
string
","
s
","
doing
:
will
only
remove
the
leading
and
trailing
newline
characters
.
What
you
want
is
try
this
a
is
the
output
string
Have
you
tried
the
replace
function
?
Output
Above
is
my
output
.
Now
I
want
to
get
rid
of
""""
\
n
""""
in
Python
.
How
can
I
realize
this
?
Should
I
use
re
module
?
I
use
text
to
represent
all
the
above
text
and
"text.strip(""\n"")"
have
no
use
at
all
.
Why
?
thank
you
!
As
Matthias
says
in
the
comment
","
it
doesn't
look
as
if
you're
actually
invoking
Django
at
all
-
those
are
normal
Apache
directory
indexes
(
so
naturally
Django
won't
be
parsing
the
templates
","
since
it's
not
even
being
called
)
.
Since
you're
just
starting
out
","
you
should
not
be
using
Apache
at
all
","
but
start
up
the
local
development
server
(
.
/
manage.py
runserver
)
as
explained
in
the
tutorial
.
Try
this
:
Imports
:
Views.py
:
Call
like
this
in
template
:
I
could
not
find
TEMPLATE_DIR
in
settings.py
","
to
tell
where
my
template
files
are
.
Also
","
when
I
go
to
my
index.html
(
that
you
can
see
below
)
","
parsing
does
not
work
(
for
exapmle
{
{
title
}
}
","
which
you
can
see
below
in
views.py
)
.
Why
does
it
(
{
{
title
}
}
)
not
work
?
I
think
that
I
am
so
close
to
make
it
work
","
but
I
can't
.
Also
","
I
could
not
find
any
related
topic
in
SO
.
SO
","
What
I
want
to
see
is
this
:
But
I
see
this
:
index.html
looks
like
:
index.html
source
:
view.py
seems
like
this
:
models.py
seems
like
this
:
Here
is
what
I
did
before
:
Python
2.7.2
installed
Django
1.5
installed
I
had
problems
about
creating
my
project
(
django-admin.py
startpriject
mysite
)
","
it
was
a
PATH
issue
","
and
solved
at
the
end
","
thanks
to
SO
","
I
have
successfully
created
my
project
.
Then
I
modified
models.py
like
this
(
to
create
a
simple
blog
)
:
MySQL-python
1.2.4
installed
I
have
created
a
mysql
database
and
a
user
","
added
user
to
the
database
and
gave
all
the
privileges
(
done
all
in
bluehost
frontend
panel
)
.
I
had
problems
about
database
update
","
thanks
to
SO
","
that
has
been
also
solved
.
Why
does
it
(
{
{
title
}
}
)
not
work
?
What
should
I
change
?
Should
I
re-install
something
?
You
should
use
the
render
shortcut
.
views.py
This
has
the
added
advantage
of
adding
the
request
instance
into
the
context
which
forces
the
use
of
RequestContext
.
Read
about
it
here
:
https://docs.djangoproject.com/en/dev/topics/http/shortcuts/#render
A
PKCS7
has
a
collection
of
SignerInfos
.
Each
SignerInfo
may
have
a
different
Message
Digest
Algorithm
.
See
https://github.com/erny/pyx509.
This
needs
pyasn1
and
pyasn1-modules
.
This
extracts
the
digest
algorithm
for
each
Signer
Info
.
I
need
to
get
the
digest
of
a
PKCS#7
envelop
to
manually
check
it
.
Usually
when
you
want
to
validate
the
signature
of
a
pkcs#7
envelop
you
do
that
:
But
in
my
case
","
the
digest
type
is
md5sha1
that
is
not
recognized
by
openssl
:
What
I
need
to
do
I
to
get
the
pkcs#7
signedContent
and
to
manually
check
it
.
What
I
need
is
a
Python
equivalent
to
org.bouncycastle.cms.CMSSignedDataParser
.
How
can
I
get
the
digest
to
be
able
to
validate
it
manually
without
having
to
use
sm_obj.verify
?
Try
the
following
","
heavily
inspired
by
this
blog
post
:
Ok
so
I
was
able
to
do
it
in
bash
:
Just
need
to
convert
it
in
Python
now
.
As
other
answers
have
stated
","
check
that
port
3050
is
open
.
However
","
fdb
only
supports
Firebird
2.0
or
higher
.
For
Firebird
1.5
","
you
can
use
either
pyodbc
or
pyfirebirdsql
Note
that
","
among
other
issues
(
like
not
handling
Firebird
INTEGER
datatypes
properly
)
","
pyfirebirdsql
isn't
100
%
compliant
with
PEP
249
-
-
Python
DB
API
2.0
as
calls
to
Cursor.rowcount
always
return
-
1
.
Edit
:
After
posting
this
","
I
took
it
upon
myself
to
write
the
code
for
pyfirebirdsql's
rowcount
function
","
so
now
it
works
as
expected
","
instead
of
always
returning
1
.
Shortly
after
that
","
the
pyfirebirdsql
author
fixed
the
INTEGER
issues
as
well
.
You
must
to
check
the
port
open
in
your
host
as
ain
said
.
BTW
","
fdb
is
just
for
firebird
2.0
and
higher
.
Check
it
out
.
https://fdb.readthedocs.org/en/latest/getting-started.html#installation
Assuming
that
the
IP
192.168.40.28
is
correct
my
next
quess
would
be
that
you
don't
have
the
port
3050
open
(
thats
the
default
port
for
Firebird
)
.
Check
your
server's
firewall
and
open
the
port
.
You
can
use
some
other
port
instead
of
3050
by
seting
the
RemoteServicePort
parameter
in
the
firebird.conf
file
","
but
then
you
have
to
set
the
port
parameter
in
the
connect
method
too
.
I'm
trying
to
connect
to
a
Firebird
1.5
database
that
is
located
on
a
server
","
from
my
local
machine
with
Python
fdb
libary
.
but
I'm
having
no
luck
.
the
server
is
windows
2008
server
R1
running
Firebird
1.5.6
as
a
service
.
It
also
has
a
System
DSN
called
firebird
.
How
can
i
connect
to
it
via
python
?
I'm
using
this
code
:
but
it
generates
this
result
:
what
am
I
doing
wrong
here
?
If
you
need
to
special-case
these
types
","
use
the
name
of
the
class
instead
:
Both
EmailField
and
URLField
are
Charfield
subclasses
","
just
with
a
maximum
length
pre-set
and
a
dedicated
validator
.
I
have
fields
in
models.py
which
are
EmailField
and
URLField
.
When
I
call
get_internal_type()
on
these
the
method
returns
CharField
instead
of
EmailField
and
URLField
.
I
want
it
to
be
EmailField
and
URLField
for
further
processing
.
Yep
","
this
is
because
you
are
not
waiting
for
threads
to
finish
their
job
","
so
the
main
thread
just
exits
.
Try
this
:
EDIT
Ha
","
I
don't
think
I'm
right
here
","
because
you
didn't
mark
your
threads
as
daemons
.
It
should
work
even
without
joining
.
But
have
a
look
at
this
code
:
It
will
always
reach
print
""""
test
""""
line
no
matter
what
.
So
in
your
code
it
should
always
reach
print
""""
+
%
s
start
""""
%
(
self.getName()
)
no
matter
what
.
Are
you
sure
it
doesn't
work
?
:
)
If
it
doesn't
","
then
there
are
only
two
possibilities
:
There
is
a
blocking
operation
and
/
or
exception
in
your
__init__
method
.
But
then
it
would
not
reach
final
print
;
concurrent
variable
is
0
for
some
reason
(
which
is
not
consistent
with
the
final
print
)
.
I
am
new
to
python
.
I
am
trying
out
Hbase
thrift
client
using
thrift
.
I
got
some
code
on
net
","
which
I
just
modify
to
work
with
latest
version
of
thrift
but
when
I
run
the
code
","
it
just
exit
","
no
threads
are
started
.
Here
is
the
code
.
I
just
get
a
message
10
thread
created
","
each
thread
will
write
6000
records
Since
you
have
app
in
C:\myap
you
need
to
run
appcfg.py
update
C:\myap
.
It's
just
a
path
to
you
app
on
your
machine
.
In
windows
command
line
.
For
example
","
""""
C:\Program
Files
(
x86)\Google\google_appengine\appcfg.py
""""
update
C:\myap
No
","
appcfg
uses
SSL
while
uploading
.
It's
safe
.
If
you
mean
to
call
application
uploading
-
it's
not
really
safe
.
I
don't
know
why
you
need
this
.
You
can
add
app
developers
in
App
Engine
admin
console
","
so
they
will
be
able
to
deploy
application
from
their
accounts
.
I
have
the
myapp.py
and
app.yaml
in
my
windows
C:\myap
directory
.
The
docs
say
to
use
:
appcfg.py
update
myapp
/
to
upload
the
app
.
I've
downloaded
/
installed
Python
and
the
Google
python
kit
.
Sorry
","
for
these
noobish
questions
","
but
:
Is
the
myapp
/
listed
above
refer
to
c:\myapp
on
my
windows
machine
?
Or
is
it
the
name
of
my
app
on
the
google
side
?
How
/
where
do
I
type
the
appcfg.py
to
upload
my
directory
?
Are
there
any
security
issues
associated
with
using
my
gmail
account
and
email
address
?
I'd
like
anybody
from
Second
Life
to
be
able
to
call
this
from
in-world
.
There
will
be
about
a
dozen
calls
a
week
.
Are
they
going
to
have
to
authenticate
with
my
email
/
password
to
use
it
?
Thanks
for
any
help
you
can
provide
!
I
have
two
lists
of
sports
players
.
One
is
structured
simply
:
The
second
is
a
list
of
lists
structured
:
I
ultimately
want
to
search
the
contents
of
the
second
list
and
pull
the
info
if
there
is
a
matching
name
from
the
first
list
.
I
need
to
swap
'
Lastname
","
Firstname
'
to
'
Firstname
Lastname
'
to
match
list
2's
formatting
for
simplification
.
Any
help
would
be
great
.
Thanks
!
You
can
swap
the
order
in
the
list
of
names
with
:
An
explanation
:
this
is
a
list
comprehension
that
does
something
to
each
item
.
Here
are
a
few
intermediate
versions
and
what
they
would
return
:
I
have
a
dataframe
with
many
many
columns
.
I
want
to
reduce
this
dataframe
to
one
with
only
the
columns
I
require
.
Instead
of
using
for
all
the
columns
that
I
don't
need
","
is
there
a
way
to
select
the
ones
I
do
and
create
a
new
dataframe
?
I
have
tried
:
You
can
select
the
columns
in
a
list
:
As
@DSM
points
out
","
in
general
there
could
be
many
rows
with
name
'
World
'
","
so
somewhere
down
the
line
we'll
need
to
pick
one
.
One
way
to
do
this
which
seems
kind
of
nice
could
be
to
use
where
(
and
then
max
)
:
Note
:
if
there
is
no
row
with
name
'
World
'
this
will
return
NaN
.
I
have
this
dataframe
in
pandas
:
I
want
to
select
an
element
based
on
its
string
value
in
""""
name
""""
column
and
then
get
the
value
as
a
string
.
To
select
the
element
:
The
problem
is
that
it
doesn't
give
a
simple
string
but
a
series
.
Casting
to
a
string
won't
help
-
-
how
can
I
just
get
the
string
""""
World
""""
out
of
this
?
Is
this
the
only
way
?
thanks
.
There's
one
method
that
no
one
mentioned
that
might
be
worth
noting
.
This
was
a
problem
I
was
having
where
I
was
doing
multiple
criteria
checks
and
getting
back
a
single
item
Series
(
basically
a
unique
row
result
)
.
If
you
have
a
single
item
in
a
Series
and
just
need
that
item
OR
know
the
index
of
the
particular
item
you
want
to
gather
","
just
do
this
:
for
the
first
(
and
only
)
item
in
a
single
item
Series
.
Or
this
:
where
index
is
the
index
of
the
item
you
are
looking
for
in
the
Series
.
If
you
want
it
as
a
string
","
you
may
have
to
cast
as
a
string
if
it
is
not
already
stringified
by
default
.
This
what
I
asked
to
do
:
"save_friends(filename, friends_list)"
takes
a
file
name
and
a
friends
list
and
writes
that
list
to
thefile
in
the
correct
format
.
So
","
for
example
","
"save_friends('friends.csv', load_friends('friends.csv')"
)
should
overwrite
the
friends
le
with
exactly
the
same
content
.
This
is
my
code
:
The
problem
is
when
I
run
the
text
code(a code provided by school that dose some simple tests)
","
it
tells
me
So
","
how
can
I
return
'
None
'
while
the
input
is
not
really
a
list(Like the input 'd' in this circumstance)
?
You
replace
the
friends_list
argument
with
an
empty
list
at
the
top
of
your
function
:
Remove
that
line
.
Also
","
your
function
should
not
return
anything
;
you
need
to
remove
the
return
line
too
to
pass
the
assignment
test
","
which
expected
None
from
the
function
(
which
is
the
default
)
.
Note
that
by
returning
early
","
you
never
call
.
close()
on
the
file
either
.
I
have
a
total
of
10
files
(
could
be
more
at
some
point
-
but
it
will
be
a
fixed
number
)
.
They're
small
-
at
around
80
byte
.
While
reading
from
them
is
all
good
and
works
-
its
slow
.
I
guess
its
because
the
script
handles
them
one
by
one
and
waits
for
the
IO
-
so
I
started
to
read
into
Threading
and
Queue
but
I
couldnt
come
up
with
a
working
solution
...
Can
anyone
give
me
an
example
of
opening
several
files
threaded
?
Code
I'd
like
to
put
into
several
threads
:
Reading
small
files
isn't
slow
","
provided
you
do
it
in
one
go
.
First
","
let's
create
a
80
byte
test
file
;
Then
we
define
a
function
to
read
all
of
it
;
Then
","
a
timing
run
(
reading
from
a
normal
harddisk
","
not
an
SSD
)
:
So
it
takes
18
μs
to
read
such
a
file
.
I
wouldn't
call
that
slow
.
When
I
create
9
of
these
test
files
and
read
them
in
a
loop
:
With
the
loop
overhead
it
is
still
only
about
21
μs
per
file
.
Edit
:
Having
seen
your
code
","
it
seems
pretty
complicated
for
what
it
does
.
I
would
structure
it
like
this
:
Advantages
:
This
reads
every
sensor
file
in
one
go
.
It
also
removes
two
function
calls
per
sensor
.
Much
less
typing
.
In
PySide
","
I
want
to
emit
a
signal
with
the
class
that
defines
the
signal
as
a
parameter
.
E.g
.
:
However
","
the
second
line
gives
a
NameError
because
MyWidget
doesn't
exist
yet
at
that
point
.
For
now
I
work
around
the
issue
by
emitting
a
QWidget
like
this
but
I
would
prefer
a
solution
with
MyWidget
as
type
.
Any
ideas
?
It
is
possible
to
define
a
signal
after
the
class
definition
.
Your
code
would
look
like
:
You
just
don't
need
this
parameter
.
Use
QObject::sender
inside
a
slot
to
find
out
which
object
emitted
the
signal
.
This
happens
because
you
put
a
slash
at
the
end
of
the
filename
.
The
trailing
slash
causes
the
OS
to
follow
the
link
","
so
that
the
result
is
the
target
directory
which
is
not
a
link
.
If
you
remove
the
slash
","
islink
will
return
True
.
The
same
thing
happens
in
Bash
as
well
:
In
os
there's
a
function
os.path.islink(PATH)
which
checks
if
PATH
is
symlink
.
But
if
fails
when
PATH
is
a
symlink
to
some
directory
.
Instead
-
-
python
thinks
it
is
directory
(
os.path.isdir(PATH)
)
.
So
how
do
I
check
if
a
dir
is
link
?
Edit
:
Here's
what
bash
thinks
:
and
here's
what
python
thinks
:
Build
lists
of
matched
lines
-
several
flavors
:
Build
generator
of
matched
lines
(
memory
efficient
)
:
Print
all
matching
lines
(
find
all
matches
first
","
then
print
them
)
:
Print
all
matching
lines
(
print
them
lazily
","
as
we
find
them
)
Generators
(
produced
by
yield
)
are
your
friends
","
especially
with
large
files
that
don't
fit
into
memory
.
I
searched
around
but
i
couldn't
find
any
post
to
help
me
fix
this
problem
","
I
found
similar
but
i
couldn't
find
any
thing
addressing
this
alone
anyway
.
Here's
the
the
problem
I
have
","
I'm
trying
to
have
a
python
script
search
a
text
file
","
the
text
file
has
numbers
in
a
list
and
every
number
corresponds
to
a
line
of
text
and
if
the
raw_input
match's
the
exact
number
in
the
text
file
it
prints
that
whole
line
of
text
.
so
far
It
prints
any
line
containing
the
the
number
.
Example
of
the
problem
","
User
types
20
then
the
output
is
every
thing
containing
a
2
and
a
0
","
so
i
get
220
foo
200
bar
etc.
How
can
i
fix
this
so
it
just
find
""""
20
""""
here
is
the
code
i
have
Thanks
.
you
should
use
regular
expressions
to
find
all
you
need
:
regular
expression
will
return
you
all
numbers
in
a
line
as
a
list
","
for
example
:
so
you
don't
match
'
200
'
or
'
220
'
for
'
20
'
.
It's
very
easy
:
The
check
has
to
be
like
this
:
If
file.txt
has
a
layout
like
this
:
We
split
up
""""
1
foo
""""
into
[
'
1
'
","
'
foo
'
]
and
just
use
the
first
item
","
which
is
the
number
.
To
check
for
an
exact
match
you
would
use
num
=
=
line
.
But
line
has
an
end-of-line
character
\
n
or
\
r\n
which
will
not
be
in
num
since
raw_input
strips
the
trailing
newline
.
So
it
may
be
convenient
to
remove
all
whitespace
at
the
end
of
line
with
I
have
installed
Python
2.7.3
and
PyScripter
.
When
I'm
trying
to
run
this
simple
code
","
the
interupter
doesn't
show
anything
:
(
tried
also
without
the
encoding
)
Works
for
me
in
the
interactive
shell
:
Try
putting
it
in
an
.
py
file
and
then
run
from
cmd
You
should
pass
a
reference
to
a
function
(
without
the
parentheses
)
to
the
command
option
.
If
you
don't
remove
the
parentheses
","
what
you
are
doing
is
calling
the
function
:
I
am
trying
to
develop
a
program
that
a
user
enters
in
some
text
then
press
submit
and
it
would
take
the
text
in
the
entry
field
and
compare
it
to
another
value
.
The
code
is
intended
to
print
to
the
console
""""
hello
""""
if
""""
hello
""""
is
written
in
the
entry
field
before
the
user
presses
submit
.
However
","
the
code
does
not
print
""""
hello
""""
to
the
console
","
any
suggestions
?
How
to
retrieve
the
line
number
in
which
a
error
occured
?
In
the
next
script
the
line
outputs
(
as
the
result
of
the
last
raise
)
but
there
is
no
line
num
.
in
JSError
object
.
The
result
:
If
you
only
need
a
line
number
","
you
can
retrieve
it
from
traceback.extract_tb
function
.
Probably
we
can
make
the
next
hack
:
If
you
know
more
elegant
method
please
publish
it
.
I
am
trying
to
learn
pygtk
and
I
am
developing
a
GUI
for
axel
.
My
problem
is
I
need
to
show
progress
of
download
in
my
main
window
.
I
used
two
threads
one
for
downloading
and
second
for
progress
calculation
.
Download
is
done
by
axel
but
my
second
thread
is
not
running
until
first
thread
stops
.
and
second
thread
is
not
updating
the
gui
I
used
gobject.idel_add
it
stucks
the
main
window
upon
download
start
tried
to
use
Gtk.gdk.thread_init()
/
thread_enter()
/
thread_leave()
it
says
no
module
gdk
in
Gtk
.
On
top
of
the
page
Gtk
is
imported
from
gi.repository
By
the
way
I
am
using
quickly
to
develop
the
app
Is
there
any
way
to
solve
the
problem
.
or
any
similiar
examples
.
Check
this
thread
separate
threads
in
pygtk
application
may
be
usefully
for
you
.
Here
left
a
pygtk
manage
multithreading
example
to
you
.
Just
a
style
question
:
Is
there
a
build-in
method
to
get
the
combinations
under
the
assertion
of
commutative
property
and
excluding
elements
paired
with
itself
?
Doesn't
seem
very
Python
.
I
know
of
itertools.product
which
is
the
Cartesian
product
.
I
could
convert
it
to
a
set
which
would
exclude
the
identity-pairings
but
it's
still
the
non-commutative
product
.
you
mean
this
?
output
:
Assuming
a
and
b
are
identical
.
itertools.combinations
","
if
both
lists
are
the
same
like
here
.
Or
in
the
general
case
itertools.product
","
followed
by
some
filtering
:
Also
","
I
think
the
term
combination
already
means
that
the
order
of
elements
in
the
result
doesn't
matter
.
Ok
","
Theodros
is
right
.
For
compensation
","
here's
a
version
which
should
work
on
a
any
list
of
lists
:
gives
(
appropriately
sorted
)
And
it
also
works
on
the
previous
counterexample
l
=
[
"[1,2,3]"
","
"[1,3,4,5]"
]
:
This
will
work
if
you
do
not
care
that
the
ordering
in
the
resulting
tuples
does
not
map
to
the
input
lists
(
you
do
not
care
whether
(
1
","
2
)
or
(
2
","
1
)
)
.
Here
you'll
get
the
combination
with
the
smaller
element
first
:
gives
With
strings
you
get
The
apparent
difference
in
the
ordering
comes
from
the
different
hashes
for
the
strings
and
the
numbers
.
I
have
2
input
files
","
input.txt
and
datainput.txt
.
I
check
if
the
2nd
column
of
input.txt
matches
the
1st
column
of
datainput.txt
","
and
if
they
match
","
then
I
put
it's
orthodb_id
at
the
end
relevant
row
in
the
output
file
.
input.txt
:
datainput.txt
:
code.py
:
This
is
the
output
I
get
with
my
code
:
You
need
just
add
an
else
block
in
your
code
to
get
the
desired
output
:
Bryan
Oakley
is
correct
","
the
image
is
not
a
jpg
in
terms
of
its
content
","
even
though
your
filesystem
thinks
it's
a
gif
.
On
my
end
I
tried
opening
a
jpg
with
your
program
and
got
the
same
error
'
TclError
:
couldn't
recognize
data
in
image
file
""""
hello.jpg
""""
.
'
So
you
can
do
this
:
Open
your
image
with
mspaint
","
then
go
to
File
>
Save
As
and
from
the
""""
Save
As
Type
""""
dropdown
","
choose
GIF
.
Then
the
code
should
work
.
This
is
what
I
used
:
(
btw
","
if
I
changed
line
7
above
to
photo
=
PhotoImage(imgPath)
then
like
you
","
no
image
appears
.
So
leave
it
as
photo
=
PhotoImage(file = imgPath)
)
I
have
tried
two
different
things
to
try
to
get
an
image
to
show
in
a
label
and
The
image
is
in
the
same
folder
as
all
the
code
.
Any
suggestions
on
how
to
show
an
image
?
Pickle
recreates
objects
by
looking
up
callables
(
commonly
classes
","
but
really
any
callable
)
and
calling
them
with
fixed
arguments
.
This
means
that
unpickling
may
run
any
callable
anywhere
","
with
any
arguments
.
Even
if
the
relevant
module
hasn't
been
imported
yet
","
pickle
will
gladly
import it
.
It
takes
only
a
couple
dozen
characters
to
execute
os.system
","
for
example
.
So
you're
screwed
before
you
even
get
an
object
back
.
Unpickling
a
pickle
amounts
to
executing
arbitrary
code
.
An
attacker
can
create
a
pickle
which
will
execute
system
commands
during
unpickling
.
Here's
an
example
from
a
good
blog
post
on
the
topic
:
This
clearly
states
that
pickle
is
insecure
.
Many
think
this
is
because
it
can
load
classes
other
than
what
you
expect
and
may
trick
you
to
run
their
functions
.
But
the
actual
security
risk
is
far
more
dangerous
.
Unpickling
can
be
exploited
to
execute
arbitrary
commands
on
your
machine
!
The
pickle
format
uses
a
full-blown
virtual
machine
.
If
the
code
fed
into
the
machine
is
controlled
by
an
attacker
","
it
can
do
things
well
beyond
object
serialization
(
including
executing
arbitrary
OS
commands
)
.
For
a
discussion
","
see
Why
Python
Pickle
is
Insecure
.
In
the
Documentation
of
the
python3
pickle
module
it
says
:
""""
The
pickle
module
is
not
intended
to
be
secure
against
erroneous
or
maliciously
constructed
data
.
Never
unpickle
data
received
from
an
untrusted
or
unauthenticated
source
.
""""
What
is
the
risk
of
unpickling
data
from
untrusted
sources
?
Is
the
unpickling
of
untrusted
data
itself
dangerous
?
Or
is
it
only
dangerous
to
use
the
unpickled
object
?
So
is
it
safe
to
unpickle
untrusted
data
and
then
(
before
doing
anything
with
the
resulting
object
)
proving
wether
it's
a
save
object
?
scope
!
however
I
am
writing
a
simple
text
based
adventure
game
in
python
.
I
will
include
all
of
the
code
because
I
have
no
idea
what
the
problem
is
(
it
isn't
too
long
and
it's
quite
simple
)
.
In
order
to
save
","
I
am
writing
to
a
text
file
the
name
of
the
'
currentRoom
'
and
each
item
in
'
you.inventory
'
.
Then
when
I
load
I
am
using
'
readline()
'
to
retrieve
that
data
and
reset
the
variables
.
I've
got
a
""""
game
loop
""""
where
I
get
the
next
command
and
perform
whatever
function
matches
the
command
.
When
I
wrote
the
following
code
for
'
load
'
","
it
wouldn't
work
correctly
.
To
test
the
save
/
load
","
I
start
the
game
pick
up
a
few
items
and
then
go
north
to
get
into
a
position
that
is
not
the
starting
room
","
and
save
.
Then
I
quit
","
run
the
game
again
","
enter
the
same
name
of
course
","
and
then
give
command
'
load
'
.
Note
","
the
saved
file
is
correct
.
I
even
included
a
print
statement
in
the
load
function
that
prints
currentRoom.name
","
and
it's
correct
","
but
then
when
I
call
the
look
function
and
it
does
the
exact
same
'
print
currentRoom.name
'
","
I've
been
moved
back
to
the
starting
room
!
?
Interestingly
","
the
inventory
is
loaded
fine
","
just
the
location
is
wrong
.
I
can't
figure
it
out
for
the
life
of
me
.
In
order
to
fix
this
","
I
had
to
move
the
load()
function
into
the
game
loop
body
","
under
'
elif
com
=
=
""""
load
""""
...
","
rather
than
calling
it
as
a
function
","
and
then
it
works
!
?
I
suspect
I'm
doing
something
wrong
with
global
vs
local
variables
","
but
I
can't
see
what
.
Here's
the
code
:
(
sorry
for
the
indentation
errors
","
cut
and
paste
didn't
work
well
)
It's
fairly
straightforward
if
you
use
a
little
subquery
:
Note
the
use
of
the
OR
not
exists
clause
to
handle
the
case
when
there
are
no
entries
with
flag=true
.
How
about
I
have
a
table
similar
to
this
:
I
need
to
find
sum
of
count
for
a
given
name
with
the
following
conditions
:
For
a
given
name
Flag
should
be
false
Entry
should
be
added
after
the
last
flag=true
entry
for
that
name
In
this
case
I
want
sum
of
count
for
entries
03
and
04
.
How
should
I
do
this
in
Django
?
EDIT
:
I
am
looking
for
creating
a
query
like
this
:
EDIT
:
Also
if
there
was
no
entry
with
flag=true
then
I
need
all
the
entries
for
that
name
.
I
use
this
(
somewhat
kludgy
)
R
function
to
merge
data.frames
and
keep
the
order
of
one
of
them
:
How
can
I
accomplish
the
same
thing
in
pandas
?
Is
there
a
less
kludgy
way
or
should
I
just
rewrite
the
same
function
?
Thanks
","
-
N
In
pandas
a
merge
resets
the
index
","
but
you
can
easily
work
around
this
by
resetting
the
index
before
doing
the
merge
.
Resetting
the
index
will
create
a
new
column
called
""""
index
""""
which
you
can
then
use
to
re-create
your
index
after
the
merge
.
For
example
:
See
this
question
/
answer
for
more
discussion
(
hat
tip
to
@WouterOvermeire
)
Use
Python's
list
comprehensions
and
define
a
helper
function
that
joins
the
text
with
the
years
if
they
are
not
yet
present
.
You
can
use
the
optional
second
parameter
of
enumerate
to
indicate
the
start
value
â
€
“
your
first
year
.
suppose
:
then
this
will
do
it
:
something
like
that
:
and
you
may
use
dict(d)
to
get
a
dictionary
afterwards
If
you
have
a
list
of
winners
","
like
:
You
can
use
enumerate
to
associate
these
with
numbers
:
And
from
this
you
can
make
a
dict
","
or
a
list
of
strings
","
or
whatever
:
You
can
strip
the
""""
in
YYYY
""""
part
easily
enough
","
but
the
best
way
to
do
that
depends
upon
how
variable
the
phrases
are
.
For
example
","
if
you
know
it's
in
YYYY
","
then
you
could
use
something
like
and
then
use
a
dictionary
comprehension
(
python
>
=
2.7
)
:
This
question
maybe
trivial
.
How
do
I
add
a
year
starting
with
1903
ending
with
2009
to
106
items
on
a
list
without
creating
a
long
HUGE
list
of
years
But
bypass
ones
with
a
year
?
For
Example
:
To
this
:
While
I
know
you
can
add
a
number
count
to
each
item
on
list
I
tried
:
I
get
TypeError
:
'
function
'
object
is
not
iterable
But
this
is
new
to
me
.
I
would
normally
have
more
to
show
but
I
really
don't
have
any
Idea
how
to
code
this
at
all
.
You
can
check
whether
the
cursor
is
null
easily
:
Otherwise
the
usual
thing
in
Python
is
to
""""
ask
for
forgiveness
not
permission
""""
:
use
your
database
connection
and
if
it
has
timed
out
catch
the
exception
and
handle
it
(
otherwise
you
might
just
find
that
it
times
out
between
your
test
and
the
point
where
you
use
it
)
.
The
problem
with
checking
before
an
operation
is
that
the
bad
condition
you're
checking
for
could
happen
between
the
check
and
the
operation
.
For
example
","
suppose
you
had
a
method
is_timed_out()
to
check
if
the
connection
had
timed
out
:
On
the
face
of
it
","
this
looks
like
you've
avoided
the
possibility
of
a
CursorTimedOut
exception
from
the
execute
call
.
But
it's
possible
for
you
to
call
is_timed_out
","
get
a
False
back
","
then
the
cursor
times
out
","
and
then
you
call
the
execute
function
","
and
get
an
exception
.
Yes
","
the
chance
is
very
small
that
it
will
happen
at
just
the
right
moment
.
But
in
a
server
environment
","
a
one-in-a-million
chance
will
happen
a
few
times
a
day
.
Bad
stuff
.
You
have
to
be
prepared
for
your
operations
to
fail
with
exceptions
.
And
once
you've
got
exception
handling
in
place
for
those
problems
","
you
don't
need
the
pre-checks
any
more
","
because
they
are
redundant
.
I
have
functions
in
which
I
am
doing
database
operations
.
I
want
to
do
something
special
","
before
trying
to
fetch
data
from
database
","
I
want
to
check
whether
cursor
object
is
null
and
whether
connection
is
dropped
due
to
time
out
.
How
can
I
do
pre
checking
in
the
Python
?
my
functions
","
foo
","
bar
:
I
am
looking
for
a
way
to
extract
basic
stats
(
total
count
","
density
","
count
in
links
","
hrefs
)
for
words
on
an
arbitrary
website
","
ideally
a
Python
based
solution
.
While
it
is
easy
to
parse
a
specific
website
using
","
say
BautifulSoup
and
determine
where
the
bulk
of
the
content
is
","
it
requires
you
to
define
the
location
of
the
content
in
the
DOM
tree
ahead
of
processing
.
This
is
easy
for
","
say
","
hrefs
or
any
arbitraty
tag
but
gets
more
complicated
when
determining
where
the
rest
of
the
data
(
not
enclosed
in
well
defined
markers
)
is
.
If
I
understand
correctly
","
robots
used
by
the
likes
of
Google
(
GoogleBot
?
)
are
able
to
extract
data
from
any
website
to
determine
the
keyword
density
.
My
scenario
is
similar
","
obtain
the
info
related
to
the
words
that
define
what
the
website
is
about
(
i.e.
after
removing
js
","
links
and
fillers
)
.
My
question
is
","
are
there
any
libraries
or
web
APIs
that
would
allow
me
to
get
statistics
of
meaningful
words
from
any
given
page
?
There
is
no
APIs
but
there
could
be
few
libraries
that
you
can
use
it
as
a
tool
.
you
should
count
the
meaningful
words
and
record
them
by
the
time
.
you
can
also
Start
from
something
like
this
:
There
are
multiple
libraries
that
deal
with
more
advanced
processing
of
web
articles
","
this
question
should
be
a
duplicate
of
this
one
.
There
are
multiple
problems
here
:
Your
JSON
data
is
actually
multiple
JSON
datas
.
That
will
be
hard
to
fix
if
you
have
a
lot
of
data
","
although
Martijns
suggestion
of
reading
per
line
might
help
","
assuming
the
data
really
is
one
JSON
mapping
per
line
.
Otherwise
the
data
needs
to
be
fixed
","
like
this
:
Note
the
opening
and
closing
brackets
","
and
the
comma
after
each
JSON
{
}
(
except
the
last
one
)
.
The
script
you
have
been
given
is
not
particularly
generic
.
It
assumes
there
is
an
'
item_id
'
in
the
first
JSON
object
given
","
which
there
is
not
.
That's
fixable
","
though
.
Your
invoice_payments
data
is
a
list
of
dictionaries
.
This
means
your
data
is
hierarchical
.
How
do
you
want
to
convert
to
CVS
","
which
is
just
a
flat
list
of
data
?
It's
not
obvious
.
The
script
you
show
doesn't
deal
with
that
","
it's
generic
and
assumes
your
json
data
is
flat
.
A
fixed
converter
:
yields
this
result
as
CSV
:
Note
how
your
invoice_payments
data
has
been
converted
to
a
string
:
Nothing
that
imports
CSV
will
make
any
practical
sense
of
that
.
Your
JSON
data
can
not
be
trivially
converted
to
CSV
","
you
have
to
decide
and
specify
how
the
CSV
data
should
look
.
Someone
sent
me
this
code
to
convert
from
json
to
csv
format
.
Here's
the
code
for
json2csv
.
Here's
my
json
data.Which
I
saved
as
transaction.json
I
tried
doing
c:\python.exe
c:\json2csv.py
c:\transaction.json
c:\transaction.txt
I
got
the
error
If
someone
can
correct
the
code
to
fetch
all
fields
then
that's
great
.
I
dont
even
need
all
fields
in
csv
.
I
need
only
client_external_id
","
invoice_clinician_id
","
invoice_id
","
location
","
item_name
","
item_unit_price
","
item_description
","
quantity
","
billing_date
.
This
has
been
pending
for
a
long
time.I
need
to
get
this
completed
today.Pls
help
.
I
am
trying
to
learn
currying
in
Python
for
my
class
and
I
have
to
overload
the
(
)
operator
for
it
.
However
","
I
do
not
understand
how
can
I
can
go
about
overloading
the
(
)
operator
.
Can
you
explain
the
logic
behind
overloading
the
parentheses
?
Should
I
overload
first
(
and
then
)
or
can
I
do
any
of
these
?
Also
","
is
there
special
name
for
parentheses
operator
?
You
can
make
an
object
callable
by
implementing
the
__call__
method
:
My
function
is
made
to
get
the
area
of
any
arbitrary
triangle
.
Here
is
the
way
that
I
know
works
However
","
I
think
this
is
crap
so
here's
what
I
had
as
a
sketched
out
thought
","
However
","
this
apparently
doesn't
play
too
nice
with
lists
.
I
thought
this
would
work
in
the
way
that
once
can
get
keys
and
values
from
dictionaries...but
lists
don't
have
the
iteritems()
method
.
Then
I
thought
about
converting
the
lists
into
dictionaries
","
but
the
keys
are
unique
in
dicts
and
hence
they
only
pop
up
once....which
would
make
my
function
not
work
properly
.
You
use
tuple
unpacking
:
Python
can
unpack
nested
sequences
into
separate
variables
","
as
long
as
you
create
the
same
nesting
structure
on
the
left-hand-side
.
The
looping
will
not
work
because
you
are
trying
to
unpack
2-value
tuples
per
loop
iteration
","
where
the
iteration
would
only
yield
only
1
value
.
This
works
across
python
versions
.
Demonstration
:
In
2.x
","
use
raw_input()
instead
of
input()
-
this
is
because
in
Python
2.x
","
input()
parses
the
user's
input
as
Python
code
","
which
is
dangerous
and
slow
.
raw_input()
just
gives
you
the
string
.
In
3.x
","
they
changed
input()
to
work
that
way
as
it
is
what
you
generally
want
.
This
is
a
simple
list
comprehension
that
takes
the
split
components
of
the
input
(
using
str.split()
","
which
splits
on
whitespace
)
and
makes
each
component
an
integer
.
In
python
3.x
In
python
2.x
In
Ruby
I
can
use
how
to
write
in
Python
self.tags
+
[
'
tag
'
]
creates
a
new
object
.
And
then
it
is
assigned
to
self.tags
.
self.tags.append
works
with
the
same
tags
object
from
the
class
.
So
all
the
objects
share
the
same
tags
object
and
append
to
it
.
I
wish
I
could
better
title
the
question
.
Anyway
","
Here
is
test.py
Here
is
the
output
:
[
'
tag
'
]
Now
I
change
class
test
to
Here
is
the
output
:
[
'
tag
'
","
'
tag
'
]
I
expected
the
first
result
[
'
tag
'
]
in
both
cases
.
Ahh
","
okay
i
found
a
solution
","
but
i'm
open
to
smarter
alternatives
:
)
This
should
do
the
trick
:
i
coded
a
little
program
and
i
am
currently
in
the
process
of
making
it
work
on
different
operating
systems
.
My
program
heavily
relies
on
the
function
count_nonzero
introduced
with
numpy
version
1.6
.
My
primary
workstation
is
Debian
Linux
with
numpy
(
ver
.
1.6.2
)
and
scipy
(
ver
.
0.11
)
.
However
on
windows
the
setup
could
involve
older
numpy
version
below
1.6
","
where
the
count_nonzero
command
isn't
available
.
Therefore
i
am
in
need
of
a
custom
count_nonzero
function
which
works
on
systems
with
older
numpy
versions
.
"def count_nonzero(self,array):"
How
can
i
achieve
this
?
First
problem
is
that
when
i
click
'
+
$16
'
button
it
doesn't
show
the
increase
","
although
you
can
see
it
after
closing
the
window
and
typing
money
into
Python
Shell
.
Second
problem
is
that
after
I
added
those
sticky=SE
and
sticky=SW
window
won't
appear
at
all
(
without
error
messages
)
.
You
store
a
new
string
based
on
the
money
variable
in
the
budget
Label
;
the
label
doesn't
keep
a
reference
to
the
money
variable
for
you
.
Simply
set
the
label
value
each
time
you
call
your
addMoney
function
:
First
of
all
","
you
are
storing
the
return
value
of
grid
or
pack
","
which
is
always
None
","
instead
of
the
reference
to
the
widgets
.
Besides
","
you
shouldn't
use
both
geometry
managers
at
the
same
time
(
if
you
are
new
to
Tkinter
","
I'd
suggest
grid
instead
of
pack
)
.
To
update
the
text
of
the
widget
","
you
have
to
use
config
with
the
text
keyword
or
budget
[
'
text
'
]
:
I
have
created
a
listctrl
with
some
of
the
data
in
the
listctrl
are
very
long
","
and
instead
of
showing
all
of
the
text
it
ends
with
....
For
example
Att
PSSM_r1_0_T
is
[
-
10.179077
","
0.944198
]
|
Att
PSSM_r1_0_Y
is
....
How
would
i
be
able
to
make
it
so
it
shows
all
of
the
text
.
Something
like
Att
PSSM_r1_0_T
is
[
-
10.179077
","
0.944198
]
|
Att
PSSM_r1_0_Y
is
[
-
4.820935
","
9.914433
]
|
Att
PSSM_r1_2_I
is
[
-
8.527803
","
1.953804
]
|
Att
PSSM_r1_2_K
is
[
-
12.083334
","
-
0.183813
]
|
Att
PSSM_r1_2_V
is
[
-
14.112536
","
5.857771
]
|
1
As
the
text
is
very
long
I
would
prefer
if
it
covered
more
than
one
line
.
I
don't
think
that
is
possible
to
do
with
a
standard
listctrl
.
Try
poking
around
at
the
UltimateListCtrl
","
being
a
full
owner
drawn
listctrl
it
has
the
ability
to
change
the
way
its
looks
far
more
than
a
standard
listctrl
.
I
figured
out
how
to
do
it
.
One
way
I
found
is
to
send
mail
through
html
.
Okay
here
is
what
I
have
so
far
and
it
doesn't
work
.
What
can
I
do
to
encode
utf-8
to
mail
api
and
make
it
works
for
using
more
than
one
language
in
the
one
email
?
Thanks
in
advance
.
.
encode('utf-8')
makes
binary
out
of
the
string
.
You
have
to
use
.
decode('utf-8')
See
:
http://blog.notdot.net/2010/07/Getting-unicode-right-in-Python
and
also
http://docs.python.org/2/howto/unicode.html
Sadly
","
this
is
Windows
","
so
WinPcap
won't
help
;
libpcap
1.1.0
and
later
can
read
pcap-ng
files
","
but
there
is
currently
no
version
of
WinPcap
based
on
a
version
of
libpcap
with
pcap-ng
support
.
I.e
.
","
currently
","
the
first
piece
of
software
to
try
would
be
VirtualBox
","
Parallels
Workstation
","
or
VMware
Workstation
","
and
the
next
piece
of
software
to
try
would
be
some
Linux
distribution
running
on
your
Windows
machine
under
the
virtualization
software
.
Sorry
.
(
Yes
","
I
have
to
find
the
time
to
do
some
libpcap
infrastructure
work
to
make
it
easier
for
the
WinPcap
people
to
make
their
remote
capture
support
work
with
newer
versions
of
libpcap
.
Sadly
","
there
are
only
24
hours
in
a
day
","
and
a
ton
of
other
things
to
do
competing
for
the
hours
that
remain
after
such
frivolities
as
eating
and
sleeping
.
:
-
)
)
A
workaround
for
that
particular
issue
would
be
either
to
save
a
capture
from
Wireshark
in
pcap
format
rather
than
pcap-ng
format
or
to
use
Wireshark
tools
such
as
editcap
(
which
can
handle
pcap-ng
files
on
Windows
","
as
they
don't
use
libpcap
/
WinPcap
to
read
capture
files
)
to
convert
from
pcap-ng
to
pcap
.
There
are
a
couple
of
Python
wrappers
for
libpcap
/
WinPcap
-
python-libpcap
and
Pcapy
.
The
python-libpcap
page
has
a
daemon
and
a
penguin
on
it
","
so
they're
only
advertising
BSD
and
Linux
support
;
it
might
work
on
other
UN*Xes
","
as
there
shouldn't
be
major
differences
in
the
way
you
hook
into
libpcap
","
but
might
not
work
on
Windows
.
Pcapy
","
however
","
explicitly
mentions
WinPcap
and
Win32
binaries
.
Do
try
out
the
pcap
binary
that
@dirkloss
compiled
for
Python
2.7
|
Windows
.
Here
I
tried
it
","
but
I'm
getting
this
error
-
SO
qn
I've
tried
several
different
things
and
crawled
around
on
lots
of
forums
looking
for
an
answer
to
this
question
.
My
goal
is
to
simply
parse
through
a
wireshark
.
pcap-ng
trace
file
using
Python
.
From
what
I
can
gather
","
it
seems
like
I
need
a
libpcap
wrapper
for
Python
or
perhaps
WinCap
(
PyCapy
?
)
I'm
relatively
new
to
Python
","
so
if
you
all
have
any
pointers
on
how
to
install
this
module
please
let
me
know
.
Earlier
I
had
a
32bit
version
of
Python
installed
and
found
a
win32
installer
and
was
able
to
parse
through
.
pcap
files
but
I
really
want
to
be
able
to
parse
through
the
.
pcapng
files
.
Thanks
guys
","
~
Kyte
I
have
an
object
as
such
and
want
to
sort
it
by
time
(
line
first
","
point
second
)
in
each
dimension
(
simplified
json
)
:
This
dimention
could
be
deeper
and
have
much
more
points
/
lines
within
each
other
.
The
sorted
output
would
be
something
like
this
:
Thanks
You'd
need
to
process
this
recursively
:
You
don't
.
Setting
the
speed
in
WebDriver
is
not
possible
and
the
reason
for
this
is
that
you
generally
shouldn't
need
to
","
and
the
'
waiting
'
is
now
done
at
a
different
level
.
Before
it
was
possible
to
tell
Selenium
","
don't
run
this
at
normal
speed
","
run
it
at
a
slower
speed
to
allow
more
things
to
be
available
on
page
load
","
for
slow
loading
pages
or
AJAX'ified
pages
.
Now
","
you
do
away
with
that
altogether
.
Example
:
I
have
a
login
page
","
I
login
and
once
logged
in
I
see
a
""""
Welcome
""""
message
.
The
problem
is
the
Welcome
message
is
not
displayed
instantly
and
is
on
a
time
delay
(
using
jQuery
)
.
Pre
WebDriver
Code
would
dictate
to
Selenium
","
run
this
test
","
but
slow
down
here
so
we
can
wait
until
the
Welcome
message
appears
.
Newer
WebDriver
code
would
dictate
to
Selenium
","
run
this
test
","
but
when
we
login
","
wait
up
to
20
seconds
for
the
Welcome
Message
to
appearing
","
using
explicit
waits
.
Now
","
if
you
really
want
access
to
""""
set
""""
Selenium's
speed
","
first
off
I'd
recommend
against
it
but
the
solution
would
be
to
dive
into
the
older
","
now
deprecated
code
.
If
you
use
WebDriver
heavily
already
","
you
can
use
the
WebDriverBackedSelenium
which
can
give
you
access
to
the
older
Selenium
methods
","
whilst
keeping
the
WebDriver
backing
the
same
","
therefore
much
of
your
code
would
stay
the
same
.
https://groups.google.com/forum/#!topic/selenium-users/6E53jIIT0TE
Second
option
is
to
dive
into
the
old
Selenium
code
and
use
it
","
this
will
change
a
lot
of
your
existing
code
(
because
it
is
before
the
""""
WebDriver
""""
concept
was
born
)
.
The
code
for
both
Selenium
RC
&
WebDriverBackedSelenium
lives
here
","
for
the
curious
:
https://code.google.com/p/selenium/source/browse/py/selenium/selenium.py
Something
along
the
lines
of
:
You'd
then
get
access
to
do
this
:
To
run
my
functional
tests
i
use
LiveServerTestCase
.
I
want
to
call
set_speed
(
and
other
methods
","
set_speed
is
just
an
example
)
that
aren't
in
the
webdriver
","
but
are
in
the
selenium
object
.
http://selenium.googlecode.com/git/docs/api/py/selenium/selenium.selenium.html#module-selenium.selenium
my
subclass
of
LiveServerTestCase
How
to
get
that
?
I
can't
call
the
constructor
on
selenium
","
i
think
.
I
am
importing
the
datetime
library
in
my
python
program
and
am
taking
the
duration
of
multiple
events
.
Below
is
my
code
for
that
:
Now
I
have
a
value
in
the
variable
""""
duration
""""
.
The
output
of
this
will
be
:
I
want
to
take
the
standard
deviation
of
all
the
durations
and
determine
if
there
is
an
anomaly
.
For
example
","
the
00:45:22
is
an
anomaly
and
I
want
to
detect
that
.
I
could
do
this
if
I
knew
what
format
datetime
was
in
","
but
it
doesn't
appear
to
be
digits
or
anything..I
was
thinking
about
splitting
the
values
up
from
:
and
using
all
the
values
in
between
","
but
there
might
be
a
better
way
.
Ideas
?
You
have
datetime.timedelta()
objects
.
These
have
.
microseconds
","
.
seconds
and
.
days
attributes
","
all
3
integers
.
The
str()
string
representation
represents
those
as
[
D
day
[s]
","
]
[H]
H:MM:SS
[
.
UUUUUU
]
as
needed
to
fit
all
values
present
.
You
can
use
simple
arithmetic
on
these
objects
.
Summing
and
division
work
as
expected
","
for
example
:
Unfortunately
","
you
cannot
multiply
two
timedeltas
and
calculating
a
standard
deviation
thus
becomes
tricky
(
no
squaring
of
offsets
)
.
Instead
","
I'd
use
the
.
total_seconds()
method
","
to
give
you
a
floating
point
value
that
is
calculated
from
the
days
","
seconds
and
microseconds
values
","
then
use
those
values
to
calculate
a
standard
deviation
.
The
duration
objects
you
are
getting
are
timedelta
objects
.
Or
durations
from
one
timestamp
to
another
.
To
convert
them
to
a
total
number
of
microseconds
use
:
Then
calculate
the
standard
deviation
:
So
:
This
is
how
the
code
is
This
works
fine
on
Linux
systems
but
not
on
Windows
.
Its
showing
EOFError
.
I
have
to
use
rb
mode
to
make
it
work
on
Windows
.
.
now
this
isn't
working
on
Linux
.
Why
this
is
happening
","
and
how
to
fix
it
?
Always
use
b
mode
when
reading
and
writing
pickles
(
"open(f, 'wb')"
for
writing
","
"open(f, 'rb')"
for
reading
)
.
To
""""
fix
""""
the
file
you
already
have
","
convert
its
newlines
using
dos2unix
.
I'm
using
Selenium
to
try
and
get
data
from
a
website
.
But
the
data
I
want
is
stored
in
'
hidden
'
tags
","
so
it's
not
visible
when
I
pull
the
source
.
Is
there
any
way
to
get
around
this
?
Are
there
different
types
of
hidden
?
I
presume
it's
hidden
because
I'm
also
using
Firebug
","
which
can
see
the
source
on
the
page
I'm
trying
to
scrape
","
but
it
'
greys-out
'
that
source
","
which
I've
read
is
an
indication
of
that
source
being
hidden
with
the
style:hidden
tag
.
One
of
my
specific
reasons
for
scraping
with
Selenium
is
to
make
sure
the
javascript
created
parts
of
each
page
are
fully
rendered
before
I
start
searching
for
content
.
I
use
this
line
to
wait
for
the
content
I
want
to
be
loaded
:
The
'
30
'
is
a
30
second
wait
timer
","
if
this
is
exceeded
then
a
TimeoutException
occurs
so
you
will
want
to
put
it
in
a
try
...
except
:
block
.
Change
my_xpath
to
match
the
tags
you
want
.
Even
if
the
style
is
marked
as
hidden
","
Selenium
can
still
see
it
.
What
is
probably
happening
is
that
the
Web
site
is
loading
the
additional
data
through
JavaScript
and
/
or
XMLHttpRequest
or
through
CSS
.
Firebug
shows
you
the
DOM
once
it
has
been
completed
.
With
a
Webdriver
you
can
pilot
the
browser
in
loading
a
page
and
interact
with
it
.
The
issue
comes
when
some
of
the
additional
information
is
given
once
there
is
a
specific
user
interaction
.
So
a
way
to
mitigate
this
would
be
to
use
webdriver
to
pilot
the
browser
and
do
the
same
sequence
of
actions
so
that
the
DOM
will
change
accordingly
.
You
might
want
to
play
with
the
CSS
to
change
the
properties
and
make
the
element
visible
too
.
Given
that
you
didn't
provide
any
code
examples
of
what
you
are
trying
to
do
","
it
is
not
realistic
to
precisely
help
you
.
But
you
will
find
plenty
of
webdriver
code
examples
in
python
in
the
official
documentation
.
Using
the
following
code
:
"printformatted(""1,Some summary"")"
gives
me
the
error
ValueError
:
need
more
than
1
value
to
unpack
","
whereas
"printformatted([""1,Some summary""])"
does
not
.
Why
?
In
the
first
case
","
you're
passing
a
string
","
so
for
status
in
statuses
iterates
over
the
string
","
character
by
character
","
which
is
not
what
you
want
.
In
the
second
case
","
you're
passing
a
list
","
so
for
status
in
statuses
iterates
over
its
elements
","
first
element
being
""""
1
","
Some
summary
""""
.
I
am
trying
to
request
a
specific
betting
amount
with
a
minimum
","
maximum
and
specific
increments
.
When
I
run
the
following
code
","
the
program
loops
over
every
element
in
the
array
rather
than
stopping
.
It
looks
like
your
code
would
be
better
off
as
an
expression
than
a
loop
:
I
believe
everything
you've
stated
is
correct
.
On
Linux
","
os.pipe
is
just
a
Python
interface
for
accessing
traditional
POSIX
pipes
.
On
Windows
","
it's
implemented
using
CreatePipe
.
When
you
call
it
","
you
get
two
ordinary
file
descriptors
back
.
It's
unidirectional
","
and
you
just
write
bytes
to
it
on
one
end
that
get
buffered
by
the
kernel
until
someone
reads
from
the
other
side
.
It's
fairly
low-level
","
at
least
by
Python
standards
.
multiprocessing.Pipe
objects
are
much
more
high
level
interface
","
implemented
using
multiprocessing.Connection
objects
.
On
Linux
","
these
are
actually
built
on
top
of
POSIX
sockets
","
rather
than
POSIX
pipes
.
On
Windows
","
they're
built
using
the
CreateNamedPipe
API
.
As
you
noted
","
multiprocessing.Connection
objects
can
send
/
receive
any
picklable
object
","
and
will
automatically
handle
the
pickling
/
unpickling
process
","
rather
than
just
dealing
with
bytes
.
They're
capable
of
being
both
bidirectional
and
unidirectional
.
Recently
I'm
studying
parallel
programming
tools
in
Python
.
And
here
are
two
major
differences
between
os.pipe
and
multiprocessing.Pipe.(despite
the
occasion
they
are
used
)
os.pipe
is
unidirectional
","
multiprocessing.Pipe
is
bidirectional
;
When
putting
things
into
pipe
/
receive
things
from
pipe
","
os.pipe
uses
encode
/
decode
","
while
multiprocessing.Pipe
uses
pickle
/
unpickle
I
want
to
know
if
my
understanding
is
correct
","
and
is
there
other
difference
?
Thank
you
.
I
am
using
SimPy
","
and
I
try
to
simulate
a
network
.
This
is
my
main
module
:
I
want
an
object
of
class
message
","
which
is
activated
by
an
object
of
class
node
","
to
interrrupt
an
object
of
class
Network
(
topology
)
.
But
I'm
getting
an
error
:
And
I
don't
know
how
to
make
an
object
global
.
And
if
I
type
topology
in
python
shell
then
it
shows
me
object
topology
","
so
why
can't
message
see
it
?
I'm
pretty
sure
the
issue
is
that
your
Message
class
is
defined
in
a
different
module
than
where
your
topology
variable
is
.
So
called
""""
global
""""
variables
in
Python
are
not
really
global
(
in
the
sense
that
there's
just
one
global
namespace
)
","
but
just
at
the
top
of
a
specific
module's
namespace
.
So
the
global
variable
topology
in
your
main
module's
namespace
is
not
accessible
as
a
global
variable
from
a
different
module
.
My
suggestion
for
working
around
this
by
passing
the
topology
value
to
the
Message
as
a
parameter
to
the
__init__
method
.
If
the
message
is
being
created
by
something
other
than
your
own
code
(
e.g
.
by
your
Node
class
)
","
you
might
need
to
pass
it
around
a
bit
more
","
so
that
it
will
be
available
when
needed
.
If
that
is
not
possible
","
you
might
be
able
to
put
the
topology
value
in
the
namespace
of
a
module
that
can
be
imported
by
your
Message
code
.
This
can
get
messy
though
","
as
circular
imports
can
break
things
if
you're
not
careful
.
Python
applies
line
buffering
when
connected
to
a
TTY
","
otherwise
a
larger
buffer
is
needed
.
Redirecting
your
Python
program
to
a
pipe
means
there
is
no
TTY
connected
to
the
stream
","
and
you'll
have
to
use
.
flush()
even
when
sending
newlines
.
You
can
run
Python
with
-
u
to
turn
off
buffering
of
stdout
.
I
have
a
wrapper
to
redirect
outputs
when
I
call
python
python-wrapped
C
+
+
.
The
basic
idea
is
to
use
dup
and
dup2
","
which
are
the
only
ways
I've
been
able
to
catch
the
printf
outputs
from
the
C
+
+
.
The
wrapper
works
fine
with
no
calls
to
flush()
as
long
as
I'm
running
the
job
interactively
","
but
when
I
send
the
job
to
a
TORQUE
batch
I
get
the
unwelcome
outputs
again
.
My
understanding
","
in
part
from
this
question
","
is
that
some
well-placed
flush()
calls
should
fix
this
","
but
where
exactly
do
they
need
to
go
?
Should
I
flush
the
buffer
before
dup'ing
to
the
tempfile
?
Before
dup'ing
back
?
Both
?
The
wrapper
I'm
using
is
as
follows
:
Simply
use
string
formatting
:
The
version
of
your
program
is
not
going
to
change
during
it's
(
short
","
showing
commandline
help
only
)
lifetime
","
so
you
only
need
to
interpolate
this
once
.
Currently
I
use
I'd
like
to
have
the
'
1.0
'
to
be
a
variable
.
I
tried
various
things
with
no
luck
...
I
am
trying
to
use
AFHTTPClient
to
connect
with
a
tornado
server
.
Here
is
my
code
from
client
:
On
the
server
side
","
I
setup
the
handler
as
bellow
:
Then
built
a
subclass
of
this
handler
:
But
when
I
run
this
code
","
it
failed
with
following
ERROR
message
:
The
strange
thing
is
","
in
the
request
header
","
it's
clearly
showing
'
Content-Type
'
:
'
application
/
json
.
Then
why
the
handler
still
has
no
attribute
'
json_args
'
?
I
am
using
tornado
3.0
.
Thanks
you
very
much
if
you
could
give
me
any
advice
.
It
turns
out
this
is
cause
by
the
Content-Type
setting
:
On
the
BaseHandler
:
But
actually
in
the
request
header
:
'
Content-Type
'
:
'
application
/
json
;
charset=utf-8
'
This
string
doesn't
match
at
all
.
But
even
after
setting
the
Content-Type
=
""""
application
/
json
""""
from
the
client
","
the
string
appended
""""
;
charset=utf-8
""""
again
.
The
only
way
I
fixed
this
is
change
the
basehandler
code
to
:
It
fixed
my
problem
.
But
still
","
anybody
know
where
""""
charset=utf-8
""""
come
from
?
Is
it
automatically
set
up
by
AFNetworking
?
Thanks
!
One
way
is
just
:
You
can
also
do
this
:
You
want
to
import the
User
modules
in
the
package
__init__.py
files
to
make
them
available
as
attributes
.
So
in
both
Helper
/
__init_.py
and
Controller
/
__init__.py
add
:
This
makes
the
module
an
attribute
of
the
package
and
you
can
now
refer
to
it
as
such
.
Alternatively
","
you'd
have
to
import the
modules
themselves
in
full
:
so
refer
to
them
with
their
full
names
.
Another
option
is
to
rename
the
imported
name
with
as
:
I
have
started
to
learn
python
and
writing
a
practice
app
.
The
directory
structure
looks
like
The
src
directory
is
in
PYTHONPATH
.
In
a
different
file
","
lets
say
main.py
","
I
want
to
access
both
User
classes
.
How
can
I
do
it
.
I
tried
using
the
following
but
it
fails
:
That's
certainly
ambiguous
.
The
other
(
c
+
+
way
of
doing
it
)
way
that
I
can
think
of
is
But
when
above
script
is
run
","
it
gives
the
following
error
I'm
not
able
to
figure
out
why
is
it
erroring
out
?
The
directories
ShutterDeck
","
Helper
and
Controller
have
__init__.py
in
them
.
This
might
also
help
(
struggled
with
similar
problem
today
)
:
in
ShutterDeck
/
{
Controller
","
Helper
}
/
__init__.py
:
And
then
:
i
have
pages
with
multiple
languages
...
something
like
this
:
/
-
>
en
","
us
","
pt
","
es
and
the
default
is
US
/
foo
-
>
pt
","
en
and
default
is
PT
/
bar
->
pt
and
default
is
PT
on
the
database
i
have
:
on
the
__init__.py
i've
set
it
like
this
:
but
that's
not
working
","
only
the
home
it's
working
fine
if
i'm
missing
any
info
please
warn
me
.
thanks
thanks
It
looks
to
me
like
you
are
searching
for
'
entity
'
in
request.matchdict
when
in
reality
you
should
be
searching
for
'
page
'
","
given
your
route
'
/
{
page
}
'
.
of
course
I
agree
with
@Martijn
because
doc
says
so
","
but
if
you
are
focused
on
unix
like
systems
","
then
you
can
make
use
of
shared
memory
:
If
you
create
file
in
/
dev
/
shm
folder
","
all
files
create
there
are
mapped
directly
to
RAM
","
so
you
can
use
to
access
the-same
database
from
two-different
processes
.
it
takes
that
much
time
:
for
at
least
2
million
records
","
doing
the
same
on
HDD
takes
(
this
is
the
same
command
but
FILE
=
/
tmp
/
test.db
)
:
so
basically
this
allows
you
accessing
the
same
databases
from
different
processes
(
without
loosing
r
/
w
speed
)
:
Here
is
demo
demonstrating
this
what
I
am
talking
about
:
Is
it
possible
to
access
database
in
one
process
","
created
in
another
?
I
tried
:
IDLE
#
1
IDLE
#
2
Error
:
No
","
they
cannot
ever
access
the
same
in-memory
database
.
Instead
","
a
new
connection
to
:
memory
:
always
creates
a
new
database
.
From
the
SQLite
documentation
:
Every
:
memory
:
database
is
distinct
from
every
other
.
So
","
opening
two
database
connections
each
with
the
filename
""""
:
memory
:
""""
will
create
two
independent
in-memory
databases
.
This
is
different
from
an
on-disk
database
","
where
creating
multiple
connections
with
the
same
connection
string
means
you
are
connecting
to
one
database
.
Within
one
process
it
is
possible
to
share
an
in-memory
database
if
you
use
the
file::memory:?cache=shared
URI
:
but
this
is
still
not
accessible
from
other
another
process
.
I'm
trying
to
write
a
program
that
will
give
me
a
nice
.
txt
file
displaying
this
year's
calendar
","
however
","
I
don't
want
to
use
the
calendar
function
","
but
datetime
.
I
want
it
to
have
the
following
format
(
I
want
to
have
the
first
3
letter
of
every
day
of
the
week
next
to
it
)
:
Tue
Jan
01
Wed
Jan
02
all
the
way
to
Tue
Dec
31
(
basically
365
lines
altogether
with
10
characters
in
every
line
","
and
every
line
ends
in
a
newline
""""
\
n
""""
)
.
This
is
what
I
have
gathered
going
from
various
stackflow
questions
","
tutorials
and
modules
.
So
far
no
success
.
My
main
issue
is
that
I
am
unfamiliar
with
how
I
get
python
to
give
me
a
range
of
dates
(
here
being
the
year
2013
","
but
it
could
also
be
just
any
increment
in
time
such
as
June
2011
to
December
2014
)
and
of
course
to
print
the
day
of
the
week
next
to
it
.
This
way
it
could
be
adapted
to
any
time
period
you
might
need
this
small
calendar
for
.
I
was
thinking
maybe
assigning
every
day's
number
(
1
being
Monday
","
2
being
Tuesday
is
the
pattern
if
I'm
correct
)
the
first
3
letters
of
a
day
in
the
week
so
it's
all
neat
and
of
the
same
length
.
Here's
one
way
to
do
the
looping
:
this
will
print
to
stdout
so
you
can
redirect
it
to
a
file
from
the
console
","
which
is
the
usual
but
if
you
want
to
write
to
the
file
directly
from
the
script
replace
the
last
two
lines
with
this
:
In
pandas
","
how
can
I
convert
a
column
of
a
DataFrame
into
dtype
object
?
Or
better
yet
","
into
a
factor
?
(
For
those
who
speak
R
","
in
Python
","
how
do
I
as.factor()
?
)
Also
","
what's
the
difference
between
pandas.Factor
and
pandas.Categorical
?
There's
also
pd.factorize
function
to
use
:
You
can
use
the
astype
method
to
cast
a
Series
(
one
column
)
:
Or
the
entire
DataFrame
:
Update
Since
version
0.15
","
you
can
use
the
category
datatype
in
a
Series
/
column
:
Note
:
pd.Factor
was
been
deprecated
and
has
been
removed
in
favor
of
pd.Categorical
.
Factor
and
Categorical
are
the
same
","
as
far
as
I
know
.
I
think
it
was
initially
called
Factor
","
and
then
changed
to
Categorical
.
To
convert
to
Categorical
maybe
you
can
use
pandas.Categorical.from_array
","
something
like
this
:
I'd
suggest
you
write
a
function
to
walk
your
datastructure
and
call
a
function
on
each
node
.
Updated
to
avoid
the
""""
deleting
item
from
iterated
sequence
""""
bug
E.g
.
Notes
Walk
function
uses
three
arguments
","
the
child
node
","
the
parent
node
and
the
work
function
.
The
walk
function
calls
the
work
function
after
visiting
the
child
nodes
.
The
work
function
takes
both
child
and
parent
nodes
as
arguments
so
pruning
the
child
is
as
easy
as
parent
[
'
children
'
]
.
remove(child)
Update
:
As
noticed
in
the
comments
","
if
you
delete
from
a
sequence
while
iterating
","
it
will
skip
elements
.
for
child
in
"list(node.get('children',[])"
)
in
the
walk
function
copies
the
list
of
children
allowing
the
entries
to
be
removed
from
the
parent's
key
without
skipping
.
Then
:
I
have
a
tree
of
nested
lists
and
dictionaries
that
I
need
to
recursively
go
through
and
remove
entire
dictionaries
that
match
specific
criteria
.
For
instance
","
I
need
to
remove
all
dictionaries
with
the
'
type
'
of
'
Folder
'
that
have
no
children
(
or
an
empty
list
of
children
)
.
I
am
still
a
beginner
Pythonist
so
please
forgive
the
brute-forceness
.
Here's
a
sample
dictionary
formatted
for
easy
copy
and
paste
.
In
this
dictionary
the
only
tree
that
should
remain
is
/
root
/
dc-1
/
group-3
.
The
group-11
folder
should
be
deleted
first
","
then
its
parent
(
since
the
child
is
no
longer
there
)
","
etc.
I
have
tried
many
different
recursive
methods
but
can't
seem
to
get
it
to
work
properly
.
Any
help
would
be
greatly
appreciated
.
For
one
","
you
could
catch
the
exception
","
print
it
and
see
what
it
is
:
)
Do
this
","
for
instance
by
surrounding
it
all
with
a
try
/
except
clause
and
printing
whatever
exception
occurs
.
I'm
guessing
the
reason
for
this
is
client
disconnect
.
This
will
cause
an
exception
and
you
should
handle
it
appropriately
.
If
a
client
can
disconnect
in
many
ways
.
By
telling
you
","
by
timing
out
","
by
dropping
the
connection
while
you're
trying
to
send
something
etc.
All
these
scenarios
are
plausible
exception
cases
","
and
you
should
test
for
them
and
handle
them
.
Hopefully
this
will
help
you
move
on
","
if
not
","
please
comment
:
)
Questions
What
is
the
reason
for
the
exception
?
Did
the
client
cause
any
errors
?
If
at
all
possible
","
please
explain
other
errors
.
Background
I
am
creating
a
Python
GUI
socket
Server
.
When
a
client
connects
to
my
server
","
the
GUI
window
will
open
(
I
am
still
working
on
this
)
.
But
","
when
a
client
does
connect
","
I
get
an
error
:
Since
the
actual
script
is
rather
long
","
I
have
provided
a
pastebin
link
.
Here
is
the
thread
code
.
s
is
the
name
of
my
socket
object
.
Traceback
Thanks
for
the
suggestion
Morten
.
Here
is
the
traceback
.
Personally
","
I
believe
that
many
errors
are
due
to
the
GUI
.
Thanks
!
It
has
to
break
r
=
a
[
r+1
]
[
c+2
]
you're
reassigning
the
value
of
'
r
'
in
that
line
.
please
also
note
that
you
shouldn't
name
your
string
""""
str
""""
as
it
is
the
name
of
the
python
string
module
and
you
won't
be
able
to
use
it
.
The
code
is
generating
the
following
error
But
according
to
me
it
should
not
","
and
as
I
am
extremely
new
to
python
","
I
am
not
able
to
debug
it
.
Please
help
sample
input
taken
:
i
have
the
following
main.py
file
:
app.yaml
file
:
handlers
:
url
:
/
images
/
(
.
.
(
gif|png|jpg
)
)
static_files
:
static
/
img
/
\
1
upload
:
static
/
img
/
(
.
.
(
gif|png|jpg
)
)
url
:
/
css
/
(
.
.
css
)
mime_type
:
text
/
css
static_files
:
static
/
css
/
\
1
upload
:
static
/
css
/
(
.
.
css
)
url
:
/
js
/
(
.
.
js
)
mime_type
:
text
/
javascript
static_files
:
static
/
js
/
\
1
upload
:
static
/
js
/
(
.
.
js
)
url
:
/
(
.
.
html
)
mime_type
:
text
/
html
static_files
:
static
/
\
1
upload
:
static
/
(
.
.
html
)
url
:
.
*
script
:
main.app
libraries
:
name
:
webapp2
version
:
""""
2.5.2
""""
this
is
the
list
of
directories
and
files
.
.
so
why
i
get
404
?
?
if
i
write
url
:
/
index.html
mime_type
:
text
/
html
static_files
:
index.html
upload
:
index.html
it
show
me
the
index.html
page
but
without
images
...
this
is
pretty
strange
what
is
happening
with
regex
of
images
?
well
i
solved
like
this
:
thank
you
very
much
to
everyone
for
NOT
answering
me
because
i
had
to
make
it
alone
:
)
i
m
more
satisfied
now
of
myself
:
D
I
have
some
classes
in
Python
:
and
a
list
myList
whose
elements
are
all
either
instances
of
Class1
or
Class2
.
I'd
like
to
create
a
new
list
whose
elements
are
the
return
values
of
method
called
on
each
element
of
myList
.
I
have
tried
using
a
""""
virtual
""""
base
class
But
if
I
try
"map(Class0.method, myList)"
I
just
get
[
0
","
0
","
0
","
...
]
.
I'm
a
bit
new
to
Python
","
and
I
hear
that
""""
duck
typing
""""
is
preferred
to
actual
inheritance
","
so
maybe
this
is
the
wrong
approach
.
Of
course
","
I
can
do
but
I
like
the
brevity
of
map
.
Is
there
a
way
to
still
use
map
for
this
?
This
is
best
:
Map
seems
to
be
favored
by
people
pining
for
Haskell
or
Lisp
","
but
Python
has
fine
iterative
structures
you
can
use
instead
.
You
can
use
But
I
think
this
is
better
:
PS
.
:
I
don't
think
there
is
ever
a
need
for
range(len(collection)
)
.
The
operator.methodcaller
tool
is
exactly
what
you're
looking
for
:
Alternatively
you
can
use
a
list
comprehension
:
I
am
trying
to
make
a
visualization
of
a
graph
using
Sage
.
I
need
to
make
the
visualization
exactly
as
I
am
writing
the
Python
code
.
I
have
downloaded
and
installed
the
Sage
for
Ubuntu
and
Sage
Notebook
is
working
perfectly
.
But
I
want
to
take
user
input
from
Tkinter
and
then
show
those
input
on
the
Graph
(
generated
by
Sage
)
.
However
","
I
am
unable
to
import sage
in
the
Python
Shell
.
How
can
I
do
so
?
From
looking
at
the
faq
","
it
looks
like
what
you
need
to
do
is
add
the
following
line
to
your
Python
file
:
Then
","
it
looks
like
you
need
to
run
your
script
by
using
the
Python
interpreter
bundled
with
Sage
from
the
command
line
/
console
:
However
","
if
you
want
to
use
Sage
directly
from
the
shell
","
you
should
probably
try
using
the
interactive
shell
.
(
just
type
in
sage
or
maybe
sage
-
python
from
the
command
line
)
Caveat
:
I
haven't
tested
any
of
this
myself
","
so
you
might
need
to
do
a
bit
of
experimenting
to
get
everything
to
work
.
This
entirely
depends
on
the
authentication
policy
that
you
use
.
The
default
AuthTktAuthenticationPolicy
sets
a
cookie
in
the
browser
which
(
by
default
)
does
not
expire
.
Again
though
","
this
depends
on
how
you
are
tracking
authenticated
users
.
I
am
making
a
pyramid
webapp
running
in
apache
webserver
using
mod_wsgi
.
Is
there
anyway
I
could
make
user
session
never
timed
out
?
(
The
idea
is
so
that
once
user
logged
in
","
the
system
will
never
kicked
them
out
unless
they
logged
out
themselves
)
.
I
cant
find
any
information
regarding
this
in
apache
","
mod_wsgi
or
pyramid
documentation
.
Thanks
!
Attribute
names
with
double
underscores
are
""""
mangled
""""
to
make
it
harder
to
have
conflicting
name
in
subclasses
.
So
use
single
underscores
.
This
is
because
name
mangling
is
happening
.
expressionType
is
defined
in
Expression
.
n.__expressionType
will
translate
to
n._Expression__expressionType
.
if
you
copy-paste
the
same
expressionType
method
to
the
class
Number
","
then
because
of
method
resolution
order
","
it
would
go
the
the
definition
that
appears
in
Number
where
self.__expressionType
would
mean
self._Number__expressionType
.
n._Expression__expressionType
!
=
n._Number__expressionType
.
This
would
actually
work
:
However
","
there
is
code
duplication
","
so
using
a
single
underscore
is
better
because
it
won't
mangle
the
name
of
the
attribute
.
As
the
others
have
pointed
out
","
""""
namemangling
""""
occurs
.
But
what
the
hell
is
that
?
Let's
see
with
an
example
.
Create
an
instance
of
A
:
Access
the
first
variable
of
A
:
Access
the
second
variable
of
A
:
Notice
the
error
A
instance
has
no
attribute
'
__var2
'
.
However
you
can
can
access
it
fine
with
:
So
simply
put
","
whenever
you
have
two
doublescores
(
or
more
)
infront
of
a
method
or
a
variable
","
Python
changes
the
name
of
that
method
or
variable
by
appending
an
underscore
and
the
classname
to
it
.
This
a
sort
of
trick
so
programmers
don't
screw
up
things
by
mistakenly
changing
values
.
I
am
unsure
what
I
am
doing
wrong
;
or
why
this
is
the
case
.
I've
the
following
code
:
For
a
number
object
say
n
=
Number(KST.Constant(..)
)
","
I
am
always
returned
None
for
the
following
statement
—
Now
if
I
change
the
double
underscores
to
single
ones
","
it
all
works
.
I
understand
the
difference
between
private
and
semi-private
variables
but
why
this
is
happening
—
I've
no
idea
.
Also
","
I've
used
""""
__
""""
in
a
number
of
other
places
and
it
all
seems
to
work
fine
.
Can
http.client.HTTPConnection
on
python
3.2
download
big
file
about
1G
?
I'm
get
source
of
class
HTTPResponse
when
i
read
content
","
all
data
will
save
to
variable
and
return
it
","
can
variable
save
1G
data
to
memory
?
I
want
save
data
to
order
socket
as
tunnel
","
i
don't
see
yield
keyword
any
where
on
HTTPResponse
?
can
http.client.HTTPConnection
run
this
task
?
tks
:
D
Read
the
response
in
chunks
.
It
can
download
them
.
If
you
want
to
add
something
different
than
always
'
a
'
you
can
try
this
too
:
Another
alternative
","
the
map
function
:
This
would
have
to
be
the
'
easiest
'
way
Suppose
I
have
a
list
of
suits
of
cards
as
follows
:
suits
=
[
""""
h
""""
","
""""
c
""""
","
""""
d
""""
","
""""
s
""""
]
and
I
want
to
add
a
type
of
card
to
each
suit
","
so
that
my
result
is
something
like
aces
=
[
""""
ah
""""
","
""""
ac
""""
","
""""
ad
""""
","
""""
as
""""
]
is
there
an
easy
way
to
do
this
without
recreating
an
entirely
new
list
and
using
a
for
loop
?
I
am
trying
to
select
multiple
objects
using
mouse
just
like
in
windows
click
and
drag
.
i
am
using
tkinter
in
python
to
buils
this
gui
.
i
am
creating
objects
as
shown
in
below
code
.
what
i
am
trying
to
do
is
if
i
drag
my
mouse
over
multiple
objects
some
def
should
return
the
tags
of
the
objects
.
how
can
i
do
this
.
Thank
you
Save
the
coordinates
on
a
button-down
event
","
and
then
on
a
button-up
event
use
the
find_enclosed
or
find_overlapping
method
of
the
canvas
to
find
all
items
enclosed
by
the
region
.
If
order
doesn't
matter
","
a
set
operation
can
be
used
:
If
I
have
a
list
of
card
suits
in
arbitrary
order
like
so
:
and
I
want
to
return
a
list
without
the
'
c
'
is
there
a
simple
way
to
do
this
?
One
possibility
would
be
to
use
filter
:
Instead
of
the
partial
one
could
also
use
the
__ne__
method
of
'
c
'
here
:
However
","
the
latter
approach
is
not
considered
very
pythonic
(
normally
you
shouldn't
use
special
methods
-
starting
with
double
underscores
-
directly
)
and
it
could
give
weird
results
if
the
list
contains
mixed
types
but
it
could
be
a
bit
faster
than
the
partial
approach
.
Python
3.5.2
tested
with
IPythons
magic
%
timeit
command
.
Without
using
for
loops
or
lambda
functions
and
preserving
order
:
I'm
aware
that
internally
it'll
still
use
loops
","
but
at
least
you
don't
have
to
use
them
externally
.
you
can
use
filter
(
or
ifilter
from
itertools
)
you
can
also
filter
using
list
construct
There
doesn't
seem
to
be
anything
like
this
built
into
Python
by
default
unfortunately
.
There
are
several
answers
but
I
though
I'd
add
one
using
iterators
.
If
changing
in
place
is
acceptable
","
that's
going
to
be
fastest
.
If
you
don't
want
to
change
the
original
and
just
want
to
loop
over
a
filtered
set
","
this
should
be
pretty
fast
:
Implementation
:
Usage
:
So
it
is
a
generator
-
you'll
need
to
call
list
or
something
to
make
it
permanent
.
If
you
only
ever
want
to
remove
a
single
index
","
you
can
of
course
make
it
even
faster
by
using
k
=
=
remove_index
instead
of
a
set
.
If
it
is
important
that
you
want
to
remove
a
specific
element
(
as
opposed
to
just
filtering
)
","
you'll
want
something
close
to
the
following
:
You
may
also
consider
using
a
set
here
to
be
more
semantically
correct
if
indeed
your
problem
is
related
to
playing
cards
.
This
question
has
been
answered
but
I
wanted
to
address
the
comment
that
using
list
comprehension
is
much
slower
than
using
.
remove()
.
Some
profiles
from
my
machine
(
using
Python
2.7.6
)
.
If
you
use
the
fastest
way
to
copy
a
list
(
which
isn't
very
readable
)
","
you
will
be
about
36
%
faster
than
using
list
comprehension
.
But
if
you
copy
the
list
by
using
the
list()
class
(
which
is
much
more
common
and
Pythonic
)
","
then
you're
going
to
be
26
%
slower
than
using
list
comprehension
.
Really
","
it's
all
pretty
fast
.
I
think
the
argument
could
be
made
that
.
remove()
is
more
readable
than
list
a
list
comprehension
technique
","
but
it's
not
necessarily
faster
unless
you're
interested
in
giving
up
readability
in
the
duplication
.
The
big
advantage
of
list
comprehension
in
this
scenario
is
that
it's
much
more
succinct
(
i.e.
if
you
had
a
function
that
was
to
remove
an
element
from
a
given
list
for
some
reason
","
it
could
be
done
in
1
line
","
whilst
the
other
method
would
require
3
lines
.
)
There
are
times
in
which
one-liners
can
be
very
handy
(
although
they
typically
come
at
the
cost
of
some
readability
)
.
Additionally
","
using
list
comprehension
excels
in
the
case
when
you
don't
actually
know
if
the
element
to
be
removed
is
actually
in
the
list
to
begin
with
.
While
.
remove()
will
throw
a
ValueError
","
list
comprehension
will
operate
as
expected
.
If
you
don't
need
a
seperate
noclubs
The
proper
way
to
do
this
is
to
use
a
loop
as
you
mentioned
.
Any
other
solution
will
be
semantically
incorrect
.
(
What
you
are
saying
is
""""
For
each
item
in
L
","
do
f
.
""""
That
is
","
you
are
being
imperative
.
A
for
loop
says
exactly
that
.
)
Question
:
How
to
define
a
compact
construction
for
this
loop
:
Where
:
L
:
is
a
list
of
objects
of
a
given
type
A
f()
:
a
method
of
A
.
For
example
:
I
found
a
way
of
doing
that
with
a
map
call
and
a
lambda
function
:
However
","
this
call
returns
a
list
","
which
I
wish
not
to
construct
.
Is
there
a
more
elegant
way
of
doing
this
?
If
you
don't
want
a
list
as
a
return
value
","
then
your
for
loop
is
the
best
way
.
Alternatively
","
you
could
use
one
of
python's
best
features
","
list
comprehensions
","
and
simply
not
assign
it
to
any
variable
.
List
comprehensions
are
usually
faster
.
I
need
to
enter
earnings
of
employees
","
employee
wise
by
click
them
from
my
tree
view
(
tree
shows
workers
relevant
to
above
selected
division.)after
their
worked
hours
/
total
kgs
&
over
kgs
enters
then
form
allows
to
enter
wages
","
incentives
","
overtime
&
other
things
so
need
to
add
that
form
below
section
of
my
form.(not
as
a
popup
window
)
currently
employees
'
worked
hours
/
total
kgs
&
over
kgs
saved
in
a
table
&
wages
","
incentives
","
overtime
&
other
things
saved
in
another
table.both
have
worker_id
column
for
identify
workers
.
please
advice
me
to
implement
this
in
openerp
ver
7
refer
this
link
for
my
form
picture
Can
you
check
on
your
field
'
selected_tea_workers_line_ids
'
you
have
used
editable='bottom
'
remove
it
then
it
open
form
view
Suppose
that
I
have
the
following
variables
:
I'd
like
to
write
a
function
that
","
given
a
specific
card
","
generates
the
possible
combinations
of
pairs
.
For
instance
","
generatePairs('a')
should
return
something
like
:
[
'
ahac
'
","
'
ahad
'
","
'
ahas
'
","
'
acad
'
","
'
acas
'
","
'
adas
'
]
but
I'm
not
sure
how
to
approach
writing
that
function
.
you
require
something
like
I
have
searched
into
google
and
found
some
contradiction
.
Does
xlwt
support
xlsx
file
(
MS
office
2007
)
.
I
heard
that
xlwt
0.7.4
support
xlsx
file
.
Does
anyone
tried
xlsx
file
writing
operation
with
xlwt
0.7.4
The
purpose
of
this
question
is
","
I
do
not
have
permission
to
install
library
if
I
need
to
install
I
need
to
provide
more
detail
info
.
I
need
to
write
xlsx
file
in
python.So
if
anyone
has
done
similar
thing
it
will
help
to
provide
better
inforamtion
I
have
looked
into
this
wiki
page
.
https://pypi.python.org/pypi/xlwt
But
did
not
find
that
it
support
xlsx
file
or
Should
I
use
https://pypi.python.org/pypi/openpyxl
for
writing
xlsx
File
The
xlwt
module
doesn't
support
the
xlsx
format
.
The
xlsx
file
format
is
completely
different
from
the
xls
format
supported
by
xlwt
.
As
an
alternative
have
a
look
at
XlsxWriter
which
is
a
Python
module
for
creating
xlsx
files
.
It
supports
a
lot
of
Excel
features
.
Have
a
look
at
the
documentation
or
start
with
the
examples
.
openpyxl
is
guaranteed
to
write
xlsx
files
.
From
a
cursory
read
through
some
of
the
xlwt
code
and
docs
/
examples
","
I
don't
think
xlwt
supports
xlsx
.
If
openpyxl
does
what
you
need
it
to
do
","
why
look
elsewhere
?
Edit
:
with
xlwt
version
0.7.4
I
attempted
to
save
a
file
as
sample.xlsx
.
Upon
attempting
to
open
it
I
got
a
not
valid
error
message
","
so
no
.
xlsx
files
for
now
.
The
easiest
is
to
use
the
pickle
module
.
There
are
some
examples
in
the
manual
.
Either
use
pickle
as
suggested
or
coerce
the
object
to
a
str
","
which
is
what
write
is
expecting
.
Note
","
if
you
decide
to
coerce
to
a
str
and
want
to
recover
the
object
","
you'll
need
to
overwrite
__str__
in
the
object
you
are
serializing
to
output
enough
data
to
reconstruct
the
object
.
I
want
to
know
how
i
can
store
a
class
object
in
a
file
so
as
to
retrive
it
later
.
eg
)
I
have
a
stack
class
and
i
need
to
store
the
object
of
that
class
to
retrive
it
later
.
I
tried
the
following
:
Where
x
is
the
object
of
the
stack
class
.
The
whole
stack
program
works
well
and
when
i
come
to
this
storing
part
","
i
get
an
error
as
:
How
can
i
correct
this
mistake
?
what
is
the
best
way
to
store
?
I
am
using
a
simple
python
script
to
get
reservation
results
for
my
CID
:
simple.py
:
Now
on
the
command
prompt
if
I
do
python
simple.py
it
runs
perfectly
and
prints
the
req.text
variable
However
when
I
try
to
do
python
simple.py
|
grep
pattern
I
get
UnicodeEncodeError
:
'
ascii
'
codec
can't
encode
character
u'\xe4
'
in
position
1314
:
ordinal
not
in
range(128)
print
needs
to
encode
the
string
before
sending
to
stdout
but
when
the
process
is
in
a
pipe
","
the
value
of
sys.stdout.encoding
is
None
","
so
print
receives
an
unicode
object
and
then
it
tries
to
encode
this
object
using
the
ascii
codec
-
-
if
you
have
non-ASCII
characters
in
this
unicode
object
","
an
exception
will
be
raised
.
You
can
solve
this
problem
encoding
all
unicode
objects
before
sending
it
to
the
standard
output
(
but
you'll
need
to
guess
which
codec
to
use
)
.
See
these
examples
:
File
wrong.py
:
Result
:
File
right.py
:
Result
:
If
sys.stdout.isatty()
is
false
(
the
output
is
redirected
to
a
file
/
pipe
)
then
configure
PYTHONIOENCODING
envvar
outside
your
script
.
Always
print
Unicode
","
don't
hardcode
the
character
encoding
of
your
environment
inside
your
script
:
I'm
using
boto
to
connect
dynamodb
in
python
.
I'm
not
seeing
any
proper
tutorials
for
querying
dynamodb
.
""""
What
i
need
is
that
i
need
to
fetch
the
content
of
the
table
where
given
name
is
present
in
firstname
or
last
name
where
first
name
and
last
name
are
the
two
fields
using
dynamodb
""""
DynamoDB's
Query
operation
only
allows
you
to
specify
the
hash
and
range
keys
.
Scan
will
let
you
provide
other
fields
","
but
it
isn't
recommended
for
general
application
use
.
I'm
not
familiar
with
boto
","
but
if
you
want
to
further
filter
your
results
","
you'll
have
to
query
if
you
can
","
then
post-process
the
results
in
your
application
.
Otherwise
","
you'll
have
to
scan
","
which
would
allow
you
to
use
the
CONTAINS
comparison
on
one
field
at
a
time
.
You
can't
check
both
fields
at
the
same
time
because
the
name
would
have
to
be
found
in
both
fields
","
not
just
one
.
See
the
table
entries
called
ScanFilter
on
the
Scan
page
for
more
information
on
what's
possible
.
Suppose
","
I
need
to
count
collisions
for
various
schemes
of
hashes
.
Number
of
elements
in
input
sequence
is
1e10^8
and
more
.
Don't
know
how
to
count
this
analytically
","
so
using
brute-force
.
It's
obvious
not
to
keep
this
list
of
hashes
in
RAM
.
That
is
the
best
way
to
write
a
code
for
my
needs
?
Should
i
dump
it
in
database
or
something
?
What
libraries
are
preferred
to
use
?
Thank
you
!
I'd
suggest
keeping
a
set
of
files
","
each
one
named
with
a
prefix
of
the
hashes
contained
within
it
(
for
example
","
if
you
use
a
prefix
length
of
6
","
then
the
file
named
ffa23b.txt
might
contain
the
hashes
ffa23b11d4334
","
ffa23b712f3
","
et
cetera
)
.
Each
time
you
read
a
hash
","
you
append
it
to
the
file
with
the
name
corresponding
to
the
first
N
characters
of
the
hash
.
You
can
also
use
bloom
filters
to
quickly
rule
out
a
large
fraction
of
the
hashes
as
unique
","
without
having
to
store
all
of
the
hashes
in
memory
.
That
way
","
you
only
have
to
fall
back
to
searching
through
a
given
prefix
file
if
the
check
against
the
bloom
filter
says
that
you
might
have
actually
seen
it
before
-
something
that
will
happen
rarely
.
Short
answer
:
if
you
have
some
gigabytes
of
RAM
","
use
Python
dictionaries
","
it's
the
easiest
way
to
implement
(
and
probably
the
faster
to
run
)
.
You
can
do
something
like
this
:
And
then
check
if
the
key
exists
in
the
mapping
:
Long
answer
:
you
can
use
a
persistent
key-value
store
","
like
GDBM
(
it
looks
like
Berkeley
DB
)
or
another
kind
of
database
-
-
but
this
approach
will
be
way
slower
than
using
just
Python
dictionaries
;
on
the
other
hand
","
with
this
approach
you'll
have
persistance
(
if
you
need
)
.
You
can
understand
GDBM
as
a
dictionary
(
key-value
store
)
that
is
persisted
in
a
single
file
.
You
can
use
it
as
follows
:
Then
the
file
my.db
will
be
created
(
see
Python
GDBM
documentation
to
understand
what
cf
means
)
.
But
it
has
some
limitations
","
as
supporting
only
strings
as
keys
and
values
:
You
can
populate
a
gdbm
database
with
all
your
keys
having
a
dummy
value
:
And
then
check
if
the
key
exists
in
the
mapping
:
Trying
to
use
Python
mechanize
to
log
in
to
a
webpage
.
Since
I
have
a
problem
with
finding
the
correct
form
","
instead
of
posting
the
URL
(
which
the
code
might
change
)
I
will
copy
some
of
the
code
here
for
future
users
.
I
read
this
","
but
there
didn't
seem
too
be
a
definitive
answer
.
Anyways
","
I
did
everything
with
mechanize
in
this
tutorial
up
until
where
I
am
getting
the
forms
.
When
I
call
:
I
get
back
:
Looking
at
the
raw
HTML
I
see
:
However
","
when
I
try
to
select
the
email
field
I
get
a
form
not
found
error
.
I
have
also
tried
using
the
form
ID
","
and
many
other
possible
form
names
.
I
don't
understand
why
printing
the
forms
with
br.forms()
returns
those
strange
results
","
does
this
mean
the
site
is
using
javascript
for
login
forms
?
Thank
you
in
advance
!
affiliate
[email]
is
not
the
name
of
a
form
","
but
of
an
input
within
the
form
.
Try
using
:
if
the
form
has
no
name
and
is
the
first
/
only
form
on
the
page
.
If
you
'
don't
declare
fields
","
they
won't
be
accessible
in
the
bundle
.
However
","
they'll
always
be
accessible
in
the
request
.
You'll
need
to
have
at
least
one
predefined
field
","
to
be
used
as
a
primary
key
.
While
you
don't
explicitly
need
to
create
it
","
you
should
have
a
way
of
knowing
what
object
to
return
if
the
user
issues
a
GET
request
for
some
object
.
I
have
a
class
which
returns
json
I
want
to
create
Tastypie
Model
Resource
based
on
the
returned
Json
I
tried
the
Below
Url
this
worked
but
i
dont
want
to
declare
fields
it
should
be
dynamic
http://thehungrycoder.com/python/using-non-orm-data-sources-with-tastypie-in-django.html
This
prints
For
this
work
","
you
need
to
know
the
length
of
the
longest
string
(
59
in
the
example
above
)
.
I
want
to
print
following
lines
in
a
pretty
way
.
I
want
all
the
OK
should
be
in
one
column
.
could
be
also
a
way
to
go
.
maybe
i
think
the
hours
is
:
here
it
is
another
way
:
I
have
2
datetime
objects
:
The
difference
is
3:20
How
can
I
obtain
this
in
python
?
I
do
:
But
I
get
3
instead
of
3:20
Is
manual
construction
acceptable
?
If
you
subtract
the
two
datetimes
you'll
get
a
timedelta
object
.
Then
just
grab
the
values
from
that
:
Use
filter()
instead
of
get()
.
get
fetches
only
one
result
.
I'm
guessing
you're
not
using
any
relationship
between
your
models
.
I
suggest
you
start
doing
that
.
HI
i
need
to
fetch
the
data
with
the
reference
to
some
value
","
but
the
problem
is
the
value
exists
multiple
times
i
need
to
fetch
the
all
values
with
referencs
to
userid
the
function
in
my
views
is
but
it
throws
the
error
multiple
values
exist
how
can
i
do
this
please
suggest
BY
USING
FILTER
IT
GIVES
Rewardpoints_log
:
202
>
","
IN
THE
HTML
FILE
BY
USING
THIS
{
{
reward
}
}
WHEN
I
TRY
{
{
reward.rewardpoints
}
}
IT
RETURNS
NOTHING
This
may
not
work
until
all
depended
libraries
be
installed
","
so
install
ffmpeg
library
with
apt-get
:
This
will
install
all
dependenties
of
'
ffmpeg
encoding
/
decoding
'
what
may
needs
the
pyffmpeg
library
.
I
have
problem
with
running
pyfmpeg
that
when
I
try
to
import code
below
it
gives
error
:
libavformat.so
can
not
found
How
can
I
solve
this
problem
I
could
not
find
enough
information
about
it.Thanks
.
.
Operating
Systen
:
Ubuntu
12.04
Python
version
:
2.7
Problem
Solved
:
I
downloaded
libs
from
here
http://rpmfind.net/linux/rpm2html/search.php?query=libavformats52
and
copy
to
/
usr
/
lib
/
But
new
problem
exist
now
it
gives
error
:
wrong
ELF
class
:
ELFCLASS32
The
simple
answer
is
that
you
can't
reconstruct
the
input
string
given
its
hash
.
The
function
has
209569
distinct
output
values
","
and
there
are
many
more
possible
input
strings
.
See
Pigeonhole
principle
.
If
the
task
is
to
find
a
string
that
has
the
given
hash
","
that
is
a
different
problem
...
I've
made
it
.
I
just
made
6
loops
and
tried
every
combination
of
the
letter
.
And
an
exception
to
break
out
when
i
found
it
.
It
was
found
within
5
seconds
.
This
is
the
code(python)
my
teacher
wrote
.
(
with
the
prime
numbers
i've
found
using
a
bruteforce
)
.
I
also
have
some
test
data
with
passwords
that
have
matching
hashes
(
not
that
it
really
matters
now
that
i've
found
the
two
primes
used
in
the
function
)
.
Can
anyone
help
me
on
the
way
of
finding
a
way
to
""""
reverse
""""
this
or
atleast
give
me
a
tip
?
I've
tried
to
implement
WebSocket
frame
unmasking
algorithm
(
based
on
this
:
How
can
I
send
and
receive
WebSocket
messages
on
the
server
side
?
)
on
the
server
side
.
Here's
what
I've
got
:
However
I
get
very
strange
results
.
On
JavaScript
side
:
produces
on
the
server
side
:
More
calls
to
"w.send(""test"")"
;
produces
"[2, 97, 0, 0]"
.
Also
first
two
arrays
have
length
>
4
(
number
of
characters
in
word
test
)
.
And
none
of
these
gets
converted
to
word
test
.
It
seeems
that
I
must
be
doing
something
wrong
in
my
decoding
code
.
What's
causing
that
?
Any
help
?
EDIT
Have
look
at
raw
frames
:
Of
course
these
numbers
are
a
bit
random
(
due
to
masking
)
","
but
note
that
second
byte
(
which
is
supposed
to
represent
the
length
of
payload
)
is
134
","
then
133
and
then
always
132
.
Also
first
two
frames
are
longer
then
other
frames
.
What's
going
on
here
?
I've
just
looked
at
this
frames
one
more
time
and
I've
realized
that
the
first
byte
:
which
according
to
the
specification
means
that
RSV1
is
used
(
the
second
bit
is
1
)
.
This
seems
to
mean
that
the
extension
is
used
and
indeed
in
my
handshake
code
I've
found
the
following
line
:
and
in
Chrome
I
can
see
:
when
I
do
the
handshake
.
I'm
not
sure
yet
how
these
extensions
work
but
removing
these
3
lines
from
the
handshake
code
fixes
the
issue
.
I
have
a
NumPy
array
that
stores
the
information
of
a
signal
.
In
the
code
below
I
have
just
created
it
from
random
numbers
for
convenience
.
Now
I
need
to
select
positions
from
the
signal
array
","
based
on
criteria
.
Again
","
I
have
simplified
the
criteria
below
","
for
sake
of
brevity
.
I
am
trying
to
store
those
positions
by
appending
them
to
an
empty
NumPy
array
called
positions
.
I
know
that
my
use
of
the
NumPy.append()
function
must
be
wrong
.
I
tried
to
debug
it
","
but
I
cannot
find
the
bug
.
What
am
I
doing
wrong
?
EDIT
:
I
actually
need
this
to
run
in
a
for
loop
","
since
my
condition
in
the
if()
is
a
mathematical
expression
that
will
be
changed
according
to
some
rule
(
that
would
be
way
to
complicated
to
post
here
)
if
the
condition
is
met
.
You
don't
need
an
explicit
loop
for
this
:
Here
","
(
signal
%
3
=
=
0
)
|
(
signal
%
5
=
=
0
)
evaluates
the
criterion
on
every
element
of
signal
","
and
np.where()
returns
the
indices
where
the
criterion
is
true
.
If
you
have
to
append
to
positions
in
a
loop
","
here
is
one
way
to
do
it
:
Yes
","
there
is
a
good
reason
not
to
start
all
your
Python
programs
like
that
.
First
of
all
:
sys.stdout.encoding
is
None
if
Python
doesn't
know
what
encoding
the
stdout
supports
.
This
","
in
most
cases
","
is
because
it
doesn't
really
support
any
encoding
at
all
.
In
your
case
it's
because
the
stdout
is
a
file
","
and
not
a
terminal
.
But
it
could
be
set
to
None
because
Python
also
fails
to
detect
the
encoding
of
the
terminal
.
Second
of
all
:
You
set
the
environment
variable
and
then
start
a
new
process
with
the
smae
command
again
.
That's
pretty
ugly
.
So
","
unless
you
plan
to
be
the
only
one
using
your
programs
","
you
shouldn't
start
them
like
that
.
But
if
you
do
plan
to
be
the
only
using
your
program
","
then
go
ahead
.
More
in-depth
explanation
A
better
generic
solution
under
Python
2
is
to
treat
stdout
as
what
it
is
:
An
8-bit
interface
.
And
that
means
that
anything
you
print
to
to
stdout
should
be
8-bit
.
You
get
the
error
when
you
are
trying
to
print
Unicode
data
","
because
print
will
then
try
to
encode
the
Unicode
data
to
the
encoding
of
stdout
","
and
if
it's
None
it
will
assume
ASCII
","
and
fail
","
unless
you
set
PYTHONIOENCODING
.
But
by
printing
encoded
data
","
you
don't
have
this
problem
.
The
following
works
perfectly
even
when
the
output
is
piped
:
(
However
","
this
will
fail
Under
Python
3
","
because
under
Python
3
","
stdout
is
no
longer
8-bit
IO
","
you
are
supposed
to
give
it
Unicode
data
","
and
it
will
encode
by
itself
.
If
you
give
it
binary
data
","
it
will
print
the
representation
.
Therefore
on
Python
3
you
don't
have
this
problem
in
the
first
place
)
.
Is
there
a
good
reason
why
I
shouldn't
start
all
my
python
programs
with
this
?
Is
there
something
special
lost
when
doing
exec
like
this
?
There
are
60
questions
about
PYTHONIOENCODING
so
I
guess
it's
a
common
problem
","
but
in
case
you
don't
know
","
this
is
done
because
when
sys.stdout.encoding
=
=
None
then
you
can
only
print
ascii
chars
","
so
e.g
.
print
""""
åäö
""""
will
throw
an
exception
.
.
EDIT
This
happens
to
me
when
stdout
is
a
pipe
;
python
encoding.py|cat
will
set
encoding
to
None
Another
solution
is
to
change
the
codec
of
stdout
sys.stdout
=
codecs.getwriter('utf8')
(
sys.stdout
)
which
I'm
guessing
is
the
correct
answer
dispite
the
comments
on
that
question
.
The
issue
with
me
was
that
I
installed
python
","
cx_oracle
as
root
but
Oracle
client
installation
was
done
by
""""
oracle
""""
user
.
I
got
my
own
oracle
installation
and
that
fixed
the
issue
.
Later
I
ran
into
PyUnicodeUCS4_DecodeUTF16
issues
with
Python
and
for
that
I
had
to
install
python
with
â€”enable-unicode=ucs4
option
I
have
installd
Python
2.7.3
on
Linux
64
bit
machine
.
I
have
Oracle
11g
client(64bit)
as
well
installed
.
And
I
set
ORACLE_HOME
","
PATH
","
LD_LIBRARY_PATH
","
and
installed
cx_oracle
5.1.2
version
for
Python
2.7
&
Oracle
11g
.
But
ldd
command
on
cx_oracle
is
unable
to
find
libclntsh.so.11.1
.
I
tried
creating
symlinks
to
libclntsh.so.11.1
under
/
usr
/
lib64
","
updated
oracle.conf
file
under
/
etc
/
ld.so.conf.d
/
.
Tried
all
possible
solutions
that
have
been
discussed
on
this
issue
on
the
forums
","
but
no
luck
.
Please
let
me
know
what
am
missing
.
I
use
the
code
to
strip
a
line
of
text
from
punctuation
:
The
problem
is
that
words
like
doesn't
turn
to
doesnt
so
now
I
want
to
remove
the
punctuation
only
between
words
but
can't
seem
to
figure
out
a
way
to
do
so
.
How
should
I
go
about
this
?
Edit
:
I
thought
about
using
the
strip()
function
but
that
will
only
take
effect
on
the
right
and
left
trailing
of
the
whole
sentence
.
For
example
:
Should
become
:
Instead
of
the
current
output
:
Is
this
what
u
want
?
Assuming
you
consider
words
as
groups
of
characters
separated
by
spaces
:
or
I
have
tried
using
textwrap
on
the
labels
and
it
works
for
me
.
Inserting
this
in
your
code
gives
us
:
I've
been
trying
to
wrap
text
for
long
labels
in
my
code
.
I
tried
the
textwrap
method
suggested
earlier
here
","
but
my
code
defines
yticklabels
through
an
array
imported
from
a
csv
using
the
pyplot.setp()
method
.
I'm
using
tight_layout()
for
the
formatting
otherwise
.
So
the
question
is
-
is
there
a
way
to
wrap
the
really
long
y
labels
to
newlines
easily
?
Here
is
some
sample
code
that
I'd
like
a
fix
for
:
This
plots
something
like
this
I'd
like
the
labels
to
go
to
newlines
after
a
fixed
width
.
Any
ideas
?
New
to
scikit-learn
and
I
am
working
with
some
data
like
the
following
.
For
single
lines
of
text
there
is
CountVectorizer
and
DictVectorizer
in
the
pipeline
before
TfidfTransformer
.
The
output
of
these
could
be
concatenated
","
I'm
hoping
with
the
following
caveat
:
The
arbitrary
text
I
don't
want
to
be
equal
in
importance
to
the
specific
","
limited
and
well-defined
parameters
.
Finally
","
some
other
questions
","
possibly
related
might
this
data
structure
indicate
which
SVM
kernel
is
best
?
Or
would
a
Random
Forest
/
Decision
Tree
","
DBN
","
or
Bayes
classifier
possibly
do
better
in
this
case
?
Or
an
Ensemble
method
?
(
The
output
is
multi-class
)
I
see
there
is
an
upcoming
feature
for
feature
union
","
but
this
is
to
run
different
methods
over
the
same
data
and
combine
them
.
Should
I
be
using
feature
selection
?
See
also
:
Implementing
Bag-of-Words
Naive-Bayes
classifier
in
NLTK
Combining
feature
extraction
classes
in
scikit-learn
http://scikit-learn.org/dev/modules/label_propagation.html
All
classifiers
in
scikit-learn
(
*
)
expect
a
flat
feature
representation
for
samples
","
so
you'll
probably
want
to
turn
your
string
feature
into
a
vector
.
First
","
let
get
some
incorrect
assumptions
out
of
the
way
:
DictVectorizer
is
not
for
handling
""""
lines
of
text
""""
","
but
for
arbitrary
symbolic
features
.
CountVectorizer
is
also
not
for
handling
lines
","
but
for
entire
text
documents
.
Whether
features
are
""""
equal
in
importance
""""
is
mostly
up
to
the
learning
algorithm
","
though
with
a
kernelized
SVM
","
you
can
assign
artificially
small
weights
to
features
to
make
its
dot
products
come
out
differently
.
I'm
not
saying
that's
a
good
idea
","
though
.
There
are
two
ways
of
handling
this
kind
of
data
:
Build
a
FeatureUnion
consisting
of
a
CountVectorizer
(
or
TfidfVectorizer
)
for
your
textual
data
and
a
DictVectorizer
for
the
additional
features
.
Manually
split
the
textual
data
into
words
","
then
use
each
word
as
a
feature
in
a
DictVectorizer
","
e.g
.
Then
the
related
questions
:
might
this
data
structure
indicate
which
SVM
kernel
is
best
?
Since
you're
handling
textual
data
","
try
a
LinearSVC
first
and
a
polynomial
kernel
of
degree
2
if
it
doesn't
work
.
RBF
kernels
are
a
bad
match
for
textual
data
","
and
cubic
or
higher-order
poly
kernels
tend
to
overfit
badly
.
As
an
alternative
to
kernels
","
you
can
manually
construct
products
of
individual
features
and
train
a
LinearSVC
on
that
;
sometimes
","
that
works
better
than
a
kernel
.
It
also
gets
rid
of
the
feature
importances
issue
as
a
LinearSVC
learns
per-feature
weights
.
Or
would
a
Random
Forest
/
Decision
Tree
","
DBN
","
or
Bayes
classifier
possibly
do
better
in
this
case
?
That's
impossible
to
tell
without
trying
.
scikit-learn's
random
forests
and
dtrees
unfortunately
don't
handle
sparse
matrices
","
so
they're
rather
hard
to
apply
.
DBNs
are
not
implemented
.
Should
I
be
using
feature
selection
?
Impossible
to
tell
without
seeing
the
data
.
(
*
)
Except
SVMs
if
you
implement
custom
kernels
","
which
is
such
an
advanced
topic
that
I
won't
discuss
it
now
.
One
solution
is
to
provide
the
__name__
explicitly
in
your
execution
dict
:
You
could
use
imp.load_module
instead
:
This
imports
the
file
as
the
__main__
module
","
executing
it
.
Note
that
it
takes
an
actual
file
object
when
the
type
is
set
to
imp.PY_SOURCE
","
so
you'd
need
to
create
a
temporary
file
for
this
to
work
if
your
source
code
comes
from
somewhere
other
than
a
file
.
Otherwise
","
can
always
set
__name__
manually
:
When
I
run
:
it
prints
__main__
.
But
when
I
run
:
it
prints
__builtin__
.
How
to
make
the
second
example
to
also
print
__main__
?
What
I
try
to
achieve
is
to
run
a
piece
of
code
with
exec
so
that
from
the
perspective
of
the
it
looks
like
it
was
run
from
command
line
.
I
would
like
to
tun
the
code
with
clean
scope
but
the
second
example
breaks
the
code
relying
on
if
__name__
=
=
""""
__main__
""""
.
How
to
fix
this
?
You
are
confused
between
Windows
and
UNIX
filesystems
.
Find
out
where
sqllite.exe
file
exists
on
the
computer
.
lets
say
it
is
in
C:\sqllite
.
Then
you
also
need
to
determine
where
you
will
create
the
database
file
.
/
tmp
/
flaskr.db
is
for
the
UNIX
filesystem
.
On
windows
","
you
should
provide
the
exact
path
or
in
your
current
working
directory
.
lets
say
it
is
C:\flasktutorial
.
To
be
safe
","
you
might
want
to
create
a
blank
flaskr.db
file
first
.
Now
you
can
run
:
Also
make
sure
that
in
your
flaskr.py
file
","
change
the
DATABASE
to
:
Following
Flask
tutorial
","
running
Win
7
","
Python
2.7.3
","
virtualenv
","
and
I
am
stuck
in
Step
3
:
Creating
The
Database
http://flask.pocoo.org/docs/tutorial/dbinit/#tutorial-dbinit
Such
a
schema
can
be
created
by
piping
the
schema.sql
file
into
the
sqlite3
command
as
follows
:
How
to
run
this
command
","
because
CMD
<
venv
>
returns
:
""""
sqlite3
""""
is
not
recognized
as
internal
or
external
command
","
operable
program
or
batch
file
.
Is
this
step
necessary
?
Folder
Project
","
2
files
schema.sql
and
flaskr.py
.
schema.sql
flaskr.py
<
venv
>
python
As
you
can
observe
from
the
error
logs
","
the
error
is
coming
while
connecting
DB
sqlite3.OperationalError
:
unable
to
open
database
.
And
since
this
step
failed
for
you
","
sqlite3
/
tmp
/
flaskr.db
<
schema.sql
You
can
easily
interpret
that
this
step
is
necessary
.
Now
to
solve
this
error
","
simply
you
have
to
install
sqlite3
","
In
ubuntu
you
can
install
sqlite3
as
","
apt-get
install
sqlite3
After
installation
your
program
will
work
fine
as
expected
.
For
linux
user
only
:
First
you
install
Sqlite3
from
here
.
After
installation
you
need
to
create
a
file
called
flaskr.db
","
location
of
this
file
is
to
be
/
myproject
/
venv
/
bin
/
flaskr
/
tmp
.
Next
step
is
run
command
I
took
file
address
from
home
.
Next
step
is
start
python
shell
","
run
this
code
Thats
it
.
I'm
not
sure
if
these
tips
are
directly
applicable
to
the
PO
","
but
they
landed
me
here
","
so
maybe
it
will
be
helpful
.
Be
sure
to
check
the
README
file
within
the
flaskr
directory
which
specifies
running
flask
-
-
app=flaskr
initdb
from
the
command
line
.
If
this
returns
AttributeError
:
'
Flask
'
object
has
no
attribute
'
cli
'
","
it
may
be
due
to
click
not
being
installed
.
If
the
command
returns
flask
:
command
not
found
","
it
may
be
due
to
having
an
older
version
of
Flash
installed
","
which
is
what
pip
install
flask
does
.
As
of
today
this
command
pip
install
"https://github.com/mitsuhiko/flask/tarball/master,"
will
install
the
most
recent
version
.
Did
you
activate
virtualenv
and
installed
flask
?
flask
should
have
sqlite3
by
default
.
I
got
following
error
though
:
To
fix
that
I
had
to
do
the
following
(
in
Windows
)
:
Change
DATABASE
=
'
/
tmp
/
flaskr.db
'
to
DATABASE
=
'
.
\
\
tmp\\flaskr.db
'
Create
tmp
folder
in
current
folder
(
flaskr
)
Create
an
empty
flaskr.db
file
in
tmp
After
that
it's
working
for
me
.
My
setup
looks
like
this
:
A
64-bit
box
running
Windows
7
Professional
is
connected
to
a
Beaglebone
running
Angstrom
Linux
.
I'm
currently
controlling
the
beaglebone
via
a
putty
command
line
on
the
windows
box
.
What
I'd
like
to
do
is
run
an
OpenCV
script
to
pull
some
vision
information
","
process
it
on
the
windows
box
","
and
send
some
lightweight
data
(
e.g
a
True
or
False
","
a
triplet
","
etc.
)
over
the
(
or
another
)
USB
connection
to
the
beaglebone
.
My
OpenCV
program
is
running
using
Python
bindings
","
so
any
piping
I
can
do
with
python
would
be
preferable
.
I've
played
around
with
pyserial
to
receive
data
on
a
windows
box
via
a
COM
port
","
so
it
seems
like
I
could
use
that
on
the
windows
side
...
at
a
total
loss
though
on
the
embedded
linux
front
Normally
on
the
linux
front
","
if
the
usb
dongle
is
of
the
right
type
","
you
will
see
something
like
/
dev
/
usbserial
or
similar
device
.
Maybe
check
dmesg
after
plugging
the
cable
.
(
on
linux
you
can
run
find
/
dev
|
grep
usb
to
list
all
usb
related
devices
)
Just
a
side
note
","
I've
seen
the
beaglebone
has
an
ethernet
port
","
why
not
just
using
a
network
socket
?
It's
all
easier
than
reinventing
a
protocol
on
usb
.
If
you
want
to
use
python
","
take
a
look
o
PyUSB
","
as
you
can
see
for
example
in
Sending
data
via
USB
using
PyUSB
.
A
related
post
is
PyUSB
for
the
Raspberry
Pi
.
I
would
like
to
crop
out
an
image
from
an
existing
image
.
I've
taken
an
image
and
applied
monochrome
on
it
with
threshold
98
%
using
imagemagick
(
is
this
doable
in
openCV
?
)
The
resulting
image
is
this
:
Now
from
this
Image
I
would
like
to
crop
out
another
image
so
that
the
final
image
looks
like
this
:
Question
How
can
I
do
this
in
OpenCV
?
Note
","
the
only
reason
I
want
to
crop
the
image
is
so
that
I
can
use
this
answer
to
get
the
part
of
the
text
.
If
there
is
no
need
to
crop
out
a
new
image
and
instead
just
concentrate
on
black
part
of
the
image
to
begin
with
","
that
would
be
great
.
If
the
text
at
the
top
and
at
the
bottom
are
the
regions
that
you
want
to
crop
out
","
if
they
are
always
at
the
same
location
the
solution
is
easy
:
just
set
a
ROI
that
ignores
those
areas
:
Output
image
:
If
the
position
of
the
black
rectangle
","
which
is
your
area
of
interest
","
is
not
present
on
a
fixed
location
then
you
might
want
to
check
out
another
approach
:
use
the
rectangle
detection
technique
:
On
the
output
above
","
the
area
you
are
interested
will
be
2nd
largest
rectangle
in
the
image
.
On
a
side
note
","
if
you
plan
to
isolate
the
text
later
","
a
simple
cv::erode
(
)
could
remove
all
the
noises
in
that
image
so
you
are
left
with
the
white
box
&
text
.
Another
technique
to
remove
noises
is
to
use
cv::medianBlur().You
can
also
explore
cv::morphologyEx
(
)
to
do
that
trick
:
A
proper
solution
might
even
be
a
combination
of
these
3
.
I've
demonstrated
a
little
bit
of
that
on
Extract
hand
bones
from
X-ray
image
.
A
simple
solution
:
scan
lines
from
top
down
","
bottom
up
","
left-right
and
right-left
.
Terminate
when
the
number
of
dark
pixels
in
the
line
exceeds
50
%
of
the
total
amount
of
pixels
in
the
line
.
This
will
give
you
the
xmin
","
xmax
","
ymin
","
ymax
coordinates
to
bound
your
cropping
rectangle
.
If
you
dont
have
the
Cliente
yet
","
you
can
create
him
","
using
the
Imovel
data
:
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
*
*
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
*
*
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
*
*
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
*
*
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
*
*
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
*
*
-
-
-
-
-
-
-
-
-
If
you
already
have
the
Cliente
instance
in
the
relation
with
your
Captacao
:
*
*
and
this
will
create
a
new
Imovel
using
the
Cliente
relation
with
you
Imovel
I
Have
an
problem
:
Exists
3
models
in
my
app
:
I
Want
populate
the
models
""""
Cliente
""""
and
""""
Imovel
""""
through
of
model
""""
Captacao
""""
...
i
know
there
post_save
for
this
","
but
I
do
not
know
how
to
do
this
","
save
those
fields
in
each
respective
model
.
Thanks
","
In
the
Image
Object
section
of
the
GIMP
Python
Documentation
it
mentions
a
image.base_type
member
that
sounds
like
what
you
want
.
I'm
trying
to
use
the
GIMP
python
plugin
Density
Brush
Fill
.
In
one
place
","
it
assumes
that
it's
being
used
on
a
color
image
and
crashes
when
used
with
a
greyscale
one
:
The
fix
would
be
quite
easy
however
I
haven't
found
any
API
documentation
for
image
.
How
to
get
the
type
(
color
or
greyscale
)
of
the
image
?
What
about
getting
number
of
channels
?
1
","
2
","
3
","
4
=
GRAY
","
GRAYA
","
RGB
","
RGBA
But
I
must
admit
I
dont
know
if
channels
has
names
at
all
and
/
or
if
positions
of
channels
defines
the
content
or
names
do
it
.
You
want
to
send
a
string
instead
of
a
tuple
to
render
as
first
argument
:
Should
be
or
(
""""
Your
health
:
""""
","
self.a
","
)
is
a
tuple
.
By
passing
a
string
","
you
can
solve
your
problem
.
See
here
to
understand
what
I
have
done
...
You
are
passing
the
tuple
(
""""
Your
health
:
""""
","
self.a
","
)
as
the
first
argument
to
render
.
I'm
assuming
there
should
be
a
string
instead
.
There
are
several
ways
to
format
a
string
with
a
variable
","
one
approach
is
this
:
I'm
having
trouble
with
pygame...again...one
of
these
days
I
swear
I'll
be
good
enough
to
not
have
to
come
here
for
easy
answers
...
Anyway
","
the
problem
this
time
is
I'm
trying
to
print
text
on
the
screen
with
a
variable
inside
it
.
It
says
that
it
has
to
be
a
string
or
unicode...but
maybe
there
is
some
way
?
Once
again
","
I
remind
everyone
to
not
correct
anything
that
I'm
not
asking
about
.
I
know
there
is
probably
some
easier
way
to
do
these
things
...
I
changed
from
the
old
datastore
API
to
NDB(a bit of a chore changing the code)
.
The
automatic
caching
seems
to
have
solved
the
problem
","
this
may
suggest
that
the
problem
was
with
my
code
","
but
still
doesn't
explain
why
everything
worked
fine
when
using
app
engine
1.7.5
and
not
with
1.7.6
.
I'll
remove
this
answer
if
anyone
has
an
alternative
","
just
thought
I'd
post
my
progress
in
case
anyone
else
is
having
the
same
problem
.
I've
been
having
some
trouble
with
the
memcache
viewer
after
updating
the
Python
Dev
Appserver
to
Google
App
Engine
1.7.6
(
with
Python
2.7
)
.
It
appears
that
my
memcache
isn't
updated
or
isn't
readable
.
I
have
tried
to
view
memcache
with
the
app
engine
memcache
viewer
but
when
I
input
the
memcache
key
I
get
an
error
.
when
I
flush
the
cache
everything
proceeds
as
normal
until
memcache
needs
to
be
read
again
...
The
hit
ratio
and
memcache
size
increases
as
normal
","
so
there
is
something
in
the
cache
.
Also
when
I
revert
back
to
app
engine
1.7.5
everything
works
just
fine
.
Perhaps
someone
else
has
had
this
issue
?
When
I
input
the
memcache
key
I
get
the
following
:
I
tried
including
an
""""
import pickle
""""
in
my
main.py
but
this
was
in
vain
.
I've
included
some
samples
of
my
code
but
hopefully
this
isn't
necessary
I
hope
its
something
more
to
do
with
the
app
engine
update
than
my
code
...
some
of
my
main.py
file
:
and
a
sample
function
for
how
I
handle
memcache
:
and
if
I
want
to
query
the
users
in
my
db
I
use
:
UPDATE
:
I
added
""""
import pickle
""""
to
the
memcache_viewer.py
file
in
Google\google_appengine\google\appengine\tools\devappserver2\admin\memcache_viewer.py
(
is
this
a
bug
?
?
)
and
now
when
I
type
in
a
memcache
key
I
get
the
following
error
under
the
memcache
key
input
field
:
Error
fetching
USERS
:
Failed
to
retrieve
value
from
cache
:
No
module
named
main
Any
help
would
be
greatly
appreciated
","
thanks
in
advance
.
I
wrote
a
class
which
is
inherited
from
xml.etree.ElementTree.Element
class
to
extend
that
class
with
methods
to
show
the
complete
tag
name
of
the
etree
element
and
an
easy
way
to
replace
the
complete
content
of
an
element
with
a
XML
value
from
a
string
.
.
In
the
init
method
of
the
new
class
i
just
want
to
have
a
copy
of
an
etree
Element
passed
as
parameter
when
instanciated
.
First
try
was
:
But
then
the
""""
subtags
""""
list
of
the
passed
""""
Element
""""
got
lost
.
The
solution
below
works
but
is
there
a
""""
smarter
""""
way
to
copy
the
complete
""""
elem
""""
into
the
new
object
?
This
corrects
the
error
in
your
original
__init__()
method
:
This
copies
the
whole
elem
into
your
instance
and
you
can
now
access
its
properties
","
such
as
self.elem.tag
or
self.elem.text
.
I
recommend
using
a
dictionary
with
names
as
keys
and
lists
including
both
the
iterations
(
as
a
sub-list
)
and
the
sums
as
values
","
e.g
.
:
I
have
a
csv
named
temp.csv
where
3
columns
now
i
want
a
new
csv
named
accumuated.csv
like
You
should
maintain
a
dictionary
with
""""
Name
""""
as
the
key
.
The
value
can
be
a
list
with
""""
Iteration
""""
and
""""
Value
""""
.
Traverse
through
the
input
csv
file
and
update
the
dictionary
based
on
the
""""
Name
""""
.
Example
code
:
Now
write
it
back
to
the
output
file
.
I
have
a
simple
python
script
however
it
displays
a
much
higher
execution
time
when
it's
run
for
the
first
time
in
a
while
.
If
I
execute
it
immediately
after
it's
faster
by
a
few
factors
.
This
script
is
run
on
a
private
test
server
with
no
applications
running
on
it
so
I
don't
think
a
lack
of
system
resources
is
what
is
causing
it
to
run
slower
.
Can
anyone
explain
the
variance
in
the
execution
time
?
I've
ran
similar
tests
for
php
scripts
that
include
external
scripts
and
there's
negligible
variance
in
the
execution
time
of
that
script
.
This
variance
affects
my
application
because
such
scripts
are
called
several
times
and
cause
the
response
to
be
delivered
between
70ms
and
450ms
.
There
can
be
several
factors
.
Two
I
can
think
off
of
right
now
:
Initial
byte
compilation
.
Python
caches
the
compiled
bytecode
in
.
pyc
files
","
on
a
first
run
that
file
needs
to
be
created
","
subsequent
runs
only
need
to
verify
the
timestamp
on
the
byte
code
cache
.
Disk
caching
The
Python
interpreter
","
the
3
libraries
you
refer
to
directly
","
anything
those
libraries
use
","
all
need
to
be
loaded
from
disk
","
quite
apart
from
the
script
and
it's
bytecode
cache
.
The
OS
caches
such
files
for
faster
access
.
If
you
ran
other
things
on
the
same
system
","
those
files
will
be
flushed
from
the
cache
and
need
to
be
loaded
again
.
The
same
applies
to
directory
listings
;
the
checks
for
where
to
find
the
modules
in
the
module
search
path
and
tests
for
bytecode
caches
all
are
sped
up
by
cached
directory
information
.
If
such
startup
times
affect
your
application
","
consider
creating
a
daemon
that
services
these
tasks
as
a
service
.
RPC
calls
(
using
sockets
or
localhost
network
connections
)
will
almost
always
beat
those
startup
costs
.
A
message
queue
could
provide
you
with
the
architecture
for
such
a
daemon
.
You
can
use
a
dictionary
:
Then
it's
just
a
matter
of
indexing
the
dictionary
:
You
can
transform
your
current
list
to
the
proposed
format
with
a
dictionary
comprehension
:
I
have
a
list
of
vertices
and
the
length
between
them
.
I
want
to
create
a
function
which
finds
the
distance
between
2
vertices
in
the
list
by
searching
through
the
list
.
the
list
will
look
something
like
:
Say
the
vertices
are
X
and
Y
could
I
index
the
list
using
X
","
Y
to
find
the
length
between
X
and
Y
?
Currently
I
am
doing
it
like
this
:
But
as
the
list
of
lengths
grows
this
could
take
time
.
Thanks
Suppose
I
have
a
Python
program
that
runs
slow
-
after
profiliing
and
I
have
identified
the
bottleneck
.
One
particular
function
from
a
3rd
party
module
I
imported
is
particularly
slow
.
For
this
particular
case
","
I
know
that
function
is
implemented
in
Python
(
Used
Eclipse
and
it's
easy
to
jump
to
the
function
definition
)
.
So
I
know
that
I
can
convert
that
function
into
Cython
as
a
speed-up
option
.
(
If
it
is
already
implemented
in
C
","
there
is
no
point
in
writing
it
in
Cython
...
)
.
If
I
don't
have
an
IDE
","
what
would
be
an
easy
option
to
determine
this
?
I
know
that
I
can
go
to
the
directory
where
the
module
is
installed
and
infer
that
it
is
in
C
if
the
module
is
in
.
so
.
But
is
there
any
alternative
?
Thanks
Check
whether
it
is
an
instance
of
types.FunctionType
:
Probably
you'd
be
safer
to
check
for
"isinstance(X, (types.FunctionType, types.LambdaType)"
.
C
functions
are
instances
of
builtin_function_or_method
:
You
can
access
this
type
as
types.BuiltinFunctionType
/
types.BuiltinMethodType
.
Alternatively
you
can
check
whether
the
function
has
a
__code__
attribute
.
Since
C
functions
do
not
have
bytecode
","
they
can't
have
__code__
.
Note
sometimes
what
seems
like
a
function
is
actually
a
class
","
e.g
.
enumerate
but
some
3rd
party
library
may
do
the
same
.
This
means
that
you
should
also
check
whether
a
class
is
implemented
in
C
or
not
.
This
one
is
harder
since
all
classes
are
instances
of
type
.
A
way
may
be
to
check
whether
the
class
has
a
__dict__
in
its
dir
","
and
if
it
doesn't
have
you
should
check
for
__slots__
.
Something
like
the
following
should
be
pretty
accurate
:
Example
usage
:
Well
","
you
wrote
your
function
with
a
required
argument
.
Therefore
","
you
have
to
pass
the
argument
.
You
can
write
it
so
the
argument
is
optional
by
specifying
the
value
that
will
be
used
when
nothing
is
passed
:
etc.
i'm
trying
define
a
function
that
return
a
list
when
i
specify
an
object
","
and
it
returns
a
list
of
all
the
objects
in
the
scene
with
*
_control
when
i
don't
specify
anything
.
.
that's
my
function
but
it
doesn't
work
....
i'm
working
with
maya
then
.
.
when
i
don't
specify
anything
it
returns
an
error
:
Error
:
line
1
:
TypeError
:
file
line
1
:
correct_value()
takes
exactly
1
argument
(
0
given
)
what's
wrong
?
?
To
handle
a
default
parameter
even
if
it
might
be
None
Here's
a
really
useful
pair
of
patterns
for
this
kind
of
stuff
:
If
you
use
both
of
these
tricks
consistently
","
you
can
transparently
handle
cases
where
your
maya
queries
have
returned
None
instead
of
a
list
As
an
aside
","
you
can
mimic
the
default
maya
behaviour
(
where
passing
no
arguments
uses
the
current
selection
)
like
this
:
If
you
want
a
parameter
to
be
optional
","
you
need
to
provide
a
default
value
:
regexes
are
strings
","
so
feel
free
to
use
your
favorite
string
formatting
construct
:
As
to
your
edited
question
","
I
can't
find
an
easy
way
to
do
that
with
regexps
","
how
about
the
following
?
Say
for
example
I
want
to
match
a
substring
if
it
contains
a
certain
number
of
some
character
.
However
","
I
don't
know
the
exact
amount
that
character
is
","
but
I
know
it
is
not
negative
.
How
would
I
write
this
regex
?
Is
this
possible
to
do
?
If
so
","
how
do
I
do
it
?
EDIT
:
Example
input
:
k
=
2
string
=
01010
Expected
output
:
""""
101
""""
","
""""
0101
""""
","
""""
1010
""""
","
""""
01010
""""
So
in
each
substring
","
it
contains
exactly
2
characters
'
1
'
Perhaps
the
weight
parameter
would
be
of
help
in
your
problem
.
Or
","
as
tcaswell
said
","
you
could
just
make
a
bar
plot
and
change
the
x-axis
.
Using
matplotlib
how
could
I
plot
a
histogram
with
given
data
in
python
Is
a
link
.
All
the
matplotlib
examples
with
hist()
generate
a
data
set
","
provide
the
data
set
to
the
hist
function
with
some
bins
(
possibly
non-uniformly
spaced
)
and
the
function
automatically
calculates
and
then
plots
the
histogram
.
I
already
have
histogram
data
and
I
simply
want
to
plot
it
","
how
can
I
do
that
?
!
For
example
","
I
have
the
bins
(
half
open
ranges
are
denoted
by
the
square
and
curved
bracket
notation
)
","
The
error
message
in
the
question
does
not
come
from
the
subprocess
.
It
was
generated
before
the
subprocess
execution
.
You
cannot
capture
that
error
using
stderr
option
.
Make
sure
there's
ding
program
in
the
path
.
Based
on
the
answer
provided
here
","
I
wanted
to
save
the
error
as
a
string
:
However
","
it
seems
that
redirecting
stderr
is
not
working
:
What
is
the
correct
way
to
capture
the
error
as
a
string
?
When
I
try
to
install
mongoengine
in
virtualenv
","
I've
got
problem
:
Would
be
nice
to
find
out
how
to
install
it
inside
virtualenv
.
The
error
message
says
that
installer
tries
to
delete
file
/
var
/
www
/
msgapp
/
backend
/
lib
/
python2.7
/
site-packages
/
bson
/
json_util.py
but
it
fails
because
you
don't
have
permissions
.
There
are
two
possible
reasons
:
You
are
trying
to
install
mongoengine
as
a
different
user
from
the
one
who
own
the
virtualenv
.
Let's
assume
that
you
are
logged
in
as
user
holms
so
your
bash
prompt
looks
like
:
[
holms@localhost
~
]
$
Now
check
ownership
of
virtualenv
with
:
ls
-
la
/
var
/
www
/
msgapp
/
|
grep
backend
If
the
output
should
look
like
:
drwxr-xr-x
.
9
holms
holms
4096
05-06
15:49
backend
If
instead
of
holms
you
get
for
example
bruce
then
virtualenv
is
owned
by
this
user
and
you
should
perform
installation
as
bruce
:
sudo
su
-
bruce
source
/
var
/
www
/
msgapp
/
backend
/
bin
/
activate
pip
install
mongoengine
You
are
logged
in
as
the
correct
user
but
/
var
/
www
/
msgapp
/
backend
/
lib
/
python2.7
/
site-packages
/
bson
/
json_util.py
is
owned
by
someone
else
.
Again
I'm
assuming
that
your
username
is
holms
.
Check
permissions
:
ls
-
la
/
var
/
www
/
msgapp
/
backend
/
lib
/
python2.7
/
site-packages
/
bson
/
|
grep
json_util.py
If
you
see
that
someone
else
e.g
.
bruce
owns
this
specific
file
then
change
the
ownerschip
:
sudo
chown
holms:holms
/
var
/
www
/
msgapp
/
backend
/
lib
/
python2.7
/
site-packages
/
bson
/
Now
you
should
be
able
to
install
mongoengine
.
Your
issue
lies
in
your
post_save
hook
on
Product
.
When
you
save
a
Product
in
your
ProductAdmin
","
save_model()
is
called
and
then
save_related()
.
This
in
turn
calls
save_formset
with
a
formset
for
ProductMetricals
that
contains
a
key
to
a
ProductMetricals
that
is
now
deleted
.
It
is
now
invalid
(
since
you
deleted
it
with
your
save
on
Product
)
.
I
ran
into
a
similar
issue
with
deleting
on
an
inline
that
had
a
relationship
with
another
inline
in
the
admin
view
.
I
ended
up
setting
on_delete=models.SET_NULL
for
my
foreign
key
relationship
since
by
default
Django
cascade
deletes
.
Another
option
would
be
to
manually
override
the
formset
.
It
looks
similar
to
what
was
discussed
in
bug
#
11830
I
have
2
django
models
-
main
(
product
)
and
linked
details
(
PhysicalProperty
are
many-to-many
link
through
additional
model
ProductMetricals
)
.
In
main
model
Product
I
wrote
the
post_save
reciever
","
where
I
check
and
clean
the
data
in
details
.
If
I
try
from
IDLE
","
it
works
fine
.
But
if
I
change
and
save
main
Product
in
Admin
form
","
I've
got
the
exception
I've
try
to
debug
it
","
but
steel
have
no
idea
-
why
Admin
raise
exception
?
Here
is
a
code
models.py
admin.py
If
I
create
the
some
property
","
create
product
","
add
a
one
property
to
product
","
then
change
the
name
of
product
and
save
it
-
I'v
got
the
exception
Exception
come
(
I
think
)
from
ProductMetricals.objects.filter( product = instance )
.
delete()
For
my
website
I
am
trying
to
design
a
search
bar
that
will
filter
more
than
one
attributes
of
a
certain
class
","
e.g
.
name
and
telephone
.
However
","
I
don't
know
how
to
get
multiple
values
from
one
page
in
python
.
I
have
made
many
attempts
but
all
have
faied
.
I
am
posting
the
most
recent
(
which
also
doesn't
work
)
.
I
could
really
use
some
help
.
I
also
post
part
of
the
the
html
code
where
i
get
the
input
.
In
case
anyone
cares
i
find
the
following
mistake
on
my
code
:
the
input
should
be
inside
the
form
that
specifies
the
""""
action
""""
plus
i
fixed
my
code
so
i
am
posting
it
too
:
I
made
a
few
changes
to
your
code
.
First
I
used
get()
to
fetch
the
parameters
so
you
don't
need
to
use
the
if
in
request.GET
.
Next
I
made
use
of
chaining
QuerySet
filters
.
Finally
","
I
have
it
returning
both
q1
and
q2
to
the
template
.
It
doesn't
make
sense
to
only
return
a
query
when
you
can
enter
both
.
Their
is
an
option
introduced
in
0.13.1
(
might
have
been
0.13.0
)
","
where
you
can
set
dropna=False
on
a
put
/
append
to
avoid
dropping
an
all-NaN
row
.
This
is
done
for
efficiency
","
as
most
of
the
time
in
say
storing
a
Panel
","
you
have
lots
of
all-NaN
rows
","
but
no
reason
to
store
them
.
Otherwise
the
frequency
information
will
be
preserved
.
Note
that
if
you
are
appending
the
frequency
information
will
NOT
be
preserved
if
you
append
multiple
times
.
You
can
always
pd.infer_freq(an_index)
if
you
need
to
re-infer
the
freqency
(
if
possible
)
.
Normally
this
is
done
automatically
in
any
event
if
needed
.
I
have
a
Pandas
DataFrame
in
which
the
index
is
(
notice
the
Freq
:
H
)
-
There
are
multiple
columns
but
the
first
few
rows
(
and
others
scattered
throughout
)
have
all
NA
entries
.
If
I
write
this
to
a
HDF
file
thus
:
and
then
read
it
back
with
:
and
look
at
the
index
","
I
see
:
Notice
that
the
Freq
is
now
None
and
also
that
there
are
less
rows
and
a
later
start
date-time
.
The
first
row
is
now
the
first
row
of
the
original
DataFrame
that
contains
at
least
one
non-NA
column
value
.
Firstly
","
is
this
expected
behaviour
due
to
limitations
of
the
HDF5
format
and
how
DataFrames
are
stored
","
or
a
bug
?
Is
there
a
clean
way
to
avoid
this
happening
","
or
do
I
just
need
to
'
fix
'
up
the
index
after
load
.
Not
sure
what
the
best
way
to
do
that
is
either
.
i
found
there
are
several
posts
here
about
unloading
a
dll
using
ctypes
","
and
i
followed
exactly
the
way
said
to
be
working
from
ctypes
import
*
however
","
i
am
on
python
64
bit
and
my
dll
is
also
compiled
for
x64
","
and
i
got
an
error
from
the
last
line
above
saying
:
and
i
checked
the
handle
to
be
a
long
int
(
int64
)
of
'
8791681138688
'
","
so
does
that
mean
windll.kernel32
only
deals
with
int32
handle
?
Google
search
shows
kernal32
is
also
for
64bit
windows
.
how
should
i
deal
with
this
then
?
FreeLibrary
takes
a
handle
","
defined
as
a
C
void
*
pointer
.
Refer
to
Windows
Data
Types
.
Set
this
in
the
function
pointer's
argtypes
:
The
default
conversion
of
a
Python
int
or
long
(
renamed
int
in
Python
3
)
is
to
a
C
long
","
which
is
subsequently
cast
to
a
C
int
.
Microsoft
uses
a
32-bit
long
even
on
64-bit
Windows
","
which
is
why
the
conversion
raises
OverflowError
.
On
platforms
that
have
a
64-bit
long
(
i.e.
pretty
much
every
other
64-bit
OS
)
","
passing
a
pointer
as
a
Python
integer
without
defining
the
function's
argtypes
may
actually
segfault
the
process
.
The
initial
conversion
to
long
works
fine
because
it's
the
same
size
as
a
pointer
.
However
","
subsequently
casting
to
a
32-bit
C
int
may
silently
truncate
the
value
.
This
program
is
written
to
solve
Project
Euler
Problem
27
:
Project
Euler
Problem
27
Here's
the
code
:
But
when
I
run
this
code
","
the
code
stops
at
a=-63
without
giving
any
output
.
Can
anybody
suggest
why
that
happens
?
I
think
you
could
have
thought
about
the
problem
a
little
bit
more
.
Also
","
your
code
looks
bit
sluggish
.
Maybe
it
would
help
if
you
coded
some
functions
instead
of
doing
everything
in
one
go
.
I
see
a
lot
of
code
repetition
in
there
Anyway
","
here's
my
two
cents
:
The
program
is
probably
stuck
in
the
while
True
:
cycle
infinitely
","
as
you
only
break
out
of
the
cycle
when
number
<
=
0
.
I
haven't
read
your
code
with
too
much
attention
","
but
do
you
intend
to
break
out
of
the
while
True
:
loop
with
the
break
statements
that
are
inside
the
while
div*div<=number
:
cycle
?
That
is
not
possible
.
You
would
most
probably
need
to
have
a
variable
to
detect
if
you
exit
your
loop
prematurely
and
then
break
from
the
outer
loop
.
Otherwise
","
maybe
the
else
clause
of
python
loops
may
help
somehow
(
see
Control
Flow
in
Python
)
.
Something
like
this
?
Edit
:
Not
what
the
poster
wanted
after
all
.
I
don't
think
it
is
possible
to
do
what
you
want
with
your
table
structure
","
without
using
more
than
one
query
.
So
","
I
have
this
forum
I
am
building
.
I'm
having
trouble
writing
an
elegant
select
query
to
get
all
the
posts
for
a
certain
topic
from
the
db
.
Here's
the
schema
:
Table
posts
have
the
following
structure
:
Now
","
there's
also
a
table
called
attachments
","
where
post
attachments
are
referenced
.
I
want
to
be
able
to
select
the
attachments
of
a
particular
post
with
just
one
query
.
Here's
the
attachments
table
structure
:
This
is
not
what
my
tables
look
like
really
.
I
just
want
a
basic
idea
how
to
do
it
.
I
can't
even
think
of
the
least
of
solutions
.
Now
the
attachment
table
has
a
foreign
key
that
references
the
post
table
.
But
","
a
post
could
possibly
have
many
attachments
.
So
how
do
I
select
a
post
and
all
its
attachment
in
just
one
query
?
Maybe
subqueries
could
work
?
onliner
trivial
recursive
lambda
:
Let's
say
I
have
the
lists
[
"[1,2]"
","
"[3,4]"
]
and
[
"[5,6]"
","
"[7,8]"
]
I
expect
[
"[6, 8]"
","
"[10, 12]"
]
as
the
result
.
I'm
trying
to
sum
up
numbers
according
to
their
indexes
.
Another
easy
job
for
ndarrays
:
I'm
completely
stumped
dealing
with
something
quite
simple
.
The
following
lines
are
a
part
of
a
much
much
larger
program
.
Thing
and
stuff
are
two
objects
on
a
grid
","
and
I
need
to
find
the
angle
between
them
","
in
relation
to
the
y-axis
.
math.atan
takes
in
a
floating
point
number
","
which
I
why
I
need
to
type
cast
the
distance
between
the
objects
.
The
.
position
calls
all
return
integers
.
I
get
different
results
while
running
my
program
","
for
each
line
.
The
first
two
should
return
exactly
the
same
result
","
a
float
.
What
I
don't
understand
","
is
how
I
can
possibly
get
different
results
depending
on
whether
I
run
line
1
or
line
2
:
/
This
is
a
part
of
a
simulation
","
To
avoid
all
confusion
","
use
No
division
included
","
thus
no
integer-to-float
conversion
necessary
.
This
will
give
","
in
difference
to
the
atan
function
","
the
correct
angle
if
the
angle
is
in
the
II
.
or
III
.
quadrant
.
The
atan
function
gives
the
result
mod
180
°
","
the
atan2
function
mod
360
°
.
Depending
on
the
application
","
the
one
or
the
other
could
be
the
intended
result
.
Difficult
to
answer
at
the
rate
you
are
editing
but
:
the
bit
inside
the
atan
is
equivalent
to
something
/
(
a-b
)
the
bit
inside
the
atan
is
equivalent
to
(
something
/
a)-b
I
tried
to
parse
the
table
from
https://www.neb.com/tools-and-resources/usage-guidelines/nebuffer-performance-chart-with-restriction-enzymes
with
Pythons
library
lxml
but
if
I
try
it
with
some
snippets
of
code
from
similar
extracting
versions
(
How
to
extract
tables
from
websites
in
Python
)
I
run
into
problems
with
<
a>-tags
and
images
which
are
displayed
in
this
table
.
.
In
the
end
I
want
a
text
file
with
the
following
columns
of
this
restriction
enzyme
table
from
NEB
without
any
formatting
","
just
plain
text
:
Enzyme
|
Sequence
|
NEBuffer
|
%
Activity
in
NEBuffer
|
Heat
Inac
.
|
Incu
.
Temp
.
I
wanted
to
try
to
extract
each
td
of
a
row
by
its
own
and
combine
the
information
in
a
list
entry
:
But
it
mixes
up
everything
in
just
one
entry
and
I
do
not
know
how
to
deal
with
those
special
characters
like
'
u
and
\
u2122
The
first
lines
of
the
output
:
and
I
think
I
do
not
have
coded
that
columns
like
the
images
in
column
2
are
skipped
:
/
I
hope
my
question
is
detailed
enough
so
you
are
able
to
understand
what
I
am
trying
to
do
.
First
of
all
the
\
u2122
is
only
the
ASCII-friendly
representation
of
the
™
unicode
character
.
If
you
print()
the
string
","
you'll
see
that
character
instead
of
that
.
So
no
worries
!
then
","
your
code
does
not
work
for
me
:
is
returning
a
list
","
which
makes
it
impossible
to
do
:
so
I
don't
get
how
you're
getting
a
result
.
And
even
if
it
was
working
","
there's
something
you
don't
get
with
XPath
","
it's
that
/
/
makes
the
search
start
at
the
root
of
the
document
","
which
why
you're
getting
every
content
of
a
a
tag
within
a
td
tag
","
not
the
one
inside
the
tr
you're
in
.
On
the
contrary
","
if
you
use
a
relative
xpath
","
the
following
would
work
:
But
thing
is
that
doing
so
is
too
generic
and
you
won't
be
able
to
keep
element
in
the
order
of
interest
.
And
sadly
","
there's
no
automatic
way
to
get
that
interesting
stuff
out
of
it
.
Then
You
need
to
take
the
HTML
","
and
see
that
you
want
the
alt
of
the
image
in
that
td
","
that
you
want
to
take
the
content
of
the
span
in
that
other
one
:
The
following
is
getting
the
values
of
interest
from
the
document
you
linked
:
which
gets
all
the
fields
.
In
the
end
","
to
make
it
easily
reuseable
","
here's
what
I'd
do
:
and
for
the
first
enzime
","
here's
the
result
:
HTH
So
","
I
had
to
add
http.php
after
api
/
(
/
api
/
http.php/tickets.json
)
and
now
I
can
create
tickets
.
Check
http://tmib.net/using-osticket-1812-api.
The
sample
used
has
this
info
in
the
comments
.
The
two
really
important
parts
are
on
lines
18
and
19
.
Besides
changing
lines
18
and
19
you
need
to
make
sure
that
you
have
the
correct
IP
address
.
You
can
check
that
by
going
to
your
site
here
:
http://your.domain.tld/support/scp/logs.php
Then
look
to
see
if
you
get
the
error
API
Error
(
401
)
.
If
you
do
then
look
at
the
IP
address
and
create
a
new
API
key
for
that
IP
address
.
The
problem
I
was
facing
was
I
typed
in
my
IPv4
address
","
but
the
server
was
getting
my
IPv6
address
.
I'm
trying
to
create
a
ticket
in
osticket
through
its
REST
API
(
https://github.com/osTicket/osTicket-1.7/blob/develop/setup/doc/api/tickets.md
)
The
problem
is
/
api
/
tickets.json
returns
404
.
I
have
it
installed
in
a
server
on
osticket
folder
(
something
like
http://my.net.work.ip/osticket/api/tickets.json
-
404
)
Also
","
I've
tried
to
CURL
the
server
(
logged
in
through
ssh
)
","
created
an
APIKey
for
the
server's
IP
address
and
had
the
same
response
.
What
am
I
missing
here
?
Thank
you
The
piece
that
seems
to
be
missing
from
the
docs
is
how
to
pass
the
API
key
.
Through
a
bit
of
testing
and
the
script
mentioned
above
","
I
found
the
X-API-Key
header
.
That
means
you
can
create
tickets
without
using
a
script
","
you
can
use
curl
as
easily
as
:
This
will
only
work
if
it
is
run
from
the
IP
address
you
specified
when
you
created
the
API
key
.
How
to
add
another
column
to
Pandas
'
DataFrame
with
percentage
?
The
dict
can
change
on
size
.
If
indeed
percentage
of
10
is
what
you
want
","
the
simplest
way
is
to
adjust
your
intake
of
the
data
slightly
:
For
real
percentages
","
instead
:
First
","
make
the
keys
of
your
dictionary
the
index
of
you
dataframe
:
Then
","
compute
the
percentage
and
assign
to
a
new
column
.
This
gives
you
:
This
piece
of
code
seems
to
set
a
rather
high
priority
on
the
path
searched
through
when
loading
DLLs
.
You
can
put
it
at
the
very
beginning
of
your
entry
point
into
your
programme
.
I
had
a
similar
issue
and
it
works
for
me
:
)
Windows
7
64-bit
-
Python
2.6
32-bit
-
Pymunk
4.0.0
Ok
","
Thanks
to
Using
Pymunk
with
Pyinstaller
.
It
took
me
a
long
time
but
I
now
understand
how
to
throw
anything
I
want
into
an
exe
with
Pyinstaller
.
However
","
a
particular
dll-that
is
there-still
fails
to
load-chipmunk.dll
.
Heres
my
.
spec
file
for
Pyinstaller
.
This
all
packages
no
problem
.
The
image
loads
fine
as
long
as
I
have
the
dll
next
to
the
exe
so
I
dont
error
.
I
confirmed
the
dll
was
in
by
comparing
before
and
after
versions
of
including
the
dll
.
160
kb
difference
.
Then
I
used
this
to
check
if
the
dll
was
in
the
current
path
when
launched
under
Pyinstallers
exe
environment
.
I
get
an
exact
1
dlls
on
output
but
pymunk
complains
it
couldn't
find
it
.
Its
in
the
_MEIPASS
PATH
dir
so
how
come
pymunk
can't
find
it
?
The
dll
is
in
the
root
so
no
searching
should
be
required
.
How
can
I
get
pymunk
to
search
the
right
location
?
I
think
it
has
to
do
with
how
pymunk
tries
to
find
the
path
of
chipmunk.dll
when
frozen
.
Apparently
special
code
is
required
when
it
all
packs
into
one
file
.
Can
you
replace
your
libload.py
file
with
this
one
and
try
again
:
https://gist.github.com/viblo/44ccd6af88d9f050403b
(
At
the
moment
I
cannot
try
this
myself
","
therefor
the
gist
.
If
it
works
Ill
commit
it
to
the
real
pymunk
repo
)
Thanks
to
unutbu
and
Denis
.
The
whole
solution
is
:
You
can
use
the
psycopg2.extensions.AsIs
to
pass
in
parameters
for
database
name
etc.
As
said
here
:
http://initd.org/psycopg/docs/usage.html#the-problem-with-the-query-parameters
Warning
Never
","
never
","
NEVER
use
Python
string
concatenation
(
+
)
or
string
parameters
interpolation
(
%
)
to
pass
variables
to
a
SQL
query
string
.
Not
even
at
gunpoint
.
Your
code
would
look
like
:
The
conn.autocommit=True
is
there
because
you
can't
create
a
database
inside
a
transaction
.
EDIT
:
Note
that
you
will
still
be
vulnerable
to
SQL
injection
when
using
AsIs
","
but
you
can
at
least
execute
queries
in
the
usual
way
(
and
CREATE
DATABASE
cannot
be
executed
from
multi-command
string
","
so
it
would
still
fail
for
new_dbname
=
my_db;DROP
DATABASE
another_db
;
-
-
)
In
the
first
query
","
your
variables
are
identifiers
","
whereas
in
the
second
they're
literal
strings
.
You
shouldn't
be
putting
parenthesis
with
the
identifiers
","
and
they
shouldn't
be
wrapped
in
single
quotes
unless
you
further
wrap
them
with
double
quotes
.
For
instance
","
this
would
create
a
table
called
'
foo
'
rather
than
foo
:
And
these
would
all
spit
out
errors
:
If
you
really
want
to
use
parameters
for
identifiers
","
you'll
need
to
use
a
dynamic
SQL
statement
.
In
this
case
","
you'll
also
need
to
worry
about
quoting
the
identifiers
using
PG
if
applicable
","
e.g
.
:
In
the
above
","
the
outer
string
is
delimited
by
$$
","
and
the
inner
strings
are
delimited
by
'
.
After
parameters
get
replaced
","
it'll
look
like
:
And
then
:
And
finally
:
This
particular
statement
of
yours
","
however
","
cannot
be
part
of
a
transaction
so
cannot
go
in
a
do
statement
.
As
such
","
you'll
need
to
resort
to
doing
the
string
manipulation
in
Python
directly
.
What
quote_ident()
does
is
","
in
essence
","
wrap
in
double
quotes
after
doubling
all
double
quotes
in
the
input
.
For
instance
:
I'm
not
familiar
enough
with
Python
to
write
that
for
you
","
but
that's
basically
the
subroutine
you'll
need
to
make
it
work
.
Then
pass
the
final
SQL
string
—
with
the
parameters
in
it
replaced
—
to
Postgres
directly
.
I
am
trying
to
execute
this
query
with
psycopg2
in
Python3
:
And
I
am
getting
this
error
message
:
syntax
error
at
or
near
""""
'
my_new_database
'
""""
LINE
1
:
CREATE
DATABASE
(
'
my_new_database
'
)
WITH
TEMPLATE
'
original
...
On
the
other
hand
","
this
query
works
without
any
problem
:
Why
the
second
one
has
not
syntax
errors
and
the
first
one
yes
?
I
tried
to
remove
parenthesis
in
the
first
one
and
then
the
problem
is
the
quotes
...
Anyone
knows
what
is
happening
?
Table
and
database
names
can
not
be
parametrized
.
As
far
as
I
know
","
only
column
values
can
be
parametrized
.
Therefore
","
you'll
need
to
string
formatting
:
Be
sure
that
new_dbname
","
original_dbname
and
owner
do
not
come
from
user
input
","
since
string
formatting
makes
this
code
vulnerable
to
sql
injection
.
You
can
use
any
queryset
you
want
.
I
need
to
make
fields
with
autocomplete
for
my
custom
forms
.
It's
just
a
drop-down
menu
with
matched
cities
","
countries
etc.
I
also
use
django-cities
for
that
.
So
","
I
subclassed
AutocompleteModelBase
.
Here
","
I
can
declare
search_fields
attribute
with
list
of
fields
from
model
","
on
which
the
search
will
be
applied
.
But
what
if
I
want
to
search
not
by
the
fields
","
but
by
","
for
example
","
info
from
related
managers
?
In
django-cities
","
there
is
special
related
manager
for
so-called
alternative
names
.
It
returns
queryset
with
names
for
some
geo
object
in
foreign
languages
.
Something
like
this
:
Country.objects.get(code='US')
.
alt_names.filter(language='de')
So
","
I
need
to
search
by
the
results
of
such
operations
.
Not
by
the
fields
of
the
model
","
but
by
translated
names
","
which
don't
belong
to
actually
fields
.
Thanks
.
Explanation
:
first
I
am
taking
the
all
the
words
in
colA
and
making
a
regular
expression
;
similarly
with
col2
.
Now
with
that
regular
expression
I
am
searching
the
input
file
and
printing
the
result
'
\
b
'
is
word
boundary
.
If
you're
searching
for
a
word
'
cat
'
but
it
may
find
'
catch
'
","
'
\
b
'
is
useful
so
to
find
only
word
'
cat
'
.
I
want
to
extract
certain
information
from
a
large
file
using
python
.
I
have
3
input
files
.
The
first
input
file
(
input_file
)
is
the
data
file
","
which
is
a
3-column
tab-separated
file
that
looks
like
this
:
The
second
input
file
(
colA_file
)
is
a
1-column
list
","
which
looks
like
this
:
The
third
input
file
(
colB_file
)
is
also
a
1-column
list
(
idential
to
colA_file
with
different
information
)
","
which
looks
like
this
:
I
want
to
extract
information
from
the
input
file
that
is
found
in
both
colA
and
colB
.
With
the
example
data
that
I
have
provided
","
this
would
mean
filtering
all
of
the
information
except
for
the
following
lines
:
I
have
written
the
following
code
in
Python
to
solve
this
task
:
However
","
it
only
outputs
1-line
","
as
if
it
is
not
iterating
over
the
list
inputs
provided
.
It
also
seems
that
my
lists
are
being
appended
upon
each
other
","
starting
with
one
item
and
incrementing
itself
with
each
appended
item
.
Thus
","
I
have
also
tried
to
reference
the
lists
as
follows
:
with
the
same
result
as
above
.
I
would
use
pandas
or
numpy
if
you
don't
mind
the
dependency
.
With
a
pandas.DataFrame
you
can
then
perform
isin
checks
on
its
columns
.
Otherwise
I'd
recommend
using
sets
since
regex
should
be
much
slower
.
Something
like
this
:
I'm
trying
to
create
a
n
by
n
array
of
objects
with
NumPy
and
here
are
my
problems
:
Let's
assume
the
dimension
of
the
array
are
set
to
m=n=3
.
To
each
element
of
I
would
like
to
assign
an
instance
of
my
class
Vector
","
which
is
a
n-dimensional
vector
type
.
In
my
first
attempt
I
tried
to
assign
a
2-dim
.
vector
in
the
following
way
:
Instead
of
an
array
of
Vector
instances
Python
returns
:
The
above
is
an
array
of
shape
(
3
","
3
","
2
)
","
not
of
(
2
","
2
)
.
Numpy
converted
the
Vectorinstances
to
a
third
array
dimension
.
This
is
not
what
I
wanted
","
so
I
tried
another
thing
:
I
first
defined
an
empty
object
array
and
then
assigned
the
instances
to
it
:
The
funny
thing
about
it
is
that
I
have
to
cast
the
array
to
a
list
.
Then
it
works
pretty
well
and
returns
exactly
what
I
need
:
It
is
then
possible
to
cast
the
list
back
to
an
array
.
So
","
the
array
can(!)
hold
objects
in
the
way
I
want
it
to
.
But
there
seems
to
be
no
straightforward
way
to
assign
the
instances
.
And
","
isn't
there
a
more
pythonic
to
do
it
?
I
really
want
to
get
rid
of
this
nested
loop
.
I
also
tried
out
np.nditer
:
Instead
of
an
array
","
this
one
returns
an
error
:
ValueError
:
assignment
to
0-d
array
which
I
do
not
understand
.
Does
anybody
have
a
good
solution
concerning
the
nested
loop
?
An
explanation
of
the
value
error
would
also
be
welcome
.
Your
loop
should
work
if
you
don't
cast
to
a
list
","
and
if
you
do
the
assignment
like
:
In
your
first
attempt
","
np.array
is
just
doing
its
job
.
It
sees
a
nested
collection
of
data
that
can
be
converted
to
a
3-D
array
","
so
it
does
so
.
To
get
a
2-D
array
of
objects
","
you
can
change
this
:
to
this
:
Here's
a
concrete
example
:
I
don't
have
your
Vector
class
","
so
I'll
just
use
numpy
arrays
as
the
objects
that
I
want
in
the
array
of
objects
.
Here's
the
data
that
I
want
to
put
into
an
array
of
objects
.
It
is
a
nested
list
of
numpy
arrays
","
each
with
two
elements
:
You
could
change
"np.array([10*j + k, 10*j + k + 1])"
to
"Vector(10*j + k, 10*j + k + 1)"
to
try
this
with
your
Vector
class
.
a
is
the
array
of
objects
that
I'm
creating
:
Assign
the
vectors
to
a
like
this
:
Problem
is
Recursively
calling
the
directory
and
sub
directory
and
pass
it
as
a
string
.
In
line
input_file_list
=
sys.argv
[
1
:
]
I
can
pass
the
argument
as
a
single
directory
but
what
I
want
to
pass
all
the
directory
and
sub
directory
Any
suggestion
will
be
greatly
appreciate
.
Thanks
.
Try
to
use
this
function
:
I
don't
think
it's
possible
with
xlwt
only
.
Use
xlrd
(
Sheet.nrows
)
or
xlutils
.
Example
from
xlutils
docs
:
How
can
I
get
amount
of
rows
in
Excel
file
via
xlwt
?
Is
it
unable
and
I
should
open
this
file
for
read
via
xlrd
just
for
get
this
info
?
I
know
this
topic
is
a
bit
old
but
it's
the
first
google
result
when
lookink
for
this
issue
.
I'm
using
python
3.6
with
xlwt
1.2.0
and
to
get
worksheet
length
I
use
this
:
In
a
function
:
This
should
get
you
started
.
This
is
a
copy
of
my
Json
file
.
How
can
I
loop
through
the
json
and
make
a
list
/
or
print
a
particular
key
(
example
:
""""
hteamName
""""
)
","
which
is
similar
in
different
arrays
but
with
different
values
.
From
a
previous
question
","
I
got
some
codes
that
could
iterate
through
a
nested
dictionary
","
but
it
only
finds
unique
keys
with
unique
values
.
Try
this
simpler
version
:
It
should
get
you
started
with
the
rest
of
your
work
.
I
installed
cache-machine
for
my
Django
1.6
project
as
described
here
:
http://cache-machine.readthedocs.org/en/latest/
Now
in
django's
admin
","
I
cannot
see
new
entries
anymore
.
Cache
invalidation
does
not
work
.
I
can
see
that
the
entry
is
beeing
generated
in
the
DB
","
but
it
does
not
show
up
in
admin's
list-view
.
What
am
I
doing
wrong
?
Thx
in
advance
!
django-cache-machine
caches
count
queries
apart
from
regular
model
queries
.
You
have
different
methods
described
in
the
documentation
to
workaround
this
:
http://cache-machine.readthedocs.org/en/latest/#count-queries
The
tricky
part
is
to
know
that
new
entries
don't
appear
because
of
this
","
but
I
hope
your
problem
is
solved
with
this
.
It
seems
you
can
use
pywin32
library
for
Python
.
I
rewrote
the
code
","
but
not
sure
about
the
value
of
TextFileTextQualifier
.
You
can
see
its
value
(
and
other
variables
)
by
running
your
VB
script
.
Perhaps
it
is
easier
to
see
if
I
rewrite
without
the
With
And
End
With
statements
.
With
is
there
so
that
VBA
doesn't
have
to
check
that
the
object
(
whose
method
is
being
called
)
is
Nothing
.
I
have
the
following
VBA
script
:
Can
someone
help
me
with
translating
with
pywin32
?
And
first
of
all
","
what
should
I
do
with
vb
statement
:
WITH
END
WITH
?
How
i
can
translate
it
?
I'm
calling
an
iron
python
script
from
c
#
using
This
works
fine
and
using
print
outs
i've
proven
that
the
code
runs
correctly
.
However
inside
the
iron
python
script
is
a
call
to
another
python
script
(
i
have
tried
it
with
just
(
ipython
""""
GenerateKitImages.py
""""
)
THE
ISSUE
:
When
i
run
the
first
iron
python
script
using
Ipy.exe
both
run
perfectly
fine
and
do
as
expected
.
HOWEVER
when
i
run
the
first
iron
python
script
using
the
c
#
script
engine
it
only
runs
the
first
script
and
the
second
script
is
never
called
.
i'm
lost
on
this
one
.
I
can
confirm
it
is
nothing
to
do
with
:
permissions
","
dependencies
.
a
gold
medal
and
pizza
for
the
man
who
can
fix
this
problem
.
First
","
the
os.system
call
should
use
a
raw
(
r
""""
""""
)
string
:
Otherwise
the
backslashes
will
cause
issues
.
However
","
since
you
said
it
works
from
ipy.exe
","
my
guess
is
that
ipython
is
on
your
PATH
in
the
first
case
and
not
the
second
case
.
You
can
set
the
PATH
environment
variable
from
C
#
","
include
the
complete
path
to
ipython
in
the
os.system
call
","
or
ensure
that
the
PATH
is
set
before
calling
the
C
#
program
","
by
changing
it
globally
.
if
you
are
using
Anaconda3
","
please
do
:
update
your
C:\Anaconda3\Lib\site-packages\notebook\static\custom\custom.css
update
your
C:\Anaconda3\Lib\site-packages\notebook\static\custom\custom.js
","
and
we
add
a
shortcut
ctrl
+
for
toggle
the
header
If
you
have
a
recent
IPython
","
like
v3.0.0
or
higher
","
and
are
seeing
only
sporadic
success
with
this
method
","
you'll
need
to
hook
into
the
RequireJS
dependency
loader
","
and
put
the
following
in
your
common.js
:
common.js
is
loaded
at
the
bottom
of
the
page
","
so
there's
no
need
to
wait
for
the
DOM
ready
event
","
i.e.
","
$(function
(
)
{
...
}
)
.
For
further
discussion
see
my
answer
at
Turn
off
auto-closing
parentheses
in
ipython
and
its
comments
.
If
it
doesn't
already
exist
","
create
a
file
named
custom.js
in
/
Users
/
YOURUSERNAME
/
.
ipython
/
profile_default
/
static
/
custom
/
(
You
may
have
to
run
ipython
profile
create
","
if
you
have
never
run
this
command
.
)
In
custom.js
","
put
the
following
lines
of
JavaScript
If
you
would
like
to
also
hide
the
toolbar
by
default
","
use
these
lines
of
JavaScript
instead
I
want
to
save
some
space
for
my
14
inch
screen
.
What
should
I
write
in
e.g
.
ipython_notebook_config.py
to
trigger
this
?
I
believe
the
try-catch
mentioned
in
a
similar
post
here
on
SO
could
be
adapted
to
cover
it
.
If
you
wrap
the
pool.map
call
in
the
try-catch
and
then
call
terminate
and
join
I
think
that
would
do
it
.
[Edit]
Some
experimentation
suggests
something
along
these
lines
works
well
:
I
simplified
your
example
somewhat
to
avoid
having
to
load
numpy
on
my
machine
to
test
but
the
critical
part
is
the
two
try-except
calls
which
handle
the
CTRL+C
key
presses
.
Jon's
solution
is
probably
better
","
but
here
it
is
using
a
signal
handler
.
I
tried
it
in
a
VBox
VM
which
was
extremely
slow
","
but
worked
.
I
hope
it
will
help
.
When
I
run
a
python
script
that
uses
multiprocessing
I
find
it
hard
to
get
it
to
stop
cleanly
when
it
receives
Ctrl-C
.
Ctrl-C
has
to
be
pressed
multiple
times
and
all
sorts
of
error
messages
appear
on
the
screen
.
How
can
you
make
a
python
script
that
uses
multiprocessing
and
quits
cleanly
when
it
receives
a
Ctrl-C
?
Take
this
script
for
example
SHA-1
is
a
simple
cryptographic
hash
function
","
why
don't
you
just
convert
bellow
url
Image
:
u'http
:
/
/
www.market1
/
ScaledImages
/
180x180x2
/
Global-BlaBand_Bistro_kastikkeet.jpg
'
in
sha-1
and
then
fine
the
image
in
directory
with
that
SHA-1
?
other
then
that
you
can
simply
override
the
image
storage
mechanism
","
and
can
store
images
with
their
url
rather
then
SHA-1
i
used
dynamic
django
scraper
to
scrape
items
from
some
site
.
I
downloaded
image
and
are
saved
in
desired
location
but
i
cannot
access
them
from
django
template
.
The
images
are
saved
in
SHA1
hash
.
Is
there
a
way
to
access
them
from
django
template
?
In
my
models
the
images
are
saved
as
","
but
when
i
see
the
folder
where
i
have
saved
them
","
the
filename
is
in
the
form
of
SHA1
Hash
.
In
my
django
template
i
did
But
still
","
I
can't
access
the
image
.
However
if
i
do
i.e
refer
to
SHA1
hash
i
can
access
them
.
Is
there
a
way
to
access
images
without
using
hard-coded
code
?
Thanks
Lets
assume
the
following
structure
of
directories
for
a
project
where
root
and
moduleOne
are
directories
Content
of
helloworld.py
:
Content
of
moduleOne
/
printfile
My
issue
:
From
moduleOne
/
the
execution
of
printfile
is
ok
","
but
from
root
/
","
if
i
run
helloworld.py
the
following
error
happens
:
How
to
solve
this
in
python
?
[Edited]
I
solved
(
more
or
less
)
this
issue
with
a
""""
workaround
""""
","
but
stil
have
a
problem
:
My
solution
:
In
moduleOne
/
printfile
But
....
lets
say
i
have
a
new
directory
","
from
the
root
","
called
etc
","
then
the
new
structure
is
:
And
now
i
need
to
acess
etc
/
f2.txt
from
moduleOne
/
printfile
.
how
?
You
need
more
abstraction
.
Don't
hard-code
the
file
path
in
printfile.py
Don't
access
a
global
in
the
printf
function
.
Do
accept
a
file
handle
as
a
parameter
to
the
printf
function
:
In
a
script
that
does
actually
need
to
know
the
path
of
f.txt
(
I
guess
helloworld.py
in
your
case
)
","
put
it
there
","
open
it
","
and
pass
it
to
printf
:
Better
yet
","
get
the
file
path
from
the
command
line
EDIT
:
You
said
on
your
Google
+
cross-post
:
full
path
is
problem
","
the
program
will
run
on
differents
environments
.
If
you're
trying
to
distribute
your
program
to
other
users
and
machines
","
you
should
look
into
making
a
distribution
package
(
see
side
note
3
below
)
","
and
using
package_data
to
include
your
configuration
file
","
and
pkgutil
or
pkg_resources
to
access
the
configuration
file
.
See
How
do
I
use
data
in
package_data
from
source
code
?
Some
side-notes
:
Represent
directories
as
the
directory
name
with
a
trailing
slash
","
Ã
la
the
conventions
of
the
tree
command
:
/
instead
of
<
root
>
","
moduleOne
/
instead
of
<
moduleOne
>
You're
conflating
""""
module
""""
with
""""
package
""""
.
I
suggest
you
rename
moduleOne
/
to
packageOne
/
.
A
directory
with
an
__init__.py
file
constitutes
a
package
.
A
file
ending
in
a
.
py
extension
is
a
module
.
Modules
can
be
part
of
packages
by
physically
existing
inside
a
directory
with
an
__init__.py
file
.
Packages
can
be
part
of
other
packages
by
being
a
physical
subdirectory
of
a
parent
directory
with
an
__init__.py
file
.
Unfortunately
","
the
term
""""
package
""""
is
overloaded
in
Python
and
also
can
mean
a
collection
of
Python
code
for
distribution
and
installation
.
See
the
Python
Packaging
Guide
glossary
.
do
I
need
to
gut
what
I
have
to
start
passing
signals
?
yes
you
do
.
You
can
always
have
your
""""
other
thread
""""
be
QObject
based
and
have
a
QObject
from
QApplication's
children
based
object
as
parent
","
and
call
the
moveToThread()
method
but
it's
better
to
have
the
main
thread
handle
GUI
","
and
other
threads
send
signals
to
the
main
thread
to
have
it
updated
.
Take
the
opportunity
of
moving
from
GTK
to
QT
","
to
actually
redesign
correctly
your
application
","
with
real
decoupling
between
the
main
components
of
your
software
.
I
have
almost
completed
converting
a
pyGTK
application
to
pyQT4
.
One
of
the
last
aspects
I
am
struggling
with
is
generating
a
dialog
from
a
separate
thread
.
I
have
a
main
GUI
and
a
background
thread
.
On
the
offchance
of
thread-specific
issues
","
the
thread
generates
a
dialog
.
For
pyGTK
what
I
do
is
:
Called
as
:
"update_gui(gui_error,'help')"
Is
there
an
equivelent
in
pyQT4
or
do
I
need
to
gut
what
I
have
to
start
passing
signals
?
config.ini
.
py
This
works
fine
when
I
tested
it
locally
","
but
I
pushed
it
to
Heroku
and
got
a
ConfigParser.NoSectionError
when
I
check
the
Heroku
logs
.
Both
my
.
py
file
and
config.ini
file
are
in
the
same
directory
.
Googling
this
issue
shows
that
it
might
be
fixable
my
explicitly
stating
the
file
path
;
how
do
I
do
this
when
app
is
on
Heroku
?
Are
there
any
other
fixes
I
can
try
?
Try
:
dirname
:
returns
the
directory
of
a
file
.
__file__
:
refers
to
the
script
file
name
I
ran
into
a
similar
problem
.
Here
is
the
code
I
used
to
get
past
the
issue
.
As
user966588
stated
","
the
directory's
timestamp
is
updating
as
the
directory
changes
.
In
the
post
I
linked
","
I
held
onto
any
directory
metadata
updates
until
after
my
directory
was
fully-populated
in
order
for
the
timestamp
change
to
stay
.
Base
to
the
directory
permission
is
this
question
on
SO
.
The
directory
only
changes
its
timestamp
when
the
directory
itself
changes
for
ex
:
when
you
create
a
new
file
in
it
.
So
to
update
the
timestamp
of
folder
you
can
create
a
temp
file
and
then
delete
it
.
There
should
be
a
better
way
but
till
you
find
it
you
can
manage
using
this
.
I
am
taking
zip
file
as
input
which
contains
multiple
files
and
folders
","
I
am
extracting
it
and
then
I
want
to
change
the
last
modified
time
of
each
content
in
zip
to
some
new
date
and
time
set
by
user
.
I
am
using
os.utime()
to
change
the
date
and
time
","
but
changes
get
reflected
only
to
the
files
and
not
to
the
folders
inside
zip
.
I
am
using
Python
2.7
as
platform
This
is
more
of
a
workaround
but
you
can
use
qplot
for
quick
","
shorthand
plots
using
series
.
I
run
into
this
problem
frequently
in
Python's
ggplot
when
working
with
multiple
stock
prices
and
economic
timeseries
.
The
key
to
remember
with
ggplot
is
that
data
is
best
organized
in
long
format
to
avoid
any
issues
.
I
use
a
quick
two
step
process
as
a
workaround
.
First
let's
grab
some
stock
data
:
First
understand
that
ggplot
needs
the
datetime
index
to
be
a
column
in
the
pandas
dataframe
in
order
to
plot
correctly
when
switching
from
wide
to
long
format
.
I
wrote
a
function
to
address
this
particular
point
.
It
simply
creates
a
'
Date
'
column
of
type=datetime
from
the
pandas
series
index
.
From
there
run
the
function
on
the
df
.
Use
the
result
as
the
object
in
pandas
pd.melt
using
the
'
Date
'
as
the
id_vars
.
The
returned
df
is
now
ready
to
be
plotted
using
the
standard
ggplot()
format
.
From
here
you
can
now
plot
your
data
however
you
want
.
A
common
plot
I
use
is
the
following
:
I'm
experimenting
with
pandas
and
non-matplotlib
plotting
.
Good
suggestions
are
here
.
This
question
regards
yhat's
ggplot
and
I
am
running
into
two
issues
.
Plotting
a
series
in
pandas
is
easy
.
I
don't
see
how
to
do
this
in
the
ggplot
docs
.
Instead
I
end
up
creating
a
dataframe
:
I
would
expect
ggplot
-
-
a
project
that
has
""""
tight
integration
with
pandas
""""
-
-
to
have
a
way
to
plot
a
simple
series
.
Second
issue
is
I
can't
get
stat_smooth()
to
display
when
the
x
axis
is
time
of
day
.
Seems
like
it
could
be
related
to
this
post
","
but
I
don't
have
the
rep
to
post
there
.
My
code
is
:
Any
help
regarding
non-matplotlib
plotting
would
be
appreciated
.
Thanks
!
(
I'm
using
ggplot
0.5.8
)
If
your
problems
allows
transitive
relations
","
i.e.
x
is
in
the
group
as
long
as
it's
at
most
7
away
from
any
element
in
the
group
","
then
this
seems
to
me
like
a
graph
theory
problem
.
To
be
more
specific
","
you
need
to
find
all
connected
components
.
The
problem
itself
is
pretty
easy
to
solve
with
a
recursive
algorithms
.
You
would
first
create
a
dictionary
in
which
every
key
would
be
one
of
the
elements
and
every
value
would
be
a
list
of
elements
which
are
at
most
7
apart
from
that
element
.
For
your
example
","
you
would
have
something
like
this
:
Which
would
give
you
something
like
this
Now
you
need
to
write
a
recursive
function
which
will
take
as
input
an
element
and
a
list
","
and
it
will
keep
adding
elements
in
a
list
as
long
as
it
can
find
an
element
which
is
at
most
7
apart
from
the
current
element
.
Somewhere
in
the
code
you
also
need
to
remember
which
elements
you
have
already
visited
to
make
sure
that
you
don't
get
into
an
infinite
loop
.
You
need
to
call
that
function
for
every
element
in
your
list
.
This
code
should
give
the
following
output
for
your
input
:
This
is
algorithm
problem
","
try
this
:
Output
:
I
have
a
list
of
numbers
in
Python
.
It
looks
like
this
:
I
want
to
keep
all
the
numbers
which
are
within
+
or
-
7
of
each
other
and
discard
the
rest
.
Is
there
a
simple
way
to
do
this
in
Python
?
I
have
read
about
the
list
filter
method
but
am
not
sure
how
to
get
what
I
want
working
with
it
.
I
am
new
to
Python
.
Update
Output
would
ideally
be
"[84, 86, 87, 89, 90]"
and
another
list
"[997, 999, 1000, 1002]"
.
I
want
to
retrieve
the
sequences
and
not
the
outliers
.
Hope
this
makes
sense
.
For
any
problem
like
this
my
first
port
of
call
is
the
Python
itertools
module
.
The
pairwise
function
from
that
link
that
I
use
in
the
code
is
available
in
the
more-itertools
module
.
I
haven't
tested
for
edge
cases
","
so
it's
buyer
beware
:
)
Just
use
the
built-in
filter
and
expand
the
results
in
your
function
call
:
Keep
in
mind
if
your
result
is
more
than
two
items
","
the
above
will
fail
.
actually
you
can
do
what
you
want
","
by
using
a
list
:
the
destructuring
array
syntax
is
there
for
you
:
if
you
want
to
get
the
result
as
a
list
you
can
:
if
you
want
to
get
only
one
element
:
if
you
like
to
read
","
here
are
a
few
resources
:
https://docs.python.org/release/1.5.1p1/tut/tuples.html
http://robert-lujo.com/post/40871820711/python-destructuring
In
python
","
returning
multiple
variables
corresponds
to
returning
any
iterable
","
so
there's
no
practical
difference
between
returning
a
list
or
""""
multiple
variables
""""
:
The
only
difference
between
these
two
functions
is
that
f
returns
a
tuple
","
and
g
returns
a
list
-
which
is
indifferent
","
if
you
use
multiple
assignment
on
the
return
value
.
I
have
a
complex
variable
like
the
one
below
...
I
have
written
a
function
to
extract
the
variables
from
the
desired
test
","
see
below
...
This
returns
something
like
the
following
...
I
would
like
to
be
able
to
return
the
variables
as
individual
variables
and
not
elements
of
a
list
.
How
would
I
go
about
doing
this
?
How
do
I
return
variable
number
of
variables
?
(
sort
of
like
*
args
but
for
return
instead
of
arguments
)
I
am
trying
to
write
an
Go_nogo
Task
in
Psychopy
.
Even
though
I
managed
to
write
a
script
which
is
working
","
there
are
still
a
few
things
that
make
troubles
.
First
","
I
present
pictures
of
emotional
stimuli
(
im_n
","
neural
;
im_a
","
emotional
)
and
people
should
only
answer
by
pressing
""""
space
""""
if
neutral
emotional
pictures
are
presented
.
When
I
run
the
code
below
everything
works
well
until
I
don't
press
any
key
or
the
wrong
key
.
So
my
question
is
","
how
do
I
have
to
write
the
code
that
I
don't
get
kicked
out
of
the
run
while
not
answering
...
?
Thanks
everybody
!
I
get
the
error
message
:
My
guess
is
that
you
get
this
error
message
at
the
if
resp
[0]
=
=
correctResp
:
line
:
Is
that
true
?
If
yes
","
it
is
simply
because
event.getKeys()
returns
an
empty
list
[]
if
no
responses
were
collected
.
And
doing
[]
[0]
will
give
you
the
above
error
because
there's
no
first
element
(
index
zero
)
just
like
"[1,2,3,4]"
[1000]
will
give
you
the
same
error
.
Note
that
even
if
you
press
a
lot
of
keys
and
none
of
them
are
in
the
keyList
","
getKeys
will
return
an
empty
list
because
it
ignores
everything
but
the
contents
of
the
keyList
(
unless
you
set
keyList=None
","
in
which
case
it
accepts
all
keys
)
.
There's
a
few
simple
ways
out
of
this
.
Firstly
","
you
can
simply
check
whether
resp
is
empty
and
give
a
""""
fail
""""
score
if
it
is
and
only
check
for
correctness
if
it
is
not
.
A
more
general
solution
","
which
would
work
with
many
response
keys
and
scoring
criteria
","
is
to
do
if
correctResp
in
resp
and
then
score
as
a
success
if
yes
.
This
comparison
will
work
with
an
empty
list
as
well
","
in
which
case
it
always
returns
False
as
empty
lists
per
definition
can't
contain
anything
.
But
in
your
particular
case
","
you
only
have
one
response
option
so
it
is
even
simpler
!
Since
you've
""""
filtered
""""
responses
sing
the
keyList
","
you
KNOW
that
if
resp
is
[]
","
the
subject
answered
""""
no-go
""""
and
conversely
if
resp
is
not
[]
","
he
/
she
answered
""""
go
""""
.
So
:
Actually
","
I
suspect
that
you
also
want
to
give
feedback
in
trials
without
neutral
images
.
In
that
case
","
define
correct
as
bool(resp)
is
(
im
in
im_n
)
:
I
was
using
gdata
module
to
access
","
upload
","
download
files
from
google
doc
.
I
have
the
oauth
key
and
secret
with
me
.
Now
I
want
to
switch
to
google
drive
api
.
Learning
and
studying
a
bit
on
google
drive
api
","
it
looks
like
a
bit
different
in
the
authentication
.
I
also
have
downloaded
pydrive
module
so
as
I
can
start
things
up
.
But
I
am
not
able
to
authorize
my
server
side
python
code
to
authorize
/
authenticate
the
user
using
my
oauth
keys
and
access
my
drive
.
Do
any
one
has
any
spare
know
how
on
how
I
can
use
pydrive
to
access
my
drive
with
my
previous
auth
keys
.
I
just
need
a
simple
way
to
authenticate
.
For
using
the
gdata
module
we
use
either
of
these
credentials
-
1
>
username
&
password
or
2
>
consumer
oauth
key
and
secret
key
.
Since
you
are
trying
to
use
oauth
credentials
","
I
think
you
want
a
Domain
Wide
Delegated
Access
for
Google
Drive
","
which
will
help
you
to
achieve
uploading
/
downloading
files
into
any
user's
google
drive
through
out
the
domain
.
For
this
you
need
to
generate
a
new
Client
ID
of
a
Service
Account
Type
from
Developer's
Console
*
.
p12
file
will
get
downloaded
.
Note
the
path
where
you
save
it
.
Also
note
the
email
address
of
your
Service
account
.
These
will
be
use
while
coding
.
Below
is
the
python
code
where
u
have
to
carefully
edit
-
PATH
TO
SERIVE
ACCOUNT
PRIVATE
KEY
","
something@developer.gserviceaccount.com
","
EMAIL_ID@YOURDOMAIN.COM
in
order
to
run
it
properly
and
test
it
.
Hope
this
will
help
!
Resource
-
Google
Drive
API
This
seems
to
be
working
Which
calls
a
method
after
the
timeout
:
from
threading
import Timer
import time
def timeout():
def main():
try
:
except
:
how
to
time-out
a
statement
/
block
of
code
in
python
?
I
tried
using
the
below
looking
at
a
post
but
it
couldn't
identify
the
signal
:
code
:
Your
code
is
working
perfectly
well
for
me
:
Though
","
are
you
using
a
Unix
system
to
try
this
?
Whether
this
is
a
Linux
","
a
BSD
or
a
Mac
?
The
signal
module
may
not
be
present
at
all
on
other
OS
","
as
this
is
using
a
feature
very
specific
to
unix
systems
","
even
though
windows
does
have
a
very
basic
support
of
POSIX
processes
.
edit
:
Well
","
as
I'm
saying
you're
in
a
very
particular
case
","
which
is
neither
Unix
or
windows
or
even
symbian
;
but
you're
running
it
in
Jython
","
which
does
not
have
access
to
the
OS
features
","
whereas
the
lack
of
signal
module
.
You
should
have
tagged
your
question
with
Jython
","
to
help
us
help
you
more
efficiently
!
Then
you'd
better
use
a
thread
timer
which
should
be
well
implemented
in
Jython
.
As
a
very
basic
example
:
As
a
more
complex
example
","
where
I'm
using
a
simple
lock
semaphore
to
handle
the
signaling
between
the
main
thread
and
the
timer
thread
:
numpy.random.shuffle
is
designed
to
work
in-place
meaning
that
it
should
return
None
and
instead
modify
your
array
.
I
have
a
numpy
array
.
How
do
I
shuffle
the
rows
?
I
tried
numpy.random.shuffle()
but
it
was
returning
None
probably
due
to
absence
of
commas
I
am
not
happy
with
the
static
way
TestCase.tearDown()
works
.
Sometimes
I
create
stuff
during
an
unittest
which
I
want
to
be
cleaned
up
later
","
and
I
don't
know
in
advance
what
needs
to
be
cleaned
up
later
.
Is
it
possible
to
add
closures
to
a
tear_down_list
which
get
executed
in
tearDown()
?
Like
this
?
Say
I
have
a
class
and
I
want
to
make
a
lot
of
similar
""""
property-style
""""
attributes
.
For
example
:
Obviously
there
is
a
lot
of
repetition
here
and
it'd
be
nice
to
factor
that
out
.
One
way
would
be
to
make
a
function
that
returns
a
property
:
Alternatively
I
could
write
a
descriptor
class
:
Which
is
best
/
fastest
/
most
Pythonic
?
Taking
the
descriptor
idea
and
mixing
it
with
a
metaclass
","
as
I
mentioned
in
the
comments
.
I
am
using
the
QR
code
(
Django
)
library
and
when
I
do
{
%
qrcode_from_text
""""
{
%
url
'
foo
'
%
}
""""
""""
l
""""
%
}
","
the
inner
tag
gives
error
.
Is
there
any
way
to
do
such
nested
django
template
tags
?
Thanks
.
No
","
but
you
don't
need
to
.
The
url
tag
has
a
syntax
to
save
the
result
to
a
variable
:
and
you
can
then
use
that
in
your
custom
tag
:
I
have
a
simple
scrapy
project
.
I
am
crawling
www.anthropologie.com
for
sale
items
.
I
want
to
use
the
standard
imagePipeline
to
download
the
sale
items
I
am
scraping
.
I
have
enabled
the
standard
imagePipeline
in
the
settings.py
file
as
well
as
a
valid
directory
for
the
IMAGE_STORE
.
I
have
the
required
fields
image_url
and
images
.
My
spider
is
scrapping
the
correct
url
for
the
images
as
validated
by
checking
the
urls
in
the
browser
.
When
I
run
the
spider
it
is
indicated
that
the
pipeline
is
enabled
.
However
I
see
no
indication
that
images
are
being
downloaded
and
I
find
no
images
in
the
correct
directory
.
below
are
examples
of
my
code
:
settings.py
:
items.py
anthro_spider.py
:
there
are
no
errors
reported
so
I
can't
figure
out
what
I'm
missing
.
In
the
items.py
file
","
it
should
be
image_urls
=
Field()
.
not
image_url
=
Field()
I
need
to
do
a
curve
fitting
with
constraints
and
weights
.
reading
around
","
mostly
here
","
I
created
a
function
From
what
I
gather
","
this
should
work
.
The
results
","
however
","
are
very
bad
.
Is
this
method
right
?
Did
I
miss
anything
?
I
should
mention
that
I
tested
the
function
itself
using
xmgrace
and
it
works
fine
.
Seeing
how
you
are
calling
this
routine
might
help
","
as
would
definitions
of
self.k0
and
self.hq_func
.
What
is
the
purpose
of
as
opposed
to
?
In
general
","
there
are
many
potential
cases
for
which
a
change
in
the
value
for
p
[0]
will
change
the
residual
.
That
makes
it
hard
to
determine
the
derivatives
","
which
is
likely
to
cause
unstable
results
.
Returning
Inf
would
definitely
cause
trouble
.
If
the
idea
is
to
put
upper
and
/
or
lower
bounds
on
the
value
a
parameter
may
take
","
you
may
find
the
lmfit
package
(
http://lmfit.github.io/lmfit-py/
)
helpful
.
You
should
investigate
Java
serialization
To
serialize
an
object
means
to
convert
its
state
to
a
byte
stream
so
that
the
byte
stream
can
be
reverted
back
into
a
copy
of
the
object
.
A
Java
object
is
serializable
if
its
class
or
any
of
its
superclasses
implements
either
the
java.io.Serializable
interface
or
its
subinterface
","
java.io.Externalizable
.
Deserialization
is
the
process
of
converting
the
serialized
form
of
an
object
back
into
a
copy
of
the
object
.
Java
can
serialize
natively
to
a
byte
stream
.
That
bytestream
can
be
a
file
","
a
network
stream
etc.
Additionally
a
number
of
3rd
party
libraries
exist
to
serialise
to
XML
","
JSON
etc.
I
made
this
method
in
Python
:
Now
I
would
like
to
migrate
it
to
a
Java
program
","
but
it
looks
like
there's
no
Pickle
module
in
Java
.
Does
anyone
know
a
way
of
doing
this
in
Java
?
(
a
","
b
and
c
are
Integers
)
use
master
/
0.14
(
coming
shortly
)
","
vastly
speeds
up
count
","
see
here
here's
the
benchmark
on
master
/
0.14
vs
0.13.1
:
Setup
v0.13.1
v0.14.0
I'm
trying
to
count
duplicate
rows
in
pandas
DataFrame
.
I
read
data
from
a
csv
file
that
looks
like
this
The
desired
output
for
the
example
input
above
is
The
code
I
have
now
is
:
This
works
as
expected
","
but
takes
a
long
time
.
I
profiled
it
and
it
looks
like
almost
all
of
the
time
is
spent
grouping
and
counting
.
Any
ideas
how
this
piece
of
code
could
be
sped
up
?
With
minimal
modification
to
existing
code
:
(
range(len)
)
would
iterate
over
indices
","
not
actual
chars
","
and
yield
instead
of
return
makes
it
a
generator
thus
allowing
to
return
all
values
)
Use
it
as
so
:
can
you
please
tell
me
how
to
get
every
character
in
a
string
repeated
using
the
for
loop
in
python?my
progress
is
as
far
as
:
and
this
returns
only
the
first
letter
of
the
string
repeated
I
believe
you
want
to
print
each
character
of
the
input
string
twice
","
in
order
.
The
issue
with
your
attempt
is
your
use
of
return
.
You
only
want
to
return
the
final
string
","
not
just
a
single
repetition
of
one
character
from
inside
the
loop
.
So
you
want
your
return
statement
to
be
somehow
outside
the
for-loop
.
Since
this
sounds
like
a
homework
problem
","
try
to
figure
out
how
to
do
that
above
before
continuing
below
.
Still
stuck
?
Print
each
character
in
a
string
twice
","
in
order
:
Without
using
a
for-loop
:
It
looks
like
you
tried
to
search
for
'
Lang
'
key
through
all
dictionaries
in
your
PDF
file
.
To
check
the
language
information
from
a
PDF
file
","
you
need
to
check
'
Lang
'
entry
in
the
catalog
.
However
the
existance
of
this
entry
depends
on
PDF
creating
software
which
is
used
to
create
the
PDF
file
and
most
PDF
files
do
not
have
this
entry
.
I
do
not
understand
Python
code
but
I
belive
that
the
PDF
library
you
are
using
will
provide
you
the
access
to
the
trailer
(
dictionary
)
or
catalog
(
root
)
dictionray
.
If
you
have
an
access
to
the
trailer
dict
","
get
'
Root
'
value
from
the
dict
.
This
is
the
indirect
reference
to
Catalog
(
Root
)
dictionary
.
Then
resolve
this
reference
to
dict
to
get
the
catalog
dictionary
.
Taking
/
Lang
value
from
this
catalog
dict
will
give
you
the
attribute
.
Try
","
the
following
Please
note
that
I
am
not
a
Python
programmer
and
the
code
snippet
above
is
my
first
Python
code
(
I
am
not
sure
it
is
working
.
:
-
)
Please
refer
pypdf
reference
at
http://sourcecodebrowser.com/python-pypdf/1.10/classpy_pdf_1_1pdf_1_1_pdf_file_reader.html#a92be75503c895367083a846b3060e632
As
explained
in
the
PDF
specification
:
http://www.adobe.com/devnet/pdf/pdf_reference.html
There
is
a
""""
/
Lang
""""
key
in
the
Document
Catalog
.
In
my
version
of
the
PDF
specification
this
is
explained
in
section
7.7.2
.
This
language
key
defines
the
language
assumed
for
the
complete
document
","
with
the
exception
of
those
parts
that
are
marked
differently
.
So
","
two
caveats
:
1
)
This
""""
/
Lang
""""
key
is
optional
.
If
it's
not
there
the
PDF
specification
says
the
language
is
undefined
.
2
)
This
""""
/
Lang
""""
key
may
be
overwritten
by
other
elements
in
the
file
.
So
the
entire
document
may
be
English
","
but
specific
sentences
on
page
101
may
redefine
the
language
as
French
for
example
.
In
your
case
","
your
algorithm
should
first
try
to
find
the
overall
document
language
as
defined
above
.
If
that's
not
there
it's
up
to
you
what
to
do
.
You
could
search
the
complete
document
for
""""
/
Lang
""""
keys
to
see
if
you
find
any
other
","
but
if
you
find
multiple
","
you'll
have
to
decide
what
that
means
...
I
am
trying
to
extract
the
language
of
any
general
pdf
document
and
set
it
in
CMS
using
python
.
I
am
trying
to
extract
it
using
/
Lang
attribute
","
here
is
the
code
sample
:
But
when
I
am
seeing
the
LOGS
I
find
no
such
attribute
""""
/
Lang
""""
in
the
dictionary
.
Log
the
messages
using
\
n
to
separate
the
lines
","
as
you've
already
considered
","
then
write
a
custom
Formatter
subclass
which
splits
the
message
into
separate
lines
","
inserts
the
appropriate
whitespace
prefix
for
the
subsequent
lines
and
joins
them
back
together
again
.
I
have
been
using
the
logging
module
in
Python
for
a
while
.
My
doubt
is
not
so
much
a
technical
one
","
but
that
I
am
not
sure
of
the
best
practices
with
the
messages
.
The
issue
is
that
sometimes
an
event
happens
and
I
want
to
dump
information
about
it
in
the
log
","
but
a
lot
of
information
.
I
can
build
a
single
long
string
","
and
use
it
in
an
log.info()
command
.
That
is
alright
","
except
for
the
fact
that
the
result
is
hardly
human-readable
.
Introducing
the
character
'
\
n
'
in
the
string
indeed
breaks
the
message
into
separate
lines
in
the
log
file
","
but
left-justified
","
it
looks
ugly
because
in
the
beginning
of
the
logging
formatter
I
am
writing
the
date
/
time
","
user
","
etc.
On
the
other
hand
I
can
split
the
message
into
different
calls
to
log.info()
","
but
that
makes
it
look
as
if
they
were
different
events
.
EDIT
:
Additional
information
.
If
I
do
different
log
calls
","
it
looks
like
What
is
the
usual
/
best
practice
here
?
I
think
I
would
like
something
like
I
have
a
dict
object
as
:
I
want
this
device
dict
to
map
to
another
dict
","
namely
","
master_device
as
:
Where
all
the
fields
in
sub
document
having
key
data
","
are
mapped
from
device
dict
.
What
is
the
'
one-liner
'
way
to
achieve
this
?
P.S
.
:
I
don't
want
it
to
be
like
:
Or
some
other
""""
for-loop
""""
way
.
(
I
simply
dont
want
this
way
)
Thanks
in
advance
.
This
is
all
I
got
:
seems
convoluted
but
at
least
it's
a
one
liner
!
!
:
-
)
This
is
for
Python
3.3
.
More
info
on
ChainMap
here
Maybe
this
approach
?
I'm
currently
doing
a
""""
Spell
Checker
""""
problem
.
I
have
already
created
a
list
that
holds
the
dictionary
.
Now
I'm
trying
to
see
if
word
is
in
the
list
.
I'm
new
to
using
classes
with
Python
so
I
am
probably
missing
something
tiny
.
Recent
error
has
been
""""
TypeError
:
'
list
'
object
cannot
be
interpreted
as
an
integer
""""
Here
is
an
excerpt
of
code
.
This
is
how
you
would
iterate
over
a
list
:
and
this
is
a
simpler
way
to
do
what
I
think
you
are
trying
to
do
.
Note
that
it
doesn't
require
an
explicit
loop
:
range
is
a
built-in
function
that
creates
a
list
(
in
python
2
)
and
an
iterable
in
python
3
.
If
in
doubt
","
use
the
built-in
help
:
Python
2
range
Python
3
range
A
small
problem
with
the
Caeser
Cypher
program
I'm
writing
.
I'm
new
to
programming
.
So
","
when
the
offset
points
to
a
character
past
126
I
want
to
wrap
back
to
the
beginning
","
so
that
it
works
with
the
ASCII
character
set
.
I've
managed
to
do
that
while
encrypting
like
this
:
Which
works
fine
.
But
while
decrypting
","
it
doesn't
seem
to
work
.
I'm
still
getting
characters
that
are
outside
of
the
ASCII
character
set
.
E.g
:
Here
is
the
code
I've
written
:
If
the
subtracted
offset
results
in
a
number
less
than
32
","
we're
supposed
to
add
94
to
the
result
.
I
thought
that
is
exactly
what
I've
done
here
","
but
I
must
be
going
wrong
somewhere
.
It's
also
not
decrypting
strings
properly
.
I
can
encrypt
strings
","
but
when
I
try
to
decrypt
the
string
I
just
encrypted
","
it
returns
the
wrong
result
.
Any
help
would
be
greatly
appreciated
:
)
Thanks
in
advance
.
It
looks
like
your
offsetD
value
has
the
wrong
sign
(
or
you
need
to
be
subtracting
it
","
rather
than
adding
)
.
If
it
is
positive
93
","
you'll
get
the
output
you
show
in
your
example
run
.
Either
change
how
its
value
is
set
(
e.g
.
offsetD
=
-
"int(input(""Please enter offset value (1 to 94)"
:
""""
)
)
or
subtract
the
value
when
you
go
to
use
it
(
e.g
.
decryption
-
=
offsetD
)
.
I
have
wrote
a
program
in
Python
about
threading
and
listed
as
follow
.
However
","
it
seems
that
the
thread
does
not
run
.
When
I
run
the
program
","
it
shows
message
""""
Thread
Completed
""""
.
What
am
I
doing
wrong
?
change
while
loop
inside
run
method
to
this
:
Don't
forget
to
import time
module
at
the
beginning
of
the
script
.
With
this
","
your
thread
will
loop
for
10
sec
.
Start
thread
witht.start()
as
@Tim
Castelijns
suggested
.
See
virtualenv
documentation
-
https://virtualenv.pypa.io/en/latest/virtualenv.html
Under
""""
The
-
-
extra-search-dir
option
""""
Haven't
tried
it
but
in
the
docs
it
says
:
""""
This
option
allows
you
to
provide
your
own
versions
of
setuptools
and
/
or
pip
to
use
instead
of
the
embedded
versions
that
come
with
virtualenv
.
""""
I'm
trying
to
start
an
environment
using
virtualenv
but
i
can't
install
the
packages
i
need
because
pip
wont
download
them
.
According
to
what
I've
found
the
problem
was
that
the
version
of
pip
i
was
using
didn't
work
well
because
of
SSL
veryfying
reasons
so
I
downgraded
to
pip
1.2.1
and
pip
would
then
download
the
packages
with
no
problems
.
However
whenever
I
try
to
use
virtualenv
it
automatically
installs
the
newest
version
of
pip
which
gives
me
the
SSL
veryfying
errors
.
Is
there
any
way
to
make
virtualenv
install
an
older
version
of
pip
?
I'm
fairly
new
to
python
and
it's
the
first
time
ive
used
virtualenv
.
Also
I'm
using
the
terminal
on
OSX
Lion
Considering
I
have
millions
of
objects
with
3
__slots__
Is
it
more
memory
efficient
to
have
short
slot
names
like
x
vs
.
long
like
would_you_like_fries_with_that_cheeseburger
?
Or
are
the
names
allocated
only
once
per
class
(
opposed
to
once
per
instance
?
)
Names
for
slots
only
take
memory
per
class
","
not
per
instance
.
Slots
use
descriptors
that
map
directly
into
the
memory
reserved
for
an
instance
","
and
the
attribute
names
are
mapped
to
these
descriptors
on
the
class
.
Thus
","
the
length
of
the
names
has
no
influence
on
how
much
memory
each
instance
uses
for
the
slots
;
the
names
only
take
space
in
the
__dict__
attribute
on
the
class
(
mapping
name
to
descriptor
)
","
and
in
the
descriptor
object
itself
(
to
provide
a
string
representation
of
the
object
)
;
the
string
is
interned
even
.
You
can
check
how
the
custom
descriptors
are
given
their
state
in
the
C
source
code
for
type.__new__()
:
where
mp->offset
is
the
index
into
the
memory
for
the
instance
.
The
descriptor
used
is
a
PyMemberDescr_Type
object
","
whose
member_get
function
uses
the
(
very
generic
)
PyMember_GetOne()
function
;
with
the
offset
the
pointer
is
retrieved
from
the
instance
:
addr
is
the
memory
address
of
the
instance
.
The
rest
of
the
function
deals
with
various
types
of
members
;
the
slot
member
is
always
set
to
type
T_OBJECT
:
which
the
function
then
returns
.
Recently
","
I'm
working
on
a
gevent
demo
and
I
try
to
compare
the
efficiency
between
gevent
and
thread
.
Generally
speakingï¼Œthe
gevent
code
should
be
more
efficient
than
the
thread
code
.
But
when
I
use
time
command
to
profile
the
program
","
I
get
the
unusual
"result(my command is time python FILENAME.py 50 1000,the last two parameters means pool number or thread number,so I change the two number in the table below)"
.
The
result
shows
that
the
thread
is
more
efficient
than
the
gevent
code
","
so
I
want
to
know
why
this
happen
and
what's
wrong
with
my
program
?
Thanks
.
gevent
VS
thread
My
code
is
below(The main idea is use thread or gevent to send multi HTTP request)
:
*
*
*
*
*
*
This
is
the
thread
version
code
*
*
*
*
*
*
*
*
*
*
*
*
This
is
the
gevent
verison
code
*
*
*
*
*
*
Try
It
seems
that
by
default
gevent
does
not
patch
httplib
(
have
a
look
at
http://www.gevent.org/gevent.monkey.html
:
httplib=False
)
so
you
are
actually
doing
blocking
requests
and
you
lose
all
advantages
of
the
asynchronous
framework
.
Although
I'm
not
sure
whether
requests
uses
httplib.
If
that
doesn't
work
","
then
have
a
look
at
this
lib
:
https://github.com/kennethreitz/grequests
Re
:
httplib=False
You
are
already
using
requests
library
to
make
web
calls
.
It
has
gevent
flavour
called
grequests
:
https://github.com/kennethreitz/grequests
Overall
I
don't
immediately
see
much
reason
to
prefer
one
style
of
threading
to
the
other
","
if
your
pool
is
so
small
.
Of
course
real
threads
are
relatively
heavy
(
start
with
8MB
stack
)
","
but
you
have
to
take
that
into
proportion
to
the
size
of
your
job
.
My
take
","
try
both
(
done
)
","
verify
you
are
doing
both
right
(
to
do
)
and
let
numbers
do
the
talking
.
I
have
a
Mainwindow
and
from
there
I'm
calling
a
popup
.
The
popup
has
to
options
to
click
.
Now
","
how
can
I
""""
return
""""
the
clicked
option
to
my
mainwindow
?
I'm
importing
the
file
where
the
popup
is
to
my
main
:
pbarpre.py
:
Add
method
to
return
some
values
split()
takes
an
optional
maxsplit
argument
:
In
Python
3
:
In
Python
2
","
you
need
to
specify
the
maxsplit
argument
as
a
positional
argument
:
Read
the
docs
:
Syntax
for
splitting
is
:
"str.split([sep[, maxsplit]])"
'
sep
'
is
the
seperator
used
to
split
strings(by default it matches any white space character)
'
maxsplit
'
argument
can
be
used
to
limit
no
.
of
splits
as
mentioned
by
Tim
Here
if
you
are
using
'
\
t
'
in
between
your
columns
","
you
can
just
use
'
\
t
'
as
seperator
As
per
standard
practice
","
'
\
t
'
is
used
as
seperator
for
columns
so
that
splitting
won't
interfere
with
other
spaces
in
strings
.
And
moreover
there
won't
be
any
compatibility
issues
with
whatever
python
version
you
are
using
.
Hope
this
helps
:
)
may
be
u
can
try
using
"re.split('your patter, string)"
","
which
should
give
you
proper
list
based
on
your
regex
.
I
have
a
textfile
that
I
want
to
put
into
lists
.
The
textfile
looks
like
this
:
I
have
tried
splitting
the
list
with
this
code
:
Output
:
But
it
does
not
give
me
the
output
i
want
It
splits
on
the
spaces
between
the
Title
.
How
can
I
split
it
into
a
list
without
breaking
up
the
title
?
Expected
output
:
I
have
got
an
excel
file
","
that
was
created
by
some
rather
old
soft
.
This
file
couldn't
be
opened
in
OpenOffice(some encoding errors)
and
in
Excel
2010
at
first
it
could
only
be
opened
in
Protected
View
.
When
I
tried
to
open
it
by
xlrd
:
I
got
an
error
:
Same
thing
with
encoding
by
cp1252
","
utf-7
.
utf_16_le
","
that
was
adviced
in
similar
topic
returns
Without
encoding
I
got
additional
string
in
traceback
After
saving
file
in
Excel
2010
(
in
xlsx
)
format
this
problem
had
disappeared
-
file
can
be
opened
both
in
xlrd
and
OO
.
Is
there
any
way
to
open
such
file
by
xlrd
without
resaving
?
Upd
.
There
is
no
such
problem
for
python2.7
xlrd
.
However
I
still
don't
know
what's
wrong
with
python3.3
xlrd
.
The
problem
is
in
different
behaviour
between
python2
and
python3
:
To
fix
this
you
can
edit
xlrd
/
sheet.py
around
line
1543
:
Change
to
So
behaviour
will
be
like
in
python2
A
simple
way
of
doing
this
is
to
just
modify
the
if
statement
to
check
that
the
candidate
element
isn't
the
last
one
","
avoiding
the
need
for
a
exception
clause
","
and
keeping
the
code
short
.
The
other
answers
are
good
for
the
specific
case
where
you
can
avoid
raising
the
exception
in
the
first
place
.
The
more
general
case
where
the
exception
cannot
be
avoided
can
be
handled
a
lambda
function
as
follows
:
The
key
point
here
is
that
making
it
a
lambda
defers
evaluation
of
the
expression
that
might
raise
an
exception
until
inside
the
try
/
except
block
of
the
test()
function
where
it
can
be
caught
.
test()
returns
either
the
result
of
the
evaluation
","
or
","
if
an
exception
in
exception_list
is
raised
","
the
on_exception
value
.
This
comes
from
an
idea
in
the
rejected
PEP
463
.
lambda
to
the
Rescue
presents
the
same
idea
.
(
I
offered
the
same
answer
in
response
to
this
question
","
but
I'm
repeating
it
here
because
this
isn't
exactly
a
duplicate
question
.
)
Check
out
this(As Tom Ron suggested)
:
I
am
checking
consecutive
indices
of
a
list
and
I
want
to
execute
same
piece
of
code
in
case
if
the
consecutive
elements
are
not
equal
or
if
the
list
index
is
out
of
range
.
Here's
what
I'm
trying
Is
there
a
way
to
do
this
elegantly
without
having
to
repeat
the
same
code
in
the
except
block
somehow
?
I
am
trying
to
write
a
fab
file
which
will
do
the
auto
deployment
of
my
code
on
amazon
server
.
So
for
that
I
am
giving
the
SSH
connection
path
at
the
top
of
the
fabric
method
.
But
its
not
connecting
and
asking
for
password
.
I
have
the
.
pem
file
using
which
I
am
able
to
connect
successfully
from
my
Ubuntu
terminal
.
But
the
same
thing
is
not
working
when
I
am
trying
to
do
through
fabric
and
its
asking
for
password
.
Example
Code
snippet
of
my
fabric
file
.
Is
there
any
way
to
make
it
password
less
.
Thanks
Set
up
your
public
and
private
keys
SSH
authentication
(
the
id_rsa
and
id_rsa.pub
files
)
.
Check
here
if
you
have
any
problem
.
Then
","
put
the
id_rsa.pub
content
into
~
/
.
ssh
/
authorized_keys
of
your
remote
server
.
Lastly
","
configure
Fabric
to
use
the
key
env.key_filename
=
""""
~
/
.
ssh
/
id_rsa
""""
Question
:
instead
of
specifying
""""
names
""""
of
the
columns
using
Designinfo
(
which
basically
makes
my
code
less
re-usable
)
","
can
I
not
READ
the
names
given
by
this
DesignMatrix
so
that
I
can
feed
this
into
a
DataFrame
later
","
without
needing
to
know
pre-hand
what
the
""""
reference
level
/
control
group
""""
level
was
?
ie
.
When
I
do
"dmatrix(""C(carbs, Treatment(reference='lo')"
)
+
score
""""
","
dta
)
so
g
would
be
the
transformed
dataframe
I
can
do
logistic
modelling
on
without
needing
to
keep
a
note
of
(
or
hard-coding
thereof
)
of
the
column
names
&
their
reference
levels
.
I
think
the
information
you're
looking
for
is
in
design_info.column_names
:
and
so
I
want
to
install
scipy
in
python
2.6
in
linux
.
But
when
I
am
installing
scipy
0.14.1
it
throws
error.How
can
I
resolve
it.The
traceback
is
as
follows
.
You
can
use
virtualenv
as
a
tool
to
use
any
version
of
python
including
2.6
or
2.7
or
3.4
As
per
my
understanding
of
your
question
","
your
application
is
not
closing
when
you
click
on
the
close
button
(
The
red
button
with
X
on
the
right
top
corner
.
)
By
default
when
you
click
the
close
button
your
application
should
close
.
In
your
case
it
seems
to
me
that
you
have
bind
the
EVT_CLOSE
to
some
method
","
which
has
no
code
in
it
to
close
the
app
window
.
For
eg
.
consider
the
code
snippet
below
","
I
have
intentionally
bind
the
EVT_CLOSE
event
to
a
method
named
as
closeWindow()
.
This
method
does
nothing
that
is
why
I
have
the
pass
keyword
there
.
Now
if
you
execute
the
code
snippet
below
you
can
see
that
the
app
window
won't
close
.
Code
:
So
","
in
order
to
close
the
app
window
you
need
to
change
the
closeWindow()
.
For
eg
:
Following
code
snippet
will
use
the
Destroy()
close
the
app
window
when
you
click
on
the
close
button
.
I
hope
it
was
useful
.
I
developed
a
GUI
with
wxGlade
and
its
still
working
.
But
to
start
the
GUI
-
I
coded
a
script
with
some
choices
.
So
everything
works
but
when
I
push
the
red
button
with
the
""""
x
""""
to
close
the
window
-
the
application
doesnÂ´t
stop
.
I
made
a
method
","
called
by
a
separate
exit
button
","
which
calls
an
exit-function
in
my
script
.
But
normally
the
users
are
using
the
close
button
(
red
button
with
X
)
so
my
method
is
not
being
used
to
close
the
window
and
the
window
doesn't
get
closed
ultimately
.
This
is
the
exit-function
.
How
can
I
use
this
function
with
the
red
button
with
the
""""
x
""""
?
I
am
very
new
to
python
and
looking
for
a
way
to
simplify
the
following
:
Obviously
_total
and
_initial
are
predefined
.
Thanks
in
advance
for
any
help
.
You
can
use
two
dictionaries
:
A
similar
way
is
first
determining
not
changed
populations
:
Or
","
you
can
have
a
single
structure
:
You
could
organize
all
the
pairs
into
a
dictionary
and
cycle
though
all
the
elements
:
Another
way
(
2.7
)
using
an
ordered
dictionary
:
well
i
didnt
not
use
your
method
and
i
tried
this
it
seems
to
work
fine
","
however
there
is
a
samll
gap
between
the
2
instances
of
Space
","
i'm
trying
to
fix
it
know
.
Here
is
the
modified
code
:
So
this
is
a
class
to
store
your
sprite
in
.
I
don't
believe
you
need
2
","
because
you
can
have
a
list
of
all
of
your
instances
of
space
for
example
in
main
","
instead
of
having
...
I
would
recommend
doing
something
like
.
and
then
change
to
then
instead
of
do
finally
instead
of
do
I
hope
that
this
works
for
you
","
as
I
see
this
as
a
reasonable
way
to
store
your
instances
of
space
and
allows
you
to
more
easily
have
the
code
with
less
code
.
If
you
have
any
problems
implementing
this
","
please
just
leave
a
comment
and
I
will
help
you
fix
it
.
Cheers
!
i've
been
looking
into
game
development
in
python
and
i'm
very
new
to
it
","
i
tryed
to
copy
a
space
invaders
and
here
is
what
i
came
up
with
","
it
is
massively
inspired
from
tutorials
and
code
i
found
online
.
What
im
trying
to
do
know
is
to
get
the
background
moving
so
i
thought
it
would
be
a
good
idea
to
create
2
sprites
with
2
background
image
and
make
them
move
one
after
the
other
.
As
soon
as
one
is
out
of
the
screen
it
should
reappear
at
the
bottom
and
so
on
.
But
it
does
not
work
i
am
not
sure
what
is
going
wrong
.
Here
is
what
i
have
to
far
:
What
you're
trying
to
do
is
import an
external
module
","
which
is
not
supported
by
GAE
.
What
you
can
do
though
","
is
copy
web.py
into
your
app
directory
","
and
then
use
it
.
See
""""
How
to
include
third
party
Python
libraries
in
Google
App
Engine
""""
.
You
can
get
the
source
code
from
here
I'm
trying
to
get
web.py
app
running
on
local
Google
App
Engine
.
My
yaml
:
My
code.py
:
When
I
start
the
server
all
I
get
is
a
blank
page
.
So
what's
wrong
?
Edit
:
Edit
2
:
Error
from
console
:
If
you
want
to
preserve
the
length
","
then
you
want
to
use
a
stream
cipher
Note
that
both
parties
need
to
know
key
(
secret
)
and
initialization
vector
(
not
secret
","
but
must
be
unique
)
.
It
is
not
safe
to
reuse
same
key
without
a
different
IV
","
thus
you'd
need
to
communicate
new
IV
across
somehow
","
which
effectively
lengthens
your
encrypted
package
.
Furthermore
","
you
really
need
to
consider
replay
attacks
-
-
what
happens
if
someone
intercepts
your
message
and
then
sends
it
again
and
again
.
Here's
simple
example
for
RC4
","
using
pycrypto
","
no
IV
(
RC4
doesn't
support
it
)
:
From
the
post
here
:
Encrypt
string
in
Python
using
http://code.google.com/p/keyczar/
I
have
strings
about
500
chars
long
and
I'd
like
to
encrypt
them
so
that
resulting
string
has
the
same
length
(
or
just
slightly
different
)
.
I
need
two
way
enryption
and
decryption
using
another
""""
secret
""""
string
known
to
both
sides
.
It
doesn't
need
to
be
very
secure
","
I
prefer
faster
solutions
.
You
don't
need
the
db.Blob
for
ndb
-
just
use
self.profile_photo
=
image
.
Here's
a
very
quick
example
:
I
am
trying
to
upload
a
profile
picture
for
a
user
of
my
app
using
a
form.But
i
get
a
server
side
error
saying
AttributeError
:
'
module
'
object
has
no
attribute
'
Blob
'
I
followed
this
example
given
by
Google
where
they
have
something
like
In
my
own
app
","
i
created
a
method
in
my
Member
class
like
this
:
and
then
created
a
Handler
to
handle
this
:
I
modified
the
imports
of
my
app
by
adding
:
from
google.appengine.api
import images
(
is
this
all
i
need
?
)
The
glaring
issue
here
is
","
my
app
uses
*
ndb
*
while
the
tutorial
uses
*
db*.What
is
the
best
way
to
go
around
this
?
Hello
all
…
I
am
using
Python
to
try
to
simulate
login
my
work
email
","
web
base
.
Now
a
problem
lies
on
the
first
page
of
login
.
I
observe
the
login
process
through
IE9’s
F12
(
Developer
tool
)
and
found
there
are
in
total
3
cookies
generated
.
However
when
I
use
below
scripts
to
visit
the
first
page
of
login
","
there
are
only
2
cookies
found
.
How
could
this
be
?
How
can
I
have
all
the
cookies
retrieved
?
(
I’ve
cleared
all
the
cache
","
cookie
history
","
and
empty
the
fold
of
Temporary
Internet
Files
before
catching
)
Thanks
.
This
is
because
the
first
two
cokies
was
returned
from
server
read
about
this
:
the
third
one
was
generated
by
javascript
:
I'm
trying
to
plot
a
surface
over
several
points
it
should
look
like
a
deformed
sphere
.
I
used
the
scatter
function
and
plotted
the
points
","
but
the
surface
function
is
not
working
(
the
window
is
empty
)
.
To
plot
the
surface
","
I
think
i
need
a
mesh
function
.
I
try
ed
to
mash
x
","
y
","
z
but
it
was
not
working
.
How
i
can
generate
the
code
","
to
put
a
surface
over
my
points
?
.
Thanks
for
helping
me
.
I
have
the
points
xyz
stored
in
a
list
.
They
are
describing
a
deformed
sphere
and
i
have
to
plot
somthing
like
this
http://ej.iop.org/images/1367-2630/12/3/033037/Full/nj332389fig1.jpg
This
question
is
hard
to
answer
without
any
sample
code
of
what
you're
doing
","
you
might
want
to
edit
it
to
include
a
working
example
.
I
suppose
you
are
using
the
mplot3d
class
","
have
you
checked
the
examples
that
are
provided
online
here
","
here
","
and
here
?
These
to
me
look
like
what
you're
trying
to
produce
.
I
have
a
series
of
strings
which
are
identifiable
by
finding
a
substring
""""
p
""""
tag
followed
by
at
least
two
CAPITAL
letters
.
Input
:
I
want
to
change
the
""""
p
""""
tag
to
an
""""
i
""""
tag
if
it's
followed
by
those
two
capital
letters
(
so
not
the
last
one
","
'
Eric
'
)
.
Desired
output
:
I've
tried
this
using
regular
expressions
in
Python
:
But
the
output
uses
""""
i
""""
tag
+
JI
in
every
instance
","
rather
than
interating
through
to
use
SA
and
then
RO
in
entries
2
and
3
.
I
believe
the
problem
is
that
I
don't
understand
the
.
group()
method
properly
.
Can
anyone
advise
what
I've
done
wrong
?
Thank
you
.
Another
way
using
look-ahead
assertion
:
Your
inner
re.search
is
only
evaluted
once
","
and
the
result
is
passed
as
one
of
the
parameters
to
re.sub
.
This
can't
possible
capture
all
the
capital-letters-pairs
","
only
the
first
one
.
This
means
your
approach
cannot
work
","
not
merely
your
understanding
of
groups
.
Furthermore
","
using
groups
is
unnecessary
.
You
need
to
capture
the
capital
letters
using
parenthesis
","
and
reference
it
as
\
1
in
the
substitution
expression
:
\
1
here
means
:
replace
with
the
substring
matched
by
the
first
(
...
)
in
the
regular
expression
.
(
docs
)
Note
the
leading
r
in
front
of
the
substitution
string
","
to
make
it
raw
.
I'm
writing
a
Django
view
that
gets
the
latest
blog
posts
of
a
wordpress
system
.
I
tried
in
a
terminal
to
use
ETags
:
I'd
like
to
avoid
requesting
the
feed
for
every
user
of
the
web
app
.
maybe
etag
aren't
the
best
option
?
Once
the
first
user
sees
this
view
","
can
I
store
the
etag
and
use
it
for
all
the
other
users
?
is
there
a
thread
for
every
user
and
therefore
I
can't
share
the
value
of
a
variable
this
way
?
Etag
allows
to
mark
unique
status
of
a
web
resource
","
so
that
you
have
a
chance
to
ask
for
the
resource
expressing
latest
status
you
already
have
.
But
to
have
some
version
already
at
your
client
","
you
have
to
fetch
it
the
first
time
","
so
for
the
first
request
is
use
of
etag
irrelevant
.
See
HTTP
Etag
at
wikipedia
","
it
explains
it
all
.
Typical
scenario
is
:
fetch
your
page
the
first
time
and
read
value
of
Etag
header
for
future
use
next
time
you
ask
for
the
same
page
","
you
add
header
If-None-Match
with
value
of
Etag
from
your
last
fetch
.
Server
will
check
","
if
there
is
something
new
","
if
the
Etag
you
provide
and
Etag
at
current
version
of
resource
are
the
same
","
it
will
not
return
complete
page
back
","
but
rather
returh
HTTP
Status
code
304
Not
Modified
.
If
the
page
has
different
status
on
the
server
","
you
get
the
page
with
HTTP
Status
code
200
and
with
new
value
of
Etag
in
the
response
header
.
If
you
want
to
optimize
your
app
not
to
generate
initial
request
for
the
same
feed
by
each
user
","
you
shall
somehow
share
the
Etag
value
for
given
resource
globally
across
your
application
.
The
first
request
the
client
will
never
be
able
to
use
any
local
caches
","
so
at
the
first
request
ETag
isn't
necessary
.
Remember
that
ETag
needs
to
be
passed
into
the
conditional
request
headers
(
If-None-Match
","
If-Match
","
etc
)
","
the
semantic
of
non
conditional
requests
are
clear
.
If
your
feed
is
a
public
feed
","
then
an
intermediate
caching
proxy
are
also
allowed
to
return
an
ETagged
result
for
non
conditional
request
","
although
it
will
always
have
to
contact
origin
server
if
the
conditional
header
doesn't
match
.
I
am
new
in
django
","
and
I
am
creating
table
in
postgresql
.
I
want
to
perform
insert
","
update
and
delete
operation
using
django
.
I
want
creating
followng
code
.
Models.py
viwes.py
This
is
the
minimum
code
you
need
.
Then
you
can
add
fields
verification
","
or
whatever
you
need
:
You
need
to
create
an
instance
of
the
model
class
(
Publisher
in
this
case
)
","
instantiate
it
with
the
appropriate
values
(
name
and
address
)
and
then
call
save()
","
which
composes
the
appropriates
SQL
INSERT
statement
under
the
hood
.
I
recommend
you
read
the
model
docs
.
You
need
to
add
a
static
file
mapping
on
the
web
app
.
Look
for
the
""""
Static
files
""""
heading
on
the
web
app
tab
.
From
what
I
can
tell
of
your
setup
","
you'd
need
to
put
""""
/
s
/
""""
for
the
URL
and
""""
/
home
/
*
username
*
/
*
projectname
*
/
Static
/
""""
for
the
directory
.
The
problem
looks
similiar
to
the
problem
here
:
Pythonanywhere
","
how
to
use
static
files
?
url
?
","
but
I
cannot
comment
there
.
I've
started
learning
Django
and
when
everything
worked
on
localhost
that
on
PythonAnywhere
it
does
not
.
At
projectname
/
settings.py
I've
set
:
and
even
URL's
from
static
folders
in
apps
.
After
trying
to
run
every
file
*
.
js
","
*
.
css
and
images
were
coppied
to
the
projectname
/
Static
folder
.
But
...
none
of
them
were
recognized
after
launch
of
the
app
.
I've
set
used
tags
At
the
source
code
I
can
see
the
proper
link
to
css
file
:
And
everything
would
be
fine
","
but
the
""""
/
s
/
""""
isn't
recognized
by
django
and
it
tries
to
find
the
view
in
urls.py
.
After
opening
the
link
to
:
username.pythonanywhere.com
/
s
/
assets
/
js
/
seen.min.js
I've
got
standard
","
debug
404
page
with
the
path
of
urls.py
tries
.
How
to
solve
this
annoying
problem
?
You're
passing
your
initial
data
to
the
form
in
the
wrong
place
in
the
GET
block
:
it's
being
interpreted
as
the
data
.
You
should
do
this
:
Except
that
you
shouldn't
be
trying
to
pass
initial
data
at
all
with
a
ModelForm
","
you
should
pass
the
actual
instance
you
want
to
edit
:
There
are
a
couple
of
other
issues
with
your
code
.
Firstly
","
you
don't
need
to
query
User
via
request.user.username
:
request.user
is
already
the
relevant
User
object
.
Secondly
","
when
you
come
to
save
","
you
should
again
use
the
instance
argument
and
save
the
ModelForm
directly
","
rather
than
setting
values
individually
:
Finally
","
you
should
not
be
extending
User
with
your
UserProfile
model
","
but
you
should
extend
AbstractUser
and
set
the
AUTH_MODEL
setting
to
point
to
UserProfile
:
then
request.user
will
be
an
instance
of
your
custom
user
model
.
You
initialize
Your
arg
variable
in
case
when
request.method
is
different
than
'
POST
'
.
To
avoid
this
error
initialize
arg
variable
at
the
top
of
method
.
Generally
@daniel-roseman
explain
it
very
well
.
please
help
to
solve
the
problem
.
I
made
a
form
in
which
there
is
a
field
select
:
models.py
:
views.py
:
if
the
user
makes
a
selection
in
the
""""
select
""""
(
0
","
1
","
or
2
)
","
then
the
form
is
submitted
.
unless
the
user
makes
a
selection
in
the
""""
select
""""
(
None
)
","
the
shape
of
the
break
and
displays
the
following
error
message
:
I
need
that
in
any
case
","
the
form
did
not
break
Is
there
a
way
to
generate
sitemaps
on
the
fly
and
regularly
submit
them
to
Google
using
Pyramid
?
I've
seen
2
code
snippets
(
here
and
here
)
for
doing
this
in
Flask
","
but
they
don't
seem
applicable
to
Pyramid
.
Specifically
","
when
I
include
"config.add_route('sitemap', '/sitemap.xml')"
in
__init__.py
and
then
the
following
view
:
I
get
an
error
:
Changing
the
view
to
:
Gets
past
the
KeyError
from
before
","
but
gives
me
a
404
when
I
try
navigating
to
mysite.com
/
static
/
sitemap.xml
.
What's
going
on
?
EDIT
:
This
is
my
sitemap.jinja2
file
.
Have
a
look
here
when
I
look
into
source
there
is
method
get_routes_mapper
in
pyramid.config.Configurator
.
Maybe
it
helps
You
to
generate
sitemap
on
the
'
fly
'
:
)
Given
you
want
to
establish
http://example.com/sitemap.xml
as
your
sitemap
URL
do
that
.
Add
this
line
to
init.py
to
register
URL
pattern
http://example.com/sitemap.xml
as
route
sitemap
register
view
code
for
route
sitemap
and
render
response
with
your
custom
jinja2
template
sitemap.jinja2
.
The
file
extension
`
jinja2
'
will
trigger
usage
of
jinja2
renderer
.
This
will
fix
your
errors
resulting
from
trying
to
name
your
templates
like
URLs
.
But
that
mixed
up
renderer
conventions
shown
below
.
*
.
pt
triggers
Chameleon
renderer
*
.
jinja2
triggers
Jinja2
renderer
*
.
mako
triggers
Mako
renderer
*
.
xml
triggers
XML
renderer
(
that
raised
your
first
error
)
Now
it
is
still
up
to
you
to
create
XML
based
on
sitemaps
protocol
.
But
your
code
looks
promising
.
You
pass
your
resource
tree
to
the
XML
template
.
Every
resource
usually
has
access
to
their
properties
like
URL
or
last_changed
stuff
.
Don't
use
unichr()
;
it
produces
a
unicode
string
with
one
character
.
Don't
mix
Unicode
strings
and
byte
strings
(
binary
data
)
","
as
this'll
trigger
implicit
encoding
or
decoding
.
Here
an
implicit
decode
is
triggered
and
fails
.
Your
codepoints
are
limited
to
values
0-255
","
so
a
simple
chr()
will
do
:
I'm
trying
to
replace
hex
representation
(
#
.
.
)
with
its
representation
in
ASCII
in
a
pdf
file
when
I
run
it
using
""""
Geany
""""
it
gives
me
the
following
error
:
The
better
way
of
doing
this
now
is
to
use
pip
install
with
the
-
e
option
.
It
uses
a
directory
with
the
setup.py
file
.
The
""""
.
""""
indicates
this
directory
.
This
works
the
same
way
as
the
setuptools
develop
method
.
I
believe
that
the
develop
creates
an
egg
link
in
your
sight
packages
folder
which
points
to
the
folder
of
the
library
.
http://pythonhosted.org/setuptools/setuptools.html#develop-deploy-the-project-source-in-development-mode
I
believe
this
is
why
you
get
the
absolute
path
.
There
may
be
a
conflict
with
a
develop
link
and
an
install
.
Things
could
have
also
been
moved
.
For
double
clicking
just
have
something
that
checks
sys.argv
.
If
there
is
no
value
for
sys.argv
[1]
append
build
","
install
","
or
develop
.
In
addition
","
I've
always
heard
that
you
want
to
import the
modules
then
call
the
functions
from
the
modules
.
from
package
import lib
.
lib.foo()
that
way
you
know
where
the
method
came
from
.
I
believe
the
import does
the
same
thing
for
both
ways
;
this
may
clean
up
your
import
.
Python
pathing
and
packaging
can
be
a
pain
.
You
may
want
to
carefully
read
the
module
search
path
documentation
.
Note
that
the
search
path
will
include
""""
the
directory
containing
the
input
script
(
or
the
current
directory
)
""""
.
When
you
invoke
python
-
m
package.script
","
the
current
directory
is
used
since
there
is
no
input
script
(
script.py
is
used
as
a
module
)
.
When
you
run
.
\
package\script.py
","
it
ads
.
\
package
to
the
search
path
.
A
solution
to
your
situation
is
to
put
all
executable
scripts
in
the
base
directory
of
your
library
hierarchy
.
I.e
.
move
script.py
up
one
directory
.
I
want
to
revisit
the
question
as
specified
in
PYTHONPATH
vs
.
sys.path
.
Basically
it
is
concerned
with
developing
a
package
like
:
Assuming
script.py
does
from
package.lib
import foo
it
does
work
when
calling
:
from
the
directory
where
setup.py
is
sitting
","
but
not
when
calling
(
on
windows
","
CPython
2.7
)
:
In
first
case
when
printing
sys.path
first
entry
is
'
'
","
but
in
second
case
the
first
entry
is
the
absolute
path
to
where
script.py
is
sitting
.
And
of
course
in
that
case
it
does
not
know
anything
about
package
and
the
import fails
.
This
will
also
be
the
case
when
double-clicking
in
Explorer
.
The
original
stackoverflow
question
recommends
package
installation
by
setup.py
develop
.
However
","
in
current
setuptools
3.5
(
I
am
aware+confused
of
the
distutils
/
setuptools
renaming
)
this
option
is
not
even
documented
(
I
have
setuptools
3.4.x
","
but
did
not
try
it
)
.
Can
anyone
point
out
to
me
what
the
recommended
(
""""
…
only
obvious
way
to
do
it
…
""""
)
procedere
is
on
windows
(
for
CPython
2.7
but
also
having
Python
3
in
mind
)
for
double-clicking
the
file
and
having
it
work
.
Relative
imports
?
Why
not
use
a
Default
dict
here.e.g
.
Now
var
[
'
anything
'
]
would
always
return
with
0
.
I
guess
that
is
the
functionality
you
require
for
testing
","
Right
?
}
this
is
already
a
object
","
if
you
said
you
want
to
add
some
field
inside
you
can
do
it
like
isn't
this
what
you
want
?
Namedtuples
are
on
way
","
if
they
don't
have
to
be
mutable
:
If
it's
for
testing
","
then
using
the
Mock
library
is
also
handy
(
see
http://www.voidspace.org.uk/python/mock/
;
unittest.mock
is
part
of
the
standard
library
since
Python
3.3
as
well
)
.
Note
that
yourobject
now
has
any
field
or
method
you
try
to
access
on
it
","
and
their
values
are
all
yet
another
MagicMock
instance
.
It's
a
very
powerful
unit
testing
tool
.
Lastly
you
can
just
create
a
boring
dummy
class
:
For
test
purposes
I
need
to
create
something
like
an
empty
object
with
some
particular
fields
.
Let
me
clarify
by
example
:
Currently
there
is
a
structure
:
The
thing
I
want
is
for
'
this_has_to_be_object
'
to
be
an
actual
object
to
which
I
can
add
some
fields
and
set
values
.
The
objective
of
such
behaviour
is
to
test
another
function
that
operates
with
'
this_has_to_be_object'.field
.
So
:
Im
trying
to
create
a
sql
table
:
I
want
to
give
an
id
to
every
movie
title
and
rating
and
the
create
a
tabel
with
3
fields
Â
¨
My
code
so
fare
is
:
My
list
look
like
this
:
How
can
i
get
this
into
a
SQL
db
?
Update
:
Output
:
If
you
don't
want
to
insert
movie
id
manually
","
change
creating
your
imdb
table
definition
with
:
"c.execute('''CREATE TABLE imdb (mov_id ROWID, Title, Rating)"
'
'
'
)
which
lets
you
omit
specifying
mov_id
in
your
insert
.
Then
","
just
add
your
record
using
just
instead
of
your
#
Insert
a
row
of
data
line
.
BTW
:
I'd
personally
do
rstrip()
on
your
Title
to
remove
newline
character
","
if
you
don't
need
it
:
Title
=
d
[4]
.
rstrip()
Here
is
an
example
of
how
to
do
it
.
It
should
be
fast
on
large
datasets
","
since
it
does
not
use
any
loop
or
specific
functions
.
It
uses
pandas
loc
function
.
school_detail
will
now
contain
I
am
using
Pandas
to
work
with
large
number
of
Data
.
I
want
to
find
the
fastest
way
to
get
the
first
row
in
DataFrame
with
id
I
need
to
loop
through
all
rows
in
school_detail
to
set
type
for
each
row
.
I
have
use
%
prun
to
check
time
for
function
get
school
by
id
.
It
is
about
0.03
sec
When
I
run
with
10000
rows
of
school_detail
.
It
takes
43
sec
.
If
I
run
with
20
mil
rows
.
It
may
take
several
hours
.
My
questions
:
I
want
to
find
the
better
way
to
get
school
by
id
to
make
it
run
faster
.
The
id
column
is
unique
.
Do
pandas
use
binary
search
in
this
column
?
According
to
the
source
code
","
Scrapy
uses
the
following
CSS
selector
to
parse
the
inputs
out
of
the
form
:
In
other
words
","
all
of
your
hidden
inputs
are
successfully
parsed
(
and
sent
with
the
request
later
)
with
the
values
equal
to
value
attributes
.
So
","
Scrapy
does
what
it
should
here
.
The
login
using
from_response()
doesn't
work
because
__EVENTTARGET
has
a
empty
value
attribute
.
If
you
make
the
login
using
a
real
browser
","
__EVENTTARGET
parameter
value
would
be
set
to
b_Login
via
javascript
__doPostBack()
function
call
.
And
","
since
Scrapy
cannot
handle
javascript
(
cannot
call
js
functions
)
","
__EVENTTARGET
is
sent
with
an
empty
value
which
causes
login
failure
.
__EVENTARGUMENT
has
an
empty
value
too
","
but
it
is
actually
set
to
the
empty
string
in
the
__doPostBack()
function
","
so
it
doesn't
make
a
difference
here
.
Hope
that
helps
.
I'm
using
Scrapy
to
scrape
some
gold
that's
behind
an
authentication
screen
.
The
website
uses
ASP.net
and
ASP's
got
some
stupid
hidden
fields
littered
all
over
the
form
(
like
__VIEWSTATE
","
__EVENTTARGET
)
.
When
I
call
FormRequest.from_response(response
","
...
I'm
expecting
that
it
reads
these
hidden
fields
automatically
from
the
response
and
populates
them
in
the
formdata
dictionary
-
which
is
what
Scrapy's
FormRequest
documentation
says
it
should
do
.
But
if
that's
the
case
","
then
why
does
the
login
process
only
work
when
I
explicitly
list
these
fields
and
populate
them
?
Edit
:
Adding
form
HTML
I'm
quite
new
to
Kivy
&
Python
so
please
excuse
the
poor
code
/
explanation
!
When
the
touch
down
event
is
activated
I
want
a
section
of
code
to
continuously
run
until
the
touch
up
event
is
activated
.
My
current
method
is
using
a
global
variable
which
is
manipulated
by
both
events
.
But
the
loop
doesn't
seem
to
register
the
change
in
the
global
variable
.
Is
there
anyway
to
make
this
method
work
.
Or
alternatively
another
method
which
would
be
more
suitable
?
Thanks
!
1
)
Don't
use
globals
.
This
should
technically
work
","
but
it's
bad
practice
and
in
this
case
there's
no
downside
to
simply
adding
a
variable
to
your
class
.
NotNone
has
this
very
moment
posted
an
example
of
how
to
do
so
with
a
kivy
property
","
which
adds
some
other
behaviour
you
may
find
convenient
later
.
2
)
Your
move
function
isn't
compatible
with
an
eventloop
driven
gui
like
kivy
(
or
many
other
gui
libraries
)
.
The
problem
is
that
the
while
function
is
a
blocking
call
-
all
of
kivy's
normal
tasks
can't
be
performed
until
the
move
function
terminates
.
Detecting
and
propagating
touch
is
one
of
these
things
","
so
on_touch_up
will
never
be
called
","
and
the
touched_down
variable
will
never
become
False
.
You
probably
instead
want
to
do
something
like
:
Using
the
clock
like
this
inserts
the
movement
into
the
normal
kivy
eventloop
","
so
it's
called
every
frame
without
blocking
everything
else
.
It
might
also
be
more
natural
to
have
an
update
function
that
is
always
scheduled
(
and
does
other
things
too
)
","
and
use
a
self.touched_down
attribute
as
above
to
optionally
perform
movement
during
that
function
.
I
think
using
a
custom
property
could
solve
your
problem
.
I
am
not
able
to
test
this
","
but
something
like
this
should
work
.
In
my
opinion
properties
are
some
of
the
very
best
features
of
Kivy
","
so
take
a
look
at
them
if
they
aren't
familiar
.
I
have
a
data
set
for
clusters
.
And
the
output
seems
something
like
this
:
-
1
"[1,2]"
2
"[1,6]"
1
"[2,4]"
Where
1
","
2
...
is
the
cluster
id
and
"[1,2]"
.
.
so
on
are
the
points
.
So
i
want
to
plot
the
points
x
co-ordinate
and
y
co-ordinate
on
both
the
axis
and
corresponding
to
that
a
point
in
graph
depicting
the
cluster
id
as
label
and
for
different
id
the
color
of
points
should
be
different
.
How
do
i
go
about
it
?
Thanks
If
one
axis
is
the
cluster
id
I
don't
get
how
you
fit
both
the
x
and
y
coordinates
onto
the
other
axis
.
So
I
plotted
the
x
","
y
on
the
x
and
y
axis
and
used
the
cluster
id
as
a
label
;
you
can
swap
which
value
goes
into
which
axis
","
I
guess
:
N.b
.
:
if
you
have
a
lot
of
data
","
don't
call
scatter
for
each
point
separately
;
append
x
","
y
","
cluster
to
three
separate
lists
and
scatter
the
lists
.
My
Python
code
is
:
Is
it
possible
to
show
the
labels
""""
Jan
""""
","
""""
Feb
""""
","
""""
Mar
""""
","
etc.
and
the
percentages
","
either
:
without
overlapping
","
or
using
an
arrow
mark
?
Alternatively
you
can
put
the
legends
beside
the
pie
graph
:
EDIT
:
if
you
want
to
keep
the
legend
in
the
original
order
","
as
you
mentioned
in
the
comments
","
you
can
set
sort_legend=False
in
the
code
above
","
giving
:
The
below
code
has
an
error
in
function
U=set(p.enum
(
)
)
which
a
type
error
of
unhashable
type
:
'
set
'
actually
if
you
can
see
the
class
method
enum
am
returning
'
L
'
which
is
list
of
sets
and
the
U
in
function
should
be
a
set
so
can
you
please
help
me
to
resolve
the
issue
or
How
can
I
convert
list
of
sets
to
set
of
sets
?
This
error
is
raised
because
a
set
can
only
contain
immutable
types
.
Or
sets
are
mutable
.
However
there
is
the
frozenset
type
:
The
individual
items
that
you
put
into
a
set
can't
be
mutable
","
because
if
they
changed
","
the
effective
hash
would
change
and
thus
the
ability
to
check
for
inclusion
would
break
down
.
Instead
","
you
need
to
put
immutable
objects
into
a
set
-
e.g
.
frozensets
.
If
you
change
the
return
statement
from
your
enum
method
to
...
...
then
it
should
work
.
I
am
wondering
why
for
the
code
shown
below
results
in
passing
self
to
function
?
The
call
is
:
Resulting
in
:
TypeError
:
function()
takes
exactly
1
argument
(
2
given
)
It
appears
that
somewhere
is
a
class
called
ObjT
with
a
method
called
function
that
is
not
designed
to
accept
an
argument
.
I
suspect
the
definition
of
ObjT
includes
this
line
:
Where
it
should
be
something
like
this
:
I
am
building
a
small
micro-web
application
using
bottle
.
First
step
is
that
the
user
inputs
an
ID
into
a
form
in
order
to
retrieve
a
record
.
Once
the
ID
has
been
verified
as
part
of
the
SQLite
database
further
steps
will
be
taken
","
they
can
modify
records
","
add
data
to
another
.
db
with
the
same
PRIMARY
KEY
and
so
on
and
so
forth
.
My
question
is
","
within
the
session
how
do
I
preserve
the
ID
once
verified
and
pass
it
around
the
various
functions
.
Should
it
be
a
global
created
inside
the
function
that
verifies
the
user
input
?
I
am
wary
of
globals
","
just
because
so
much
junk
is
talked
about
them
.
Also
","
I
am
nervous
that
if
not
very
well
managed
the
user
could
restart
the
session
enter
new
data
but
the
global
ID
variable
might
not
have
been
properly
disposed
of
etc....
What
is
the
correct
way
to
approach
this
?
Im
not
sure
it
helps
","
but
here
is
the
code
I
am
using
so
far
....
If
you
want
to
use
the
id
for
a
single
request
then
passing
around
the
local
variable
as
you
seem
to
be
doing
is
better
than
using
globals
.
If
you
want
to
preserve
the
id
across
requests
you
can
save
the
value
in
cookies(if safety of id is not a concern)
and
access
it
then
easily
.
The
cookie's
are
saved
on
the
user's
computer
and
are
accessible
only
during
the
request
.
For
more
on
using
cookies
in
bottle
refer
bottle's
documentation(it is explained in the tutorial)
.
Also
:
instead
of
embedding
the
html
in
your
code
use
templates
!
It
will
make
your
code
neater
.
I
have
Titan
(
with
embedded
cassandra
running
on
my
system
)
.
I
have
rexster
server
running
I
am
able
to
create
a
graph
from
the
rexter
shell
using
Gremlin
queries
.
I
have
installed
bulbs
on
my
system
as
follows
.
If
I
try
the
following
from
ipython
on
my
machine
I
get
the
following
error
.
What
does
these
mean
?
Titan
has
its
own
module
...
I
have
list
which
is
already
sorted
.
I
often
need
to
check
to
see
Is
there
a
way
to
teach
'
in
'
that
sortedlist
is
sorted
and
that
it
should
binary
search
the
list
?
Python
favours
explicit
over
implicit
.
Your
options
are
to
explicitly
use
the
bisect
module
if
you
know
your
data
is
sorted
","
or
create
a
subclass
of
list
that
implements
__contains__
by
using
that
module
.
For
example
:
could
be
used
as
a
substitute
for
list
","
and
in
will
use
__contains__
automatically
.
You'd
probably
want
to
override
__setitem__
","
.
extend()
and
.
append()
to
maintain
the
list
in
sorted
order
","
too
.
My
suggestion
would
be
to
subclass
list
to
use
sorted
list
:
Then
:
You
are
using
a
3rd-party
Python
library
","
inspyred
is
an
AI
package
for
Python
.
You
appear
to
have
a
sequence
of
Individual
instances
;
you'd
look
up
the
documentation
and
see
what
attributes
such
instances
have
.
You
are
looking
at
the
string
representation
of
these
instances
","
but
you
should
be
able
to
access
various
aspects
directly
.
The
documentation
states
there
are
candidate
","
fitness
","
birthdate
and
maximize
attributes
;
try
these
to
see
which
one
represents
the
list
you
seek
.
A
quick
peek
at
the
source
code
suggests
you
want
the
candidate
attribute
:
When
dealing
with
new
objects
in
Python
","
you
can
always
introspect
them
by
using
tools
such
as
the
type()
","
dir()
and
vars()
functions
","
which
tell
you
about
the
type
of
object
you
have
","
the
methods
the
class
defines
","
and
the
attributes
the
instance
has
.
I
have
a
string
of
some
data
.
string
[0]
prints
"[31, 37, 3]"
:
[
0.12704417954091404
]
.
I
want
to
get
the
first
array
which
is
"[31, 37, 3]"
Yes
manufacturer
need
to
exist
as
an
instance
already
.
you
can
create
car
instance
like
this
:
you
can
get
the
company_created
data
from
the
civic
instance
by
:
There
is
something
that
is
tripping
me
up
with
models
","
and
I
guess
SQL
tables
in
general
.
Let
us
suppose
you
have
these
models
:
When
you
create
an
instance
of
Car
like
say
","
the
following
A
couple
questions
:
When
you
create
an
instance
of
Car
","
are
you
also
creating
an
instance
of
Manufacturer
as
a
by-product
?
Or
does
'
honda
'
need
to
exist
as
an
instance
already
.
If
not
","
is
there
a
way
to
make
an
instance
of
both
","
in
say
","
one
form
.
Can
I
make
calls
on
'
civic
'
for
things
pertaining
to
the
manufacture
?
For
example
","
could
I
get
the
'
company_created
'
data
from
the
civic
instance
?
If
not
","
why
bother
having
the
relationship
in
the
first
place
?
Thanks
so
much
in
advance
.
I
would
really
appreciate
a
thorough
answer
so
I
can
understand
models
and
relationships
fully
.
And
yes
","
I
have
read
the
docs
.
Firstly
","
the
thing
to
remember
is
that
these
classes
are
representations
of
underlying
database
tables
.
A
ForeignKey
field
in
a
Django
model
represents
a
one-to-many
relationship
in
the
database
","
with
an
_id
field
representing
the
ID
of
another
table
.
Those
tables
are
themselves
independent
","
but
are
linked
via
the
FK
field
(
and
the
relevant
index
constraint
","
if
the
database
supports
them
)
.
That
said
","
in
your
Car
model
manufacturer
is
a
required
field
(
because
you
haven't
defined
it
as
null=True
)
.
So
when
you
create
a
Car
","
you
must
point
it
at
an
already
existing
Manufacturer
-
and
that
manufacturer
must
have
been
saved
already
","
so
that
Django
can
populate
the
underlying
manufacturer_id
field
with
the
ID
of
the
related
object
Because
Django
is
aware
of
the
foreign
key
relationship
between
the
two
objects
","
you
can
use
them
when
querying
.
In
SQL
this
would
be
done
via
JOINs
:
Django
gives
you
a
special
syntax
to
do
queries
across
those
joins
","
via
double
underscores
.
So
","
for
example
","
if
you
wanted
to
get
all
the
cars
made
by
a
manufacturer
created
in
1990
(
assuming
that's
what
you
mean
by
the
company_created
field
)
","
you
would
do
this
:
Django
translates
this
into
something
like
""""
:
If
you
already
have
your
""""
civic
""""
instance
and
just
want
to
get
access
to
the
related
data
","
this
is
pure
Python
object
access
:
civic.manufacturer
is
the
related
Manufacturer
object
","
so
you
can
simply
do
civic.manufacturer.company_created
to
get
the
relevant
data
.
Again
","
Django
translates
that
into
the
database
access
","
but
from
your
point
of
view
this
is
simple
object
composition
.
Note
that
really
all
this
is
fully
explained
in
the
tutorial
","
with
relationships
between
Poll
and
Choice
which
exactly
match
your
Manufacturer
and
Car
models
.
I
have
been
writing
a
proof-of-concept
type
program
that
sends
a
screenshot
over
the
network
","
and
for
some
reason
","
the
dataRecieved
method
isn't
being
called
after
data
is
sent
.
Here
is
my
code
SERVER
CLIENT
When
you
call
reactor.stop
at
the
end
of
connectionMade
","
you
immediately
shut
down
the
whole
process
before
any
data
is
sent
.
Remove
it
and
you
should
be
fine
.
How
can
i
get
[
1
","
2
","
3
","
4
","
""""
hello
""""
]
with
list
comprehension
in
python
?
You'd
use
a
conditional
expression
instead
:
This
is
part
of
the
left-hand
element-producing
expression
","
not
the
list
comprehension
syntax
itself
","
where
if
statements
act
as
a
filter
.
The
conditional
expression
uses
the
form
true_expression
if
test_expression
else
false_expression
;
it
always
produces
a
value
.
I
simplified
the
expression
a
little
;
if
you
are
replacing
""""
-
-
""""
with
""""
hello
""""
you
may
as
well
just
return
""""
hello
""""
.
[
"cell.replace(""--"",""hello"")"
if
cell
=
=
""""
-
-
""""
else
cell
for
cell
in
row
]
When
using
the
if
at
the
end
of
the
for
","
it
restricts
which
items
are
considered
","
so
that
version
will
only
return
one
item
","
since
only
one
item
in
the
source
list
matches
the
condition
.
Also
in
this
case
you
don't
need
to
use
replace
","
you
could
just
use
""""
hello
""""
if
cell
=
=
""""
-
-
""""
instead
","
but
you
would
use
this
form
if
you
had
multiple
items
you
wanted
to
manipulate
.
I
have
my
webapp
written
in
Python
running
on
Google
App
Engine
;
so
far
only
testing
in
my
localhost
.
My
script
needs
to
read
text
files
located
in
various
directories
.
So
in
my
Python
script
I
would
simply
use
"os.chdir(""./""+subdirectory_name)"
and
then
start
opening
and
reading
the
files
and
using
the
text
for
analysis
.
However
I
cannot
do
that
when
trying
to
move
it
over
to
App
Engine
.
I
assume
I
need
to
make
some
sort
of
changes
to
app.yaml
but
I'm
stuck
after
that
.
Can
anyone
walk
me
through
this
somewhat
thoroughly
?
From
the
answer
below
","
I
was
able
to
fix
this
part
","
I
have
a
followup
question
about
the
app.yaml
structure
though
.
I
have
my
folder
set
up
like
the
following
","
and
I'm
wondering
what
I
need
to
put
in
my
app.yaml
file
to
make
it
work
when
I
deploy
the
project
(
it
works
perfectly
on
localhost
right
now
)
.
Main_Folder
:
python_my_app.py
app.yaml
text_file1
text_file2
text_file3
subfolder1_with_10_text_files_inside
subfolder2_with_10_text_files_inside
subfolder3_with_10_text_files_inside
...
how
do
I
specify
this
structure
in
my
app.yaml
and
do
I
need
to
change
anything
in
my
code
if
it
works
right
now
on
localhost
?
You
don't
need
to
change
your
working
directory
at
all
to
read
files
.
Use
absolute
paths
instead
.
Use
the
current
file
location
as
the
starting
point
","
and
resolve
all
relative
paths
from
there
:
If
you
want
to
be
able
to
read
static
files
","
do
remember
to
set
the
application_readable
flag
on
these
","
as
Google
will
otherwise
only
upload
them
to
their
content
delivery
network
","
not
to
the
app
engine
.
You
can
package
your
text
files
inside
your
application
and
then
do
something
like
this
:
Here
is
an
easy
one-liner
that
does
the
same
transcription
from
DNA
to
RNA
(
T->U
)
:
Answered
by
Martijn
Pieters
.
Thank
you
.
It
is
because
statement
vs
expression
.
And
because
.
join()
does
not
mutate
(
is
a
pure
function
)
","
so
it
needs
to
be
assigned
to
a
variable
.
Question
:
What
is
the
reason
for
this
oddity
?
Goal
:
This
following
method
works
:
However
the
following
does
NOT
work
:
or
The
results
are
the
same
in
both
Python2
and
Python3
.
Like
all
assignment
","
+
=
is
a
statement
.
You
cannot
ever
put
statements
in
an
expression
.
The
right-hand
expression
(
everything
after
+
=
)
is
evaluated
first
","
the
result
of
which
is
then
used
for
the
augmented
assignment
.
You
can
do
:
Now
the
expression
resolves
either
to
'
U
'
","
or
base
","
depending
on
the
value
of
base
.
If
'
U
'
if
base
=
=
'
T
'
else
RNA_seq.join(base)
worked
","
then
that
means
that
RNA_seq.join()
returns
a
new
value
and
doesn't
mutate
RNA_seq
in-place
.
RNA_seq.join('U')
if
base
=
=
'
T
'
else
RNA_seq.join(base)
would
then
also
return
a
new
value
","
leaving
the
original
value
bound
by
RNA_seq
unchanged
","
and
you
didn't
assign
it
back
to
RNA_seq
.
I'm
not
very
familiar
with
Python
","
and
I
am
just
discovering
GDB
python
scripting
capabilities
;
the
motivation
of
my
question
is
to
enhance
the
GDB
printing
of
values
inside
the
MELT
monitor
which
will
later
be
connected
to
GCC
MELT
.
But
here
is
a
simpler
variant
.
My
system
is
Linux
/
Debian
/
Sid
/
x86-64
.
the
GCC
compiler
is
4.8.2
;
the
GDB
debugger
is
7.6.2
;
its
python
is
3.3
I
want
to
debug
a
C
program
with
a
""""
discriminated
union
""""
type
:
Here
is
my
Python
file
to
be
read
under
gdb
I'm
stuck
with
the
following
basic
questions
.
How
to
install
my
pretty
printer
in
python
under
GDB
?
(
I'm
seeing
several
ways
in
the
documentation
","
and
I
can't
choose
the
appropriate
one
)
.
How
to
ensure
that
GDB
pretty-prints
both
union
my_un
and
its
typedef-ed
synonym
myval_t
the
same
way
.
How
should
the
pretty
printer
detect
NULL
pointers
?
How
can
my
pretty
printer
recurse
for
struct
boxsequence_st
?
This
means
detecting
that
the
pointer
is
non-nil
","
then
dereferencing
its
ptag
","
comparing
that
tag
to
tag_sequence
","
pretty
printing
the
valtab
flexible
array
member
.
How
to
avoid
recursing
too
deeply
the
pretty
printing
?
I
don't
have
enough
experience
with
the
gdb
Python
api
to
call
this
an
answer
;
I
consider
this
just
some
research
notes
from
a
fellow
developer
.
My
code
attached
below
is
quite
crude
and
ugly
","
too
.
However
","
this
does
work
with
gdb-7.4
and
python-2.7.3
.
An
example
debugging
run
:
All
of
the
above
are
bog-standard
pretty-printed
outputs
-
-
my
reasoning
is
that
I
often
want
to
see
what
the
pointers
are
","
so
I
didn't
want
to
override
those
.
However
","
dreferencing
the
pointers
uses
the
prettyprinter
shown
further
below
:
The
last
line
shows
that
when
debugging
tiny
","
tiny-gdb.py
in
the
same
directory
gets
loaded
automatically
(
although
you
can
disable
this
","
I
do
believe
this
is
the
default
behaviour
)
.
The
tiny-gdb.py
file
used
for
above
:
The
reasoning
behind
my
choices
are
as
follows
:
How
to
install
pretty-printers
to
gdb
?
There
are
two
parts
to
this
question
:
where
to
install
the
Python
files
","
and
how
to
hook
the
pretty-printers
to
gdb
.
Because
the
pretty-printer
selection
cannot
rely
on
the
inferred
type
alone
","
but
has
to
peek
into
the
actual
data
fields
","
you
cannot
use
the
regular
expression
matching
functions
.
Instead
","
I
chose
to
add
my
own
pretty-printer
selector
function
","
typefilter()
","
to
the
global
pretty-printers
list
","
as
described
in
the
documentation
.
I
did
not
implement
the
enable
/
disable
functionality
","
because
I
believe
it
is
easier
to
just
load
/
not
load
the
relevant
Python
script
instead
.
(
typefilter()
gets
called
once
per
every
variable
reference
","
unless
some
other
pretty-printer
has
already
accepted
it
.
)
The
file
location
issue
is
a
more
complicated
one
.
For
application-specific
pretty-printers
","
putting
them
into
a
single
Python
script
file
sounds
sensible
","
but
for
a
library
","
some
splitting
seems
to
be
in
order
.
The
documentation
recommends
packaging
the
functions
into
a
Python
module
","
so
that
a
simple
python
import module
enables
the
pretty-printer
.
Fortunately
","
Python
packaging
is
quite
straightforward
.
If
you
were
to
import gdb
to
the
top
and
save
it
to
/
usr
/
lib
/
pythonX.Y
/
tiny.py
","
where
X.Y
is
the
python
version
used
","
you
only
need
to
run
python
import tiny
in
gdb
to
enable
the
pretty-printer
.
Of
course
","
properly
packaging
the
pretty-printer
is
a
very
good
idea
","
especially
if
you
intend
to
distribute
it
","
but
it
does
pretty
much
boil
down
to
adding
some
variables
et
cetera
to
the
beginning
of
the
script
","
assuming
you
keep
it
as
a
single
file
.
For
more
complex
pretty-printers
","
using
a
directory
layout
might
be
a
good
idea
.
If
you
have
a
value
val
","
then
val.type
is
the
gdb.Type
object
describing
its
type
;
converting
it
to
string
yields
a
human-readable
type
name
.
val.type.strip_typedefs()
yields
the
actual
type
with
all
typedefs
stripped
.
I
even
added
.
unqualified()
","
so
that
all
const
/
volatile
/
etc.
type
qualifiers
are
removed
.
NULL
pointer
detection
is
a
bit
tricky
.
The
best
way
I
found
","
was
to
examine
the
stringified
.
address
member
of
the
target
gdb.Value
object
","
and
see
if
it
is
""""
0x0
""""
.
To
make
life
easier
","
I
was
able
to
write
a
simple
deref()
function
","
which
tries
to
dereference
a
pointer
.
If
the
target
points
to
(
void
*
)
0
","
it
returns
the
string
""""
NULL
""""
","
otherwise
it
returns
the
target
gdb.Value
object
.
The
way
I
use
deref()
is
based
on
the
fact
that
""""
array
""""
type
pretty-printers
yield
a
list
of
2-tuples
","
where
the
first
item
is
the
name
string
","
and
the
second
item
is
either
a
gdb.Value
object
","
or
a
string
.
This
list
is
returned
by
the
children()
method
of
the
pretty-printer
object
.
Handling
""""
discriminated
union
""""
types
would
be
much
easier
","
if
you
had
a
separate
type
for
the
generic
entity
.
That
is
","
if
you
had
and
it
was
used
everywhere
when
the
tag
value
is
still
uncertain
;
and
the
specific
structure
types
only
used
where
their
tag
value
is
fixed
.
This
would
allow
a
much
simpler
type
inference
.
As
it
is
","
in
tiny.c
the
struct
box*_st
types
can
be
used
interchangeably
.
(
Or
","
more
specifically
","
we
cannot
rely
on
a
specific
tag
value
based
on
the
type
alone
.
)
The
sequence
case
is
actually
quite
simple
","
because
valtab
[]
can
be
treated
as
simply
as
an
array
of
void
pointers
.
The
sequence
tag
is
used
to
pick
the
correct
union
member
.
In
fact
","
if
valtab
[]
was
simply
a
void
pointer
array
","
then
gdb.Value.cast(gdb.lookup_type()
)
or
gdb.Value.reinterpret_cast(gdb.lookup_type()
)
can
be
used
to
change
each
pointer
type
as
necessary
","
just
like
I
do
for
the
boxed
structure
types
.
Recursion
limits
?
You
can
use
the
@
operator
in
print
command
to
specify
how
many
elements
are
printed
","
but
that
does
not
help
with
nesting
.
If
you
add
iseq3->valtab
[2]
=
(
myval_t)iseq3
;
to
tiny.c
","
you
get
an
infinitely
recursive
sequence
.
gdb
does
print
it
nicely
","
especially
with
set
print
array
","
but
it
does
not
notice
or
care
about
the
recursion
.
In
my
opinion
","
you
might
wish
to
write
a
gdb
command
in
addition
to
a
pretty-printer
for
deeply
nested
or
recursive
data
structures
.
During
my
testing
","
I
wrote
a
command
that
uses
Graphviz
to
draw
binary
tree
structures
directly
from
within
gdb
;
I'm
absolutely
convinced
it
beats
plain
text
output
.
Added
:
If
you
save
the
following
as
/
usr
/
lib
/
pythonX.Y
/
tree.py
:
you
can
use
it
in
gdb
:
If
you
have
e.g
.
and
you
have
X11
(
local
or
remote
)
connection
and
Graphviz
installed
","
you
can
use
to
view
the
tree
structure
.
Because
it
retains
a
list
of
already
visited
nodes
(
as
a
Python
set
)
","
it
does
not
get
fazed
about
recursive
structures
.
I
probably
should
have
cleaned
my
Python
snippets
before
posting
","
but
no
matter
.
Please
do
consider
these
only
initial
testing
versions
;
use
at
your
own
risk
.
:
)
I
used
some
code
taken
from
Rosetta
Code
.
I
renamed
a
few
things
but
I
didn't
really
change
anything
.
It
pretty
closely
matches
some
psuedocode
.
However
","
when
I
test
the
number
123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901
it
returns
False
.
Other
(
python
and
java
)
implementations
of
Miller-Rabin
return
True
for
probable
prime
.
After
some
testing
","
try_composite
returns
True
after
only
2
rounds
!
I
would
really
like
to
know
any
error
","
I'm
guessing
a
mis-indent
or
some
feature
I
don't
know
about
.
In
your
try_composite
function
","
the
for
loop
should
be
for
i
in
"range(1,s)"
.
Do
not
test
the
case
where
i
is
zero
.
EDIT
:
Also
","
you
are
missing
a
test
in
your
try_composite
function
.
Here
is
my
version
of
the
pseudocode
:
It's
too
bad
that
Python
doesn't
allow
labels
on
break
or
continue
statements
.
Here's
pseudocode
for
a
much
prettier
version
of
the
function
:
Notice
the
two
places
where
the
control
flow
goes
to
next
i
.
There
is
no
good
way
to
write
that
in
Python
.
One
choice
uses
an
extra
boolean
variable
that
can
be
set
and
tested
to
determine
when
to
bypass
the
rest
of
the
code
.
The
other
choice
","
which
I
took
above
","
is
to
write
a
local
function
to
perform
the
task
.
This
""""
loop-and-a-half
""""
idiom
is
convenient
and
useful
;
it
was
proposed
in
PEP
3136
and
rejected
by
Guido
.
To
remove
the
recursion
","
you
need
to
make
your
functions
iterative
.
For
add
to
list
","
this
is
easy
.
Something
like
this
should
work
.
For
maxMatching
","
this
is
also
possible
","
but
it
takes
some
more
work
.
However
","
do
you
notice
that
your
recursion
builds
the
dp
table
from
top
left
to
bottom
right
?
And
that
you
use
the
values
of
the
dp
to
calculate
the
value
maxMatching
more
to
the
right
and
to
the
bottom
?
So
what
you
can
do
is
to
create
a
helper
table
(
like
dp
and
boolean
)
and
construct
that
from
top
to
bottom
and
left
to
right
.
For
each
of
the
cells
you
calculate
the
value
based
on
the
values
as
you
would
now
","
but
instead
of
using
recursion
","
you
use
the
value
from
the
helper
table
.
This
method
is
called
Dynamic
Programming
","
which
is
building
a
solution
based
on
the
solutions
of
smaller
problems
.
Many
problems
that
can
be
defined
using
some
form
of
mathematical
resursion
can
be
solved
using
Dynamic
Programming
.
See
http://en.wikipedia.org/wiki/Dynamic_programming
for
more
examples
.
I
am
having
a
problem
with
'
Maximum
recursion
depth
exceeded
'
in
python
I
converted
a
java(I dont know java so it wasn't easy)
function
to
python
function
and
it
did
work
for
small
lists
but
when
I
use
large
lists
I
get
that
error
.
I
tried
to
do
sys.setrecursionlimit(10000)
but
it
seem
that
there
is
a
problem
because
it
will
not
finish
","
maybe
because
I
converted
the
java
code
to
python
in
a
wrong
way
.
this
is
the
python
code
of
the
function
the
function
should
return
a
list
from
list
b
which
are
the
nearest
points
from
list
a
I
thought
that
convert
this
function
to
iterative
will
resolve
the
problem
.
my
question
is
","
how
to
make
that
function
100
%
iterative
instead
of
recursive
?
thanks
You
can
utilise
itertools.cycle
to
repeat
one
of
the
iterables
","
eg
:
Note
that
while
cycle
will
repeat
its
elements
indefinitely
","
izip
stops
on
the
shortest
iterable
(
list1
)
.
EDIT
Okay
","
that
makes
more
sense
-
How
about
this
...
Though
I
feel
the
same
as
you
-
there
is
probably
a
smoother
way
to
accomplish
that
...
I've
got
a
script
running
that
I
want
to
toggle
between
different
variables
.
Let's
say
I've
got
a
list
of
urls
and
I
want
to
concatenate
one
of
the
variables
a
","
b
or
c
.
I
don't
care
which
but
I'd
expect
the
variables
to
repeat
but
the
list
would
run
through
once
.
v
would
end
up
looking
like
url1+string1
url2+string2
url3+string3
url4+string1
url5+string2
etc
I
was
able
to
get
this
to
work
on
my
own
but
I'm
new
and
learning
","
does
anyone
have
a
more
elegant
solution
to
this
?
it
returns
what
I
was
looking
for
but
it's
messy
:
string1a
string2b
string3c
string4a
string5b
string6c
string7a
string8b
I
am
getting
the
following
error
log
for
installing
scrapy
like
pip
install
scrapy
-
-
upgrade
on
some
gooogling
I
found
a
related
issue
with
cffi
here
but
the
workaround
mentioned
there
to
pip
install
-
I
cffi==0.8.1
did
not
work
either
:
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
C:\Python27\Scripts\pip-script.py
run
on
05
/
10
/
14
10:40:16
Downloading
/
unpacking
cffi==0.8.1
Getting
page
https://pypi.python.org/simple/cffi/
URLs
to
search
for
versions
for
cffi==0.8.1
:
*
https://pypi.python.org/simple/cffi/0.8.1
*
https://pypi.python.org/simple/cffi/
Getting
page
https://pypi.python.org/simple/cffi/0.8.1
Could
not
fetch
URL
https://pypi.python.org/simple/cffi/0.8.1:
HTTP
Error
404
:
Not
Found
Will
skip
URL
https://pypi.python.org/simple/cffi/0.8.1
when
looking
for
download
links
for
cffi==0.8.1
Analyzing
links
from
page
https://pypi.python.org/simple/cffi/
Skipping
link
https://pypi.python.org/packages/cp26/c/cffi/cffi-0.8.1-cp26-none-win32.whl#md5=43c5e5dee0623bedecddda42a3244b81
(
from
https://pypi.python.org/simple/cffi/
)
;
unknown
archive
format
:
.
whl
Skipping
link
https://pypi.python.org/packages/cp26/c/cffi/cffi-0.8.2-cp26-none-win32.whl#md5=b96b84d22204db3955ea89e95a26ebc6
(
from
https://pypi.python.org/simple/cffi/
)
;
unknown
archive
format
:
.
whl
Skipping
link
https://pypi.python.org/packages/cp26/c/cffi/cffi-0.8.2-cp26-none-win_amd64.whl#md5=4ad6b4952b0aef43dc52613b3c8ce935
(
from
https://pypi.python.org/simple/cffi/
)
;
unknown
archive
format
:
.
whl
Skipping
link
https://pypi.python.org/packages/cp27/c/cffi/cffi-0.8.1-cp27-none-win32.whl#md5=a24b8aac42b4b35009bdda89ab33173d
(
from
https://pypi.python.org/simple/cffi/
)
;
unknown
archive
format
:
.
whl
Skipping
link
https://pypi.python.org/packages/cp27/c/cffi/cffi-0.8.2-cp27-none-win32.whl#md5=748720929cf4175330f6459e13b4f45a
(
from
https://pypi.python.org/simple/cffi/
)
;
unknown
archive
format
:
.
whl
Skipping
link
https://pypi.python.org/packages/cp27/c/cffi/cffi-0.8.2-cp27-none-win_amd64.whl#md5=39bef680d138f48f985bdb1de18ae8e1
(
from
https://pypi.python.org/simple/cffi/
)
;
unknown
archive
format
:
.
whl
Skipping
link
https://pypi.python.org/packages/cp32/c/cffi/cffi-0.8.1-cp32-none-win32.whl#md5=186fc3ae218025ec1ef506231cf55d0b
(
from
https://pypi.python.org/simple/cffi/
)
;
unknown
archive
format
:
.
whl
Skipping
link
https://pypi.python.org/packages/cp32/c/cffi/cffi-0.8.2-cp32-none-win32.whl#md5=2e1f0db1a890bf0ba0062f182fc0214a
(
from
https://pypi.python.org/simple/cffi/
)
;
unknown
archive
format
:
.
whl
Skipping
link
https://pypi.python.org/packages/cp32/c/cffi/cffi-0.8.2-cp32-none-win_amd64.whl#md5=999137217400787f979e85935bc8eb79
(
from
https://pypi.python.org/simple/cffi/
)
;
unknown
archive
format
:
.
whl
Skipping
link
https://pypi.python.org/packages/cp33/c/cffi/cffi-0.8.1-cp33-none-win32.whl#md5=73259a8c2eb0869d67964d3e2dba3424
(
from
https://pypi.python.org/simple/cffi/
)
;
unknown
archive
format
:
.
whl
Skipping
link
https://pypi.python.org/packages/cp33/c/cffi/cffi-0.8.2-cp33-none-win32.whl#md5=b3d7b747675d86fcf43859922e1d6d93
(
from
https://pypi.python.org/simple/cffi/
)
;
unknown
archive
format
:
.
whl
Skipping
link
https://pypi.python.org/packages/cp33/c/cffi/cffi-0.8.2-cp33-none-win_amd64.whl#md5=9461ce831b01f36c64a84c25dbf7aaac
(
from
https://pypi.python.org/simple/cffi/
)
;
unknown
archive
format
:
.
whl
Skipping
link
https://pypi.python.org/packages/cp34/c/cffi/cffi-0.8.2-cp34-none-win32.whl#md5=692578ca58384689ddf6b7c399f5dd60
(
from
https://pypi.python.org/simple/cffi/
)
;
unknown
archive
format
:
.
whl
Skipping
link
https://pypi.python.org/packages/cp34/c/cffi/cffi-0.8.2-cp34-none-win_amd64.whl#md5=5cbbe0e74cb337a032d49c3fab35174f
(
from
https://pypi.python.org/simple/cffi/
)
;
unknown
archive
format
:
.
whl
Found
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.1.tar.gz#md5=d78ad460f708ddd1f550fd65fd0803e1
(
from
https://pypi.python.org/simple/cffi/
)
","
version
:
0.1
Found
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.2.1.tar.gz#md5=32dd211d55f6891a575cf2b5471768fd
(
from
https://pypi.python.org/simple/cffi/
)
","
version
:
0.2.1
Found
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.2.tar.gz#md5=768f1214dbaf8839d120e8dfd658aa88
(
from
https://pypi.python.org/simple/cffi/
)
","
version
:
0.2
Found
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.3.tar.gz#md5=25dbc7b6182c64d08adeb6077bfa2743
(
from
https://pypi.python.org/simple/cffi/
)
","
version
:
0.3
Found
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.4.1.tar.gz#md5=e2b929397207d3ec7d88786dda7da474
(
from
https://pypi.python.org/simple/cffi/
)
","
version
:
0.4.1
Found
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.4.2.tar.gz#md5=c2a35af157006e966c67d1a725e7875e
(
from
https://pypi.python.org/simple/cffi/
)
","
version
:
0.4.2
Found
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.4.tar.gz#md5=4d5cee77fcb328ece71d794dcc38b5a9
(
from
https://pypi.python.org/simple/cffi/
)
","
version
:
0.4
Found
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.5.tar.gz#md5=b163c11f68cad4371e8caeb91d81743f
(
from
https://pypi.python.org/simple/cffi/
)
","
version
:
0.5
Found
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.6.tar.gz#md5=5be33b1ab0247a984d42b27344519337
(
from
https://pypi.python.org/simple/cffi/
)
","
version
:
0.6
Found
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.7.1.tar.gz#md5=dcfbb32d9a757d515801463602e4c533
(
from
https://pypi.python.org/simple/cffi/
)
","
version
:
0.7.1
Found
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.7.2.tar.gz#md5=d329f5cb2053fd31dafc02e2c9ef0299
(
from
https://pypi.python.org/simple/cffi/
)
","
version
:
0.7.2
Found
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.7.tar.gz#md5=2110516c65f7c9e6f324241c322178c8
(
from
https://pypi.python.org/simple/cffi/
)
","
version
:
0.7
Found
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.8.1.tar.gz#md5=1a877bf113bfe90fdefedbf9e39310d2
(
from
https://pypi.python.org/simple/cffi/
)
","
version
:
0.8.1
Found
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.8.2.tar.gz#md5=37fc88c62f40d04e8a18192433f951ec
(
from
https://pypi.python.org/simple/cffi/
)
","
version
:
0.8.2
Found
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.8.tar.gz#md5=e61deb0515311bb42d5d58b9403bc923
(
from
https://pypi.python.org/simple/cffi/
)
","
version
:
0.8
Ignoring
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.1.tar.gz#md5=d78ad460f708ddd1f550fd65fd0803e1
(
from
https://pypi.python.org/simple/cffi/
)
","
version
0.1
doesn't
match
=
=
0.8.1
Ignoring
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.2.1.tar.gz#md5=32dd211d55f6891a575cf2b5471768fd
(
from
https://pypi.python.org/simple/cffi/
)
","
version
0.2.1
doesn't
match
=
=
0.8.1
Ignoring
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.2.tar.gz#md5=768f1214dbaf8839d120e8dfd658aa88
(
from
https://pypi.python.org/simple/cffi/
)
","
version
0.2
doesn't
match
=
=
0.8.1
Ignoring
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.3.tar.gz#md5=25dbc7b6182c64d08adeb6077bfa2743
(
from
https://pypi.python.org/simple/cffi/
)
","
version
0.3
doesn't
match
=
=
0.8.1
Ignoring
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.4.1.tar.gz#md5=e2b929397207d3ec7d88786dda7da474
(
from
https://pypi.python.org/simple/cffi/
)
","
version
0.4.1
doesn't
match
=
=
0.8.1
Ignoring
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.4.2.tar.gz#md5=c2a35af157006e966c67d1a725e7875e
(
from
https://pypi.python.org/simple/cffi/
)
","
version
0.4.2
doesn't
match
=
=
0.8.1
Ignoring
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.4.tar.gz#md5=4d5cee77fcb328ece71d794dcc38b5a9
(
from
https://pypi.python.org/simple/cffi/
)
","
version
0.4
doesn't
match
=
=
0.8.1
Ignoring
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.5.tar.gz#md5=b163c11f68cad4371e8caeb91d81743f
(
from
https://pypi.python.org/simple/cffi/
)
","
version
0.5
doesn't
match
=
=
0.8.1
Ignoring
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.6.tar.gz#md5=5be33b1ab0247a984d42b27344519337
(
from
https://pypi.python.org/simple/cffi/
)
","
version
0.6
doesn't
match
=
=
0.8.1
Ignoring
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.7.1.tar.gz#md5=dcfbb32d9a757d515801463602e4c533
(
from
https://pypi.python.org/simple/cffi/
)
","
version
0.7.1
doesn't
match
=
=
0.8.1
Ignoring
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.7.2.tar.gz#md5=d329f5cb2053fd31dafc02e2c9ef0299
(
from
https://pypi.python.org/simple/cffi/
)
","
version
0.7.2
doesn't
match
=
=
0.8.1
Ignoring
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.7.tar.gz#md5=2110516c65f7c9e6f324241c322178c8
(
from
https://pypi.python.org/simple/cffi/
)
","
version
0.7
doesn't
match
=
=
0.8.1
Ignoring
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.8.2.tar.gz#md5=37fc88c62f40d04e8a18192433f951ec
(
from
https://pypi.python.org/simple/cffi/
)
","
version
0.8.2
doesn't
match
=
=
0.8.1
Ignoring
link
https://pypi.python.org/packages/source/c/cffi/cffi-0.8.tar.gz#md5=e61deb0515311bb42d5d58b9403bc923
(
from
https://pypi.python.org/simple/cffi/
)
","
version
0.8
doesn't
match
=
=
0.8.1
Downloading
from
URL
https://pypi.python.org/packages/source/c/cffi/cffi-0.8.1.tar.gz#md5=1a877bf113bfe90fdefedbf9e39310d2
(
from
https://pypi.python.org/simple/cffi/
)
Running
setup.py
egg_info
for
package
cffi
running
egg_info
creating
pip-egg-info\cffi.egg-info
writing
requirements
to
pip-egg-info\cffi.egg-info\requires.txt
writing
pip-egg-info\cffi.egg-info\PKG-INFO
writing
top-level
names
to
pip-egg-info\cffi.egg-info\top_level.txt
writing
dependency_links
to
pip-egg-info\cffi.egg-info\dependency_links.txt
writing
manifest
file
'
pip-egg-info\cffi.egg-info\SOURCES.txt
'
warning
:
manifest_maker
:
standard
file
'
-
c
'
not
found
reading
manifest
file
'
pip-egg-info\cffi.egg-info\SOURCES.txt
'
reading
manifest
template
'
MANIFEST.in
'
writing
manifest
file
'
pip-egg-info\cffi.egg-info\SOURCES.txt
'
Source
in
c:\users\ali\appdata\local\temp\pip-build-ali\cffi
has
version
0.8.1
","
which
satisfies
requirement
cffi==0.8.1
Downloading
/
unpacking
pycparser
(
from
cffi==0.8.1
)
Getting
page
https://pypi.python.org/simple/pycparser/
URLs
to
search
for
versions
for
pycparser
(
from
cffi==0.8.1
)
:
*
https://pypi.python.org/simple/pycparser/
Analyzing
links
from
page
https://pypi.python.org/simple/pycparser/
Found
link
https://pypi.python.org/packages/source/p/pycparser/pycparser-2.02.zip#md5=e484dcd6702770551c3d6cec1caf8dd8
(
from
https://pypi.python.org/simple/pycparser/
)
","
version
:
2.02
Found
link
https://pypi.python.org/packages/source/p/pycparser/pycparser-2.03.zip#md5=ae975c230d0f5c275ffc278c2a5c8862
(
from
https://pypi.python.org/simple/pycparser/
)
","
version
:
2.03
Found
link
https://pypi.python.org/packages/source/p/pycparser/pycparser-2.04.zip#md5=bc652c2ee023efdd37a67aa92ca88359
(
from
https://pypi.python.org/simple/pycparser/
)
","
version
:
2.04
Found
link
https://pypi.python.org/packages/source/p/pycparser/pycparser-2.05.zip#md5=23eb23b7f14d0361b155f25d67f82be9
(
from
https://pypi.python.org/simple/pycparser/
)
","
version
:
2.05
Found
link
https://pypi.python.org/packages/source/p/pycparser/pycparser-2.06.tar.gz#md5=b6898062ebae1ce52d355ef1039e030e
(
from
https://pypi.python.org/simple/pycparser/
)
","
version
:
2.06
Found
link
https://pypi.python.org/packages/source/p/pycparser/pycparser-2.07.tar.gz#md5=985cb5c0d7f357904e7fdff44d31e727
(
from
https://pypi.python.org/simple/pycparser/
)
","
version
:
2.07
Found
link
https://pypi.python.org/packages/source/p/pycparser/pycparser-2.08.tar.gz#md5=923f08a99839b4fac45d2ac395e1ef7e
(
from
https://pypi.python.org/simple/pycparser/
)
","
version
:
2.08
Found
link
https://pypi.python.org/packages/source/p/pycparser/pycparser-2.09.1.tar.gz#md5=74aa075fc28b7c24a4426574d1ac91e0
(
from
https://pypi.python.org/simple/pycparser/
)
","
version
:
2.09.1
Found
link
https://pypi.python.org/packages/source/p/pycparser/pycparser-2.09.tar.gz#md5=27b8018a93c11c93f8e845488a593db4
(
from
https://pypi.python.org/simple/pycparser/
)
","
version
:
2.09
Found
link
https://pypi.python.org/packages/source/p/pycparser/pycparser-2.10.tar.gz#md5=d87aed98c8a9f386aa56d365fe4d515f
(
from
https://pypi.python.org/simple/pycparser/
)
","
version
:
2.10
Using
version
2.10
(
newest
of
versions
:
2.10
","
2.09.1
","
2.09
","
2.08
","
2.07
","
2.06
","
2.05
","
2.04
","
2.03
","
2.02
)
Downloading
from
URL
https://pypi.python.org/packages/source/p/pycparser/pycparser-2.10.tar.gz#md5=d87aed98c8a9f386aa56d365fe4d515f
(
from
https://pypi.python.org/simple/pycparser/
)
Running
setup.py
egg_info
for
package
pycparser
running
egg_info
creating
pip-egg-info\pycparser.egg-info
writing
pip-egg-info\pycparser.egg-info\PKG-INFO
writing
top-level
names
to
pip-egg-info\pycparser.egg-info\top_level.txt
writing
dependency_links
to
pip-egg-info\pycparser.egg-info\dependency_links.txt
writing
manifest
file
'
pip-egg-info\pycparser.egg-info\SOURCES.txt
'
warning
:
manifest_maker
:
standard
file
'
-
c
'
not
found
reading
manifest
file
'
pip-egg-info\pycparser.egg-info\SOURCES.txt
'
writing
manifest
file
'
pip-egg-info\pycparser.egg-info\SOURCES.txt
'
Source
in
c:\users\ali\appdata\local\temp\pip-build-ali\pycparser
has
version
2.10
","
which
satisfies
requirement
pycparser
(
from
cffi==0.8.1
)
Installing
collected
packages
:
cffi
","
pycparser
Found
existing
installation
:
cffi
0.8.2
Uninstalling
cffi
:
Removing
file
or
directory
c:\python27\lib\site-packages\cffi
Removing
file
or
directory
c:\python27\lib\site-packages\cffi-0.8.2-py2.7.egg-info
Successfully
uninstalled
cffi
Running
setup.py
install
for
cffi
Running
command
C:\Python27\python.exe
-
c
""""
import setuptools
;
__file__='c:\\users\\ali\\appdata\\local\\temp\\pip-build-ALI\\cffi\\setup.py';exec(compile(open(__file__).read().replace('\r\n
'
","
'
\
n
'
)
","
__file__
","
'
exec
'
)
)
""""
install
-
-
record
c:\users\ali\appdata\local\temp\pip-1ctxjo-record\install-record.txt
-
-
single-version-externally-managed
running
install
running
build
running
build_py
creating
build
creating
build\lib.win32-2.7
creating
build\lib.win32-2.7\cffi
copying
cffi\api.py
->
build\lib.win32-2.7\cffi
copying
cffi\backend_ctypes.py
->
build\lib.win32-2.7\cffi
copying
cffi\commontypes.py
->
build\lib.win32-2.7\cffi
copying
cffi\cparser.py
->
build\lib.win32-2.7\cffi
copying
cffi\ffiplatform.py
->
build\lib.win32-2.7\cffi
copying
cffi\gc_weakref.py
->
build\lib.win32-2.7\cffi
copying
cffi\lock.py
->
build\lib.win32-2.7\cffi
copying
cffi\model.py
->
build\lib.win32-2.7\cffi
copying
cffi\vengine_cpy.py
->
build\lib.win32-2.7\cffi
copying
cffi\vengine_gen.py
->
build\lib.win32-2.7\cffi
copying
cffi\verifier.py
->
build\lib.win32-2.7\cffi
copying
cffi\__init__.py
->
build\lib.win32-2.7\cffi
running
build_ext
building
'
_cffi_backend
'
extension
creating
build\temp.win32-2.7
creating
build\temp.win32-2.7\Release
creating
build\temp.win32-2.7\Release\c
creating
build\temp.win32-2.7\Release\c\libffi_msvc
c:\mingw\bin\gcc.exe
-
mno-cygwin
-
mdll
-
O
-
Wall
-
Ic
/
libffi_msvc
-
IC:\Python27\include
-
IC:\Python27\PC
-
c
c
/
_cffi_backend.c
-
o
build\temp.win32-2.7\Release\c\_cffi_backend.o
In
file
included
from
c
/
_cffi_backend.c:7
:
c
/
misc_win32.h:217
:
error
:
conflicting
types
for
'
int8_t
'
c:\mingw\bin
\
.
.
/
lib
/
gcc
/
mingw32
/
4.3.3
/
.
.
/
.
.
/
.
.
/
.
.
/
include
/
stdint.h:27
:
error
:
previous
declaration
of
'
int8_t
'
was
here
c
/
misc_win32.h:219
:
error
:
conflicting
types
for
'
int32_t
'
c:\mingw\bin
\
.
.
/
lib
/
gcc
/
mingw32
/
4.3.3
/
.
.
/
.
.
/
.
.
/
.
.
/
include
/
stdint.h:31
:
error
:
previous
declaration
of
'
int32_t
'
was
here
c
/
misc_win32.h:223
:
error
:
conflicting
types
for
'
uint32_t
'
c:\mingw\bin
\
.
.
/
lib
/
gcc
/
mingw32
/
4.3.3
/
.
.
/
.
.
/
.
.
/
.
.
/
include
/
stdint.h:32
:
error
:
previous
declaration
of
'
uint32_t
'
was
here
c
/
misc_win32.h:225
:
error
:
two
or
more
data
types
in
declaration
specifiers
c
/
misc_win32.h:225
:
warning
:
useless
type
name
in
empty
declaration
error
:
command
'
gcc
'
failed
with
exit
status
1
Complete
output
from
command
C:\Python27\python.exe
-
c
""""
import setuptools
;
__file__='c:\\users\\ali\\appdata\\local\\temp\\pip-build-ALI\\cffi\\setup.py';exec(compile(open(__file__).read().replace('\r\n
'
","
'
\
n
'
)
","
__file__
","
'
exec
'
)
)
""""
install
-
-
record
c:\users\ali\appdata\local\temp\pip-1ctxjo-record\install-record.txt
-
-
single-version-externally-managed
:
running
install
running
build
running
build_py
creating
build
creating
build\lib.win32-2.7
creating
build\lib.win32-2.7\cffi
copying
cffi\api.py
->
build\lib.win32-2.7\cffi
copying
cffi\backend_ctypes.py
->
build\lib.win32-2.7\cffi
copying
cffi\commontypes.py
->
build\lib.win32-2.7\cffi
copying
cffi\cparser.py
->
build\lib.win32-2.7\cffi
copying
cffi\ffiplatform.py
->
build\lib.win32-2.7\cffi
copying
cffi\gc_weakref.py
->
build\lib.win32-2.7\cffi
copying
cffi\lock.py
->
build\lib.win32-2.7\cffi
copying
cffi\model.py
->
build\lib.win32-2.7\cffi
copying
cffi\vengine_cpy.py
->
build\lib.win32-2.7\cffi
copying
cffi\vengine_gen.py
->
build\lib.win32-2.7\cffi
copying
cffi\verifier.py
->
build\lib.win32-2.7\cffi
copying
cffi\__init__.py
->
build\lib.win32-2.7\cffi
running
build_ext
building
'
_cffi_backend
'
extension
creating
build\temp.win32-2.7
creating
build\temp.win32-2.7\Release
creating
build\temp.win32-2.7\Release\c
creating
build\temp.win32-2.7\Release\c\libffi_msvc
c:\mingw\bin\gcc.exe
-
mno-cygwin
-
mdll
-
O
-
Wall
-
Ic
/
libffi_msvc
-
IC:\Python27\include
-
IC:\Python27\PC
-
c
c
/
_cffi_backend.c
-
o
build\temp.win32-2.7\Release\c\_cffi_backend.o
In
file
included
from
c
/
_cffi_backend.c:7
:
c
/
misc_win32.h:217
:
error
:
conflicting
types
for
'
int8_t
'
c:\mingw\bin
\
.
.
/
lib
/
gcc
/
mingw32
/
4.3.3
/
.
.
/
.
.
/
.
.
/
.
.
/
include
/
stdint.h:27
:
error
:
previous
declaration
of
'
int8_t
'
was
here
c
/
misc_win32.h:219
:
error
:
conflicting
types
for
'
int32_t
'
c:\mingw\bin
\
.
.
/
lib
/
gcc
/
mingw32
/
4.3.3
/
.
.
/
.
.
/
.
.
/
.
.
/
include
/
stdint.h:31
:
error
:
previous
declaration
of
'
int32_t
'
was
here
c
/
misc_win32.h:223
:
error
:
conflicting
types
for
'
uint32_t
'
c:\mingw\bin
\
.
.
/
lib
/
gcc
/
mingw32
/
4.3.3
/
.
.
/
.
.
/
.
.
/
.
.
/
include
/
stdint.h:32
:
error
:
previous
declaration
of
'
uint32_t
'
was
here
c
/
misc_win32.h:225
:
error
:
two
or
more
data
types
in
declaration
specifiers
c
/
misc_win32.h:225
:
warning
:
useless
type
name
in
empty
declaration
error
:
command
'
gcc
'
failed
with
exit
status
1
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
Rolling
back
uninstall
of
cffi
Replacing
c:\python27\lib\site-packages\cffi
Replacing
c:\python27\lib\site-packages\cffi-0.8.2-py2.7.egg-info
Command
C:\Python27\python.exe
-
c
""""
import setuptools
;
__file__='c:\\users\\ali\\appdata\\local\\temp\\pip-build-ALI\\cffi\\setup.py';exec(compile(open(__file__).read().replace('\r\n
'
","
'
\
n
'
)
","
__file__
","
'
exec
'
)
)
""""
install
-
-
record
c:\users\ali\appdata\local\temp\pip-1ctxjo-record\install-record.txt
-
-
single-version-externally-managed
failed
with
error
code
1
in
c:\users\ali\appdata\local\temp\pip-build-ALI\cffi
Exception
information
:
Traceback
(
most
recent
call
last
)
:
File
""""
c:\python27\lib\site-packages\pip\basecommand.py
""""
","
line
139
","
in
main
status
=
"self.run(options, args)"
File
""""
c:\python27\lib\site-packages\pip\commands\install.py
""""
","
line
271
","
in
run
"requirement_set.install(install_options, global_options, root=options.root_path)"
File
""""
c:\python27\lib\site-packages\pip\req.py
""""
","
line
1185
","
in
install
"requirement.install(install_options, global_options, *args, **kwargs)"
File
""""
c:\python27\lib\site-packages\pip\req.py
""""
","
line
592
","
in
install
cwd=self.source_dir
","
filter_stdout=self._filter_install
","
show_stdout=False
)
File
""""
c:\python27\lib\site-packages\pip\util.py
""""
","
line
662
","
in
call_subprocess
%
(
command_desc
","
proc.returncode
","
cwd
)
)
InstallationError
:
Command
C:\Python27\python.exe
-
c
""""
import setuptools
;
__file__='c:\\users\\ali\\appdata\\local\\temp\\pip-build-ALI\\cffi\\setup.py';exec(compile(open(__file__).read().replace('\r\n
'
","
'
\
n
'
)
","
__file__
","
'
exec
'
)
)
""""
install
-
-
record
c:\users\ali\appdata\local\temp\pip-1ctxjo-record\install-record.txt
-
-
single-version-externally-managed
failed
with
error
code
1
in
c:\users\ali\appdata\local\temp\pip-build-ALI\cffi
Skipping
link
https://pypi.python.org/packages/cp26/c/cffi/cffi-0.8.1-cp26-none-win32.whl#md5=43c5e5dee0623bedecddda42a3244b81
(
from
https://pypi.python.org/simple/cffi/
)
;
unknown
archive
format
:
.
whl
-
it
seems
your
pip
is
old
and
doesn't
support
wheels
.
Try
updating
pip
first
.
I
do
not
think
it
is
the
language
data
structure
type
that
should
be
differentiated
","
but
it
is
more
about
the
data
you
send
over
.
Note
that
","
different
languages
may
have
different
language
structures
and
so
on
.
That
is
just
really
low-level
details
.
What
is
more
important
is
what
you
send
.
You
could
look
into
the
following
example
how
the
serialization
/
deserialization
works
with
json
format
in
QtCore
.
Json
is
also
supported
in
python
quite
well
by
the
json
module
","
so
you
would
have
no
issue
on
the
server
side
to
deserialize
it
:
JSON
Save
Game
Example
This
is
basically
the
important
part
that
would
give
you
some
hint
on
the
client
side
.
Do
not
get
lost
at
saving
into
a
file
.
It
is
basically
writing
the
raw
bytes
to
the
file
","
which
you
would
replace
by
sending
over
the
network
:
...
and
then
you
would
do
something
like
this
on
the
server
side
","
again
instead
of
reading
from
file
","
you
would
read
from
the
network
","
but
that
is
not
a
biggie
as
both
are
IO
.
You
could
use
raw
protocol
to
design
your
own
","
or
just
use
an
extending
.
I
would
suggest
to
go
with
something
standard
","
like
http
(
tcp
/
udp
)
.
Then
","
you
would
only
need
to
define
the
json
format
for
your
own
data
","
and
not
deal
with
all
the
rest
","
like
one-way
or
two-way
communication
","
transaction
identifier
against
reply
attack
","
timestamp
","
data
size
and
so
on
.
This
would
allow
you
to
truly
concentrate
on
the
important
stuff
for
you
.
Once
","
you
have
your
own
json
format
defined
","
you
could
look
into
the
QtNetwork
module
to
send
post
","
get
","
put
and
delete
requests
as
you
wish
.
You
would
probably
work
closely
with
the
QNetworkManager
","
QNetworkReply
classes
","
and
so
on
.
Here
you
can
find
a
simple
client
implementation
in
Qt
with
QtCore's
json
for
a
simple
pastebin
functionality
:
The
JSON
is
defined
in
here
:
http://sayakb.github.io/sticky-notes/pages/api/
For
sure
","
it
is
not
the
only
way
of
doing
it
","
e.g
.
if
you
need
efficiency
","
you
may
well
look
into
a
binary
format
like
capnproto
.
I'm
a
begginer
in
network
programming
","
so
","
sorry
if
my
questions
may
appear
a
little
obvious
.
I'm
trying
to
send
some
data
from
Qt
application
to
a
Python
server
which
will
process
them
and
send
back
some
answer
.
the
methods
that
allows
me
to
send
data
in
the
QTcpSocket
class
are
:
my
application
will
manage
:
authentification
","
sending
and
receiving
some
complexe
data
like
struct
","
and
files
.
I've
many
questions
about
this
situation
:
Are
the
methods
mentioned
above
sufficient
to
send
complexe
data
","
and
how
?
how
to
deal
with
the
data
types
in
the
server
side
(
with
Python
)
?
do
you
think
I
should
use
an
other
protocole
like
HTTP
(
with
the
QNetworkAccessManager
class
)
?
Trying
to
answer
your
questions
:
Are
the
methods
mentioned
above
sufficient
to
send
complexe
data
","
and
how
?
Well
","
yes
","
sending
a
raw
byte
array
is
the
lowest
level
format
.
However
","
you
need
a
something
that
can
get
you
uniquely
from
your
complex
data
to
the
byte
array
and
from
the
byte
array
back
to
your
complex
data
.
This
process
is
called
in
different
ways
","
encoding
","
serializing
","
marshalling
....
But
in
general
it
just
means
creating
a
system
for
encoding
complex
structures
into
a
sequence
of
bytes
or
characters
There
are
many
you
can
choose
from
:
ASN.1
","
JSON
","
XML
","
Google's
protocol
buffers
or
MIME
....
You
can
even
design
your
own
(
e.g
.
a
simple
schema
is
using
TLV
:
(
Tag-Length-Value
)
","
where
Tag
is
an
identifier
of
the
Type
and
Value
can
be
a
basic
type
[
and
you
have
to
define
a
representation
for
each
type
that
you
consider
basic
]
or
again
one
or
more
TLV
)
","
Length
indicates
how
many
bytes
/
characters
are
used
to
encode
the
Value
.
What
to
choose
depends
a
lot
of
where
you
encode
(
language
/
platform
)
and
where
you
decode
(
language
/
platform
)
and
your
requirements
for
speed
","
bandwidth
usage
","
transport
","
whether
messages
should
be
inspected
...
etc.
If
you're
dealing
with
heterogenous
architectures
you
might
need
to
think
about
endianness
.
Finally
","
you
should
distinguish
between
the
format
(
i.e.
how
the
complex
structure
is
expressed
as
a
sequence
of
bytes
in
the
line
)
and
the
library
used
for
encoding
(
or
the
library
used
for
decoding
)
.
Sometimes
they
will
be
linked
","
sometimes
for
the
same
format
you
will
have
a
choice
of
libraries
to
use
.
how
to
deal
with
the
data
types
in
the
server
side
(
with
Python
)
?
So
","
here
you
have
a
requirement
...
if
you're
going
for
a
externally
provided
format
","
you
must
make
sure
it
has
a
python
library
able
to
decode
it
.
if
you're
going
for
a
home-grown
solution
","
one
of
things
you
should
define
is
the
expression
of
your
complex
C
+
+
structures
as
Python
structures
.
An
additional
possibility
is
to
do
everything
in
C
+
+
","
and
for
the
python
server
side
use
one
of
the
systems
for
creating
python
extensions
in
C
+
+
(
e.g
.
boost-python
or
swig
....
)
do
you
think
I
should
use
an
other
protocol
like
HTTP
(
with
the
QNetworkAccessManager
class
)
?
It
depends
on
what
you
try
to
do
.
There
are
many
HTTP
libraries
widely
available
that
you
can
use
on
different
languages
and
different
architectures
.
You
still
need
to
solve
the
problem
of
deciding
the
formatting
of
your
information
(
although
HTTP
have
some
defined
practices
)
.
In
addition
HTTP
is
clearly
biased
towards
the
communication
of
a
client
with
a
server
","
with
the
action
always
initiated
by
the
client
.
Things
get
complex
(
or
less
widely
supported
)
when
is
the
server
the
one
that
needs
to
initiate
the
communication
or
the
one
that
needs
to
send
spontaneous
information
.
With
Scrapy
","
it
seems
you
can't
do
this
.
I
have
my
own
function
to
remove
specific
node
(
with
its
children
)
:
Hope
it
help
I'm
using
Scrapy
to
crawl
a
site
with
some
odd
formatting
conventions
.
The
basic
idea
is
that
I
want
all
the
text
and
sub-elements
of
a
certain
div
","
EXCEPT
a
few
div
in
the
middle
.
Here
is
the
piece
of
code
below
:
-
The
final
output
should
look
like
:
-
Here
is
the
piece
of
my
Scrapy
code
.
Please
suggest
the
addition
to
this
script
:
-
Here
are
the
following
xpath
that
I
experimented
with
but
did
not
get
the
desired
results
:
-
Thanks
in
advance
...
You
can
use
non-recursive
algorithm
to
calculate
Fibonacci
numbers
Or
use
the
formula
:
System
:
Ubuntu
14
IDE
:
PyCharm
Community
Edition
3.1.1
Python
:
2.7.6
Algorithm
with
recurrent
call
:
Test
:
Value
in
test
is
intended
.
Around
value
26175
test
sometimes
pass
but
sometimes
it
is
terminated
with
message
:
Process
finished
with
exit
code
139
I
understand
that
test
result
somehow
depends
from
hardware
resources
but
I'm
looking
for
more
precise
answer
from
stackoverflow
seniors
:
)
I
am
trying
to
get
data
data
from
backend
through
ajax
in
django
.
But
i
am
getting
error
over
there
.
It
directly
going
to
the
error
function
of
ajax
.
But
in
view
data
is
getting
printed
.
views.py
ajax
call
is
please
help
me
out
when
i
print
print
response
i
am
getting
the
data
in
view
.
Data
is
not
getting
passed
to
the
ajax
success
function
.
I
think
you
should
use
instead
of
uninitialized
data
variable
in
your
javascript
code
.
I'm
trying
to
solve
a
second
order
ODE
using
odeint
from
scipy
.
The
issue
I'm
having
is
the
function
is
implicitly
coupled
to
the
second
order
term
","
as
seen
in
the
simplified
snippet
(
please
ignore
the
pretend
physics
of
the
example
)
:
in
this
case
I
realise
it
is
possible
to
algebraically
solve
for
the
implicit
variable
","
however
in
my
actual
scenario
there
is
a
lot
of
logic
between
F_r
and
the
evaluation
of
a
and
algebraic
manipulation
fails
.
I
believe
the
DAE
could
be
solved
using
MATLAB's
ode15i
function
","
but
I'm
trying
to
avoid
that
scenario
if
at
all
possible
.
My
question
is
-
is
there
a
way
to
solve
implicit
ODE
functions
(
DAE
)
in
python( scipy preferably)
?
And
is
there
a
better
way
to
pose
the
problem
above
to
do
so
?
As
a
last
resort
","
it
may
be
acceptable
to
pass
a
from
the
previous
time-step
.
How
could
I
pass
dydt
[1]
back
into
the
function
after
each
time-step
?
if
algebraic
manipulation
fails
","
you
can
go
for
a
numerical
solution
of
your
constraint
","
running
for
example
fsolve
at
each
timestep
:
Clearly
this
will
slow
down
your
time
integration
.
Always
check
that
fsolve
finds
a
good
solution
","
and
flush
the
output
so
that
you
can
realize
it
as
it
happens
and
stop
the
simulation
.
About
how
to
""""
cache
""""
the
value
of
a
variable
at
a
previous
timestep
","
you
can
exploit
the
fact
that
default
arguments
are
calculated
only
at
the
function
definition
","
Notice
that
in
order
for
the
trick
to
work
the
cache
parameter
must
be
mutable
","
and
that's
why
I
use
a
list
.
See
this
link
if
you
are
not
familiar
with
how
default
arguments
work
.
Notice
that
the
two
codes
DO
NOT
produce
the
same
result
","
and
you
should
be
very
careful
using
the
value
at
the
previous
timestep
","
both
for
numerical
stability
and
precision
.
The
second
is
clearly
much
faster
though
.
I'm
saving
date
in
UTC
.
I
wan't
to
change
that
date
to
automatic
user
timezone
.
my
settings
my
template
I
tried
django-tz-detect
https://github.com/adamcharnock/django-tz-detect
but
I
can't
implement
that
correctly
.
Please
suggest
a
good
way
to
solve
this
issue
.
I
got
a
solution
from
here
I
created
a
view
function
context
in
my
base
template
my
sales
template
I
don't
know
this
is
the
correct
method
.
but
this
solution
works
for
me
.
And
use
this
b_reader
to
construct
your
csv
output
.
To
write
to
a
csv
using
b_reader
(
which
contains
the
edits
)
for
python
3
and
above
for
python
2.7
So
I
have
managed
to
now
successfully
write
a
python
script
that
will
:
Read
a
CSV
file
of
coordinates
written
as
such
:
Then
using
this
:
Display
this
:
Now
I
am
able
to
write
a
CSV
file
and
output
it
but
I
am
unsure
how
to
then
write
a
new
output
CSV
file
","
or
even
just
display
it
as
a
list
in
the
shell
","
so
that
it
would
display
as
.
I've
been
reading
other
questions
/
answers
but
can't
find
anything
to
append
a
new
column
using
a
true
or
false
value
from
within
the
script
.
I
am
trying
to
get
Premier
league
table
with
this
code
:
Everything
works
perfect
","
except
I
get
results
in
unicode
.
How
to
convert
it
into
plain
text
?
You
can
change
the
line
to
"text.encode(""utf-8"")"
:
Lots
of
info
in
the
BeautifulSoup
docs
I
want
to
make
a
cluster
style
dendrogram
in
bokeh
.
I
like
this
bokeh
dot
plot
example
","
but
i
want
to
make
lines
curvy
and
show
a
cluster
strcuture
like
this
:
http://bl.ocks.org/mbostock/4063570
here
is
code
from
dot
plot
from
bokeh
example
:
Any
suggestion
on
how
to
make
lines
curvy
and
show
the
clustering
relation
like
in
dendrogram
.
This
picture
is
shown
just
to
show
the
idea
of
straight
line
with
dot
.
actual
target
is
to
show
the
clustering
relation
of
all
lines
coming
from
single
point
.
This
is
not
impossible
to
render
in
Bokeh
(
as
of
0.4.4
)
but
there
is
no
built-in
support
to
help
with
graphs
and
graph
layout
yet
.
You
would
have
to
compute
or
use
a
library
to
compute
the
line
points
for
the
edges
","
etc.
and
then
pass
those
to
Bokeh
.
Graph
support
is
on
our
roadmap
but
it
will
probably
not
be
until
later
this
year
unless
a
complete
PR
gets
dropped
in
our
lap
.
You
will
need
to
perform
a
query
","
and
depending
how
many
entities
returning
all
them
in
a
single
request
will
either
not
be
possible
or
practical
.
You
will
then
need
to
use
cursors
with
the
query
.
You
should
read
the
section
of
Queries
in
the
ndb
docs
-
they
are
clear
about
what
needs
to
be
done
-
https://developers.google.com/appengine/docs/python/ndb/queries
A
simple
query
for
all
items
and
to
return
the
details
you
want
as
a
list
of
Json
records
you
would
do
the
following
","
using
the
map
method
of
a
query
","
which
calls
the
supplied
function
or
classmethod
.
It
doesn't
expect
a
method
of
the
entity
thats
why
I
don't
use
toJSON
directly
.
You
may
need
to
fiddle
with
your
toJSON
method
","
have
a
look
at
what
results
looks
like
when
you
run
it
.
results
may
also
need
to
explicitly
converted
to
json
","
so
you
may
want
to
defer
the
explicit
json.encode
till
after
after
you
have
run
the
query
.
Given
this
model
class
I
am
trying
to
run
a
query
to
return
all
the
student
names
","
along
with
the
score
for
each
student
in
JSON
format
.
I
already
ran
a
query
on
the
datastore
and
was
able
to
retrieve
information
regarding
each
student
using
But
have
no
clue
how
to
get
ALL
of
the
students.I
have
read
examples
given
by
Google
","
but
am
still
lost.I
know
this
time
around
i'll
need
a
loop
","
but
that's
about
all
i
can
come
up
with.Any
ideas
would
be
appreaciated
.
I
â€˜ve
written
a
function
to
plot
the
histogram
for
an
1-D
array
theta
.
But
one
thing
I
do
not
like
in
this
function
is
that
the
data
is
in
the
code
.
Could
you
know
how
to
keep
the
data
in
a
file
and
to
make
the
function
read
them
from
the
file
?
Since
the
data
is
usually
much
larger
.
PS
:
The
code
is
If
you
have
a
txt
file
with
the
separated-comma
theta
values
in
the
first
line
and
the
number
of
bins
in
the
second
line
:
You
can
add
this
:
Example
:
http://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html
Assuming
each
data
entry
is
on
separate
lines
in
your
theta.dat
file
and
theta.dat
is
your
current
working
directory
.
just
run
your
code
as
you
have
","
I
have
put
in
the
print
statement
just
to
show
you
how
the
data
structure
looks
.
some
webcams
need
a
warmup
time
","
and
deliver
empty
frames
on
startup
.
you
want
to
check
for
that
.
also
","
who
said
","
that
cv2.rectangle
returns
anything
?
where
did
you
get
that
idea
?
from
SO
?
I'm
working
on
Face
Detection
in
OpenCV(2.4.6)
and
python(2.7)
.
I
have
a
very
simple
code
","
but
its
not
giving
me
the
desired
output
.
This
is
my
code
:
When
I
run
this
code
","
my
webcam
will
start
and
it
will
give
me
a
blank
window
like
this
Then
the
webcam
will
turn
off
","
and
in
the
editor
I
will
get
an
error
as
follows
:
Any
suggestions
are
welcome
.
Thanks
in
advance
.
In
absence
of
other
responses
I've
settled
down
with
a
home-made
solution
below
.
It
probably
won't
reduce
memory
footprint
when
reading
zip
files
yet
it
might
improve
latency
when
zip
header
is
read
first
.
Usage
example
:
It
seems
zipfile.ZipFile
requires
random
access
which
is
is
not
supported
by
urllib2's
returned
""""
file-like
""""
object
.
I've
tried
wrapping
it
with
io.BufferedRandom
but
got
:
IDE
:
PyCharm
Community
Edition
3.1.1
Python
:
2.7.6
I
using
DDT
for
test
parameterization
http://ddt.readthedocs.org/en/latest/example.html
I
want
to
choose
and
run
parameterized
test
method
from
test
class
in
PyCharm
->
see
example
:
When
I
navigate
to
the
first
test
method
test_print_value
in
code
and
hit
ctrl+Shift+F10
(
or
use
Run
Unittest
test_print
...
option
from
context
menu
)
then
test
is
executed
.
When
I
try
the
same
with
parameterized
test
I
get
error
:
And
output
contains
:
However
when
I
run
all
tests
in
class
(
by
navigating
to
test
class
name
in
code
and
using
mentioned
run
test
option
)
all
parameterized
and
non
parameterized
tests
are
executed
together
without
errors
.
The
problem
is
how
to
independently
run
prameterized
method
from
the
test
class
-
workaround
is
putting
one
parameterized
test
per
test
class
but
it
is
rather
messy
solution
.
Actually
this
is
issue
in
PyCharm
utrunner.py
who
runs
unittests
.
If
you
are
using
DDT
there
is
a
wrapper
@ddt
and
@data
-
it
is
responsible
for
creating
separate
tests
for
each
data
entry
.
In
the
background
these
tests
have
different
names
e.g
.
This
would
create
tests
named
:
-
test_print_1_1
-
test_print_2_2
When
you
try
to
run
one
test
from
the
class
(
Right
Click
->
Run
'
Unittest
test_print
'
)
PyCharm
has
a
problem
to
load
your
tests
print_1_1
","
print_2_2
as
it
is
trying
to
load
test_print
test
.
When
you
look
at
the
code
of
utrunner.py
:
and
you
will
debug
it
you
see
that
issue
.
Ok
.
So
my
fix
for
that
is
to
load
proper
tests
from
the
class
.
It
is
just
a
workaround
and
it
is
not
perfect
","
however
as
DDT
is
adding
a
TestCase
as
another
method
to
the
class
it
is
hard
to
find
a
different
way
to
detect
right
test
cases
than
comparing
by
string
.
So
instead
of
:
you
can
try
to
use
:
Checking
if
digit
is
found
after
the
main
name
is
a
workaround
to
exclude
similar
test
cases
:
test_print
test_print_another_case
But
of
course
it
would
not
exclude
cases
:
test_if_prints_1
test_if_prints_2
So
in
the
worst
case
","
if
we
haven't
got
a
good
name
convention
we
will
run
similar
tests
","
but
in
most
cases
it
should
just
work
for
you
.
When
I
ran
into
this
error
","
it
was
because
I
had
implemented
an
init
function
as
follows
:
When
I
changed
it
to
the
following
","
it
worked
properly
:
The
problem
was
caused
by
me
not
propagating
the
*
args
and
*
*
kwargs
through
properly
.
If
you
could
slightly
modify
your
call
patterns
","
I
could
imagine
that
it
is
feasible
:
In
this
way
","
you
could
perfectly
make
it
work
with
__getattr__
if
you
make
boot()
return
an
object
which
provides
these
methods
.
If
not
","
you
could
try
one
of
the
following
:
Just
call
cfg.instance.get('a')
Give
Service
a
metaclass
:
Here
the
metaclass
provides
an
appropriate
way
to
deflect
attribute
accesses
to
the
class
to
one
of
its
attributes
.
Be
aware
that
under
Py3
","
the
syntax
is
slightly
different
.
IIRC
","
it
is
(
However
","
it
seems
slightly
wrong
to
me
to
use
the
Service
class
just
for
this
kind
of
encapsulation
.
You
can
do
this
cfg
=
boot(...)
from
my
first
idea
at
one
place
and
subsequently
use
cfg
for
every
access
.
You
might
even
(
misleadingly
)
name
it
Config
:
or
and
subsequently
use
cfg
(
or
Config
)
everywhere
you
want
.
)
I'm
trying
to
build
a
Service
class
that
holds
the
Registry
instances
.
The
Registry
and
Service
classes
are
the
following
;
What
I
currently
want
is
after
the
boot
function
is
called
","
The
registry
functions
should
be
accessible
from
Service
class
statically
without
defining
functions
in
Service
class
.
So
","
after
booting
with
the
following
code
","
I
want
to
use
get
function
.
NOTE
:
Config
object
is
an
extension
of
Registry
I've
tried
__getattr__
magic
method
but
it
doesn't
work
on
static
.
I've
tried
__call__
but
after
that
I
need
to
call
Service()
.
get()
which
is
not
that
I
want
.
So
","
is
it
possible
to
make
this
happen
in
python
?
I
don't
want
to
redefine
class
methods
in
Service
layer
.
NOTE
:
In
PHP
","
there
exists
__callStatic()
function
that
does
exactly
that
I
want
.
However
","
I
can't
find
an
implementation
on
python
.
stack
it
first
and
then
use
value_counts
:
The
result
looks
like
:
This
question
has
already
been
asked
on
StackOverflow
I
have
a
DataFrame
like
following
.
I
want
to
count
each
item
in
the
DataFrame
.
How
to
do
it
?
You
are
creating
a
new
array
in
every
j-loop-step
.
So
only
the
last
row
will
be
filled
in
the
final
z
I'm
not
sure
what
I'm
doing
wrong
in
building
up
an
array
.
I
do
some
calculations
according
to
which
my
completed
array
","
z
","
should
look
like
this
:
But
my
array
actually
turns
out
with
the
entire
first
column
being
zeros
","
like
this
:
I
don't
understand
where
I'm
going
wrong
.
Here's
my
code
","
which
seems
simple
enough
:
As
you
can
see
","
there's
another
array
that
I'm
calculating
","
called
weights
","
but
that
doesn't
seem
to
have
any
issues
.
It
has
all
rows
and
columns
as
expected
.
What's
wrong
with
the
way
I'm
handling
z
?
I'm
preparing
exercises
for
school
classes
involving
Python's
turtle
library
.
The
students
are
already
drawing
terrific
pictures
","
but
I
want
them
to
be
able
to
detect
existing
pictures
and
colours
in
order
to
modify
the
behaviour
of
their
program
.
For
example
I
would
like
to
provide
them
with
code
which
draws
a
maze
using
turtle
","
and
then
they
can
write
the
code
to
navigate
the
turtle
around
the
maze
(
don't
worry
","
I'll
start
simpler
)
.
Is
there
a
way
to
detect
the
colour
of
the
pixels
already
drawn
by
the
turtle
?
Thanks
!
Turtle
uses
Tkinter
canvas
","
which
you
can
get
using
turtle.getcanvas()
","
and
according
to
this
you
cannot
read
the
colour
of
a
pixel
without
using
a
workaround
of
converting
the
canvas
to
a
picture
(
bitmap
)
and
read
the
bitmap
.
You
could
try
to
keep
an
open
array
to
work
as
the
bitmap
of
your
canvas
and
update
it
yourself
as
you
draw
new
elements
on
the
canvas
","
although
that
seems
impractical
unless
the
maze
is
simple
and
'
squary
'
.
You
need
to
enable
SSL
by
adding
the
following
to
your
app.yaml
:
EDIT
:
the
bug
below
hasn't
been
fixed
but
I
contributed
a
patch
to
the
stripe
python
bindings
to
get
around
it
.
There
is
currently
a
bug
in
the
app
engine
dev
server
(
It
is
on
my
to
do
list
to
try
and
fix
it
)
which
can
be
circumvented
by
doing
the
following
:
add
""""
_ssl
""""
and
""""
_socket
""""
keys
to
the
dictionary
_WHITE_LIST_C_MODULES
in
/
Applications
/
GoogleAppEngineLauncher.app
/
Contents
/
Resources
/
GoogleAppEngine-default.bundle
/
Contents
/
Resources
/
google_appengine
/
google
/
appengine
/
tools
/
devappserver2
/
python
/
sandbox.py
Replace
the
socket.py
file
provided
by
google
in
/
Applications
/
GoogleAppEngineLauncher.app
/
Contents
/
Resources
/
GoogleAppEngine-default.bundle
/
Contents
/
Resources
/
google_appengine
/
google
/
appengine
/
dis27
from
the
socket.py
file
from
your
Python
framework
.
I've
just
updated
Stripe
API
library
to
the
latest
version
and
it
stopped
working
on
Google
AppEngine
due
to
fact
that
GAE
is
blocking
imports
of
some
packages
such
as
sockets
and
SSL
.
Are
there
any
chances
to
make
it
working
on
Google
AppEngine
?
UPDATEing
requires
starting
a
transaction
","
then
keeping
a
write-ahead-log
of
the
changes
that
the
update
would
make
","
then
ensuring
that
succeeds
before
then
writing
changes
back
to
the
main
table
","
and
then
ensuring
all
of
that
completes
successfully
...
SELECTing
is
reading
then
doing
a
simple
""""
in
memory
if
""""
to
swap
the
values
over
for
each
row
...
so
it's
bound
to
be
faster
(
plus
disk
reads
are
almost
always
a
lot
faster
than
disk
writes
(
which
a
SELECT
really
doesn't
require
)
)
...
I
am
trying
to
update
a
couple
of
columns
in
my
SQLite
database
using
:
However
this
seems
to
take
a
very
long
time
to
even
update
one
column
.
I
have
resorted
to
using
:
in
my
SELECT
queries
which
works
a
lot
quicker
","
however
is
there
a
reason
why
the
UPDATE
method
is
so
slow
?
(
I
only
have
670K
rows
in
my
database
)
NOTE
:
My
computer
is
fairly
high-end
and
when
the
UPDATE
is
running
","
there
doesn't
seem
to
be
much
pressure
on
my
desktops
resources
.
Do
you
have
the
tools
available
to
do
an
""""
explain
""""
on
your
query
:
to
see
what
(
if
any
)
indexes
your
sql
query
is
using
.
if
your
sql
query
is
resulting
in
a
full
table
scan
over
670K
rows
","
then
it
could
be
slowish
.
if
you
are
using
ubuntu
or
a
similiar
based
linux
distribution
","
Sqliteman
will
do
this
for
you
.
After
reading
this
wonderful
article
about
speeding-up
looping
in
Python
by
using
built-in
iterators
and
implicit
loops
I
have
tried
it
in
my
code
.
It
worked
well
on
many
parts
","
but
one
small
part
still
annoys
me
-
the
values
I
iterate
over
are
sometimes
stored
as
fields
of
a
class
which
are
the
dictionary
values
","
and
I
can't
get
rid
of
looping
over
my
dictionary
to
retrieve
them
.
Here
is
a
simplified
version
of
my
code
:
So
the
dictionary
d
have
tuples
(
x
","
y
)
as
keys
and
Pair
instances
as
values
","
and
my
goal
is
to
sum
the
radii
of
the
given
sub_list
of
pairs
(
sub_list
could
be
the
entire
dictionary
)
.
Is
there
any
""""
broadcasting
""""
technique
for
that
","
or
the
loop
in
the
marked
line
is
inevitable
?
BTW
-
I
am
a
beginner
","
so
any
useful
comment
about
the
code
(
including
styling
and
Pythonish
will
be
appreciated
.
Thanks
!
well
your
code
is
not
so
bad
","
though
there's
one
useless
thing
","
which
is
your
object
Pair
.
You
may
just
as
well
store
the
radius
as
value
in
the
dict
:
I
can't
get
rid
of
looping
over
my
dictionary
to
retrieve
them
.
Reread
your
code
!
You're
not
looping
over
the
dictionary
","
you're
iterating
over
the
sub_list
!
Maybe
this
is
a
syntax
problem
","
here's
how
you
could
rewrite
that
iteration
:
And
think
about
it
","
what
you
want
is
to
get
the
precalculated
radius
value
for
each
element
of
the
sublist
.
So
there's
actually
no
other
way
than
iterating
over
the
sublist
!
Whether
you
do
it
using
a
list
comprehension
or
a
map
is
mostly
a
matter
of
personal
preference
","
even
though
the
list
comprehension
is
more
efficient
:
If
you
talk
about
complexity
","
consider
the
size
of
d
being
m
","
and
the
one
of
values
being
n
","
then
:
is
O(n)
!
If
you
didn't
want
A.f()
to
be
called
","
nothing
forces
you
to
use
super()
.
f()
:
The
above
does
exactly
what
your
C
#
code
does
;
create
two
separate
instances
and
invokes
f()
on
each
","
where
B.f()
completely
replaces
what
A.f()
does
.
You
can
still
take
the
unbound
function
from
A
and
pass
in
an
instance
of
B()
:
or
you
could
look
up
the
parent
method
without
a
reference
to
the
parent
class
with
:
This
is
however
quite
different
from
your
original
C
#
sample
.
I
am
going
to
implement
below
C
#
piece
of
code
in
Python
.
As
a
matter
of
fact
","
when
I
instantiate
an
object
from
class
B
","
I
am
prone
to
get
some
flexibility
to
choose
between
the
parent-version
or
child
version
of
the
overrided
method
","
just
like
below
:
Where
the
output
is
:
But
I
have
no
clear
idea
to
the
same
with
super
keyword
in
python
.
I
have
written
below
code
:
where
the
output
is
:
which
is
not
my
desired
output
;
in
the
latter
case
","
i.e.
Python
one
","
the
two
strings
are
generated
just
by
one
method
call
.
Would
you
please
help
me
to
change
the
python
code
so
that
the
desired
output
is
acquired
?
if
you
are
only
needing
COM15
","
I
would
simplify
it
and
get
the
for
loop
out
of
there
.
Does
the
code
below
work
?
If
not
","
something
on
Windows
is
hung
up
.
I'm
working
on
Windows
environment
and
with
Arduino
.
I
have
a
python
script
","
not
written
by
me
","
that
uses
the
serial
port
of
linux
.
Since
I
work
on
Windows
I
should
convert
this
code
in
order
to
let
it
work
on
windows
to
configure
and
then
use
the
serial
port
used
by
the
Arduino
attached
at
the
pc
.
My
port
is
the
COM15.i
uses
windows
7
32
bit
.
My
python
distribution
is
the
2.7.5
and
i
have
installed
the
pyserial
module
32
bit
.
The
piece
of
code
of
interest
is
the
following
:
The
""""
DEFAULT_DEVICE
""""
variable
is
the
one
that
define
the
serial
port
.
It
is
defined
as
:
I
think
I
should
simply
modify
this
variable
in
the
format
for
the
serial
in
windows
in
order
to
make
the
script
work
and
correctly
configure
the
serial
.
Searching
online
I
have
found
that
simply
putting
:
it
should
work
.
I
have
tried
in
this
way
but
when
launching
the
code
I
get
the
raise
"RuntimeError(""unable to configure serial port"")"
defined
in
the
code
.
Any
idea
of
the
problem
?
Maybe
the
format
I
give
to
DEFAULT_DEVICE
variable
is
not
correct
or
maybe
I
should
modify
something
else
in
the
code
above
.
Thank
you
for
every
help
.
Groupby
is
your
friend
.
This
will
scale
very
well
;
only
a
small
constant
in
the
number
of
features
.
It
will
be
roughly
O(number of groups)
Create
some
test
data
","
group
sizes
are
7-12
","
70k
groups
Create
a
frame
where
you
select
the
first
3
rows
and
the
final
row
from
each
group
(
note
that
this
WILL
handle
groups
of
size
<
4
","
however
your
final
row
may
overlap
another
","
you
may
wish
to
do
a
groupby.filter
to
remedy
this
)
And
pretty
fast
For
further
manipulation
you
usually
should
stop
here
and
use
this
(
as
its
in
a
nice
grouped
format
that's
easy
to
deal
with
)
.
If
you
want
to
translate
this
to
a
wide
format
I
have
the
following
problem
.
Lets
say
this
is
my
CSV
So
","
I
have
rows
which
can
be
grouped
by
id
.
I
want
to
create
a
csv
like
below
as
an
output
.
So
","
I
want
to
be
able
to
chose
the
number
of
rows
I
will
grab
to
convert
into
columns
(
always
starting
from
the
first
row
of
an
id
)
.
In
this
case
I
grabbed
3
rows
.
I
will
also
then
skip
one
or
more
rows
(
in
this
case
only
one
skip
)
to
take
the
final
columns
from
the
last
row
of
the
same
id
group
.
And
for
reasons
","
I
want
to
use
a
data
frame
.
After
struggling
for
a
3-4
hours
.
I
found
out
a
solution
as
given
below
.
But
my
solution
is
very
slow
.
I
have
about
700
","
000
rows
and
may
be
around
70
","
000
groups
of
ids
.
The
code
above
at
model=3
takes
almost
an
hour
on
my
4GB
4
Core
Lenovo
.
I
need
to
go
to
model
=
maybe
10
or
15
.
I
am
still
novice
in
Python
and
I
am
sure
there
can
be
several
changes
that
will
make
this
fast
.
Can
some
one
explain
deeply
how
I
can
improve
on
the
code
.
Thanks
a
ton
.
model
:
number
of
rows
to
grab
The
only
way
to
do
such
thing
is
to
create
a
'
generic
'
template
","
that
would
contain
exactly
what
you've
posted
.
Just
pass
the
HTML
variable
to
that
template
(
so
","
yes
","
render
it
normally
)
and
mark
it
with
the
safe
template
filter
.
EDIT
Well
","
sorry
","
it's
not
the
only
way
to
do
this
.
You
could
also
create
a
decorator
that
would
accept
a
HttpResponse
and
wrap
its
content
around
","
or
you
could
even
create
a
middleware
for
this
","
but
the
above
answer
seems
to
be
the
most
simple
one
.
I
have
","
what
I
believe
","
to
be
some
complex
processing
of
my
model
data
that
would
be
easier
to
take
care
of
inside
my
view
than
inside
of
an
html
template
.
Thus
","
I
would
be
returning
the
raw
html
inside
of
HttpResponse
.
However
","
I
would
still
like
to
get
the
benefit
of
template
inheritance
that
render_to_response
provides
.
Thus
","
I
would
like
to
do
something
like
the
following
:
I
don't
require
the
use
of
any
other
template
tags
or
variable
evaluation
.
Is
this
possible
in
Django
?
How
do
I
best
go
about
doing
this
?
I
am
getting
an
error
when
compiling
python
to
exe
.
the
error
is
showing
a
missing
module
but
when
I
install
","
the
pip
cannot
find
it
.
How
can
I
install
those
modules
?
The
following
modules
appear
to
be
missing
:
[
'
_scproxy
'
","
'
email.Encoders
'
","
'
email.MIMEBase
'
","
'
win32evtlog
'
","
'
win32evtlogutil
'
]
*
binary
dependencies
*
Your
executable(s)
also
depend
on
these
dlls
which
are
not
included
","
USER32.DLL
you
may
or
may
not
need
to
distribute
them
.
modified
from
the
example
at
the
bottom
here
:
http://www.py2exe.org/index.cgi/ListOfOptions
You
could
try
this
setup.py
But
I
personally
use
gui2exe
to
produce
setup.py
.
I
need
to
get
a
negotiated
cipher
for
DTLS
protocol
in
pyOpenSSL
.
I
was
successful
in
doing
that
for
TCP
sockets
","
but
when
it
comes
to
datagrams
","
it's
not
that
obvious
.
Please
provide
an
example
either
in
C
or
Python
.
This
is
what
I've
tried
so
far
:
The
printed
result
is
(
None
)
The
result
is
None
because
that
is
the
cipher
that
has
been
negotiated
for
your
connection
.
Or
rather
","
it
is
None
because
no
cipher
has
been
negotiated
for
your
connection
yet
.
Cipher
selection
is
part
of
the
handshake
and
the
handshake
is
not
done
anywhere
in
this
example
.
Try
con.do_handshake()
before
calling
SSL_get_current_cipher
.
Also
bear
in
mind
that
_-prefixed
names
are
private
and
you
really
shouldn't
use
them
if
you
want
your
program
to
keep
working
with
future
versions
of
pyOpenSSL
.
I
think
you
just
need
ctypes
","
there
is
a
complete
example
on
calling
a
lapack
function
on
this
page
:
http://www.sagemath.org/doc/numerical_sage/ctypes.html
You
get
your
function
like
this
:
I'm
developing
a
package
that
requires
Python
bindings
for
the
dgtsv
subroutine
from
the
LAPACK
Fortran
library
.
At
the
moment
","
I'm
distributing
the
Fortran
source
file
","
dgtsv.f
","
alongside
my
Python
code
","
and
using
numpy.distutils
to
automatically
wrap
it
and
compile
it
into
a
shared
library
","
_gtsv.so
","
that
is
callable
from
Python
.
Here's
what
my
setup.py
file
looks
like
at
the
moment
:
Note
that
in
order
to
actually
use
_gtsv.so
","
I
still
have
to
link
against
a
pre-existing
LAPACK
shared
library
(
extra_link_args
=
[
'
-
llapack
'
]
)
.
Since
this
library
should
already
contain
the
dgtsv
subroutine
","
it
seems
to
me
that
it
would
be
cleaner
to
just
wrap
the
function
in
the
existing
shared
library
","
rather
than
having
to
distribute
the
actual
Fortran
source
.
However
I've
never
come
across
any
examples
of
using
F2PY
to
wrap
functions
that
are
part
of
a
shared
library
rather
than
just
raw
Fortran
source
code
.
Is
this
possible
?
I
would
like
to
use
a
pyROOT
module
in
my
software
under
the
pycharm
IDE
.
My
problem
is
that
the
IDE
doesn't
recognize
any
of
root
modules
.
Can
you
tell
me
what
should
I
do
to
fix
this
issue
?
As
discussed
here
","
you
can
probably
fix
this
by
doing
;
""""
If
you
select
the
correct
project
and
go
to
File
>
Settings
","
under
the
Project
Settings
you
can
see
the
Project
Interpreter
which
tells
you
which
interpreter
is
being
used
.
""""
Once
you've
selected
to
the
correct
interpreter
","
you'll
also
need
to
install
PYRoot
as
explained
here
.
You
can
install
rootpy
bindings
using
pip
","
explained
here
and
here
.
I
am
wondering
if
there
any
algorithm
where
I
can
compute
the
center
of
a
polygon
in
OSM
because
I've
found
that
each
polygon
has
a
different
parameters
expression
:
""""
"POLYGON((-171379.35 5388068.23,-171378.8 5388077.59,-171368.44 5388076.82,-171368.89 5388067.46,-171379.35 5388068.23)"
)
""""
""""
"POLYGON((-171379.3 5374226.42,-171375.96 5374227.32,-171378.95 5374239.82,-171365.69 5374243.03,-171364.16 5374237.14,-171365.92 5374236.76,-171364.26 5374230.26,-171362.67 5374230.63,-171360.11 5374220.19,-171376.6 5374216.13,-171379.3 5374226.42)"
)
""""
In
this
expression
each
one
has
a
different
number
of
parameters
so
how
can
i
compute
the
centroid
of
it
?
I
am
using
the
postgis
to
get
the
polygon
information
from
the
spatial
information
.
Any
help
?
You
can
use
PostGIS
functionality
or
for
example
the
python
lib
shapely
to
realize
it
.
The
polygons
are
described
using
WKT
.
Using
Python
?
Use
shapely
.
I
think
np.r_
might
be
what
you're
looking
for
.
You
could
use
np.r_
[
b_0
","
somefunction(b_in)
","
b_n
]
","
for
example
:
I
have
a
list
(
actually
I'd
like
to
make
it
a
1D
numpy
array
)
whose
first
and
last
element
will
remain
constant
but
whose
other
elements
are
objects
of
an
optimisation
","
i.e
will
change
often
and
need
to
be
in
a
separate
variable
(
because
scipy.optimize.leastsq()
needs
it
that
way
)
.
So
I've
got
this
code
to
put
the
constant
""""
outer
""""
elements
together
with
the
changing
ones
:
This
looks
very
unelegant
to
me
","
and
I'm
convinced
there
must
be
a
way
to
do
this
in
just
one
line
that
defines
For
background
","
I
need
this
because
I
have
an
error
function
for
leastsq
that
takes
a
1D
array
of
which
only
the
inner
elements
are
optimized
.
I
am
feeding
the
result
of
the
above
to
a
lambda
function
","
together
with
other
parameters
to
the
error
function
","
so
that
I
can
pass
only
the
stuff
that
changes
to
leastsq
:
because
I
am
using
the
same
error
function
for
different
types
of
optimisation
(
there
are
different
aspects
and
different
types
of
constraints
that
the
function
can
be
fitted
to
)
","
I
do
not
want
to
re-write
the
error
function
for
a
specific
purpose
.
I'm
trying
to
write
a
class
circle
and
this
is
my
first
experience
with
OOP
","
to
do
so
I
wrote
a
Point
class
also
","
but
when
I
run
the
contains
and
intersect
function
I
get
this
error
message
:
This
is
the
code
:
instances
of
the
class
Point
do
not
have
an
attribute
center
","
in
these
three
lines
of
code
you
assume
they
do
:
If
check
is
an
instance
of
Point
don't
refer
to
check.center
.
BTW
","
please
give
the
full
stack
trace
when
asking
questions
like
these
","
not
just
the
last
line
.
With
the
full
trace
I
could
have
just
looked
at
the
line
with
the
error
instead
of
having
to
look
through
the
rest
of
the
code
.
What
you
probably
wanted
was
just
to
access
the
x
value
of
the
Circle's
center
and
the
x
of
the
check
Point
.
Note
that
using
a
method
to
access
an
attribute
is
considered
not
to
be
good
style
in
Python
.
If
you
want
to
access
the
value
of
an
attribute
just
access
the
attribute
directly
.
In
some
languages
this
can
problems
if
later
you
need
to
change
how
an
object
is
implemented
but
in
Python
you
can
change
an
attribute
into
a
property
at
any
time
without
breaking
code
.
You'd
have
to
traverse
the
many
layers
of
nesting
you
have
;
the
outer
list
contains
only
more
lists
.
A
recursive
solution
:
Demo
:
This
produces
a
new
list
with
everything
that
is
not
a
list
converted
to
float
.
I
have
a
list
that
looks
like
this
:
And
I
want
to
turn
all
these
values
into
floats
.
The
code
I
tried
is
:
But
when
I
try
this
it
gives
me
the
following
error
:
Can
anybody
tell
me
what
I'm
doing
wrong
and
how
I
can
fix
it
?
Thanks
in
advance
Python
infinite
loop
code
below
:
I
know
I
can
use
import IPython
;
IPython.embed()
or
import pdb
;
pdb.set_trace()
to
pause
loop
running
and
get
value
of
i
-
-
-
run-time
debug
.
however
","
what
I
want
is
if
I
can
""""
tapping
""""
into
this
loop
on
demand
probe
value
without
pause
it
","
similar
to
sniffer
packets
in
network
area
?
PS
:
not
as
simple
as
print
i
to
terminal
directly
.
Thanks
.
Pyringe
will
do
what
you
want
.
Just
grab
it
and
run
python
-
m
pyringe
","
then
you
should
be
able
to
inject
it
in
and
debug
like
you
wanted
.
Im
writing
some
simple
python
application
using
PyQt
and
QtDesigner
.
I've
designed
mainView
in
designer
which
has
a
QGraphicsView
and
some
buttons
inside
.
My
question
is
","
how
should
I
draw
shapes
on
that
QGraphicsView
?
Without
QtDesigner
","
I
would
have
created
class
extending
QGraphicsView
and
overriden
its
paintEvent()
method
.
However
","
Designer
generates
single
Ui_MainWindow
class
","
so
I
can't
set
it's
QGraphicsView
field
to
my
specific
subclass
.
You'll
want
to
promote
the
QGraphicsView
to
your
own
class
that
subclasses
QGraphicsView
.
To
learn
how
to
promote
widgtes
in
Qt
Designer
","
see
this
SO
post
:
How
do
I
use
promote
to
in
Qt
Designer
in
pyqt4
?
In
your
subclass
","
you
can
override
the
paintEvent
method
is
usual
.
To
split
the
string
by
blanks
you
need
to
use
For
the
input
""""
set
title
""""
you
will
get
a
parse
array
looking
like
this
where
parse
[0]
=
=
'
set
'
and
parse
[1]
=
=
'
title
'
If
you
want
to
test
whether
your
string
starts
with
""""
set
title
""""
","
either
check
the
command
string
itself
or
check
the
first
two
indexes
of
parse
.
I
know
this
is
a
basic
question
","
but
I'm
having
difficult
with
parsing
some
text
.
So
how
the
system
will
work
","
let's
take
the
following
example
:
I
should
therefore
get
:
The
problem
is
therefore
","
I
need
to
split
the
string
so
when
I
enter
","
for
example
:
Should
give
me
:
I
have
tried
the
following
:
But
this
does
not
work
and
will
not
even
recognise
that
I
am
entering
""""
set
title
""""
Any
ideas
?
You
don't
need
split
.
You
need
re
:
returns
Will
split
on
the
literal
sequence
of
three
characters
single
quote
","
space
","
single
quote
","
which
don't
appear
in
your
command
strings
.
I
think
you
will
need
to
approach
this
more
like
:
Note
the
use
of
a
second
argument
to
tell
split
how
many
times
to
do
so
;
'
set
title
""""
"foo""'.split"
(
""""
""""
","
1
)
=
=
[
'
set
'
","
'
title
""""
foo
""""
'
]
.
Precisely
how
you
implement
will
depend
on
the
range
of
things
you
want
to
be
able
to
parse
.
I
just
got
through
the
Flask
mega-tutorial's
section
on
implementing
full
text
search
with
Flask-WhooshAlchemy
(
http://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-x-full-text-search
)
and
I
have
the
posts
below
:
I
tried
using
Post.query.whoosh_search('fourth AND not')
.
all()
expecting
to
get
back
[
Post
u'not
my
fourth
'
]
as
a
result
but
I'm
getting
both
of
the
original
posts
instead
.
How
do
I
get
WhooshAlchemy
to
treat
the
not
as
a
string
rather
than
an
operator
?
I
have
recreated
your
setup
.
The
question
you
should
have
asked
is
:
Why
can't
whoosh_search
find
not
?
Try
this
.
This
should
have
returned
the
post
'
not
my
fourth
'
","
right
?
According
to
the
""""
Stop
Words
""""
section
in
this
document
","
“
Stop
”
words
are
words
that
are
so
common
it’s
often
counter-productive
to
index
them
.
This
question
has
a
link
which
shows
that
by
default
'
not
'
is
a
stop
word
","
and
whoosh_search
does
not
index
it
.
So
lets
add
another
post
with
'
fourth
'
and
a
less
common
word
-
how
about
'
cheese
'
.
And
now
lets
search
for
all
posts
with
'
fourth
'
AND
'
cheese
'
in
the
body
.
Perfect
.
BONUS
:
if
you
want
to
get
all
posts
with
'
fourth
'
OR
'
cheese
'
","
do
this
:
According
to
the
last
paragraph
on
this
page
in
the
Flask-WhooshAlchemy
docs
","
query
terms
are
treated
like
an
AND
by
default
.
So
change
your
search
to
be
If
you
are
still
having
issues
with
it
","
perhaps
you
have
to
do
as
per
Whoosh's
docs
on
making
a
term
from
literal
text
.
I
have
labeled
an
binary
image
However
when
I
check
the
number
of
labels
","
I
get
535
elements
.
As
a
solution
for
this
I
thought
about
using
measure.regionprops
in
order
to
remove
the
labels
with
a
small
pixel
area
.
How
would
you
guys
approach
this
?
I
have
tried
the
following
","
but
for
one
reason
or
another
the
new
array
is
no
longer
seen
as
a
correct
label
element
.
I
think
"morphology.remove_small_objects(image, min_px_size)"
does
what
you're
looking
for
.
Here's
an
example
that
uses
that
function
:
http://scikit-image.org/docs/dev/auto_examples/applications/plot_coins_segmentation.html#edge-based-segmentation
I
am
new
to
both
python
and
django
.
I
am
trying
to
get
python
3.4
and
django
1.6.4
working
within
a
pyvenv-3.4
environment
on
Linux
Mint
Mate
13
.
I
followed
the
procedures
of
Using
a
virtual
environment
with
Python
3.4
for
""""
Building
Python
3.4
from
source
""""
and
""""
Using
pyvenv-3.4
""""
.
Then
following
the
Django
tutorial
at
Writing
your
first
Django
app
","
I
was
able
to
create
""""
mysite
""""
with
startproject
.
But
the
command
""""
python
manage.py
runserver
""""
failed
with
django.core.exceptions
.
ImproperlyConfigured
:
Error
loading
either
pysqlite2
or
sqlite3
modules
(
tried
in
that
order
)
:
No
module
named
'
_sqlite3
'
Sorry
","
I
do
not
recall
why
I
resorted
to
compiling
from
source
","
but
it
may
be
that
I
was
unable
to
find
pyvenv
in
the
standard
Ubuntu
downloads
using
apt-get
.
In
any
case
","
the
Makefile
for
the
source
distribution
does
not
have
an
""""
uninstall
""""
target
","
so
it
seems
I
am
stuck
with
whatever
make
installed
","
and
without
a
plan
for
what
to
do
next
to
actually
get
pyvenv-python-django
working
.
I
am
updating
this
post
by
attaching
the
full
error
:
It
sounds
like
you
didn't
have
the
SQLite
development
libraries
installed
at
the
time
you
built
CPython
from
source
.
The
build
process
conditionally
includes
some
features
based
the
availability
of
underlying
development
libraries
.
On
Debian
GNU
/
Linux
and
derivatives
","
install
libsqlite3-dev
.
Then
re-run
.
/
configure
","
make
and
make
install
.
When
i
running
.
/
manage.py
from
the
virtualenv
shows
this
error
(
ENV)vijay@vijay-Ideapad-Z570
:
~
/
nightybuild
/
heroku-landing$
ls
landingpage
main
manage.py
Procfile
Readme.md
requirements.txt
static
templates
(
ENV)vijay@vijay-Ideapad-Z570
:
~
/
nightybuild
/
heroku-landing$
.
/
manage.py
Traceback
(
most
recent
call
last
)
:
File
""""
.
/
manage.py
""""
","
line
8
","
in
from
django.core.management
import execute_from_command_line
ImportError
:
No
module
named
django.core.management
(
ENV)vijay@vijay-Ideapad-Z570
:
~
/
nightybuild
/
heroku-landing$
but
without
activating
virtualenv
.
/
manage.py
gives
fine
.
here
is
pip
freeze
output
(
ENV)vijay@vijay-Ideapad-Z570
:
~
/
nightybuild
/
heroku-landing$
pip
freeze
argparse==1.2.1
distribute==0.6.24
wsgiref==0.1.2
(
ENV)vijay@vijay-Ideapad-Z570
:
~
/
nightybuild
/
heroku-landing$
but
you
know
what
my
app
up
&
running
on
heroku
fine
.
.
I
just
cant
figure
out
why
it`s
not
running
in
locally
.
.
?
That's
because
Django
is
not
installed
in
your
virtual
env
.
Confession
:
this
is
for
a
class
.
I
am
trying
to
generate
all
subsets
of
length
1
-
length(full set)
","
but
they
must
be
in
order
.
So
","
with
an
input
of
(
4
","
2
)
","
valid
results
would
be
(
4
)
","
(
4
","
2
)
","
and
(
2
)
.
Would
not
be
(
4
","
4
)
(
2
","
2
)
or
(
2
","
4
)
eta
(
4
","
2
","
2
)
should
return
(
2
","
2
)
as
well
.
Length
is
not
pre-determined
.
What
I
have
right
now
:
This
is
partially
obtained
by
a
given
function
.
I
don't
understand
how
Python
is
iterating
over
an
empty
set
in
that
2nd
""""
for
""""
loop
.
This
current
code
outputs
(
4
","
4
)
","
(
2
","
2
)
","
and
(
2
","
4
)
in
addition
to
the
""""
right
""""
answers
.
I'm
getting
hung
up
on
the
nested
loops
and
keeping
track
of
multiple
counters
and
having
different
lengths
for
the
desired
output
.
I
have
also
tried
this
:
From
the
ever-helpful
itertools
recipes
:
This
includes
the
empty
set
","
but
you
can
trivially
alter
the
range
to
handle
that
.
Here's
one
way
","
using
recursion
","
though
you
can
alternatively
do
it
iteratively
with
a
loop
","
and
join
the
lists
for
each
n
:
or
using
a
list
comprehension
:
it
is
weird
and
i
don't
know
why
urllib2
is
not
working
.
although
i
tried
this
code
working
with
selenium
and
it's
worked
for
me
.
if
you
don't
know
phantomjs
","
it's
just
a
headless
browser
.
you
can
change
phantomjs
with
FireFox
","
it's
still
working
i
want
to
get
html
document
on
specific
web
site
.
this
code
is
working
well
.
but
this
code
is
not
working
.
why
do
not
working
specific
site
?
I'd
suggest
looking
into
a
thread
pool
","
and
having
the
threads
take
tasks
from
a
suitable
shared
data
structure
(
e.g
.
a
queue
)
rather
than
starting
new
threads
all
the
time
.
Depending
on
what
it
is
you
actually
want
to
do
","
if
you're
using
CPython
(
if
you
don't
know
what
I
mean
by
CPython
","
you
will
be
)
you
might
not
actually
get
any
performance
improvement
from
using
threads
(
due
to
global
interpreter
lock
)
.
So
you
might
be
better
off
looking
into
the
multiprocessing
module
.
I
am
using
Threads
from
the
threading
class
for
the
first
time
and
they
don't
seem
to
be
freeing
themselves
up
after
the
function
runs
.
I
am
attempting
to
have
a
max
of
5
threads
running
at
once
.
Since
one
thread
creates
the
next
there
will
be
some
overlap
but
I'm
seeing
2000
+
threads
running
at
once
before
I
get
the
exception
""""
can't
start
new
thread
""""
.
I'm
open
to
any
other
cleaning
up
I
can
do
here
as
well
since
I'm
pretty
new
to
python
and
entirely
new
to
threading
.
I'm
just
trying
to
get
the
threading
set
up
properly
at
this
point
.
Eventually
this
will
scrape
the
URLS
.
You
need
to
use
the
select_related
query
modifier
:
By
using
prefetch_related
","
you
can
walk
the
one-to-many
relationship
and
fetch
all
of
the
photos
in
two
queries
.
One
for
all
of
the
answers
","
and
another
for
all
of
the
photos
which
are
related
to
the
answers
.
I
have
two
models
:
The
relationship
is
one-to-many
(
one
answer
has
many
photos
)
I
need
to
write
a
reporting
function
that
outputs
all
answers
along
with
corresponding
photos
.
Here
is
what
I
have
:
This
works
","
but
as
you
can
see
it
already
","
the
number
of
times
for
photo
in
answer.photos.all()
will
be
executed
equal
to
the
number
of
answers
in
the
db
.
Preferably
","
I
would
like
to
only
execute
two
db
querys
","
one
fetches
all
of
the
answers
and
the
other
fetches
all
of
the
photos
.
So
I
tried
this
:
This
approach
has
decreased
the
number
of
db
queries
down
to
two
but
it
takes
even
longer
to
execute
as
whole
.
9.5
seconds
for
this
approach
vs
7.5
seconds
for
the
1st
approach
.
Any
suggestions
on
how
to
be
more
effcient
?
Thank
you
!
P.S
.
I
am
using
Django
1.8.2
UPDATE
:
I
used
the
method
suggested
by
@Mark
Galloway
","
and
the
execution
time
has
dropped
to
1.6
seconds
.
The
number
of
queries
becomes
3
.
Django
performed
the
following
query
:
select
*
from
answer
select
*
from
photo
select
*
from
photo
where
'
photo.answer_id
'
in
(
19
","
20
","
3
...
)
#
the
numbers
inside
the
(
)
does
not
seem
to
be
continous
I
wonder
what
is
the
purpose
of
the
last
query
?
Your
URLconf
might
not
have
been
loaded
at
the
time
you
try
to
reverse
the
URL
.
Try
reverse_lazy
instead
I
am
trying
to
redirect
to
another
from
a
Django
view
","
like
so
And
this
is
my
URLconf
I
keep
getting
the
NoReverseMatch
error
.
What
am
I
doing
wrong
?
The
following
should
work
:
With
the
following
as
input
:
It
gives
the
following
output
:
Testing
using
Python
2.7
.
Below
is
also
a
possible
alternative
version
for
Python
3.0
:
I'd
suggest
to
use
np.loadtxt
to
read
the
file
as
ndarray
and
perform
np.min
with
a
given
axis
:
Here's
a
little
illustration
:
You
could
do
it
with
only
python
built
in
commands
and
a
transpose
achieved
by
zipping
all
the
rows
as
follows
:
I've
created
a
program
which
finds
the
minimum
of
each
row
of
a
CSV
file
and
I
would
now
like
to
do
the
same
for
each
column
","
however
I
have
been
unable
to
do
so
.
Any
advice
would
be
greatly
appreciated
thank
you
.
Assume
that
I
have
few
modules
on
my
GAE
project
(
say
A
","
B
","
C
)
.
They
shares
the
users
database
and
sessions
.
For
example
:
module
A
will
manage
the
login
/
logout
actions
(
through
cookies
)
","
module
B
","
C
will
handle
other
actions
.
FYI
","
those
modules
are
developed
in
both
PHP
and
Python
.
Now
","
I
do
not
want
to
make
user
&
session
verification
codes
on
all
3
modules
.
Is
there
anyway
for
me
to
put
a
middleware
that
run
before
all
3
modules
for
each
request
.
Such
as
X
:
it
will
add
header
for
each
request
to
set
the
user
id
and
some
user's
information
if
the
user
has
logged
in
.
I.E
:
after
I
can
implement
my
above
idea
.
Each
request
will
run
through
1
in
below
3
cases
:
X
","
A
X
","
B
X
","
C
What
do
you
say
?
Thanks
Update
1
:
more
information
The
middleware
","
I
mean
the
request
middle
ware
.
If
X
is
a
middleware
then
it
will
be
run
before
the
request
is
passed
to
the
app
(
or
module
)
","
it
will
change
the
request
only
such
as
:
Do
some
authentication
actions
Add
some
headers
:
X-User-Id
:
for
authorized
user
id
X-User-Scopes
:
for
scopes
of
authorized
user
etc
...
And
of
course
","
it
is
independent
to
the
inside
module's
language
(
PHP
or
Python
or
Java
or
...
)
The
X
middleware
should
be
configured
at
app.yaml
.
The
way
I
approached
such
scenario
(
in
a
python-only
project
","
donno
about
php
)
was
to
use
a
custom
handler
(
inheriting
webapp2.RequestHandler
which
I
was
already
using
for
session
support
)
.
In
its
customized
dispatch()
method
the
user
info
is
collected
and
stored
in
the
handler
object
itself
.
The
implementation
of
the
handler
exists
in
only
one
version
controlled
file
","
but
which
is
symlinked
(
for
GAE
accessibility
)
in
each
module
that
references
the
handler
.
This
way
I
don't
have
to
manage
multiple
independent
copies
of
the
user
and
session
verification
code
.
There
are
two
ways
you
can
get
a
row
count
in
DynamoDB
.
The
first
is
performing
a
full
table
scan
and
counting
the
rows
as
you
go
.
For
a
table
of
any
reasonable
size
this
is
generally
a
horrible
idea
as
it
will
consume
all
of
your
provisioned
read
throughput
.
The
other
way
is
to
use
the
Describe
Table
request
to
get
an
estimate
of
the
number
of
rows
in
the
table
.
This
will
return
instantly
","
but
will
only
be
updated
periodically
per
the
AWS
documentation
.
The
number
of
items
in
the
specified
index
.
DynamoDB
updates
this
value
approximately
every
six
hours
.
Recent
changes
might
not
be
reflected
in
this
value
.
As
per
documentation
boto3
""""
The
number
of
items
in
the
specified
table
.
DynamoDB
updates
this
value
approximately
every
six
hours
.
Recent
changes
might
not
be
reflected
in
this
value
.
""""
or
you
can
use
DescribeTable
:
You
can
use
this
","
to
get
count
of
entire
table
items
Refer
here
:
http://boto.cloudhackers.com/en/latest/ref/dynamodb2.html#module-boto.dynamodb2.table
query_count
method
will
return
the
item
count
based
on
the
indexes
you
provide
.
For
example
","
You
can
add
the
primary
index
or
global
secondary
indexes
along
with
range
keys
.
possible
comparison
operators
__eq
for
equal
__lt
for
less
than
__gt
for
greater
than
__gte
for
greater
than
or
equal
__lte
for
less
than
or
equal
__between
for
between
__beginswith
for
begins
with
Example
for
between
I'm
using
boto.dynamodb2
","
and
it
seems
I
can
use
Table.query_count()
.
However
it
had
raised
an
exception
when
no
query
filter
is
applied
.
What
can
I
do
to
fix
this
?
BTW
","
where
is
the
document
of
filters
that
boto.dynamodb2.table.Table.Query
can
use
?
I
tried
searching
for
it
but
found
nothing
.
I
am
trying
to
install
djangoCMS
on
my
Ubuntu
15.04
.
First
I
had
a
""""
pillow
not
compiled
with
jpeg
""""
issue
","
but
managed
to
solve
it
by
installing
another
pip
version
(
pip
install
Pillow==2.3
)
before
running
djangocms
-
f
-
p
.
myproject
.
Can
anyone
explain
to
me
why
?
And
how
to
solve
it
?
Thank
you
!
UPDATE
:
I
installed
django
CMS
inside
the
virtualenv
like
this
:
pip
freeze
:
This
problem
is
solved
in
support
/
3.1.x
branch
of
django-cms
.
You
can
checkout
it
or
wait
the
next
release
:
3.1.3
tag
.
The
recommended
way
is
using
Celery
.
If
you
want
don't
want
to
use
async
task
handling
you
can
also
just
create
a
custom
management
command
and
run
it
via
cron
.
Both
of
them
should
work
with
the
whole
projects
context
(
e.g
.
what
your
defined
in
your
settings
)
","
so
you
can
use
the
Django
ORM
to
connect
to
your
DB
etc..
I
am
setting
up
backend
for
an
application
","
with
Django
and
MySQL
.
As
a
part
of
the
set
up
","
I
need
to
keep
on
fetching
latest
content
from
Facebook
and
Twitter
Graph
APIs
and
keep
updating
my
database
with
that
.
The
user
of
my
application
would
pull
this
latest
available
data
from
the
database
.
Now
","
how
and
where
I
implant
this
code
?
Shall
I
put
it
somewhere
in
the
Django
project
","
if
yes
","
then
where
?
Or
shall
I
use
it
as
an
independent
script
i.e.
not
attached
to
Django
in
anyway
","
and
update
the
DB
directly
with
that
.
Also
since
this
would
be
a
continuous
process
","
I
need
it
to
run
as
background
task
.
It
should
not
eat
consume
any
resources
that
might
be
needed
by
the
foreground
tasks
.
I
am
trying
to
retrive
reviews
from
trip
advisor
and
instead
of
writing
code
I
diciced
to
use
the
shell
that
scrapy
comes
with
.
While
I
was
testing
this
Xpath
I
am
getting
invalid
syntax
error
.
Try
Assume
that
I
have
the
following
data
set
Now
","
I'd
like
to
find
the
last
value
of
each
month
","
move
it
monthwise
to
the
values
of
the
next
month
","
and
finally
take
the
cumulative
product
of
these
values
.
Doing
this
procedure
for
the
above
data
should
result
in
(
performing
each
step
)
:
Finding
the
last
entry
of
each
month
and
moving
them
monthwise
would
result
in
Filling
the
NaN's
with
1
","
taking
the
cumulative
product
","
and
dropping
temp
would
result
in
I
hope
that
this
is
clear
enough
.
And
if
anyone
wonders
why
on
earth
I'd
like
to
do
this
is
because
I've
got
MTD-data
","
and
it
needs
to
be
resampled
.
Thanks
","
Tingis
.
edit
The
number
of
entries
per
month
are
""""
random
""""
","
as
in
they
can
either
be
as
long
as
the
month
or
shorter
(
business
data
...
)
The
following
code
does
not
assume
that
you
have
only
two
rows
for
each
month
.
The
idea
is
that
do
the
group-wise
calculation
first
","
and
then
populate
some
NaN
using
.
reindex()
and
fill
those
NaNs
using
backward
fill
as
we've
got
the
value
for
the
very
last
entry
of
each
month
.
For
the
follow-ups
question
:
I
have
a
string
And
for
some
reason
the
same
string
with
added
characters
(
'
-
'
","
'
.
'
)
I
want
to
map
the
index
of
a
character
from
s1
to
s2
Example:s1:0
-->
s2:2
","
s1:5
-->
s2:9
...
Ok
","
so
something
like
this
should
work
:
I
solved
by
:
counting
the
number
of
occurrence
of
the
character
in
s1
at
position
i
and
then
find
the
character
s1
[i]
in
s2
that
has
the
same
number
of
occurrence
Note
I
found
the
find_nth
here
you
may
use
two
stack
for
each
(
s1
","
s2)with
index
as
key
and
character
as
value
then
pop
the
values
from
each
compare
them
and
generate
required
output
.
You
can
try
something
like
this
:
I
am
trying
to
parse
data
from
server
and
want
to
store
it
in
file
but
I
am
getting
Unexpected
EOF
while
parsing
.
I
am
very
newbie
on
python
.
Here
is
my
code
.
Please
give
me
any
reference
or
hint
.
You
forget
to
add
a
closing
bracket
.
or
You
need
a
queue
that
is
shared
between
processes
.
One
process
adds
to
the
queue
","
the
other
processes
from
the
queue
.
Here
is
a
simplified
example
:
If
you
want
to
only
process
10
things
at
a
time
","
you
can
limit
the
queue
size
to
10
.
This
means
","
the
""""
writer
""""
cannot
write
anything
until
if
the
queue
already
has
10
items
waiting
to
be
processed
.
By
default
","
the
queue
has
no
bounds
/
limits
.
The
documentation
is
a
good
place
to
start
for
more
on
queues
.
I
have
a
situation
where
I
have
to
read
and
write
the
list
simultaneously
.
It
seems
the
code
starts
to
read
after
it
completes
writing
all
the
elements
in
the
list.What
I
want
to
do
is
that
the
code
will
keep
on
adding
elements
in
one
end
and
I
need
to
keep
on
processing
first
10
elements
simultaneously
.
You
can
do
like
this
PS
:
assuming
y
is
an
infinite
stream
You
can
use
the
Flask
session
object
which
does
exactly
what
you
want
:
A
session
basically
makes
it
possible
to
remember
information
from
one
request
to
another
.
Also
","
a
session
is
basically
a
python
dictionary
where
you
can
store
info
between
requests
.
See
here
a
small
example
on
using
session
and
the
secret_key
which
it
requires
.
If
security
is
your
primary
concern
you
should
use
the
isdangerous
module
which
is
more
secure
than
the
default
werkzeug
session
.
Update
Another
possibility
is
to
use
server-side
sessions
to
store
the
session
data
in
the
database
.
There
is
even
an
extension
-
Flask-KVSession
-
which
stores
session
data
on
a
variety
of
backends
.
Here
is
the
situation
:
I
am
using
nginx
","
uwsgi
","
python3.4
","
flask
for
a
web
service
.
In
the
login
process
","
I
want
to
setup
a
global
variable
to
hold
the
{
'
token':'user_id
'
}
dictionary
for
fast
user
token
validation
.
I
have
tried
two
ways
.
The
first
is
using
flask.g
.
But
","
the
problem
is
that
the
flask.g
global
variable
can
not
share
data
between
requests
.
The
second
is
using
app's
config
dictionary
","
but
the
app.config
dictionary
will
changed
when
uwsgi
reloaded
.
Could
someone
tell
me
how
to
achieve
the
goal
?
Turns
out
it
was
quite
straight
forward
.
I
added
following
function
:
Then
I
replaced
this
line
:
By
this
:
Works
like
a
charm
so
far
.
I'm
running
a
number
of
tasks
on
the
GMail
API
and
am
getting
the
same
error
as
described
in
this
issue
.
To
resolve
it
","
I
would
like
to
implement
the
suggested
solution
.
However
I
am
not
sure
how
to
apply
this
to
my
code
.
My
code
currently
looks
as
follows
:
Now
I
am
supposed
to
run
following
code
at
some
moment
in
time
:
And
then
save
the
value
for
content
in
the
datastore
.
The
issue
explains
this
is
not
user-bound
","
however
it
is
not
clear
to
me
if
I
should
run
this
once
and
store
the
value
until
eternity
","
or
refresh
it
at
certain
moments
in
time
.
In
addition
","
since
I
am
handling
authorization
a
bit
different
","
I
have
the
feeling
I
will
run
into
issues
there
as
well
if
I
implement
it
the
exact
same
way
.
Cause
when
I
build
the
service
","
I
actually
add
the
credentials
in
the
call
:
I'm
looking
for
a
solution
with
this
small
selenium
script
.
The
problem
happens
when
on
the
list
there
are
more
than
1
occurence
of
the
text
.
i'm
looking
for
in
with
xpath
command
[
contains()
.
Then
script
stops
.
As
you
see
on
my
script
tried
to
use
css
selector
(
un-commented
)
but
it
not
valid
.
I
have
seen
some
solution
with
regular
expression
with
css
selector
with
^
and
$
but
i
don't
how
it
works
.
EDIT
:
I'm
looking
to
select
only
the
third
element
""""
LIT
""""
as
you
see
there
are
thee
times
LIT
inside
de
list
which
block
the
script
.
Here
is
the
snippet
html
and
here
my
code
Use
/
/
span
[
text()
=
'
""""
+
cat3
+
""""
'
]
.
.
Don't
use
contains
keyword
as
it
will
try
to
search
subString
LIT
.
If
you
remove
contains
","
then
it
will
try
to
match
only
whole
word
'
LIT
'
with
case
sensitive
.
Another
clean
solution
is
to
download
the
wheel
file
for
SciPy(compatiable with your architecture of Python)
and
use
pip
install
pythonModule.whl
and
it
should
install
SciPy
with
no
problems
.
I
had
to
do
this
the
other
day
and
I
stumbled
upon
this
website
","
which
has
many
Python
modules
already
compiled
to
binaries
.
Just
another
way
to
do
it
if
you
don't
want
to
download
Anaconda
.
I
did
this
for
NumPy
","
SciPy
","
and
Scikit-Learn
.
Task
I
want
to
install
Scipy
on
64bit
python
.
Already
Done
I
have
tried
following
ways
:
Using
pip
install
numpy
/
scipy
-
-
in
this
case
numpy
installs
well
but
scipy
fails
.
error
:
can't
find
lpack
.
.
Install
Scipy
from
here
-
-
in
this
it
installs
well
but
on
importing
gives
an
error
not
valid
win32
application
.
As
its
32bit
and
python
is
64bit
Use
gohlke
to
install
scipy
.
Installation
goes
well
but
when
importing
any
sub-module
like
import scipy
.
stats
throws
an
error
:
specified
module
can
not
be
found
.
Reason
to
move
on
64bit
Python
I
have
40000
data
points
csv
file
.
On
ruining
linkage
function
python
throws
a
memory
error
.
Some
posts
suggested
to
move
to
64bit
can
solve
the
issue
.
Installing
scientific
Python
modules
from
sources
on
Windows
is
a
bit
complex
and
not
very
maintainable
.
A
more
reliable
solution
is
to
use
a
scientific
python
distribution
","
such
as
Anaconda
or
Enthought
Canopy
.
Edit
:
I
understand
this
answer
is
opinion
based
","
however
I
am
not
aware
of
other
simple
and
open-source
way
to
deploy
scientific
python
modules
on
Windows
.
Sure
you
could
install
scipy
from
sources
or
with
pip
","
however
in
a
wider
picture
","
if
you
need
Numpy
","
Scipy
","
IPython
and
Matplotlib
","
etc.
that's
just
not
a
workable
solution
.
insert
a
dictionary
into
the
Counter
update
","
like
this
:
Counter.update(iterable)
takes
an
iterable
(
hint
:
a
Python
string
IS
an
iterable
)
and
updates
the
count
for
each
item
of
the
iterable
-
so
what
you
get
is
really
what
is
to
be
expected
.
If
you
really
want
to
use
Counter.update()
here
","
you
have
to
pass
either
a
dict
with
'
1:2:3
'
as
key
and
1
as
value
or
a
sequence
of
(
key
","
value
)
tuples
(
ie
[
(
""""
1:2:3
""""
","
1
)
","
]
)
.
Else
you
can
of
course
use
the
ordinary
dict
syntax
as
mentioned
by
Daniel
Roseman
-
which
really
is
the
thing
to
do
if
you
have
no
other
reason
to
use
update()
.
A
Counter
is
basically
a
dict
.
So
you
can
set
your
key
via
the
normal
dict
syntax
:
I
want
to
add
a
key
to
a
Counter
object
that
has
several
colon
(
:
)
characters
inside
.
The
problem
is
Counter
object
adds
a
key
for
every
character
in
my
key
.
for
example
:
I
want
my
counter
be
like
Counter({'1:2:3':1})
.
How
can
I
do
that
?
The
wrapper
library
you
seem
to
be
using
","
suggest
an
alternative
method
to
capture
frames
(
your
GetVideo
only
takes
single
frames
","
it
does
NOT
return
a
time-series
of
images
","
so
it's
a
bit
of
a
misnomer
)
:
Remark
that
it
starts
and
stops
the
acquisition
each
time
.
By
inspecting
the
stop_acquisition
method
","
you
will
notice
that
it
redirects
that
call
to
the
Aravis
library
","
where
most
likely
the
memory
buffers
are
correctly
torn-down
.
In
your
current
implementation
","
you
will
probably
notice
that
each
time
you
call
GetVideo
","
the
memory
increases
by
about
the
same
amount
as
the
size
(
in
MB
)
of
a
single
image
.
It
is
my
guess
that
by
using
some
of
the
more
specific
methods
available
to
cap.cam
(
an
instance
of
the
Aravis.Camera
)
","
you
will
be
able
to
set
up
a
""""
ring
""""
structure
","
which
is
typical
for
streaming
video
.
However
","
if
you
just
want
snapshots
","
then
use
the
code
above
.
Imaging
Source
Gige
camera
is
running
continuously
in
my
python
code
.
Using
software
triggering
.
It
gradually
increasing
memory
for
possessing
and
after
sometime
it
stuck
due
to
the
low
memory
.
here
is
my
python
code
for
capture
frame
.
How
i
solve
this
?
The
value
you
want
is
currently
{
}
","
the
initial
value
of
testLine
.
You
can
try
this
:
I
am
trying
to
convert
csv
to
Json
.
If
I
encounter
csv
headers
with
naming
convention
""""
columnName1.0.columnName2.0.columnName3
""""
I
need
to
create
a
nested
JSON
-->
{
ColumnName1
:
{
columnName2
:
{
columnName3
:
value
}
}
}
.
.
So
far
I
am
able
to
split
header
into
list
of
subColumnNames
and
create
a
nested
JSON
type
","
but
I
am
unable
to
assign
a
value
.
Any
Help
?
Output
:
Use
tempfile.mkstemp()
which
will
create
a
standard
temp
file
on
disk
which
will
persist
until
removed
:
EDIT
tempfile.TemporaryFile()
will
be
destroyed
as
soon
as
it's
closed
","
which
is
why
your
code
above
is
failing
.
I
wanted
to
ask
if
it's
possible
to
create
PDF
/
XLS
documents
as
temporary
files
.
I'm
doing
that
to
send
them
using
flask
afterwards
.
For
pdf
/
xls
files
creation
I
use
reportlab
and
xlsxwriter
packages
respectively
.
When
I
save
document
using
their
methods
","
I
get
the
""""
Python
temporary
file
permission
denied
""""
error
.
When
I
try
to
close
using
the
tempfile
methods
","
files
become
corrupted
.
Is
there
any
way
to
overcome
this
?
Or
any
other
suitable
solution
?
EDIT
:
Some
code
snippets
:
Similar
story
with
pdf
files
.
Short
Answer
:
Yes
","
of
course
.
All
you
need
is
to
wrap
the
C
+
+
code
with
a
Python
wrapper
.
Elaborated
:
Suppose
your
C
+
+
code
is
inside
class_file.h
.
In
the
same
directory
","
create
a
.
pxd
file
with
any
name
of
your
choice
with
the
following
content
This
is
the
wrapper
of
the
C
+
+
file
.
That's
all
you
have
to
do
using
Cython
.
Further
you
need
to
write
a
.
pyx
file
and
cimport
your
pxd
file
like
a
simple
module
.
You
also
need
to
have
a
setup.py
which
will
create
a
.
so
file
which
can
be
imported
by
Python
as
easily
as
any
other
python
module
.
Do
visit
http://docs.cython.org/src/tutorial/cython_tutorial.html
for
any
more
help
.
I
hope
you
find
Cython
very
easy
and
very
light-based
.
Yes
.
You
can
create
a
Python
C
+
+
extension
where
your
C
+
+
objects
will
be
visible
from
within
Python
as
if
they
were
built-in
types
.
There
are
two
main
ways
to
go
about
it
.
1.Create
the
extension
yourself
following
the
documentation
provided
in
the
CPython
API
Documentation
.
2.Create
the
extension
using
a
tool
such
as
boost::python
or
SWIG
.
In
my
experience
boost::python
is
the
best
way
to
go
about
it
(
it
saves
you
an
enormous
amount
of
time
","
and
the
price
you
pay
is
that
now
you
depend
on
boost
)
.
For
your
example
","
the
boost::python
bindings
could
look
as
follows
:
Compile
:
and
run
in
Python
:
I
am
writing
a
python
/
C
+
+
application
","
that
will
call
methods
in
C
+
+
extension
from
python
.
Say
my
C
+
+
has
a
class
:
It
there
anyway
that
python
can
get
a
object
in
C
+
+
and
call
its
member
function
","
i.e.
:
Any
reference
to
general
reading
is
also
appreciated
.
I
want
to
get
the
health
of
an
elasticsearch
cluster
similar
to
the
command
but
using
python
.
I
did
the
following
but
all
I
get
is
an
I
played
around
with
some
parameters
for
health()
such
us
index
and
level
but
I
mess
around
the
syntax
.
Has
someone
a
working
example
?
Regards
","
Tobi
You
can't
use
the
Cluster
API
directly
","
try
this
:
I
would
like
to
run
a
project
(
project.py
)
that
has
two
scheduling
elements
like
this
answer
I
found
on
stack
overflow
:
Periodically
execute
function
in
thread
in
real
time
","
every
N
seconds
One
function
is
to
run
every
x
seconds
the
other
every
y
seconds
where
0.03
<
x
<
0.1
and
2
<
y
<
10
.
function_x(t)
calls
an
outside
program
(
sync.py
)
that
pauses
(
SIGSTOP
)
project.py
Question
0
:
After
calling
do_every
","
the
program
continues
it's
execution
.
Is
this
a
correct
assumption
?
Question
1
:
If
sync.py
takes
2
seconds
to
execute
before
calling
SIGCONT
on
project.py
","
will
function_x's
Timer
be
'
messed
up
'
?
Question
3
:
If
function_y
is
sending
data
over
a
socket
","
will
function_x
calling
SIGSTOP
mess
up
this
execution
?
Will
the
timing
be
paused
?
How
does
the
timer
count
time
?
Answer
referenced
earlier
:
0
)
After
calling
do_every
-
it
runs
the
worker_func
once
and
then
continues
.
yes
.
So
your
output
will
be
:
1
)
I
didn't
really
know
if
the
timer
would
be
messed
up
-
so
I
ran
a
test
using
htop
where
I
can
send
SIGCONT
and
SIGSTOP
to
any
process
-
so
","
I
ran
do_every
(
2
","
print_so
","
10
)
;
and
after
5-6
""""
"stackoverflow""s"
I
stopped
it
.
After
a
minute
on
starting
it
-
it
spewed
out
4
more
stackoverflows
.
So
","
no
-
the
timer
is
not
messed
up
.
3
)
It
will
not
be
messed
up
-
what
will
happen
is
that
if
the
socket
has
a
timeout
-
and
you
pause
for
more
than
the
timeout
","
the
other
side
of
the
socket
will
assume
your
code
is
dead
and
will
give
a
timeout
exception
.
But
if
it's
for
less
than
the
timeout
","
the
socket
will
assume
it
was
a
temporary
disconnection
","
and
will
continue
streaming
the
data
after
that
.
I
want
to
drag
a
file
into
a
window
and
get
the
file
path
.
I've
tried
doing
this
:
then
in
the
main
window
:
But
this
does
nothing
.
I've
tried
running
this
tutorial
code
","
but
it
doesn't
do
anything
either
.
How
do
I
accomplish
this
?
When
you
print
self.data
","
you
should
see
a
list
of
paths
printed
out
.
Anyway
","
I
wrote
up
a
tutorial
on
drag-n-drop
a
while
ago
which
shows
how
to
do
this
.
Here's
a
slightly
modified
version
of
my
code
that
both
prints
out
the
file
paths
to
stdout
and
to
a
text
control
too
:
I
then
wanted
to
use
CNNs
in
Lasagne
","
but
didn't
get
it
to
work
the
same
way
","
as
the
predictions
were
always
0
...
recommend
you
to
look
at
the
MNIST
example
.
I
find
that
one
much
better
to
use
and
to
extend
","
as
old
code
snippets
didn't
fully
work
due
to
API
changes
over
time
.
I've
amended
the
MNIST
example
","
my
target
vector
has
labels
0
or
1
and
create
the
output
layer
for
the
NN
this
way
:
And
for
the
CNN
:
I
am
trying
to
use
a
Neural
network
for
a
classification
problem
.
I
have
6
possible
classes
and
the
same
input
may
be
in
more
than
one
class
.
The
problem
is
that
when
I
try
to
train
one
NN
for
each
class
","
I
set
output_num_units
=
1
and
on
train
","
I
pass
the
first
column
of
y
","
y
[
:
","
0
]
.
I
get
the
following
output
and
error
:
If
I
try
to
use
output_num_units=num_class
(
6
)
and
the
full
y
(
all
six
fields
)
","
first
I
get
an
error
of
the
KStratifiedFold
","
because
it
seems
that
it
does
not
expect
y
to
have
multiple
rows
.
If
I
set
eval_size=None
","
than
I
get
the
following
error
:
The
only
configuration
that
is
working
is
setting
more
than
one
output
unit
and
passing
only
one
column
to
y
.
Than
it
trains
the
NN
","
but
is
does
not
seem
to
be
right
as
it
is
giving
me
2
output
layers
","
and
I
have
only
one
Y
to
compare
to
.
What
am
I
doing
wrong
?
Why
can't
I
use
only
one
output
?
Should
I
convert
my
y
classes
from
a
vector
of
6
columns
to
a
vector
of
only
one
column
with
a
number
?
I
use
the
following
code
(
extract
)
:
I'm
using
a
seaborn
jointplot
for
scatterplotting
purposes
","
but
I
can't
seem
to
get
a
simple
diagonal
line
going
across
...
I'm
getting
a
AttributeError
:
'
JointGrid
'
object
has
no
attribute
'
get_xlim
'
.
Does
anyone
know
a
workaround
using
Seaborn
?
Here's
my
code
(
also
the
title
doesn't
show
up
!
what
gives
)
:
thanks
in
advance
everyone
.
The
error
was
a
useful
hint
:
a
JointPlot
is
a
congeries
of
subplots
","
you
have
to
find
the
specific
ax
to
plot
onto
.
Modifying
the
Seaborn
example
:
I
figured
this
out
in
an
interpreter
:
dir(g)
","
then
g.plot
?
","
g.plot_joint
?
-
-
those
are
plotting
functions
specific
to
the
jointplot
-
-
what
else
was
there
?
-
-
dir(g.ax_joint)
;
aha
","
there's
set_ylim
","
etc.
You
need
to
make
a
new
dictionary
from
your
defaultdict
.
The
children
in
your
example
code
is
just
a
list
of
strings
","
so
I
don't
know
where
the
""""
size
""""
of
each
one
comes
from
so
just
changed
it
into
a
list
of
dicts
(
which
don't
have
a
an
entry
for
a
""""
size
""""
key
)
.
Output
:
I
have
a
defaultdict(list)
and
I
used
simplejson.dums(my_defaultdict)
in
order
to
output
the
defaultdict
into
a
JSON
format
.
I
am
using
the
HTML
code
for
dendogram
from
http://bl.ocks.org/mbostock/4063570
but
I
am
trying
to
make
my
defaultdict
information
into
the
format
of
the
JSON
file
the
author
is
using
.
This
JSON
file
is
named
:
/
mbostock
/
raw
/
4063550
/
flare.JSON
and
it's
found
in
this
link
:
http://bl.ocks.org/mbostock/raw/4063550/flare.json.
So
here
is
my
defaultdict
data
:
so
my
current
json_data
looks
like
this
:
So
in
my
understanding
the
numbers
would
be
the
corresponding
""""
"name"":""5"
""""
and
then
my
JSON
format
file
would
also
have
the
children
as
""""
children
""""
.
As
what
it
is
right
now
","
my
JSON
format
output
doesn't
run
in
the
HTML
code
of
the
dendogram
.
Any
help
on
how
to
output
my
defaultdict
into
the
appropriate
JSON
format
that
would
work
with
the
HTML
code
for
producing
that
dendogram
would
be
great
.
The
code
has
to
be
Python
.
The
expected
outcome
would
be
like
this
:
Edited
:
The
answer
of
martineau
works
","
but
it's
not
exactly
what
I
want
.
I
start
with
a
defaultdict(list)
and
the
desired
output
","
as
above
should
have
the
""""
children
""""
as
a
list
of
dicts
whereas
with
martineau
kind
answer
","
the
""""
children
""""
it's
just
a
list
.
If
anybody
can
add
something
to
that
to
make
it
work
it
would
be
great
.
Don't
worry
about
the
""""
size
""""
variable
","
this
can
be
ignored
for
now
.
Thank
you
.
You
need
to
build
the
dictionary
so
that
it
contains
the
desired
'
children
'
fields
.
json.dumps
does
not
output
data
in
any
predefined
schema
.
Rather
","
the
object
passed
to
json.dumps
must
already
adhere
to
any
structure
desired
.
Try
something
like
this
:
In
the
context
of
a
model
I'm
generating
","
I
at
one
point
need
to
generate
a
probability
distribution
from
a
an
array
of
real
numbers
.
I'll
leave
out
the
relevant
details
","
but
essentially
have
a
function
(
we'll
just
call
it
""""
f
""""
for
now
)
","
that
generates
an
array
of
n
floats
:
Now
","
these
values
are
proportional
to
probabilities
I
next
need
to
use
in
a
multinomial
sampling
procedure
","
so
the
obvious
approach
is
simply
this
:
But
this
(
sometimes
)
doesn't
work
!
Basically
the
sum
of
arr
/
arr.sum()
ends
up
being
greater
than
1
.
In
principle
this
should
be
mathematically
impossible
","
but
I'm
assuming
this
boils
down
to
a
floating-point
precision
issue
.
Here's
a
trivial
example
of
how
this
can
happen
:
So
long
story
short
","
my
question
is
how
best
to
deal
with
this
.
I
can
cheat
by
simply
adding
a
very
small
number
to
the
sum
","
i.e.
:
But
this
is
really
hackish
","
and
I
fear
it
may
introduce
further
unwanted
precision
issues
.
Is
there
a
better
solution
?
Start
by
reading
https://docs.python.org/2/tutorial/floatingpoint.html
In
a
nutshell
","
floating
point
can't
really
represent
0.05
.
The
effect
is
minute
:
The
correct
solution
is
to
define
the
desired
precision
for
each
mathematical
operation
","
calculate
the
round
errors
of
each
step
and
round
accordingly
when
necessary
.
In
your
case
","
you
can
round
to
5
digits
since
you're
only
adding
a
few
numbers
.
But
for
more
complex
calculations
which
need
to
be
correct
","
you
will
have
to
do
the
error
assessment
.
SET_ATTITUDE_TARGET
is
now
available
to
be
used
in
guided_mode
.
You
can
even
test
it
using
mavproxy
attitude
command
.
I
have
Dronekit
working
properly
with
SITL
sim
","
however
for
my
project
I
want
to
be
able
to
command
the
attitude
of
the
copter
.
Obviously
I
can
do
this
via
RC
over
ride
in
ALT_HOLD
mode
","
however
I
don't
like
that
approach
.
I
have
been
trying
to
use
the
Mavlink
message
SET_ATTITUDE_TARGET
(
#
82
)
","
however
when
I
send
the
messages
to
the
sim
","
nothing
happens
.
I
have
been
able
to
set
the
velocity
and
the
position
","
and
those
work
fine
.
Here
is
my
function
:
Can
someone
help
me
out
?
SET_ATTITUDE_TARGET
is
now
implemented
","
and
will
be
released
with
ArduCopter
3.4
It
is
not
possible
to
set
the
attitude
directly
because
the
command
is
not
supported
by
Copter
in
either
guided
mode
or
AUTO
mode
/
missions
)
.
The
list
of
supported
commands
in
guided
mode
is
here
and
AUTO
commands
here
.
What
you
can
do
is
set
the
yaw
.
Another
(
hacky
)
approach
that
may
work
is
setting
the
ROI
as
this
will
point
the
camera
(
and
often
the
whole
vehicle
)
at
a
target
.
This
sounds
like
a
reasonable
requirement
-
perhaps
create
a
request
with
explanation
of
why
this
is
useful
for
you
?
Selenium
tests
in
Python
are
just
python
code
.
You
can
use
the
CSV
module
and
a
normal
loop
to
carry
out
these
actions
on
the
page
","
and
receive
the
values
from
the
new
DOM
.
You
can
use
loops
just
like
normal
Python
to
capture
the
screenshots
","
but
no
I'm
not
going
to
write
the
code
for
ya
.
Can
someone
let
me
know
1
.
How
to
write
selenium
webdriver
code
in
python
for
reading
the
data
from
CSV
files
and
input
them
into
fields
of
application
under
test
and
print
the
results
into
CSV
file
after
execution
.
2
.
I
have
written
capture
screenshot
for
every
statement
to
get
the
screenshots
.
Is
there
any
that
we
can
capture
screenshots
in
a
single
go
","
like
using
any
loop
statements
etc..
If
yes
","
then
can
you
post
the
code
Thanks
for
your
time
and
response
is
appreciated
....
The
repeat()
iterator
makes
the
load
rows
look
endless
;
you
can
iterate
over
it
forever
.
Presumably
the
actual
code
looks
like
this
:
e.g
.
the
repeat(load)
is
chained
","
so
all
the
data
in
the
CSV
file
looks
like
one
long
sequence
of
rows
","
and
when
you
reach
the
end
of
the
CSV
file
you
simply
start
at
the
beginning
again
","
as
if
the
rows
in
the
CSV
file
are
read
from
an
endless
loop
.
The
islice()
then
picks
a
subset
of
that
endless
loop
.
By
making
the
load
rows
endless
","
it
doesn't
matter
if
the
slice
tries
to
take
more
rows
from
the
file
then
are
present
.
I
have
the
following
snippet
in
Python
.
Here
","
load
is
the
list
created
from
reading
an
input
csv
file
.
After
searching
on
line
","
I
understand
what
are
the
purposes
of
function
calls
islice
and
chain_from_iterable
.
The
question
for
me
here
is
:
why
do
we
need
to
repeat(load)
here
?
Does
that
mean
we
make
copies
of
the
list
obtained
from
the
input
csv
file
and
handle
them
based
on
the
task_id
or
things
like
that
?
Sort
of
in
a
parallel
fashion
?
I
am
pretty
sure
I
can
learn
Python
very
well
","
but
right
now
I
am
kindof
new
to
Python
.
I'm
trying
to
use
pyaudio
to
play
some
wave
files
but
I'm
always
having
slow
/
crackling
/
garbled
outputs
.
When
I
play
this
wave
file
as
described
bellow
","
the
audio
plays
just
fine
:
However
","
when
I
open
the
'
play_wave.py
'
example
from
/
pyaudio
/
test
","
the
audio
is
so
slow
and
garbled
that
is
useless
for
any
application
.
To
reproduce
a
similar
poor
quality
on
your
laptop
/
PC
","
just
make
the
CHUNK
=
1
(
the
output
is
pretty
similar
on
my
Ubuntu
)
Additional
information
:
What
I
tried
:
1
-
Another
Raspberry
Pi
B
+
.
2
-
Change
the
audio
samples
per
buffer
:
As
I
was
supposing
the
problem
was
the
audio
samples
per
buffer
(
the
CHUNK
variable
in
this
example
)
","
I
made
a
loop
to
increment
the
CHUNK
by
1
and
played
the
audio
for
each
increment
.
I
could
notice
a
slight
difference
for
some
CHUNK
values
","
but
nothing
even
close
to
the
quality
that
I
get
when
I
play
it
by
aplay
.
However
","
I
could
notice
a
big
difference
between
this
two
files
:
1
-
police_s.wav
=
8
bits
","
22000Hz
","
Mono
","
176
kb
/
s
->
Way
better
than
the
beat.wav
played
by
the
same
CHUNK
(
2048
)
2
-
beat.wav
=
16bits
","
44100Hz
","
Stereo
","
1411
kb
/
s
When
I
play
the
same
audio
through
the
example
/
pyaudio
/
test
/
play_wave_callback.py
","
the
output
is
almost
perfect
","
excepting
some
interruptions
at
the
end
of
the
audio
.
So
I
saw
that
it
doesn't
set
the
CHUNK
.
It
uses
the
frame_count
parameter
in
the
callback
function
","
so
I
printed
it
and
saw
that
it
was
1024
¬
¬
","
the
same
default
value
that
came
with
the
example
/
pyaudio
/
test
/
play_wave.py
and
that
results
in
a
garbled
audio
.
3
-
pyaudio
0.2.4
:
Since
hayderOICO
mentioned
he
was
using
pyaudio
0.2.4
and
said
""""
I'm
using
PyAudio
fine
.
""""
","
I
decided
to
give
a
try
on
that
older
version
but
I
got
the
same
result
...
4
-
Added
disable_audio_dither=1
to
config.txt
I'm
using
:
Raspberry
Pi
B
+
Raspbian
python
2.7.3
pyaudio
v0.2.8
portaudio19-dev
TRRS
analog
audio
How
I
installed
everything
:
1st
try
:
2nd
try
:
3rd
try
:
From
GitHub
:
https://github.com/jleb/pyaudio
It's
very
frustrating
having
the
library's
example
not
working
properly
on
Pi
.
I
don't
think
it's
a
hardware
limitation
since
the
same
audio
plays
well
with
aplay
and
other
libraries
like
pygame
and
SDL2
.
I
am
new
to
Raspberry
Pi
and
audio
programming
","
so
I
hope
to
be
doing
something
stupid
...
As
I
am
already
using
a
specific
wrapper
for
pyaudio
","
I
really
would
like
to
keep
using
it
instead
of
moving
to
another
library
...
I
appreciate
any
help
","
suggestions
and
advice
.
Thanks
in
advance
!
I
had
a
similar
problem
on
my
raspberry
pi
(
and
on
my
mac
)
.
In
my
experience
the
pyaudio
library
is
a
pain
to
work
with
(
after
2
weeks
of
battling
it
","
I
ended
up
using
pygame
)
.
What
worked
for
me
was
to
check
the
default
sample
rate
of
the
audio
out
and
check
that
its
the
same
and
play
sounds
back
as
numpy
arrays
.
So
on
the
RPi
","
I'd
check
(
extrapolating
from
ubuntu
here
...
)
the
files
Does
direct
playback
of
numpy
arrays
work
?
If
so
you
could
do
it
in
a
roundabout
way
...
Here
is
some
code
to
test
if
you
like
This
worked
on
my
RPi
and
mac
","
but
as
said
before
I
ended
up
using
pygame
because
even
with
on
and
off
ramps
I
couldn't
get
rid
of
crackling
at
the
beginning
and
end
and
sample
rate
wasn't
something
that
could
be
easily
changed
.
I
would
really
recommend
against
pyaudio
","
but
if
you
are
set
on
it
I
wish
you
the
best
of
luck
!
:
)
I
think
you
need
to
include
gif
in
bulldozer.spec
file
.
Here
:
source.include_exts
=
py
","
png
","
...
","
gif
I
wanted
to
make
an
small
Kivy
program
with
a
GIF
.
It
worked
perfectly
on
my
PC
","
but
when
I
compiled
it
","
pushed
it
to
my
device
and
ran
it
","
all
I
got
is
a
white
screen
.
Here
is
my
code
:
I
placed
the
treeview
widget
within
a
grid
cell
","
which
uses
stick=N+S+E+W
","
in
order
to
attach
the
widget
borders
to
the
grid
cell's
borders
.
Also
","
I
set
"grid_columnconfigure(0, weight=1)"
for
the
parent
(
root
)
","
in
order
to
resize
widgets
in
column
0
(
the
one
which
also
contains
the
treeview
)
according
to
the
main
window
size
(
resizable
)
.
Despite
so
","
some
columns
(
""""
bitrate
""""
","
""""
status
""""
)
of
the
treeview
widget
do
not
show
","
because
the
whole
widget
is
bigger
than
the
grid
cell
which
contains
it
.
Here's
the
source
:
Here's
a
screenshot
:
http://i.stack.imgur.com/U1g44.png.
As
you
can
see
","
the
columns
""""
bitrate
""""
and
""""
status
""""
aren't
shown
.
They
can
be
viewed
only
by
increasing
the
width
of
the
main
window
.
I
can't
figure
out
where's
the
problem
.
Thanks
for
taking
your
time
.
The
treeview
widget
has
a
method
named
column
which
lets
you
specify
options
for
a
given
column
.
One
of
the
options
is
stretch
","
which
lets
you
determine
whether
the
column
should
stretch
and
shrink
when
the
widget
is
resized
.
You
can
also
use
the
width
attribute
to
specify
the
starting
size
.
These
two
combined
should
cause
your
treeview
to
appear
with
all
of
the
columns
fitting
on
the
screen
.
I
have
been
messing
around
with
Python
/
Panda3D
and
trying
to
get
my
first
file
to
run
.
I
am
on
OSX
and
after
installing
Panda3D
I
tried
to
run
this
file
:
That
was
followed
by
this
on
the
Terminal
output
:
I
have
been
searching
everywhere
","
but
all
the
forms
don't
really
solve
the
problem
.
Any
help
would
be
awesome
!
Thanks
!
This
is
usually
a
problem
when
you're
using
a
version
of
Python
other
than
the
one
Panda3D
is
compiled
with
.
If
you
are
using
the
1.9.0
Lion
build
of
Panda3D
","
make
sure
you
are
using
the
system-provided
copy
of
Python
2.7
","
by
explicitly
running
""""
python2.7
""""
or
""""
ppython
""""
(
which
should
be
a
symlink
to
the
correct
version
of
Python
)
.
I
was
writing
an
automation
script
in
python
that
dealt
with
sending
commands
through
a
Telnet
session
.
For
some
reason
I
couldn't
get
it
to
work
.
After
a
ton
of
frustrating
debugging
","
I
found
that
when
I
was
translating
a
command
of
:
The
-
in
the
command
became
something
weird
in
utf-8
.
I
had
to
translate
it
in
bytes
because
I
was
sending
it
using
Telnet
(
I
know
I
should
use
ssh
","
but
it
honestly
is
fine
in
my
case
)
and
I
realized
it
was
weird
because
when
I
printed
the
command
in
bytes
it
would
be
:
I
don't
remember
the
exact
numbers
","
but
I
fixed
it
by
copying
and
pasting
a
new
""""
-
""""
that
I
used
two
lines
up
in
the
function
and
worked
fine
.
I
did
copy
and
paste
in
the
part
from
two
lines
up
","
but
I
typed
the
ulimit
-
s
part
.
I
was
also
using
IDLE
Anyone
know
what
happened
?
You
managed
to
enter
something
like
an
U+2013
EN
DASH
or
an
U+2014
EM
DASH
","
which
both
look
a
lot
like
the
ASCII
character
U+002D
HYPHEN
MINUS
.
Because
either
of
these
characters
are
outside
of
the
basic
Latin-1
alphabet
","
encoding
either
one
of
these
to
UTF-8
results
in
a
3-byte
sequence
:
Those
two
are
not
the
only
confusable
characters
;
a
few
more
:
U+2010
HYPHEN
‐
(
UTF8
:
E2
80
90
)
U+2011
NON-BREAKING
HYPHEN
‑
(
UTF8
:
E2
80
91
)
U+2012
FIGURE
DASH
‒
(
UTF8
:
E2
80
92
)
U+FE58
SMALL
EM
DASH
﹘
(
UTF8
:
EF
B9
98
)
U+FE63
SMALL
HYPHEN-MINUS
﹣
(
UTF8
:
EF
B9
A3
)
etc.
Anyone
know
what
happened
?
I
can
see
two
possibilities
here
.
One
is
that
you
inadvertently
copy-pasted
a
line
of
code
from
a
Web
page
or
other
document
where
the
-
had
been
replaced
by
an
emdash
(
it
usually
happens
to
me
with
quote
signs
and
typographic
quote
signs
)
","
which
looks
like
a
minus
sign
but
it's
a
UTF8
multibyte
sequence
.
The
other
is
that
somehow
the
IDLE
editor
effected
a
""""
spelling
check
""""
like
that
of
Microsoft
Word
","
which
replaces
(
among
others
)
quote
signs
with
typographic
quote
signs
","
three
consecutive
dots
with
an
ellipsis
and
minus
signs
with
emdashes
.
This
might
have
been
triggered
by
some
rare
keystroke
combination
typed
in
error
(
for
example
I
sometimes
trigger
the
Windows
7
Screen
Magnifier
while
trying
to
type
in
","
I
think
","
a
{
symbol
-
which
on
my
keyboard
is
Shift
AltGr
[
)
.
It's
not
that
I
think
the
following
is
more
readable
than
what
you
have
","
but
it
only
uses
list
comprehensions
.
Say
you
have
Then
a
dictionary
of
the
mean
can
be
found
with
For
the
variance
","
note
that
","
by
the
definition
of
variance
V
[X]
=
E
[x2]
-
(
E
[X]
)
2
","
so
","
if
you
define
:
then
the
variance
dictionary
is
I'm
wondering
whether
there
is
a
Pythonic
way
to
compute
the
means
and
variances
of
Counters
?
For
example
","
I
have
four
Counters
sharing
the
same
keys
:
My
way
to
do
that
is
:
I
could
use
the
following
code
to
find
the
mean
/
variance
for
each
key
:
Is
there
an
easier
way
to
compute
the
mean
/
variance
for
each
key
compared
to
my
way
?
I
have
written
code
that
separates
the
characters
at
'
even
'
and
'
odd
'
indices
","
and
I
would
like
to
modify
it
so
that
it
separates
characters
by
upper
/
lower
case
.
I
can't
figure
out
how
to
do
this
for
a
string
such
as
""""
AbBZxYp
""""
.
I
have
tried
using
.
lower
and
.
upper
but
I
think
I'm
using
them
incorrectly
.
Btw
","
for
odd
/
even
index
you
could
just
do
this
:
There
is
an
itertools
recipe
called
partition
that
can
do
this
.
Here
is
the
implementation
:
From
itertools
recipes
:
Upper
and
Lowercase
Letters
You
can
manually
implement
the
latter
recipe
","
or
install
a
library
that
implements
it
for
you
","
e.g
.
pip
install
more_itertools
:
Here
partition
uses
a
predicate
function
to
determine
if
each
item
in
an
iterable
is
lowercase
.
If
not
","
it
is
filtered
into
the
false
group
.
Otherwise
","
it
is
filtered
into
the
group
of
true
items
.
We
iterate
to
expose
these
groups
.
Even
and
Odd
Indices
You
can
modify
this
to
work
for
odd
and
even
indices
as
well
:
Here
we
zip
an
itertools.count()
object
to
enumerate
the
iterable
.
Then
we
iterate
the
children
so
that
the
sub
items
yield
the
letters
only
.
See
also
more_itertools
docs
for
more
tools
.
Are
you
looking
to
get
two
strings
","
one
with
all
the
uppercase
letters
and
another
with
all
the
lowercase
letters
?
Below
is
a
function
that
will
return
two
strings
","
the
upper
then
the
lowercase
:
You
can
then
call
it
with
the
following
:
which
gives
you
two
variables
","
upper
and
lower
.
Use
them
as
necessary
.
What
others
have
said
about
not
doing
circular
imports
is
the
best
solution
","
but
if
you
end
up
absolutely
needing
them
","
it's
usually
within
just
one
method
or
function
of
one
of
the
modules
.
Thus
you
can
safely
do
this
:
There's
a
slight
overhead
to
doing
the
import each
time
the
function
is
called
","
but
it
is
rather
low
unless
it's
called
all
the
time
.
the
webhandler
module
needs
access
to
several
variables
that
are
generated
in
the
datahandler
module
It
might
make
sense
to
push
any
""""
generated
""""
data
to
a
third
location
.
So
datahandler
functions
call
"config.setvar( name, value )"
when
appropriate
and
webhandler
functions
call
config.getvar( name )
when
they
need
to
.
config
would
be
a
third
sub-module
","
containing
simple
setvar
and
getvar
functions
that
you
write
(
wrappers
around
setting
/
getting
elements
of
a
global
dictionary
would
be
the
simplest
approach
)
.
Then
the
datahandler
code
would
import webhandler
","
config
but
webhandler
would
only
need
to
import config
.
I
agree
with
poke
however
","
that
the
need
for
such
a
question
betrays
the
fact
that
you
probably
haven't
yet
got
the
design
finalized
as
neatly
and
logically
as
you
thought
.
If
it
were
me
","
I
would
re-think
the
way
modules
are
divided
up
.
Circular
dependencies
are
a
form
of
code
smell
.
If
you
have
two
modules
that
depend
on
each
other
","
then
that’s
a
very
bad
sign
","
and
you
should
restructure
your
code
.
There
are
a
few
different
ways
to
do
this
;
which
one
is
best
depends
on
what
you
are
doing
","
and
what
parts
of
each
module
are
actually
used
by
another
.
A
very
simple
solution
would
be
to
just
merge
both
modules
","
so
you
only
have
a
single
module
that
only
depends
on
itself
","
or
rather
on
its
own
contents
.
This
is
simple
","
but
since
you
had
separated
modules
before
","
it’s
likely
that
you
are
introducing
new
problems
that
way
because
you
no
longer
have
a
separation
of
concerns
.
Another
solution
would
be
to
make
sure
that
the
dependencies
are
actually
required
.
If
there
are
only
a
few
parts
of
a
module
that
depend
on
the
other
","
maybe
you
could
move
those
bits
around
in
a
way
that
the
circular
dependency
is
no
longer
required
","
or
utilize
the
way
imports
work
to
make
the
circular
dependencies
no
longer
a
problem
.
The
better
solution
would
probably
be
to
move
the
dependencies
into
a
separate
new
module
.
If
naming
is
really
the
hardest
problem
about
that
","
then
you’re
probably
doing
it
right
.
It
might
“
ruin
the
organisation
of
[your]
program
”
but
since
you
have
circular
dependencies
","
there
is
something
inherently
wrong
with
your
setup
anyway
.
Ok
so
it
is
like
this
.
I'd
rather
not
give
away
my
code
but
if
you
really
need
it
I
will
.
I
have
two
modules
that
need
a
bit
from
each
other
.
the
modules
are
called
webhandler
and
datahandler
.
In
webhandler
I
have
a
line
:
and
in
datahandler
I
have
another
line
:
Now
I
know
this
is
terrible
code
and
a
circular
import like
this
causes
the
code
to
run
twice
(
which
is
what
im
trying
to
avoid
)
.
However
the
datahandler
module
needs
to
access
several
functions
from
the
webhandler
module
","
and
the
webhandler
module
needs
access
to
several
variables
that
are
generated
in
the
datahandler
module
.
I
dont
see
any
workaround
other
than
moving
functions
to
different
modules
but
that
would
ruin
the
organisation
of
my
program
and
make
no
logical
sense
with
the
module
naming
.
Any
help
?
This
is
a
simple
scrapy
spider
which
crawls
yelp.com
and
fetches
data
I've
set
Rule(LinkExtractor(allow=('.*')
)
","
follow=True
","
"callback=""parseBusiness"
""""
)
To
follow
links
and
the
callback
as
parseBusiness
However
","
Scrapy
here
","
does
not
follow
links
This
is
the
specific
output
(
full
output
here
http://pastebin.com/BkuErvMq
)
This
is
my
code
below
What
am
i
missing
here
?
to
get
scrapy
to
follow
all
the
links
You
are
not
setting
the
rules
attribute
of
your
spider
:
You
must
be
looking
for
:
See
IDEONE
demo
Output
:
54
Mind
that
re.M
is
redundant
in
your
regex
as
you
do
not
have
anchors
^
and
$
in
your
pattern
(
only
their
behavior
is
impacted
by
that
option
)
.
If
you
use
(
\
sNumber
of
copies
:
(
\
d{1
","
2
}
)
)
","
there
are
2
capturing
groups
","
and
the
number
will
be
in
group
2
.
If
the
file
you
search
for
the
expression
does
not
contain
that
text
","
and
you
want
to
skip
it
","
check
if
you
obtained
a
match
object
:
I'm
searching
for
a
string
Number
of
copies
:
in
a
text
file
and
once
it's
found
","
I
want
to
print
the
digits
associated
with
it
","
so
I'm
searching
for
","
(
\
sNumber
of
copies
:
(
\
d{1
","
2
}
)
)
and
I
want
to
return
(
\
d{1
","
2
}
)
.
I've
been
researching
nested
groups
in
REGEX
","
but
I
don't
have
the
syntax
down
for
it
in
Python
.
Any
help
would
be
appreciated
.
Here
what
the
line
I'm
looking
for
in
a
text
file
looks
like
:
My
desired
output
would
just
be
2
and
2
because
I'm
looking
for
the
number
that
follows
the
text
.
fi2Content
represent
a
whole
text
file
read
by
Python
.
I
can
print(zDiscs)
but
I
can't
print(zDiscs.group(2)
)
.
Why
?
I
get
the
following
error
:
when
I
try
to
print(zDiscs.group(2)
)
Here's
my
whole
script
if
that
helps
troubleshooting
can
get
the
raw
text
of
the
file
if
there's
a
way
to
pass
that
directly
to
configparser
Try
configparser.ConfigParser.read_string
When
coupled
with
an
appropriate
ZIP
file
","
this
code
works
for
me
:
Upon
reflection
","
the
following
might
be
more
appropriate
:
ZipFile
not
only
supports
read
but
also
open
","
which
returns
a
file-like
object
.
So
","
you
could
do
something
like
this
:
I
am
creating
a
program
that
loads
and
runs
python
scripts
from
a
compressed
file
.
Along
with
those
python
scripts
","
I
have
a
config
file
that
I
previously
used
configparser
to
load
info
from
in
an
uncompressed
version
of
the
program
.
Is
it
possible
to
directly
read
config
files
in
zip
files
directly
with
configparser
?
or
do
I
have
to
unzip
it
into
a
temp
folder
and
load
it
from
there
?
I
have
tried
directly
giving
the
path
:
Didn't
work
.
no
surprises
there
.
Then
I
tried
using
zipfile
and
found
that
it
didn't
work
either
.
so
I've
resorted
to
creating
a
temp
folder
","
uncompressing
to
it
","
and
reading
the
conf
file
there
.
I
would
really
like
to
avoid
this
if
possible
as
the
conf
files
are
the
only
limiting
factor
.
I
can
(
and
am
)
loading
the
python
modules
from
the
zip
file
just
fine
at
this
point
.
I
can
get
the
raw
text
of
the
file
if
there's
a
way
to
pass
that
directly
to
configparser
","
but
searching
the
docs
I
came
up
empty
handed
.
Update
:
I
tried
using
stringIO
as
a
file
object
","
and
it
seems
to
work
somewhat
.
configparser
doesn't
reject
it
","
but
it
doesn't
like
it
either
.
If
I
use
read_file
instead
","
it
doesn't
error
out
","
but
doesn't
load
anything
either
.
I
get
a
Local
variable
'
first
'
referenced
before
assignment
error
when
I
run
my
code
.
That
is
just
a
part
of
my
code
where
the
error
occurs
.
However
when
I
paste
the
code
into
IDLE
it
works
no
problem
so
I
don't
know
why
this
is
happening
.
Anyways
","
my
full
code
(
unfinished
Tic
Tac
Toe
)
:
Python
scans
a
function
body
for
any
assignments
","
and
if
they
aren't
explicitly
declared
global
","
then
it
creates
a
local
scope
variable
for
that
name
.
Because
you
assign
to
first
in
your
reverse()
function
","
and
you
haven't
explicitly
declared
first
to
be
global
within
that
function's
scope
","
python
creates
a
local
variable
named
first
that
hides
the
global
one
.
It
doesn't
matter
that
the
assignment
comes
after
the
comparison
;
python
implicitly
declares
all
local
variables
at
the
beginning
of
the
function
.
To
fix
this
you
can
declare
first
to
be
global
within
the
reverse()
function
","
but
as
others
have
said
","
globals
should
be
avoided
when
possible
.
It's
a
good
question
","
with
not
an
easy
to
find
answer
.
The
main
difference
is
that
local-name
(
)
does
not
consider
prefixes
(
namespaces
)
for
tags
.
For
example
","
given
a
node
<
x:html
"xmlns:x=""http"
:
/
/
www.w3.org
/
1999
/
xhtml
""""
/
>
","
the
local-name
will
match
the
html
tag
","
while
/
/
html
will
not
work
","
and
neither
will
/
/
x:html
.
Please
consider
the
following
code
","
if
you
have
any
questions
feel
free
to
ask
.
Show
me
the
code
Setup
:
It
is
now
not
possible
to
use
the
tag
selector
:
But
using
local-name
we
can
still
get
the
element
(
considering
the
namespace
)
Or
strict
namespace
using
name()
:
Performance
I
parsed
this
website
as
a
tree
and
used
the
following
queries
:
Now
onto
actual
namespaces
.
I
parsed
a
block
from
here
.
Conclusion
I
had
to
rewrite
to
conclusion
since
after
using
the
namespace
method
it
became
obvious
that
the
gain
when
using
namespaces
is
also
there
.
Roughly
2
times
faster
when
specifying
the
namespace
(
causing
optimizations
)
","
rather
than
using
local-name
.
I'm
using
lxml
on
Python
2.7
.
Given
a
node
","
node
and
a
child
","
child_element
","
what
is
the
difference
between
these
:
node.xpath('./child_element')
"node.xpath(""*[local-name()"
=
'
child_element
'
]
""""
)
In
other
words
","
what's
going
on
under
the
hood
here
?
Is
there
any
reason
one
ought
to
be
""""
better
""""
than
another
(
in
terms
of
performance
or
correctness
)
?
I've
read
through
the
lxml
docs
and
a
good
deal
of
other
XPath
query
resources
and
am
not
finding
any
real
clarification
.
So
","
it's
a
little
hacky
","
but
should
be
relatively
robust
.
There
are
two
main
negatives
here
:
Repeated
calls
to
fromstring
means
that
this
code
isn't
extremely
fast
.
About
the
same
speed
as
parsing
each
document
individually
","
much
slower
than
if
it
were
all
one
document
Errors
are
thrown
relative
to
the
current
location
in
the
document
.
It
would
be
easy
to
add
relative
location
support
(
just
adding
an
accumulator
to
keep
track
of
current
location
)
Basically
the
approach
is
to
find
the
thrown
errors
and
then
parse
just
the
section
of
the
file
above
the
error
.
If
an
error
that
isn't
related
to
the
last
of
a
root
node
is
thrown
then
it
is
handled
like
a
typical
exception
.
The
find_nth
code
is
shamelessly
stolen
from
this
question
.
It's
possible
that
there
aren't
many
situations
where
this
code
is
deeply
useful
","
but
for
me
with
a
large
number
of
slightly
irregular
documents
(
very
common
with
academic
data
)
it's
invaluable
.
XML
documents
must
have
a
single
root
element
;
otherwise
","
they
are
not
well-formed
","
and
are
","
in
fact
","
not
XML
.
Conformant
parsers
cannot
parse
non-well-formed
""""
XML
""""
.
When
you
construct
your
single
XML
document
out
of
multiple
documents
","
simply
wrap
the
disparate
root
elements
in
a
single
root
element
.
Then
you'll
be
able
to
use
standard
parsers
such
as
lxml
.
So
","
I
assume
this
is
a
pretty
typical
use
case
","
but
I
can't
really
find
anything
about
support
for
this
in
the
lxml
documentation
.
Basically
I've
got
an
xml
file
that
consists
of
a
number
of
distinct
xml
documents
(
reviews
in
particular
)
The
structure
is
approximately
:
Basically
","
I
try
to
read
the
file
in
like
so
:
But
I
get
an
error
when
I
do
so
:
Totally
reasonable
error
","
in
fact
it
is
an
xml
error
and
should
be
treated
as
such
","
but
my
question
is
:
how
do
I
get
lxml
to
recognize
that
this
is
a
list
of
xml
documents
and
to
parse
accordingly
?
Is
magic
a
real
lxml
function
?
How
do
you
run
a
Python
script
from
within
Notepad
+
+
that
opens
in
Windows
Powershell
and
runs
the
script
from
the
directory
that
the
Python
script
is
located
even
if
there
is
a
space
in
the
directory
path
.
To
be
able
to
run
a
Python
script
in
Powershell
you
can
press
F5
to
open
the
run
dialog
box
","
to
run
the
python
script
in
powershell
normally
you
can
use
;
This
however
opens
up
Powershell
in
an
ugly
format
similar
to
what
cmd
looks
like
by
default
however
it
does
have
all
the
features
","
it
also
does
not
run
the
script
from
the
directory
so
relative
paths
don't
work
.
To
make
the
Powershell
look
normal
you
should
run
it
from
the
shortcut
that
is
in
the
start
menu
","
to
allow
it
to
work
if
the
file
path
has
a
space
in
it
you
should
put
a
\
before
the
quotation
marks
.
The
working
run
command
is
therefore
;
Everything
I
found
about
this
via
searching
was
either
wrong
or
incomplete
in
some
way
.
So
","
how
do
I
:
delete
everything
in
my
postgresql
database
delete
all
my
alembic
revisions
make
it
so
that
my
database
is
100
%
like
new
This
works
for
me
:
1
)
Access
your
session
","
in
the
same
way
you
did
session.create_all
","
do
session.drop_all
.
2
)
Delete
the
migration
files
generated
by
alembic
.
3
)
Run
session.create_all
and
initial
migration
generation
again
.
No
idea
how
to
mess
with
alembic
","
but
for
the
database
","
you
can
just
log
into
the
SQL
console
and
use
DROP
DATABASE
foo
.
Or
were
you
wanting
to
clear
out
all
the
data
","
but
leave
the
tables
there
?
If
so
","
Truncating
all
tables
in
a
postgres
database
has
some
good
answers
.
So
in
order
to
parse
a
JSON
with
bash
","
I
use
python
-
mjson.tool
to
grep
the
field
and
awk
to
get
the
value
.
I
used
to
pipe
it
right
after
curl
","
so
I
could
do
something
like
curl
$url
|
python
-
mjson.tool
|
grep
(
something
)
|
awk
(
something
)
But
I
want
to
store
the
curl
output
to
a
variable
and
send
that
to
the
python
-
mjson.tool
.
I
tried
python
-
mjson.tool
$json
or
python
-
mjson.tool
<
<
<
$json
but
neither
has
worked
.
Is
there
a
way
to
feed
in
a
json
variable
to
python
?
Also
","
I
cannot
use
jq
.
In
bash
","
you
need
to
quote
the
variable
.
If
you
don't
","
it
will
collapse
the
whitespace
:
Consider
the
sliders_app.py
example
in
Bokeh
.
I
would
like
to
have
the
ability
to
update
the
plot
at
regular
time
intervals
.
For
example
","
say
we
want
to
time-shift
the
plot
every
10
seconds
.
How
can
I
do
this
in
the
context
of
an
app
?
To
illustrate
the
goal
","
I
would
like
to
add
the
following
extremely
simple
logic
to
the
app
that
simply
shifts
X
and
Y
cyclically
.
Note
that
there
is
no
easy
way
to
insert
this
loop
into
the
original
sliders_app.py
(
where
would
it
go
?
)
.
Is
there
any
way
to
do
this
in
Bokeh
?
Does
Bokeh
perhaps
have
any
timer
widgets
to
which
one
could
hook
up
a
timer
callback
to
update
data
sources
?
If
not
","
is
there
a
plan
to
incorporate
this
functionality
some
time
down
the
road
?
Update
It
looks
like
spectrogram.py
uses
threading
to
handle
this
type
of
updates
.
For
anyone
interested
","
this
may
be
the
way
to
pull
it
off
.
There
are
a
few
options
.
The
AjaxDataSource
can
cause
the
client
to
pull
from
a
REST
endpoint
directly
","
on
a
periodic
basis
.
Here
is
an
example
that
shows
its
use
:
https://github.com/bokeh/bokeh/blob/master/examples/plotting/file/ajax_source_realtime.py
Note
that
the
spectrogram
will
probably
be
rewritten
soon
to
use
this
","
and
reduce
the
amount
of
JS
that
is
written
by
hand
.
(
The
spectrogram
is
fairly
sophisticated
and
has
some
custom
JS
","
we
are
always
trying
to
reduce
that
amount
over
time
)
Also
it
is
worth
mentioning
that
the
threading
in
the
spectrogram
has
to
do
with
the
server
side
of
things
","
it
does
not
really
have
anything
to
do
with
Bokeh
","
per
se
","
or
with
getting
updates
into
Bokeh
.
If
you
are
running
an
app
in
the
Bokeh
server
","
you
can
simply
update
the
data
source
model
however
often
you
like
","
and
the
plot
will
respond
.
This
is
how
you
get
content
of
an
e-mail
i.e.
*
.
eml
file
.
This
works
perfectly
on
Python2.5
-
2.7
.
Try
it
on
3
.
It
should
work
as
well
.
I
programmed
this
for
my
mailgroup
using
mailbox
","
that
is
why
it
is
so
convoluted
.
It
never
failed
me
.
Never
any
junk
.
If
message
is
multipart
","
output
dictionary
will
contain
a
key
""""
files
""""
(
a
sub
dict
)
with
all
filenames
of
extracted
other
files
that
were
not
text
or
html
.
That
was
a
way
of
extracting
attachments
and
other
binary
data
.
You
may
change
it
in
pullout()
","
or
just
change
the
behaviour
of
file_exists()
and
save_file()
.
construct_name()
constructs
a
filename
out
of
message
id
and
multipart
message
filename
","
if
there
is
one
.
In
pullout()
the
Text
and
Html
variables
are
strings
.
For
online
mailgroup
it
was
OK
to
get
any
text
or
HTML
packed
into
multipart
that
wasn't
an
attachment
at
once
.
If
you
need
something
more
sophisticated
change
Text
and
Html
to
lists
and
append
to
them
and
add
them
as
needed
.
Nothing
problematic
.
Maybe
there
are
some
errors
here
","
because
it
is
intended
to
work
with
mailbox.Message()
","
not
with
email.Message()
.
I
tried
it
on
email.Message()
and
it
worked
fine
.
You
said
","
you
""""
wish
to
list
them
all
""""
.
From
where
?
If
you
refer
to
the
POP3
mailbox
or
a
mailbox
of
some
nice
open-source
mailer
","
then
you
do
it
using
mailbox
module
.
If
you
want
to
list
them
from
others
","
then
you
have
a
problem
.
For
example
","
to
get
mails
from
MS
Outlook
","
you
have
to
know
how
to
read
OLE2
compound
files
.
Other
mailers
rarely
refer
to
them
as
*
.
eml
files
","
so
I
think
this
is
exactly
what
you
would
like
to
do
.
Then
search
on
PyPI
for
olefile
or
compoundfiles
module
and
Google
around
for
how
to
extract
an
e-mail
from
MS
Outlook
inbox
file
.
Or
save
yourself
a
mess
and
just
export
them
from
there
to
some
directory
.
When
you
have
them
as
eml
files
","
then
apply
this
code
.
I
found
this
code
much
simpler
I
do
not
known
how
to
load
a
eml
file
in
python
3.4
.
I
want
to
list
all
and
read
all
of
them
in
python
.
I
was
able
to
resolve
it
by
taking
out
the
need
for
the
variable
in
the
route
.
I
am
trying
to
accomplish
having
when
the
user
puts
keywords
into
the
form.search
it
passes
those
keywords
to
the
route
as
the
variable
.
But
I
have
been
unsuccessful
in
accomplishing
this
.
So
far
I
have
this
as
the
bootstrap
","
Here
is
my
flask
route
and
form
.
I
got
2
lists
to
plot
a
time
series
graph
using
matplotlib
I
wrote
a
code
to
plot
a
graph
using
these
two
lists
","
which
is
as
follows
:
I
added
this
set
of
codes
to
plot
the
average
of
the
values
of
list
r1
i.e
:
but
the
code
started
throwing
an
error
saying
:
Is
there
any
direct
method
in
matplotlib
to
add
an
average
line
into
a
graph
?
?
Thanks
for
help
.
.
r1
is
a
list
of
strings
not
actual
floats
/
ints
so
obviously
you
cannot
divide
a
string
by
a
an
int
","
you
need
to
cast
to
float
in
your
lambda
or
convert
the
list
content
to
floats
before
you
pass
it
:
The
change
does
work
:
Also
using
sum
would
be
a
lot
simpler
to
get
the
average
:
I'm
not
100
%
sure
I
understand
what
you're
asking
","
and
am
rusty
with
pyparsing
","
but
I
think
Group
will
help
you
.
I
have
some
data
that
can
be
parsed
using
the
OneorMore
function
from
pyparsing
.
Something
like
","
where
bar
and
foo
are
2
parsers
.
The
problem
with
this
function
is
that
everytime
that
OneorMore
match
foo
and
bar
parsers
in
the
data
stream
","
the
corresponding
values
associated
with
the
keys
""""
foo
""""
and
""""
bar
""""
are
updated.But
","
how
can
I
accumulate
all
the
matched
values
of
foo
and
bar
?
I'm
trying
to
implement
something
like
the
many1
monadic
parser
in
Haskell
","
saving
the
result
of
parsing
foo
and
bar
in
an
Algebraic
Data
type
","
like
How
can
I
do
this
in
python
?
I
have
some
class
instances
that
I
pickle
using
__reduce__
.
These
classes
have
members
that
I
collect
in
another
array
.
I
need
to
pickle
this
array
","
but
I
can't
find
the
right
way
to
do
it
.
Just
to
clarify
things
","
imagine
I
have
classes
that
represent
square
","
rectangle
and
circle
.
I
create
some
instances
:
I
can
pickle
the
list
but
I
need
a
way
to
pickle
this
array
of
the
class
instances
properties
keeping
the
reference
to
the
original
objects
so
that
if
one
object
changes
the
relative
property
changes
:
imagine
I
call
C.grow(2)
I
expect
to
have
dimensions
[2]
=
12
.
To
solve
the
problem
I
now
pickle
this
dictionary
:
but
I
think
this
is
a
very
poor
solution
.
I
appreciate
any
suggestion
.
If
the
issue
is
that
your
class
methods
are
not
able
to
be
pickled
","
then
you
might
try
dill
.
It
typically
can
pickle
class
instances
","
class
methods
","
and
instance
methods
.
The
only
way
to
keep
the
original
reference
of
the
objects
if
you
have
things
like
A.side
which
is
an
int
is
to
either
keep
it
in
a
list
or
a
class
of
it's
own
.
For
example
","
you
can
do
this
:
Now
","
you
can
make
Square
","
Rectangle
and
Circle
such
that
they
use
the
class
Value
for
each
of
their
variables
.
and
then
use
it
in
dimensions
like
dimensions
=
[
A.side
","
...
]
Note
","
now
that
A.side
is
a
class
-
the
value
cannot
be
seen
using
print(A.side)
but
can
be
seen
using
print(A.side.value)
I
came
out
with
this
solution
:
I
don't
think
this
is
the
ideal
solution
but
","
at
least
","
with
it
I
don't
have
to
change
the
other
classes
(
Square
","
Rectangle
","
Circle
in
the
example
)
.
Thank
you
all
for
your
help
.
I
have
a
column
called
maturity_dt
filled
with
datetime
objects
in
my
dataframe
df
","
and
I
am
just
trying
to
select
only
the
rows
in
the
column
which
have
a
maturity_dt
in
August
or
February
.
So
","
I
am
trying
to
delete
all
the
rows
that
do
not
correspond
with
these
months
dynamically
using
the
code
below
.
However
","
I
get
the
error
IndexError
:
index
109235
is
out
of
bounds
for
axis
0
with
size
44681
despite
using
reset_index
","
so
I
am
wondering
if
there
is
another
way
to
delete
rows
dynamically
.
Thank
You
Can
you
reindex
by
date
?
This
would
work
:
A
couple
of
things
:
(
a
)
you
have
to
wait
until
the
page
is
loaded
(
or
at
least
the
part
of
the
page
that
you
are
interested
in
)
(
b
)
only
maximized
browser
window
works
for
me
(
depends
on
device
/
resolution
)
(
c
)
you
are
trying
to
click
the
wrong
element
Read
the
documentation
regards
waiting
in
selenium
.
EDIT
Manged
to
get
one
step
further
(
switch
to
iframe
)
","
but
the
following
code
raises
selenium.common.exceptions.ElementNotVisibleException
:
Message
:
Element
is
not
currently
visible
and
so
may
not
be
interacted
with
To
enter
text
into
an
input
box
with
Selenium
(
e.g
","
your
user
name
into
the
username
field
)
","
use
the
send_keys
method
of
the
element
:
You
can
also
try
Selenium's
action_chains
module
","
as
in
the
following
code
(
untested
)
","
to
get
past
the
roadblock
from
@dm295's
answer
:
I
am
getting
very
frustrated
trying
to
login
to
costar.com
with
python
and
selenium
.
I
have
tried
it
on
the
chrome
browser
and
firefox
browser
","
but
can't
figure
out
the
correct
code
.
I
have
logged
into
other
websites
","
but
cannot
figure
out
how
to
input
text
into
the
login
boxes
for
this
site
.
Here's
what
I
have
so
far
:
How
is
it
possible
that
there
can
be
a
list
of
elements
","
but
yet
not
have
an
initial
element
?
If
anyone
can
figure
out
how
to
input
text
into
the
username
and
password
text
boxes
of
costar.com
","
I
will
be
greatly
appreciative
.
I
can't
figure
this
out
for
the
life
of
me
!
Hello
everyone
","
First
of
all
I
am
a
newbie
to
coding
and
I
am
learning
now
.
So
","
please
excuse
me
for
my
doubt
!
My
data
is
as
follows
:
and
so
on
....
till
Topic
100
with
the
same
format
.
Here
the
first
line
is
the
topic
number
and
it's
weight
.
The
following
are
the
words
in
that
topics
and
their
weights
in
that
topic
.
I
want
to
find
the
probability
of
each
of
the
word
.
That
is
divide
each
of
the
word's
weight
with
it's
respective
topic
weight
.
For
example
","
and
so
on
...
where
these
values
are
the
results
of
the
word's
weight
/
the
topic
weight
.
If
it
is
a
normal
column
division
then
","
I
would
have
used
col2
/
col1
technique
but
this
quite
challenging
.
So
","
please
guide
me
.
Thanks
in
advance
!
You
don't
say
anything
at
all
about
what
you
want
your
output
format
to
look
like
","
or
even
give
an
example
of
such
","
but
this
should
at
least
point
you
in
the
right
direction
...
Suggested
python
starting
point
","
which
is
what
your
edit
seems
to
indicate
is
your
desired
output
","
aside
from
floating
point
rounding
concerns
:
With
your
above
sample
input
","
this
code
produces
:
I
just
installed
scrapyd
on
Ubuntu
14.04
and
after
the
installation
","
I
simply
typed
""""
scrapyd
""""
into
the
command
line
and
got
:
Failed
to
load
application
:
No
module
named
txweb
I
used
both
the
general
and
Ubuntu
specific
installations
and
neither
seem
to
work
.
Just
for
some
background
","
I
decided
to
use
scrapyd
after
having
created
a
project
that
ran
multiple
spiders
but
was
not
able
to
pipeline
the
scraped
data
into
two
different
tables
.
I
looked
at
some
other
SO
posts
like
this
one
","
but
they
seem
to
be
addressing
older
versions
of
scrapyd
which
probably
won't
be
relevant
anymore
.
Can
someone
show
me
how
to
get
started
with
scrapyd
","
because
the
information
provided
in
the
docs
doesn't
seem
to
be
working
for
me
","
thanks
.
although
the
docs
has
apt-get
...
pip
is
the
better
way
to
install
scrapy
(
Never
failed
for
me
)
sudo
pip
install
scrapyd
works
for
me
and
then
scrapyd
doesn't
give
any
errors
.
When
you
install
with
apt-get
-
twisted
may
not
be
getting
installed
","
but
pip
takes
care
of
that
.
Try
uninstalling
your
apt-get
scrapyd
first
","
and
then
use
pip
to
install
it
-
-
-
-
-
-
-
-
-
-
-
-
-
-
UPDATE
-
-
-
-
-
-
-
-
-
-
-
-
Answering
your
next
question
in
the
comment
...
If
you
want
to
run
scrapyd-deploy
you
need
to
also
install
the
package
scrapy-client
which
is
not
installed
along
with
scrapy
.
This
is
mentioned
in
the
docs
-
http://scrapyd.readthedocs.org/en/latest/deploy.html
You
can
install
it
using
sudo
pip
install
scrapyd-client
reduce
reduces
an
iterable
down
to
a
single
object
","
so
after
this
line
args
is
not
an
iterable
.
This
reduces
args
down
to
a
single
Q
object
that
ORs
all
of
the
items
in
query_word_list
together
.
If
you
want
to
pass
it
using
the
*
syntax
","
you'll
need
to
wrap
it
in
an
iterable
","
i.e.
args
=
(
args
","
)
.
I'm
trying
to
create
a
search
form
where
a
user
can
choose
from
a
few
options
to
filter
the
search
.
I'm
getting
this
error
:
sortedByScore()
argument
after
*
must
be
a
sequence
","
not
Q
views.py
:
code
for
my
custom
manager
sortedByScore
:
Your
first
parameter
to
"sortedByScore(*args, **kwargs)"
is
a
Q
object
:
You
will
need
to
redefine
your
method
to
pass
in
different
parameters
and
/
or
perform
the
reduce
inside
the
method
.
Try
to
do
something
like
that
on
the
test
:
Perhaps
if
you
send
the
data
as
json
it
will
work
.
This
is
because
QueryDict
returns
the
last
value
of
a
list
in
__getitem__
:
QueryDict.getitem(key)
Returns
the
value
for
the
given
key
.
If
the
key
has
more
than
one
value
","
getitem()
returns
the
last
value
.
Raises
django.utils.datastructures.MultiValueDictKeyError
if
the
key
does
not
exist
.
(
This
is
a
subclass
of
Pythonâ€™s
standard
KeyError
","
so
you
can
stick
to
catching
KeyError
.
)
https://docs.djangoproject.com/en/1.8/ref/request-response/#django.http.QueryDict.getitem
If
you
post
a
form
","
in
which
a
key
maps
to
a
list
:
this
is
what
you
get
in
the
request
body
:
Since
the
test
method
post
the
data
as
a
form
","
what
you
get
from
request.data
is
a
QueryDict
(
the
same
as
request.POST
)
","
hence
you
get
the
last
value
in
the
list
when
getting
request.data
.
To
get
expected
behavior
","
post
the
data
as
JSON
in
the
request
body
(
as
in
@Vladir
Parrado
Cruz's
answer
)
.
By
default
the
QueryDict
will
return
a
single
item
from
the
list
when
doing
a
getitem
call
(
or
access
by
square
brackets
","
such
as
you
do
in
request.data
[
'
material
'
]
)
You
can
instead
use
the
getlist
method
to
return
all
values
for
the
key
:
https://docs.djangoproject.com/en/1.8/ref/request-response/#django.http.QueryDict.getlist
I'm
developing
a
REST
api
with
Django
and
REST-framework
.
I
have
endpoint
which
takes
a
POST
request
with
this
kind
of
json
:
It
is
a
machine-learning
analysis
api
and
the
json
tells
to
use
Bayes
classifier
to
provided
strings
and
return
results
.
This
works
fine
when
I
test
it
manually
by
doing
the
post
requests
.
However
","
it
breaks
down
when
I
try
to
write
an
unit
test
.
I
have
the
following
test
:
test
fails
everytime
because
of
the
latter
assert
gives
""""
AssertionError
:
'
P
'
!
=
1
""""
Here
is
my
view
code
:
The
really
interesting
part
was
when
I
fired
the
debugger
to
check
what
happens
here
.
Turns
out
that
the
line
gives
the
last
entry
of
the
list
in
in
my
request
","
in
this
case
""""
Paska
kesä
taas
.
Kylmää
ja
sataa
""""
However
","
while
I
inspect
the
contents
of
the
request.data
","
it
shows
a
querydict
with
lists
pipeline
and
material
as
they
are
in
request
.
Why
do
I
get
string
instead
of
material
list
when
I
call
request.data
[
""""
material
""""
]
?
Is
there
something
I
have
forgotten
and
I
have
to
specify
some
kind
of
serializer
?
And
why
it
works
during
normal
execution
but
not
with
tests
?
I'm
using
Django
1.8
with
Python
3
.
Also
","
I'm
not
tying
the
view
to
any
specific
model
.
Finally
here
is
what
my
debugger
shows
when
I
put
break
points
into
view
:
request.data
:
asd
=
request.data
[
""""
material
""""
]
:
I
am
trying
to
make
a
very
simple
application
that
allows
for
people
to
define
their
own
little
python
scripts
within
the
application
.
I
want
to
execute
the
code
in
a
new
process
to
make
it
easy
to
kill
later
.
Unfortunately
","
Python
keeps
giving
me
the
following
error
:
My
code
is
posted
below
What
am
I
missing
?
Is
there
something
wrong
with
my
user_input_string
?
With
my
compile
options
?
Any
help
would
be
appreciated
.
I
believe
args
must
be
a
tuple
.
To
create
a
single-element
tuple
","
add
a
comma
like
so
:
args=(user_input_code
","
)
I've
modified
a
.
py
file
on
a
production
server
and
the
changes
are
not
being
applied
.
At
first
I
thought
this
was
because
the
associated
.
pyc
file
was
not
being
updated
.
So
I
deleted
the
.
pyc
file
and
tested
hoping
the
.
py
file
would
compile
into
a
new
.
pyc
file
.
The
applications
behavior
did
not
update
and
a
new
.
pyc
file
was
not
created
.
This
was
done
based
off
a
previous
question
.
How
is
it
that
the
script
is
executing
without
generating
a
new
.
pyc
file
?
Here
are
the
two
files
in
question
:
admin_email.pyc
has
been
deleted
.
Only
admin_email.py
remains
on
there
server
.
Here
is
the
code
that
is
executing
the
script
that
has
been
changed
:
The
application
is
being
served
with
gunicorn
+
nginx
.
Any
ideas
on
what
the
issue
is
?
Why
is
a
new
.
pyc
file
being
created
?
More
importantly
why
is
the
applications
behavior
not
being
updated
?
Im
assuming
you're
suing
wsgi
or
fcgi
-
most
probably
on
apache
or
nginx
With
wsgi
-
normally
apache
will
cache
your
django
for
you
.
So
","
you
need
to
tell
apache
/
nginx
/
server-provider
that
your
django
code
has
changed
.
The
way
you
do
this
is
by
changing
the
""""
last
edited
""""
meta
information
of
your
.
wsgi
file
.
So
","
you
simple
do
touch
/
path
/
to
/
django.wsgi
and
it
will
normally
work
.
If
you
use
fcgi
","
the
same
thing
applies
.
I
need
to
search
if
any
item
of
a
list
contains
a
specific
string
.
At
the
moment
I
use
this
code
:
The
problem
is
that
if
I
try
so
search
Height
I
got
this
:
I
need
to
match
only
Height
included
in
Height
6
'
element
","
so
that
item
[0]
will
be
the
one
that
I
need
.
What's
the
best
way
to
do
that
?
You
can
split
your
items
and
then
check
:
Demo
:
As
a
more
general
way
you
can
use
regex
to
search
for
your
pattern
:
Demo
:
Just
use
a
single
dictionary
for
totalizing
...
I
have
multiple
lists
of
tuples
containing
a
name
and
a
count
and
I
want
to
merge
them
by
their
name
with
the
sum
of
their
respective
count
.
For
example
","
I
have
:
And
I
would
like
to
get
:
Given
two
numpy
array
masks
","
created
with
the
3rd
and
4th
columns
of
data
of
7
columns
total
:
How
can
I
mask
data
which
are
masked
by
either
exp_mask
or
loggf_mask
?
The
logic
of
what
I
am
trying
to
describe
is
:
I
believe
you
are
looking
for
a
bitwise
or
","
which
is
|
.
You
can
use
either
bitwise_or
","
which
also
has
the
|
shorthand
","
or
logical_or
.
Both
will
work
since
your
array
will
be
of
type
bool
:
You
can
use
np.any()
to
evaluate
boolean
or
on
masks
:
I
have
2
arrays
containing
zeros
&
ones
.
I
want
to
perform
hstack()
on
them
but
not
getting
the
desired
output
.
Python
Code
.
.
Current
Output
.
.
Expected
Output
.
.
I
can't
understand
what
silly
mistake
I'm
doing
.
np.hstack((np.zeros(8)
","
np.ones(8)
)
)
.
astype(int)
for
np.array
output
","
or
"map( int, np.hstack((np.zeros(8)"
","
np.ones(8)
)
)
)
for
list
output
You
must
tell
numpy
to
return
the
values
as
integers
To
print
out
zerosThenOnes
like
this
"[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]"
Use
:
Numpy
Zeros
If
i
understood
you
right
then
this
is
your
code
","
and
this
is
your
access
to
the
x
variable
:
Given
code
like
the
following
:
How
do
you
get
the
value
of
x
in
class
b
to
be
used
in
class
a
?
I
an
example
like
this
:
How
can
I
get
the
value
of
Y
in
class
A
The
issue
you're
having
is
that
your
code
expects
the
""""
-
mc
""""
suffix
to
appear
in
lowercase
","
but
you're
calling
the
upper()
method
on
the
input
string
","
resulting
in
text
that
is
all
upper
case
.
You
need
to
change
one
of
those
so
that
they
match
(
it
doesn't
really
matter
which
one
)
.
Either
replace
the
upper()
call
with
lower()
","
or
replace
the
string
""""
-
mc
""""
with
""""
-
MC
""""
","
and
your
code
should
work
better
(
I'm
not
certain
I
understand
all
of
it
","
so
there
may
be
other
issues
)
.
I
have
a
file
at
/
location
/
data.txt
.
In
this
file
I
have
entry
like
:
aaa:xxx:abc.com:1857:xxx1:rel5t2:y
ifa:yyy:xyz.com:1858:yyy1:rel5t2:y
I
want
to
access
'
aaa
'
from
my
code
either
I
mention
aaa
while
giving
the
input
in
caps
or
small
after
running
my
python
code
it
should
return
me
aaa
is
the
right
item
But
here
I
want
to
include
one
exception
that
if
I
give
the
input
with
-
mc
suffix
(
aaa-mc
)
either
in
small
latters
or
in
caps
it
should
ignore
the
-
mc
.
Below
is
the
my
code
and
output
as
well
which
I
am
getting
now
.
def pITEMName():
When
I
execute
above
code
it
prompts
me
like
:
Enter
pipe
separated
list
of
ITEMS
:
And
if
I
provide
the
list
like
:
Enter
pipe
separated
list
of
ITEMS
:
aaa-mc|ifa
it
gives
me
the
result
as
:
Total
Distint
item
Count
:
2
AAA-MC
is
wrong
item
Name
items
Belonging
to
other
:
Other
center
:
item
Count
From
Other
center
=
0
items
Belonging
to
Current
Centers
:
Active
items
in
US1
:
^
IFA$
Active
items
in
US2
:
^
AAA$
Ignored
item
Count
From
Current
center
=
0
You
Have
Entered
itemList
belonging
to
this
center
as
:
^
IFA$|^AAA$
Active
item
Count
:
2
Do
You
Want
To
Continue
[
YES|Y|NO|N
]
:
As
you
must
be
see
in
above
result
aaa
is
coming
as
valid
count
(
active
item
count
:
2
)
because
its
available
in
/
location
/
data.txt
file
.
but
also
its
coming
as
AAA-MC
is
wrong
item
name
(
2nd
line
from
above
result
)
.
I
want
'
-
mc
or
-
MC
'
to
ignore
with
any
item
present
or
non
present
in
/
location
/
data.txt
file
.
Please
let
me
know
what's
wrong
with
my
above
code
to
achieving
this
.
The
way
you
are
constructing
ITEMList
is
by
reading
in
a
string
","
capitalizing
it
(
with
upper()
)
","
and
stripping
all
whitespace
.
Therefore
","
something
like
'
aaa-mc
'
is
being
converted
to
'
AAA-MC
'
.
You're
later
splitting
this
uppercase
string
on
the
token
'
-
mc
'
","
which
is
impossible
for
it
to
contain
","
so
.
I'd
reccommed
either
replacing
upper()
with
lower()
when
you
are
reading
your
string
in
","
or
doing
a
hard
replace
on
the
types
of
'
-
mc
'
","
so
instead
of
try
using
in
your
list
comprension
.
I
need
some
help
on
returning
to
the
top
of
the
loop
in
python
I
know
about
the
break
statement
but
I
do
not
know
if
its
helpful
at
all
.
Here
is
my
code
:
If
what
you
want
to
do
is
run
that
part
of
your
program
over
and
over
(
effectively
jumping
back
up
to
the
top
)
","
you
can
use
a
while
true
loop
to
accomplish
that
:
There
is
no
loop
in
your
code
but
maybe
this
will
help
?
or
look
into
for
or
while
loops
.
No
.
That
widget
is
part
of
the
toolbar
.
Just
use
a
BitmapButton
or
a
PlateButton
.
They
both
support
images
and
would
be
more
flexible
to
use
anyway
.
I
want
to
create
a
ToolBarToolBase
object
without
adding
it
visibly
to
the
toolbar
.
I've
tried
instantiating
ToolBarToolBase
","
but
I'm
using
wxPython
version
2.8.12.1
(
I
can't
upgrade
","
I'm
doing
this
for
work
)
so
I
don't
think
it's
fully
implemented
.
Is
there
any
way
to
do
this
?
I
need
to
generate
1MB
image
file
to
do
a
teste
on
django
.
I
can
only
set
the
image
size
","
but
not
the
image
file
size
.
The
simplest
way
is
to
save
a
BMP
image
","
which
has
no
compression
:
And
if
you
need
to
use
PNG
","
you
can
fill
it
with
random
data
:
File
size
depends
on
what
is
on
the
picture
.
PNG
compresses
the
data
","
so
the
blank
image
would
be
way
smaller
than
a
photo
(
with
the
same
resolution
)
.
You
may
use
an
iterative
approach
:
filp
one
pixel
","
save
to
file
","
check
size
.
Then
flip
another
one
.
Faster
:
use
bisection
.
i
think
i
got
what
you
want
.
i
just
moved
the
if
statement
of
the
used
letters
to
the
top
of
your
while
","
and
instead
of
appending
to
the
lettersguessed
the
guess
variable
","
i
appended
the
userinput
variable
.
So
I
am
new
to
coding
and
as
part
of
my
course
I
am
making
a
hangman
game
.
When
I
run
the
program
","
it
says
""""
[
'
'
","
'
'
","
'
'
","
'
'
","
'
'
","
'
'
","
'
'
","
'
'
]
Input
:
""""
.
I
want
to
make
it
so
that
the
apostrophes
and
commas
are
not
printed
.
I
feel
like
the
.
join()
function
would
be
the
best
way
to
do
this
","
but
as
I
am
still
learning
","
I
am
not
sure
how
and
/
or
where
to
place
the
function
.
Please
bear
with
me
!
Thank
you
!
!
add
at
the
begining
:
and
after
the
input-one-chart-checking
add
checking
for
permitted
letter
:
From
what
I
understand
","
you
cannot
specify
five
nodes
serve
as
map
nodes
and
one
as
a
reduce
node
within
a
single
spark
cluster
.
You
could
have
two
clusters
running
","
one
with
five
nodes
for
running
the
map
tasks
and
one
for
the
reduce
tasks
.
Then
","
you
could
break
your
code
into
two
different
jobs
and
submit
them
to
the
two
clusters
sequentially
","
writing
the
results
to
disk
in
between
.
However
","
this
might
be
less
efficient
than
letting
Spark
handle
shuffle
communication
.
In
Spark
","
the
call
to
.
map()
is
""""
lazy
""""
in
the
sense
that
it
does
not
execute
until
the
call
to
an
""""
action
.
""""
In
your
code
","
this
would
be
the
call
to
.
collect()
.
See
https://spark.apache.org/docs/latest/programming-guide.html
Out
of
curiosity
","
is
there
a
reason
you
want
one
node
to
handle
all
reductions
?
Also
","
based
on
the
documentation
the
.
sample()
function
takes
three
parameters
.
Could
you
post
stderr
and
stdout
from
this
code
?
I
am
learning
Apache-Spark
as
well
as
its
interface
with
AWS
.
I've
already
created
a
master
node
on
AWS
with
6
slave
nodes
.
I
also
have
the
following
Python
code
written
with
Spark
:
My
question
is
how
I
can
set
up
the
6
slave
nodes
in
AWS
","
such
that
5
slave
nodes
do
the
mapping
work
as
I
mentioned
in
the
code
","
and
the
other
slave
node
do
the
reducing
work
.
I
really
appreciate
if
anyone
helps
me
.
I
have
a
queue
from
which
I
need
to
get
chunks
of
10
entries
and
put
them
in
a
list
","
which
is
then
processed
further
.
The
code
below
works
(
the
""""
processed
further
""""
is
","
in
the
example
","
just
print
the
list
out
)
.
This
code
is
ugly
.
I
do
not
see
how
to
improve
it
-
I
read
some
time
ago
how
to
use
iter
with
a
sentinel
but
fail
to
see
how
my
problem
could
make
use
of
it
.
Is
there
a
better
(
=
more
elegant
/
pythonic
)
way
to
solve
the
problem
?
You
could
use
iter
twice
:
"iter(q.get, 'END')"
returns
an
iterator
which
can
iterate
over
the
values
in
the
queue
until
'
END
'
is
returned
by
q.get()
.
Then
you
could
use
the
grouper
recipe
to
group
the
iterator
into
chunks
of
10
items
.
yields
You
have
tabs
","
not
spaces
","
in
your
DNS
file
.
Try
this
:
python
2.7.3
:
I'm
sure
I'm
missing
something
simple
but
not
able
to
figure
it
out
I
have
a
text
file
and
I
am
reading
file
line
by
line
:
each
line
i
am
matching
for
some
pattern
:
I
am
getting
""""
No
match
""""
for
all
in
file
I
have
following
lines
which
i
am
looking
:
I
am
trying
same
pattern
and
line
on
http://pythex.org/
and
its
matching
but
not
with
this
code
.
OK
","
got
it
figured
.
The
Artifactory
REST
call
for
Property
Search
does
indeed
return
REST
like
URIs
.
There's
a
different
REST
call
for
File
Info
.
The
results
from
the
File
Info
call
include
a
downloadUri
.
So
","
I
first
have
to
use
the
Property
Search
REST
call
","
take
the
results
from
that
","
massage
the
resulting
URI
a
bit
to
get
the
file
path
.
That
path
is
like
""""
com
/
companyname
/
appgroup
/
artifactname
/
version
/
filename
""""
.
I
used
urllib.parse
to
help
with
that
","
but
still
had
to
massage
the
result
a
bit
as
it
still
included
""""
artifactory
/
repo-key
""""
.
I
use
the
repo
key
and
file
path
to
make
another
REST
call
for
File
Info
.
The
results
of
that
include
the
downloadUri
.
This
seems
like
the
long
way
around
","
but
it
works
.
It's
still
not
as
elegant
as
I'd
like
.
Two
REST
calls
take
some
time
to
finish
.
I
want
a
list
of
URLs
that
will
download
artifacts
from
Artifactory
.
The
list
of
URLs
should
come
from
a
REST
query
.
I
am
successfully
calling
the
Artifactory
(
3.2.2
)
REST
API
for
various
data
I
need
.
In
one
case
","
I
am
doing
a
property
search
","
searching
for
artifacts
with
an
""""
application_name
""""
and
""""
release_version
""""
.
These
properties
were
added
by
TeamCity
when
the
artifacts
were
deployed
.
I
can
successfully
search
Artifactory
using
the
Artifactory
Property
Search
tool
in
the
web
console
","
and
I
can
successfully
search
with
those
same
terms
from
my
python
script
.
The
REST
call
returns
json
.
Within
that
json
is
an
array
of
dicts
","
and
each
of
those
is
a
{
uri
:
url
}
.
All
good
","
but
not
quite
.
The
URL
returns
a
404
when
pasted
into
a
web
browser
.
By
walking
thru
the
url
","
I
discover
that
the
/
api
/
storage
part
is
what's
throwing
off
the
browser
.
I
suspect
that's
because
this
URI
is
not
meant
for
browsers
","
but
for
another
REST
query
.
Sheesh
.
The
documentation
is
not
clear
on
this
.
It
sure
seems
like
I
should
be
able
to
get
a
proper
browser
URL
from
a
REST
call
.
Example
URL
:
""""
"http://ourserver.org:8081/artifactory/api/storage/our-releases/com/companyname/Training/1.7.4/Training.ipa"""
It's
easy
to
replace
""""
/
api
/
storage
""""
with
""""
/
simple
""""
in
that
URL
string
and
that
makes
the
URL
work
in
a
browser
.
I
just
think
it's
an
ugly
solution
.
I
mostly
think
I'm
missing
something
","
perhaps
obvious
.
Suggestions
welcome
!
As
explained
in
the
Artifactory
REST
API
","
optional
headers
can
be
used
to
add
extra
information
of
the
found
artifact
.
For
instance
","
you
could
try
adding
the
header
'
X-Result-Detail
:
info
'
","
which
will
return
the
downloadUri
property
as
well
:
Using
any
of
the
above
command
you
can
get
the
downloadUri
and
you
could
even
easily
parse
it
with
python
:
Artifactory
REST
API
:
https://www.jfrog.com/confluence/display/RTF/Artifactory+REST+API
Consider
the
following
cases
:
If
you
want
a
unicode
string
","
and
you
already
have
a
unicode
string
","
you
need
do
nothing
.
If
you
want
a
bytestring
","
and
you
already
have
a
bytestring
","
you
need
do
nothing
.
If
you
have
a
unicode
string
and
want
a
bytestring
","
you
encode
it
.
If
you
have
a
bytestring
and
want
a
unicode
string
","
you
decode
it
.
In
none
of
these
cases
is
it
appropriate
to
encode
or
decode
more
than
once
.
I
have
a
legacy
code
segment
that
always
encode('utf-8')
for
me
when
I
pass
in
an
unicode
string
(
directly
from
database
)
","
is
there
a
way
to
change
unicode
string
to
other
format
to
allow
it
to
be
encoded
to
'
utf-8
'
again
without
getting
an
error
","
since
I
am
not
allowed
to
change
the
legacy
code
segment
.
I've
tried
decoding
it
first
but
it
returns
this
error
If
I
leave
the
unicode
string
as
is
it
returns
If
I
change
the
legacy
code
to
not
encode('utf-8')
it
works
","
but
this
is
not
a
viable
option
Edit
:
Here
is
the
code
snippet
For
some
reason
if
I
skip
#
2
I
don't
get
the
error
that
I
mentioned
above
","
I
double
check
the
type
for
the
string
","
it
seems
like
both
is
unicode
","
and
both
is
the
same
character
","
but
the
code
I
am
working
on
does
not
allow
me
to
encode
or
decode
to
utf-8
","
while
the
same
character
in
some
snippet
allows
me
to
do
that
.
In
order
for
encode('utf-8')
to
make
sense
","
the
string
must
be
a
unicode
string
(
or
contain
all-ASCII
characters
...
)
.
So
","
unless
it's
a
unicode
instance
already
","
you
have
to
decode
it
first
from
whatever
encoding
it's
in
to
a
unicode
string
","
after
which
you
can
pass
it
into
your
legacy
interface
.
At
no
point
does
it
make
sense
for
anything
to
be
double-encoded
-
-
encoding
takes
a
string
and
transforms
it
to
a
series
of
bytes
;
decoding
takes
a
series
of
bytes
and
transforms
them
back
into
a
string
.
The
confusion
only
arises
because
Python
2
uses
the
str
for
both
plain-ASCII
strings
and
byte
sequences
.
When
you
issue
a
Request
","
passing
dont_filter=True
would
turn
off
the
OffSiteMiddleware
and
the
url
would
not
be
filtered
with
allowed_domains
:
If
the
request
has
the
dont_filter
attribute
set
","
the
offsite
middleware
will
allow
the
request
even
if
its
domain
is
not
listed
in
allowed
domains
.
I
have
been
looking
for
a
better
way
to
scrape
external
website
from
another
a
main
source
website
.
To
better
explain
it
let
me
use
an
example
with
yelp.com
to
explain
what
am
trying
to
do
(
though
my
target
is
not
yelp
)
.
I
would
scrape
title
and
address
visit
link
that
titles
leads
to
to
get
company
website
I
would
like
to
extract
emails
from
source
code
of
the
main
website
.
(
I
know
its
difficult
","
but
i
am
not
crawling
all
pages
i
am
assuming
most
site
have
contact
in
their
url
e.g
site.com
/
contact.php
)
The
point
is
while
scraping
from
yelp
and
storing
data
in
a
field
","
I
want
to
get
external
data
from
a
companies
main
website
.
Below
is
my
code
can't
figure
out
how
to
do
it
using
scrapy
.
I
am
inserting
into
Cassandra
Cassandra
2.0.13(single node for testing)
by
python
cassandra-driver
version
2.6
The
following
are
my
keyspace
and
table
definitions
:
What
I
tried
:
1
)
multiprocessing(protocol version set to 1)
each
process
has
its
own
cluster
","
session(default_timeout set to 30.0)
2
)
batch
insert
(
protocol
version
set
to
2
because
BatchStatement
is
enabled
on
Cassandra
2.X
)
the
above
function
is
invoked
by
:
tuples
are
stored
in
batch_queue
.
3
)
synchronizing
execution
Several
days
ago
I
post
another
question
Cassandra
update
fails
","
cassandra
was
complaining
about
TimeOut
issue
.
I
was
using
synchronize
execution
for
updating
.
Can
anyone
help
","
is
this
my
code
issue
or
python
cassandra-driver
issue
or
Cassandra
itself
?
Thanks
a
million
!
If
your
question
is
about
those
errors
at
the
top
","
those
are
server-side
error
responses
.
The
first
says
that
the
coordinator
you
contacted
cannot
satisfy
the
request
at
CL.ONE
","
with
the
nodes
it
believes
are
alive
.
This
can
happen
if
all
replicas
are
down
(
more
likely
with
a
low
replication
factor
)
.
The
other
two
errors
are
timeouts
","
where
the
coordinator
didn't
get
responses
from
'
live
'
nodes
in
a
time
configured
in
the
cassandra.yaml
.
All
of
these
indicate
that
the
cluster
you're
connected
to
is
not
healthy
.
This
could
be
because
it
is
overwhelmed
(
high
GC
pauses
)
","
or
experiencing
network
issues
.
Check
the
server
logs
for
clues
.
Finally
I
have
found
buggy
code
.
I
had
an
error
in
this
line
in
C
+
+
server
:
Instead
of
the
buggy
code
above
it
should
be
:
I
understood
how
to
convert
C
+
+
string
into
ZMQ
string
because
I've
found
this
function
on
the
web
:
Below
is
the
link
to
zhelpers.hpp
header
file
which
contains
the
function
pasted
above
and
many
other
useful
functions
for
C
+
+
ZMQ
based
application
:
https://github.com/imatix/zguide/blob/master/examples/C%2B%2B/zhelpers.hpp
I'm
developing
zmq
/
protobuf
application
and
I
have
a
problem
with
deserialization
of
messages
sent
from
C
+
+
to
python
.
I
easily
handle
messages
from
python
to
C
+
+
however
in
the
other
direction
I
have
a
problem
.
Protobuf
library
in
python
client
application
complains
that
it
detected
:
File
""""
C:\Python27\lib\site-packages\google\protobuf\internal\python_message.py
""""
","
line
844
","
in
MergeFromString
raise
message_mod.DecodeError('Unexpected end-group tag.')
I
presume
there
is
a
problem
between
C
+
+
serizalization
and
python
deserialization
.
I'm
wondering
if
there
is
some
problem
with
null
terminator
in
C
/
C
+
+
.
I'm
using
RaspberryPi
running
Raspian
for
C
+
+
code
and
x64
CPU
running
Windows
7
for
python
code
.
This
is
my
C
+
+
serialization
code
.
.
This
is
my
python
deserialization
code
.
.
I
debugged
that
deserialization
fails
in
this
function
\
google\protobuf\internal\python_message.py
Could
you
help
me
with
enabling
my
application
?
I
have
a
QMainWindow
app
consisting
of
a
Menu
Bar
","
Splitter
in
the
central
widget
","
and
a
status
bar
.
On
the
left
side
of
the
splitter
is
a
widget
containing
some
controls
(
list
","
combo
","
and
button
)
and
on
the
right
is
a
widget
containing
a
layout
of
Matplotlib
canvas
","
and
NavigationToolBar
.
I
was
able
to
use
QT
Stylesheets
to
set
the
various
background
colors
of
everything
...
except
the
Matplotlib
side
.
I
attempted
to
use
the
following
but
it
has
no
effect
.
I
also
tried
setting
the
NavBar
and
Canvas
stylesheets
directly
...
but
again
it
didn't
work
:
Basically
I
am
trying
to
turn
the
background
color
of
the
NavToolbar
and
the
border
around
the
canvas
to
match
everything
else
(
Currently
cyan
so
it
shows
up
easy
)
...
Any
help
would
be
appreciated
.
Full
code
&
sample
image
below
:
I
think
I
may
have
a
solution
thanks
to
some
tips
from
Maxwell
Grady
.
I
changed
the
following
2
lines
:
Note
the
lack
of
a
""""
.
""""
before
QWidget
and
Picture
of
GUI
after
changes
.
Now
that
I
can
actually
set
some
style
properties
...
time
to
make
it
less
ugly
.
I
fuzzy
matched
a
list
of
movie
titles
and
compiled
them
into
another
list
of
each
comparison
along
with
the
match
values
:
I
want
to
add
up
the
match
values
for
each
title
so
that
I
get
output
like
this
:
I
have
tried
several
implementations
utilizing
slices
but
it's
ugly
.
(
Not
my
exact
code
but
this
is
the
ugliness
)
:
what
is
a
more
efficient
way
to
accomplish
this
?
You
can
use
a
dict
to
store
the
results
you
want
","
and
then
at
the
end
if
you
want
a
list
of
tuples
","
you
can
use
dict.items()
(
Python
3.x
)
to
get
that
.
Example
-
You
do
not
need
list(...)
at
the
end
if
you
are
using
Python
2.x
.
pyspark
relies
on
the
spark
SDK
.
You
need
to
have
that
installed
before
using
pyspark
.
Once
that's
set
you
need
to
set
the
environment
variable
SPARK_HOME
to
tell
pyspark
where
to
look
for
your
spark
installation
.
If
you're
on
a
*
nix
system
you
can
do
so
by
adding
the
follow
to
your
.
bashrc
If
you're
using
Windows
there's
a
convoluted
way
of
setting
variables
via
GUI
here
.
Through
DOS
you
can
use
set
in
the
place
of
export
:
when
I
try
:
I
get
:
What
is
the
solution
?
I
need
to
be
able
to
determine
(
or
predict
)
when
a
unicode
character
won't
be
printable
.
For
instance
","
if
I
print
this
unicode
character
under
default
settings
","
it
prints
fine
:
But
if
I
print
another
unicode
character
","
it
prints
as
a
stupid
","
weird
square
:
I
really
need
to
be
able
to
determine
before
a
character
is
printed
if
it
will
display
like
this
as
an
ugly
square
(
or
sometimes
as
an
anonymous
blank
)
.
What
causes
this
","
and
how
can
I
predict
it
?
While
it's
not
very
easy
to
tell
if
the
terminal
running
your
script
(
or
the
font
your
terminal
is
using
)
is
able
to
render
a
given
character
correctly
","
you
can
at
least
check
that
the
character
actually
has
a
representation
.
The
character
\
ua62b
is
defined
as
VAI
SYLLABLE
NDOLE
DO
","
whereas
the
character
\
ua62c
has
no
definition
","
hence
why
it
may
be
rendered
as
a
square
or
other
generic
symbol
.
One
way
to
check
if
a
character
is
defined
is
to
use
the
unicodedata
module
:
As
you
can
see
above
","
a
ValueError
is
raised
for
the
\
ua62c
character
because
it
isn't
defined
.
Another
method
is
to
check
the
category
of
the
character
.
If
it
is
Cn
then
the
character
is
not
assigned
:
It
seems
that
my
issue
was
using
multiple
zmq
contexts
over
different
processes
.
While
the
PyZMQ
documentation
states
that
the
zmq
context
is
thread-safe
","
I
can
only
assume
it
meant
Python
threads
rather
than
processes
.
This
is
quite
confusing
as
in
C
","
zmq
contexts
are
thread
safe
despite
running
in
a
way
similar
to
the
Python
multiprocessing.Process
.
The
issue
was
solved
by
creating
a
zmq
context
for
each
Process
.
I
am
working
with
PyZMQ
and
I
have
what
seems
to
be
a
rather
peculiar
issue
.
I
have
two
classes
that
are
wrapping
sockets
for
communication
","
MZLSubscriber
and
MZLRequester
.
There
is
a
class
that
contains
both
of
them
","
MZLLink
.
For
each
of
these
","
I
also
have
tests
MZLSubscriberTest
","
MZLRequesterTest
","
and
MZLinkTest
.
The
tests
for
the
subscriber
and
requester
work
as
they
should
","
but
MZLinkTest
does
not
receive
any
subscriber
messages
.
Below
is
what
seems
to
be
the
relative
code
","
which
are
the
constructors
for
the
3
classes
as
well
as
run()
for
MZLSubscriber
and
the
tests
for
MZLink
and
MZLSubscriber
.
MZLink
Constructor
:
MZLink
Test
:
MZLRequester
Constructor
:
MZLSubscriber
Constructor
:
MZLSubscriber.run()
:
MZLSubscriber
Test
:
The
subscriber
thread
seems
to
block
at
datum
=
self.socket.recv()
","
which
makes
me
think
it
could
be
some
issue
with
the
socket
creation
.
However
","
it
does
seem
to
work
when
only
working
with
the
subscriber
.
The
requester
seems
to
work
in
both
cases
.
In
addition
","
everything
goes
smoothly
by
just
commenting
out
the
two
lines
dealing
with
requester
.
I
apologize
for
the
wall
of
code
","
but
I
can't
even
narrow
what
code
the
issue
is
coming
from
at
this
point
.
When
I
do
","
I'll
remove
the
irrelevant
code
.
The
test
code
that
deals
with
the
incoming
data
has
been
removed
.
As
a
bit
of
clarification
","
I
am
using
Python
2.7
with
PyZMQ
14.3.1
.
UPDATE
:
It
seems
that
running
MZLSubscriber
in
the
main
thread
rather
than
creating
another
Process
results
in
the
expected
result
","
so
it
seems
that
this
could
be
some
sort
of
thread
safety
.
To
my
knowledge
","
zmq
contexts
are
thread-safe
","
but
sockets
are
not
.
I
thought
this
wouldn't
cause
an
issue
because
I'm
explicitly
making
sure
there
is
a
socket
for
each
thread
.
UPDATE
2
:
If
the
calls
setting
up
the
socket
in
MZLSubscriber
are
moved
from
run()
to
__init__
","
the
socket
seems
to
receive
a
small
portion
of
the
published
message
","
but
does
have
an
error
:
I
have
gotten
a
workaround
to
this
by
creating
a
new
zmq.Context
in
MZLSubscriber.run()
","
although
I
feel
that
this
shouldn't
be
necessary
if
zmq
contexts
are
thread-safe
.
You
should
split
your
question
into
two
","
however
","
for
the
first
question
","
the
exception
is
raised
because
there
is
no
JOB_ID
environment
variable
set
in
your
Python
program's
environment
.
Firstly
you
can
set
it
on
the
command
line
when
invoking
Python
:
or
you
can
set
the
variable
in
you
shell's
environment
:
Either
of
these
will
fix
the
problem
","
however
","
you
should
code
more
defensively
by
catching
the
exception
","
or
by
using
os.environ.get('JOB_ID')
which
will
not
throw
an
exception
if
the
environment
variable
is
not
set
.
For
the
second
question
you
should
reask
it
as
new
question
and
in
it
explain
more
accurately
which
module
is
being
used
.
First
question
:
I
have
a
python
program
and
after
I
run
it
","
I
get
the
following
error
:
Is
it
because
I
did
not
initialize
the
environmental
variables
in
the
command
line
prompt
?
The
UserDict
is
involved
with
a
dictionary
","
right
?
Second
question
:
How
to
quickly
grasp
the
main
initialization
of
a
simulation
in
python
?
For
example
","
suppose
I
have
the
following
code
.
How
to
understand
this
then
?
I
am
relatively
new
to
python
.
Many
thanks
for
your
time
and
attention
.
Its
better
to
use
os.getenv('JOB_ID')
instead
of
os.environ
[
'
JOB_ID
'
]
.
Tell
me
how
are
you
setting
the
environment
variable
from
the
command-line
?
For
ex
.
:
you
could
do
this
:
You're
getting
a
parser
error
due
to
invalid
syntax
.
In
Python
you
don't
specify
variable
types
","
or
use
new
for
instantiation
new
objects
Should
be
From
the
Jython
docs
:
If
you
have
a
Java
class
You
use
this
from
Jython
like
so
:
I
am
stuck
at
a
part
of
my
script
that
is
supposed
to
perform
the
following
:
a
.
Iterate
through
a
source
directory
.
b
.
Move
each
file
(
name
=
GUID.file
extension
)
to
a
destination
folder
that
is
named
as
that
file's
guid
.
In
theory
","
this
problem
is
simple
enough
to
solve
in
Python
with
the
os.walk()
and
os.rename()
.
The
complication
is
that
the
file
extension
for
some
of
these
files
are
unconventional
as
shown
by
this
screenshot
:
As
a
workaround
","
I
using
Commons.IO
Java
libraries
.
Yet
my
script
is
erroring
on
the
last
3
lines
when
I
am
trying
to
instantiate
File
objects
.
What
am
I
doing
wrong
?
Script
:
Error
(
partial
string
)
:
You
can
use
array
slicing
as
follows
for
this
-
This
seems
like
a
really
simple
question
but
I
can't
find
a
good
answer
anywhere
.
How
might
I
multiply
(
in
place
)
select
columns
(
perhaps
selected
by
a
list
)
by
a
scalar
using
numpy
?
E.g
.
Multiply
columns
0
and
2
by
4
Currently
I
am
doing
this
in
multiple
steps
but
I
feel
like
there
must
be
a
better
way
especially
if
the
list
gets
larger
.
Current
way
:
An
alternate
solution
is
to
create
a
class
that
inherits
from
np.ndarray
and
add
a
method
to
it
to
make
the
in-place
mutation
more
intuitive
.
Code
:
Result
:
If
you're
set
on
Python
","
I
would
use
paramiko
:
https://github.com/paramiko/paramiko
This
seems
to
be
one
of
the
most
widely
used
Python
SSH
libraries
.
There
are
some
other
StackOverflow
questions
that
address
how
to
do
this
.
How
to
open
an
SSH
tunnel
using
python
?
SSH
Tunnel
for
Python
MySQLdb
connection
The
main
idea
is
to
create
a
tunnel
with
paramiko
and
then
connect
to
the
localhost
port
through
which
you
are
tunneling
traffic
to
the
remote
server
using
the
Python
library
MySQLdb
.
I'm
writing
a
small
python
program
locally
as
i
don't
have
root
access
on
the
server
.
It
basically
does
a
lot
of
mysql
queries
using
python
MySQLdb
module
.
The
thing
is
I
cant
use
MySQLdb
with
the
server
","
as
the
mysql
server
is
hosted
locally
and
I
need
to
ssh
into
the
server
and
then
use
mysql
from
there
.
Is
there
any
module
available
where
I
can
connect
to
a
mysql
database
via
SSH
.
At
the
moment
I
can
connect
to
the
mysql
instance
using
SSH
credentials
(
IP
","
User
","
Pass
)
I
also
have
the
user
/
pass
for
the
mysql
instance
and
I'm
pretty
sure
it
runs
on
127.0.0.1
/
localhost
.
Why
not
just
create
the
dict
directly
in
the
init
?
then
returns
To
your
second
question
","
if
you
pass
a
list
instead
of
a
string
to
the
phone
parameter
that
will
show
up
as
a
list
","
will
that
work
?
returns
Suppose
I
have
the
code
below
:
I
can
initialize
the
data
:
But
I
have
a
feeling
that
the
display_userdata()
function
is
redundant
if
I
could
re-write
__init
to
store
as
dict
.
It
returns
the
output
:
My
questions
are
:
Is
there
a
smarter
way
to
write
the
__init__
snippet
so
the
input
is
stored
directly
as
python
dictionary
?
I
don't
want
to
call
the
constructor
with
dict
key
by
using
setattr
.
Secondly
","
Suppose
the
user
has
3
phones
or
more
(
variable
)
","
how
do
I
store
this
in
an
array
while
calling
the
object
constructor
.
Think
self.phone
=
[
'
702-000-000
'
","
'
413-222-3333
'
]
You
could
achieve
it
by
using
keyword
arguments
in
python
(
*
*
kwargs
)
.
https://docs.python.org/2/tutorial/controlflow.html#keyword-arguments
If
you
have
your
data
as
list
","
you
want
to
pass
it
as
a
parameter
when
creating
instance
of
Person
this
why
:
But
Here
you
have
to
keep
your
data
organized
in
your
list
in
standard
way
","
Otherwise
you
better
use
dictionary
instead
of
list
","
this
way
:
I'm
new
to
programming
and
I
want
to
know
what
is
the
difference
between
these
two
class
definitions
?
Which
one
is
a
better
and
safer
definition
?
the
one
with
instance
variables
or
the
one
with
constructor
method
?
Thanks
MyClass1
You
may
want
to
do
the
addition
and
store
the
sum
in
the
__init__()
method
to
avoid
having
it
recalculated
every
time
doAdd()
is
called
.
And
like
@gragas
says
","
this
method
lets
you
store
the
data
for
later
use
in
other
methods
.
MyClass2
You're
not
using
any
object
oriented
features
here
","
so
it
might
as
well
just
be
a
regular
function
.
you
can
use
ipython
notebook
.
then
open
your
existing
code
and
do
a
further
modification
.
Sometimes
","
I
want
to
add
new
features
into
a
Python
application
.
Usually
","
we
can
explore
and
test
these
new
features
in
IPython
interactive
environment
.
It
is
inconvenient
","
however
","
that
type
in
the
already
finished
codes
each
time
.
So
","
is
it
possible
to
import an
exist
python
script
into
IPython
for
further
modification
?
You
can
use
regex
to
split
on
anything
which
is
not
a
digit
\
D
:
You
don't
need
to
use
the
question
mark
:
You
can
do
it
with
re
module
","
this
way
:
If
the
question
mark
is
always
at
the
end
.
regex
are
relatively
slow
for
performing
tasks
like
this
You
can
use
the
split
function
.
"Str.split(""|"")"
and
assign
the
result
to
an
array
variable
.
Considering
you
are
using
'
?
'
as
a
terminating
char
.
The
safest
way
to
do
this
would
be
:
If
I
have
a
string
like
this
:
how
would
I
take
out
the
""""
12345
""""
and
the
""""
67891
""""
","
etc
(
the
characters
between
the
pipes
)
and
add
them
to
a
list
all
the
way
until
the
question
mark
(
I
am
using
the
question
mark
in
my
code
as
a
terminating
character
)
?
A
similar
question
has
been
asked
here
:
How
do
I
find
the
string
between
two
special
characters
?
but
I
think
mine
is
different
because
I
need
to
do
it
multiple
times
in
the
same
","
one-line
string
.
Here
is
what
I
am
hoping
to
achieve
:
Thanks
in
advance
!
!
It's
fairly
easy
to
create
a
series
of
buttons
and
bind
them
to
the
same
handler
.
I
wrote
on
this
topic
a
few
years
ago
here
.
Using
that
example
","
I
created
something
simple
using
your
example
:
You
will
probably
want
to
change
it
up
slightly
so
that
the
value
of
the
quote
dictionary
is
another
data
structure
that
contains
the
label
of
the
button
and
the
quote
instead
of
using
the
same
string
for
the
button's
label
and
its
name
.
I'm
creating
a
wxPython
App
which
lists
rows
of
buttons
.
When
the
buttons
are
pressed
","
a
pop-up
message
(
which
will
be
a
quote
)
is
displayed
.
I'm
having
trouble
programming
the
buttons
to
display
a
pop
up
message
.
1
)
I'm
having
trouble
having
a
pop
up
message
displayed
after
wx.ToggleButton
is
clicked
.
2
)
The
other
problem
is
how
I
can
make
multiple
buttons
which
will
each
display
a
different
message
The
callback
can
send
data
to
a
coroutine
which
can
send
it
along
again
if
needed
.
Not
sure
if
this
is
what
you
are
looking
for
but
your
question
made
me
think
of
coroutines
.
If
the
callback
execution
is
asynchronous
","
I'm
not
sure
what
would
happen
if
the
coroutine
wasn't
back
to
the
first
yield
and
the
callback
tries
to
send
it
something
","
it
might
throw
a
ValueError
:
generator
already
executing
exception
.
I
am
looking
for
a
way
to
get
stuff
from
a
callback
into
a
generator
","
or
the
generator
to
inherit
the
callback
.
When
the
callback
is
called
","
the
generator
should
run
","
yielding
the
data
.
While
I
normally
would
use
a
queue
and
a
thread
to
run
and
get
that
callback
","
in
this
case
I
am
running
inside
a
python
extension
of
an
cli
which
calls
my_callback(data)
.
As
a
result
","
I
can't
use
threading
","
as
python
will
only
execute
that
callback
.
Afterwards
the
C
part
of
the
cli
will
do
stuff
again
","
and
python
is
not
executed
.
Edit
:
I
could
register
a
'
poll
'
function
","
which
will
get
called
periodically
.
Putting
a
wait
in
there
give
at
least
the
thread-queue
construction
some
time
to
execute
.
But
that
feels
to
dirty
for
actual
code
.
You
could
use
the
split
command
.
For
ex
","
:
-
If
you
want
to
do
this
from
python
","
you
could
use
subprocess.check_call()
i
have
a
file
""""
text.txt
""""
that
is
1.1MB
right
now
.
I
want
to
split
it
up
into
50kb
text
files
.
I
would
use
a
loop
if
I
could
readlines()
the
file
","
but
since
it's
one
long
string
","
I'm
not
sure
i
could
do
that
.
Open
the
file
","
set
up
a
byte
range
to
iterate
through
","
then
seek()
to
that
location
","
read()
in
the
content
","
and
","
if
there
was
content
","
write
it
to
a
new
file
.
If
there's
no
content
","
break
out
of
the
loop
.
I
am
implementing
a
switch
/
case
expression
through
dictionary
in
Python
.
Can
anyone
tell
me
why
the
following
recursive
call
is
wrong
?
I
am
getting
an
exception
:
RuntimeError
:
maximum
recursion
depth
exceeded
Your
dict
values
are
not
put
in
lazily
","
which
means
it
always
creates
the
dict
whether
you
look
up
'
ac
'
or
not
.
As
such
","
you
can't
just
look
up
'
a
'
in
your
recursive
call
-
-
you
have
to
make
the
dict
(
recursing
another
layer
)
first
.
You'll
have
to
write
this
code
another
way
.
The
reason
you're
getting
infinite
recursion
is
because
you're
telling
Python
to
store
the
result
of
the
recursive
call
rec('a')
in
the
dictionary
.
This
means
it
recurses
unconditionally
","
since
you
always
build
the
dictionary
before
doing
the
lookup
.
One
way
to
solve
this
would
be
to
store
lambda
functions
in
the
dictionary
","
and
then
only
call
the
one
you
get
.
In
this
version
","
the
recursive
call
is
only
made
when
the
appropriate
x
value
is
passed
in
:
Unfortunately
","
that's
a
bit
cumbersome
.
I'm
not
sure
I'd
bother
if
there
were
just
have
three
cases
and
a
default
.
I'd
just
use
a
series
of
if
statements
","
even
though
that
might
not
be
as
efficient
as
a
dictionary
lookup
.
Sorry
for
asking
such
a
basic
question
","
but
I'm
stuck
and
can't
figure
out
what
I
am
doing
wrong
.
I
am
developing
a
small
website
using
Flask
","
teaching
myself
web
coding
along
the
way
.
I
have
the
following
file
structure
:
My
mathsoc.py
looks
like
this
:
Then
mathsoc_main.html
looks
like
this
:
And
mathsoc.css
looks
like
this
:
But
mathsoc_main.html
cannot
find
the
stylesheet
","
it
appears
:
it
does
not
apply
either
of
the
defined
properties
to
the
content
.
I'm
guessing
that
I'm
doing
something
wrong
with
<
link
"rel=""stylesheet"
""""
"type=""text"
/
css
""""
"href=""mathsoc.css"
""""
/
>
","
but
I
don't
know
what
.
It
seems
so
blindingly
obvious
","
yet
no
style
is
loaded
!
Change
your
folder
structure
to
include
a
static
folder
.
See
http://flask.pocoo.org/docs/0.10/tutorial/folders/
My
question
refers
to
the
following
code
:
Whenever
I
run
my
program
","
the
text
isn't
always
in
the
center
.
My
question
is
how
do
I
always
keep
it
in
the
center
.
Using
str.format
you
can
specify
right
","
left
or
centered
alignment
","
see
Format
Specification
Mini-Language
.
Example
:
Now
using
.
format
and
specifying
center
aligned
with
a
specific
width
we
get
:
You
can
add
some
flair
:
You'll
most
likely
have
to
play
around
with
the
width
to
ensure
your
strings
are
centered
.
In
:
is
the
standard
ipython
prompt
.
It
is
ipython's
way
of
telling
you
that
it
is
waiting
for
input
(
just
like
>
>
>
in
the
standard
python
interpreter
)
.
Note
that
output_notebook
will
only
function
in
the
browser
(
i.e.
","
in
the
notebook
)
.
If
you
want
to
run
code
in
a
command
line
ipython
shell
(
I
am
assuming
that
is
what
you
mean
by
""""
in
regular
iPython
""""
)
then
you
will
have
to
use
output_file
or
one
of
the
functions
in
bokeh.embed
.
What
is
happening
when
I
get
the
following
error
?
There
is
no
error
from
within
the
notebook
.
But
when
I
do
this
in
regular
iPython
I'm
wondering
what
is
happening
....
and
what
In
:
means
...
see
error
below
....
(
last
5
lines
)
It
means
that
for
whatever
reason
","
your
account
does
not
have
permission
to
modify
the
contents
of
the
.
ipython
folder
","
which
IPython
uses
to
store
some
settings
and
cache
some
data
","
so
IPython
will
create
and
use
a
temporary
directory
instead
.
To
fix
this
","
you
should
either
modify
the
permissions
on
the
.
ipython
folder
from
the
command
line
and
give
yourself
read
/
write
permissions
","
or
modify
the
IPYTHON_DIR
environment
variable
to
point
to
a
folder
you
do
have
permissions
for
.
Or
","
if
you
don't
feel
like
fixing
the
error
+
don't
mind
that
IPython
is
using
a
temp
folder
","
don't
do
anything
.
I
added
this
to
my
code
and
it
prints
out
the
event
type
to
the
terminal
.
I
want
know
what
the
correct
event
type
for
a
electronic
smart
white
board
is
","
at
the
moment
the
following
works
on
a
regular
screen
with
mouse
","
and
with
a
mouse
attached
to
a
electronic
white
board
","
but
it
doesn't
respond
to
the
finger
tap
.
What
is
the
correct
event.type
for
smart
boards
when
using
your
finger
to
tap
the
white
board
.
Here
is
an
alternative
one(ish)
liner
:
Which
prints
:
To
see
how
this
works
","
you
can
split
the
line
up
into
its
components
to
get
:
This
prints
the
following
:
Alternatively
you
could
make
use
of
k
and
g
at
the
same
time
:
I'm
learning
python
and
recently
I
was
challenged
by
an
exercise
to
compress
a
string
.
The
input
goes
like
'
aaaabbcccca
'
the
output
has
to
be
'
a4b2c4a1
'
.
I
did
it
","
but
I
have
a
feeling
that
my
solution
is
rather
clumsy
.
I
would
like
to
know
","
what
would
be
your
answer
to
the
task
.
My
code
is
:
Out
of
my
head
I
would
do
it
in
a
similar
way
:
But
I
guess
I
on
the
page
provided
by
Vogel612
there
are
a
lot
more
examples
","
this
one
is
a
nice
to
mention
.
I
modified
the
example
from
there
(
which
uses
itertools.groupby
)
to
match
your
input
string
:
A
neat
one
liner
:
-
)
I
want
to
parse
logic
strings
and
get
all
the
combinations
of
elements
that
are
in
an
""""
and
""""
logic
.
For
instance
","
for
the
string
'
(
A
and
(
B
or
C
)
)
'
I
should
get
[
"[A,B]"
","
"[A,C]"
]
and
for
the
string
'
(
A
and
B
and
(
C
or
D
and
F
)
or
F
and
G
)
'
I
should
get
[
"[A,B,C]"
","
"[A,B,D,F]"
","
"[F,G]"
]
.
I'm
trying
to
use
pyparsing
.
Following
this
post
here
parsing
a
complex
logical
expression
in
pyparsing
in
a
binary
tree
fashion
I
manage
to
get
a
nested
list
with
the
letters
grouped
according
to
preferences
(
""""
and
""""
has
preference
over
""""
or
""""
","
and
parenthesis
overrides
this
)
:
Which
gives
:
[
[
[
[
[
'
A
'
]
","
'
and
'
","
[
'
B
'
]
","
'
and
'
","
[
[
[
'
C
'
]
","
'
or
'
","
[
[
'
D
'
]
","
'
and
'
","
[
'
F
'
]
]
]
]
]
","
'
or
'
","
[
[
'
F
'
]
","
'
and
'
","
[
'
G
'
]
]
]
]
]
Now
how
can
I
process
this
result
to
achieve
the
desired
output
?
I
would
be
greateful
for
any
help
.
I
tried
pyparsing
but
I'm
open
to
other
modules
which
may
be
better
.
Thanks
in
advance
.
Python
libraries
are
going
to
help
us
a
little
bit
:
Let's
write
the
required
function
:
And
let's
test
it
:
There
are
comments
in
the
source
code
.
Anyway
","
the
main
steps
are
:
The
expression
variables
are
found
as
single-character
uppercase
names
.
Each
variable
can
be
either
True
or
False
.
We
find
all
combinations
.
We
select
only
such
combinations
that
make
the
whole
expression
True
.
We
keep
only
minimal
solutions
","
i.e.
those
that
are
not
supersets
of
other
solutions
.
I
would
like
to
thank
you
very
much
for
a
nice
question
.
The
Python's
itertools
never
stop
surprising
me
.
;
-
)
We
can
do
this
thing
to
type
in
any
arguments
:
Is
it
possible
to
type
in
arguments
in
the
middle
of
the
code
","
not
in
the
beginning
?
I
want
to
do
smth
the
same
:
Have
a
look
at
argparse
a
python
class
for
command
line
parsing
When
the
code
is
executed
","
I
receive
the
error
-
OverflowError
:
Python
int
too
large
to
convert
to
C
long
.
How
may
I
go
about
solving
this
please
?
This
code
is
meant
to
create
a
frame
grabber
that
allows
me
to
pick
a
frame
that
I
want
and
save
it
.
Well
","
the
error
is
telling
you
that
you
are
passing
a
number
in
this
line
:
That
is
too
large
to
be
cast
to
ctypes.c_long
;
The
maximum
number
that
can
be
represented
by
this
type
is
(
2**31)-1
=
2147483647
.
This
is
presumably
your
TOTAL_FRAMES_FLAG
.
Now
that's
a
lot
of
frames
(
approx
.
500
days
@50fps
)
","
so
it
can't
be
right
.
I
notice
that
you
are
getting
this
value
before
you
open
the
video
file
","
so
it's
likely
to
be
undefined
at
that
point
.
Try
doing
it
after
you've
opened
your
file
","
and
see
if
that
fixes
things
.
You
could
print
the
value
","
as
well
","
to
see
if
it
is
indeed
too
large
(
before
you
try
the
suggested
change
)
.
Code
:
-
Output
:
-
Req
Output
:
-
Kindly
give
a
logical
solution
.
Your
function
does
not
return
anything
thus
when
you
print
the
result
of
the
function
it
prints
None
displayHand()
function
does
not
return
anything
","
hence
when
you
do
-
This
actually
prints
None
","
since
displayHand()
did
not
return
anything
.
Just
call
it
as
normal
without
the
print
-
displayHand
returns
nothing
.
Just
remove
the
print
of
the
end
of
your
code
and
it
will
work
.
Output
:
In
hadoop
streaming
","
you
can
only
run
1
map
and
1
reduce
job
at
a
time
(
at
present
)
.
You
can
essentially
run
2
mappers
(
or
any
number
of
mappers
)
in
1
job
by
piping
the
output
of
first
map
function
to
the
second
map
function
.
However
for
multiple
reducers
","
as
Ned
Rockson
said
","
you'll
have
2
independent
jobs
by
using
identity
mapper
in
the
second
job
Do
as
first
said
'
map1.py
|
map2.py
|
map3.py
'
","
this
is
not
work
","
only
map1.py
can
be
executed
.
The
right
thing
to
do
is
with
mrjob(the Python MapReduce library)
","
hope
this
helps
Probably
this
is
want
you
want
:
""""
hadoop-multiple-streaming
extends
Hadoop-Streaming
which
is
a
utility
that
comes
with
the
Hadoop
distribution
.
This
utility
allows
you
to
not
only
do
Hadoop-Streaming
","
but
also
create
and
run
'
multiple
'
Map
/
Reduce
jobs
for
'
one
'
input
with
any
executable
or
scripts
.
For
example
:
This
project
is
the
maven
project
.
So
you
can
simply
do
maven
build
command
for
making
hadoop-multiple-streaming.jar
file
.
In
more
detail
","
'
mvn
clean
package
'
command
will
compile
source
code
and
packaging
to
${project_home
}
/
target
folder
.
""""
Took
from
https://github.com/hyonaldo/hadoop-multiple-streaming.
I
am
currently
writing
codes
that
run
on
hadoop
streaming
in
Python
.
However
","
I
am
trying
to
do
one
mapping
and
two
reducing
jobs
.
When
I
try
to
run
the
code
using
the
following
command
","
only
one
reducer
-
the
first
one
-
is
working
.
I
am
using
this
command
:
Can
you
please
tell
me
how
to
work
on
it
?
For
example
:
so
that
the
output
would
be
:
What
do
I
put
in
place
of
the
*
default
*
to
make
this
happen
?
Use
default
named
arguments
.
Explicitly
mention
that
c
has
to
take
the
value
of
7
In
my
program
I
need
to
convert
a
.
png
file
to
.
jpg
file
but
I
don't
want
to
save
the
file
to
disk
.
Currently
I
use
But
this
saves
file
to
disk
.
I
dont
want
to
save
this
to
disk
but
have
it
converted
to
.
jpg
as
an
object
.
How
can
I
do
it
?
You
can
do
what
you
are
trying
using
BytesIO
from
io
:
Convert
to
a
bytearray
","
and
then
delete
the
unwanted
elements
:
I
have
an
array
of
bytes
in
python
","
which
looks
like
this
:
I
have
to
remove
each
nth
byte
(
A
","
B
","
C
in
the
example
)
.
Array
length
can
be
many
thousands
.
Typical
chunk
size
can
be
128
or
few
thounds
.
This
is
my
current
solution
:
I
am
looking
for
other
solutions
","
which
could
be
more
elegant
.
Python
3.x
only
solutions
are
fine
.
Something
as
simple
as
the
following
should
do
:
This
works
","
because
we
process
chuck_size+1
bytes
in
every
iteration
","
but
only
keep
chunk_size
of
them
","
effectively
removing
a
byte
after
every
chunk
.
I'd
pull
the
'
chunk
and
skip
'
logic
out
into
a
generator
","
something
like
:
Code
:
IMAGE
OF
DBF
TABLE
:
http://i.stack.imgur.com/1UHE1.jpg
My
problem
is
I
can
not
change
the
records
by
rows
","
cause
the
position
of
the
products
(
PROCOD
)
may
vary
.
Any
suggestions
to
get
the
PROCOD
and
change
the
value
of
PROEST
?
UPDATED
:
But
the
question
now
is
","
how
i
edit
the
value
based
on
the
CODIGO
field
(
Stock
code
)
.
I
have
multiples
stocks
ID
:
(
1
","
2
","
5
","
11
)
.
The
code
update
just
the
first
result
","
i
need
update
a
specific
record
based
in
the
CODIGO
FIELD
.
In
SQL
would
be
:
""""
UPDATE
PROEST
SET
32
where
CODIGO=11
""""
...
or
CODIGO=2
SOLVED
by
Ethan
Furman
You
don't
say
which
dbf
package
you
are
using
","
but
it
looks
like
mine
.
What
you
want
to
do
is
create
an
temporary
index
on
the
PROCOD
field
and
then
you
can
search
on
it
and
update
whichever
other
fields
you
need
to
:
If
the
product
code
is
not
unique
","
then
the
above
""""
should
only
be
one
product
with
that
code
comment
""""
is
wrong
.
Change
the
index
to
:
and
then
search
with
:
This
is
because
Django
form
fields
don't
accept
a
blank
parameter
.
The
core
parameters
accepted
by
Django
form
fields
are
:
required
label
label_suffix
initial
widget
help_text
error_messages
validators
localize
You
can
instead
pass
a
required
parameter
with
its
value
as
False
.
So
","
if
you
pass
an
empty
value
â
€
“
either
None
or
the
empty
string
""""
""""
","
the
form
will
not
raise
a
validation
error
.
I'm
trying
to
create
a
form
that
will
receive
empty
strings
in
some
of
its
values
.
This
form
is
not
backed
by
a
model
object
.
I've
defined
it
thus
:
Except
when
I
try
to
run
a
test
:
I
get
the
following
exception
:
According
to
the
docs
this
should
be
ok
.
Am
I
missing
something
?
No
","
the
docs
do
not
mention
a
blank
parameter
for
form
fields
;
that's
for
model
fields
.
Form
fields
take
a
required
parameter
that
defaults
to
True
.
Your
example
is
a
strange
if
not
an
invalid
YAML
file
","
which
makes
it
a
bit
of
guess
work
what
is
going
on
here
.
The
reason
that
it
is
a
problematic
YAML
file
is
the
line
which
has
a
different
indentation
(
i.e.
is
outdented
)
compared
to
the
lines
before
and
after
.
Up
until
LFSDetails
the
YAML
file
looks
like
it
has
a
mapping
at
the
top
level
.
So
the
intial
solution
might
be
to
just
push
LFSDetails
in
with
two
spaces
to
align
it
with
that
.
That
would
result
in
an
empty
string
as
its
literal
block
scalar
(
introduced
with
|
and
your
code
seems
to
rely
on
YAML
as
text
block
within
YAML
.
So
it
looks
like
the
indentation
following
all
of
the
lines
following
LFSDetails
is
correct
.
For
your
error
message
to
appear
your
YAML
file
has
to
start
with
a
dash
(
sequence
indicator
)
indented
3
spaces
:
gives
this
error
(
I
stripped
out
the
irrelevant
intermediate
mapping
entries
)
.
This
however
parses
","
but
completely
drops
anything
after
the
outdentation
by
LFSDetails
ยน
:
gives
you
:
Because
of
the
dash
the
mapping
starting
with
Version
:
is
the
first
element
of
a
sequence
/
list
and
after
that
trying
to
access
a
list
element
with
a
string
(
LFSDetails
)
gets
you
the
error
.
You
should
therefore
first
clean
up
your
YAML
to
make
it
correct
.
I
also
suggest
to
include
the
start
of
document
marker
(
-
-
-
)
even
if
you
only
have
one
document
in
the
example
file
","
since
you
are
using
load_all()
.
And
get
rid
of
the
extra
indentation
(
i.e.
have
things
start
in
the
first
column
of
the
line
)
.
After
removing
any
spurious
sequence
starting
dashes
and
get
something
like
this
:
which
gets
you
as
output
:
ยน
This
was
done
using
ruamel.yaml
of
which
I
am
the
author
.
It
is
an
enhanced
version
of
PyYAML
and
the
results
using
that
should
be
similar
.
I
want
to
parse
the
YAML
file
having
the
following
data
structure
:
I
have
the
following
code
but
I'm
having
a
""""
TypeError
:
list
indices
must
be
integers
","
not
str
""""
in
the
line
20
:
(
I'm
calling
this
method
later
with
iStream
set
to
sys.stdin
to
read
the
data
file
)
.
I
think
I'm
missing
something
basic
here
regarding
the
data
structure
or
the
way
the
yaml.load_all
is
working
.
What
am
I
missing
?
I
presume
you
are
using
PyYaml
.
load_all
is
for
loading
multiple
YAML
documents
in
a
single
file
","
and
so
always
returns
a
list
.
You
just
have
a
single
document
","
which
maps
to
a
Python
dict
","
so
you
should
just
use
load
.
I
removed
my
commas
before
searching
the
line
of
text
.
To
do
this
","
I
inserted
lineWoCommas
=
"line.replace(',', '')"
after
if
:
m
My
code
is
trying
to
read
all
log
files
throughout
the
specified
directory
in
rootDir
and
write
certain
pieces
of
information
from
that
log
file
to
an
outputFile
The
issue
I'm
having
is
searchObj_Archive_date.group()
","
fullpath
","
zDiscsVar
","
zCopiesVar
","
and
searchObj_Year_3or6.group()
aren't
being
read
into
my
file
from
certain
lines
within
the
log
files
.
This
happens
for
only
about
10
%
of
the
total
outputted
lines
of
text
","
so
I'm
confused
why
it's
only
happening
some
of
the
time
","
so
instead
of
E:\filepath\text.txt
|
5
/
23
/
2015
12:00
|
C:\anotherFilePath\text.txt
|
23
|
23
|
5Year
","
I
get
E:\filepath\text.txt
|
|
|
|
|
Any
insight
as
to
why
this
error
is
occuring
would
be
greatly
appreciated
.
My
code
is
below
:
After
doing
some
researched
","
I
found
that
what's
causing
my
error
is
that
whenever
a
line
has
a
comma
","
in
it
.
It
stops
reading
the
line
at
that
comma
and
skips
to
the
next
line
","
does
anybody
know
a
workaround
to
this
?
An
example
of
my
input
text
that's
giving
me
problems
:
11
/
23
/
2015
12:34:58
Adding
file
D:\fp\fp1\fp2\text
","
text
","
text.txt
Normally
these
lines
don't
have
commas
","
so
does
anyone
know
of
a
way
to
handle
commas
when
reading
in
lines
of
text
?
For
something
as
complicated
","
I
really
think
you
should
avoid
calculating
the
integral
yourself
","
especially
if
you
don't
have
experience
with
complex
integration
","
and
use
a
well
tested
existing
implementation
.
Meijer
G-function
is
implemented
in
mpmath
and
possibly
in
Sympy
.
I'm
in
need
of
a
Meijer
G
function
in
scipy
.
I
read
somewhere
on
the
internet
that
due
to
its
generality
","
the
Meier
G
function
is
not
supported
as
a
special
function
in
scipy
","
but
everyone
should
write
something
up
according
to
his
personal
use
case
.
My
problem
is
that
I
have
no
experience
whatsoever
with
complex
integration
.
As
LaTeX
is
forbidden
here
","
here's
what
I'm
trying
to
solve
numerically
:
(
the
first
line
being
the
general
case
","
the
second
line
my
case
that
I'm
trying
to
compute
)
","
with
p(a)
","
k
","
k2
given
As
wikipedia
states
","
there
are
three
ways
to
get
L
:
L
runs
from
−
i
∞
to
+
i
∞
such
that
all
poles
of
Γ(bj − s)
","
j
=
1
","
2
","
...
","
m
","
are
on
the
right
of
the
path
","
while
all
poles
of
Γ(1 − ak + s)
","
k
=
1
","
2
","
...
","
n
","
are
on
the
left
.
L
is
a
loop
beginning
and
ending
at
+
∞
","
encircling
all
poles
of
Γ(bj − s)
","
j
=
1
","
2
","
...
","
m
","
exactly
once
in
the
negative
direction
","
but
not
encircling
any
pole
of
Γ(1 − ak + s)
","
k
=
1
","
2
","
...
","
n
.
L
is
a
loop
beginning
and
ending
at
−
∞
and
encircling
all
poles
of
Γ(1 − ak + s)
","
k
=
1
","
2
","
...
","
n
","
exactly
once
in
the
positive
direction
","
but
not
encircling
any
pole
of
Γ(bj − s)
","
j
=
1
","
2
","
...
","
m
.
How
do
I
get
L
and
solve
the
integral
?
The
way
I'm
used
to
compute
integrals
over
the
reals
is
to
I'm
not
too
much
after
efficiency
","
I'd
prefer
a
simple
and
slow
working
example
that
I
can
use
to
understand
the
methodology
.
To
fix
your
specific
issue
","
you
can
do
something
like
this
:
and
you
can
call
the
format
helper
method
in
the
template
-
feel
free
to
modify
it
to
your
usecase
.
I
see
a
few
things
that
could
be
changed
in
this
app
.
I
would
also
recommend
using
slugs
-
example
this
app
is
great
(
django-autoslug-field
)
.
One
approach
for
this
would
be
:
This
would
give
you
the
flexibility
to
analyze
which
apps
are
in
an
environment
","
and
at
the
same
time
","
what
environments
does
an
app
belong
to
.
Your
template
would
be
Also
","
now
you
can
process
the
url
by
slug
","
instead
of
the
name
-
which
would
eliminate
all
the
.
replace(..)
hacks
I
need
some
help
mapping
views
in
Django
.
My
app
is
pretty
simple
-
-
it's
just
a
status
page
listing
all
of
our
environments
and
their
corresponding
apps
.
So
far
I
have
all
the
env's
listed
like
so
:
But
what
I
want
is
:
(
where
[
app
*
]
would
be
from
app_list
in
my
Environment
class
-
-
see
below
)
Here's
my
models.py
(
app_list
=
app1
","
app2
","
etc.
)
-
-
UPDATE
-
-
And
my
views.py
:
And
my
index.html
Well
","
I
figured
it
out
.
Turns
out
that
the
metafunc.parametrize
function
accepts
""""
ids
""""
as
a
parameter
.
All
I
had
to
do
was
define
the
__repr__
of
the
objects
I
was
looking
to
name
","
and
expanded
the
list
comprehension
so
I
could
return
two
things
from
the
same
loop
.
I'm
using
py.test
to
execute
a
suite
of
selenium
tests
.
I'm
essentially
running
a
collector
in
my
conftest.py
that
generates
tests
like
this
(
I
stole
this
from
the
pytest
documentation
)
:
My
test
cases
are
placed
in
objects
that
look
like
this
:
I
instantiate
them
something
like
this
:
My
browsers
are
attached
to
a
grid
server
","
I
make
a
list
of
them
like
this
:
I
store
the
target
environment
in
a
config
file
that
is
an
instantiation
of
an
object
like
this
:
Then
I
have
a
test
class
that
creates
a
list
of
test
cases
like
this
-
the
test
object
parameters
are
actually
strings
that
allow
for
self
generating
code
.
I'm
oversimplifying
as
I
pass
them
in
as
fill
ins
to
broader
exec
statements
:
When
the
collector
runs
","
it
looks
like
this
:
collected
#
items
<
Module
'
tests.py
'
>
<
Class
'
TestClass
'
>
<
Instance
'
(
)
'
>
<
Function
""""
test_function
[
environment0-test_object0-('browser_1
'
","
GRID_SERVER
)
]
""""
>
<
Function
""""
test_function
[
environment1-test_object1-('browser_2
'
","
GRID_SERVER
)
]
""""
>
<
Function
""""
test_function
[
environment2-test_object2-('browser_1
'
","
GRID_SERVER
)
]
""""
>
<
Function
""""
test_function
[
environment3-test_object3-('browser_2
'
","
GRID_SERVER
)
]
""""
>
I
want
to
have
the
collector
work
in
such
a
way
that
I
get
back
useful
information
about
each
item
-
I've
messed
around
with
setting
__str__
","
__repr__
","
and
__name__
methods
in
various
places
but
haven't
had
the
results
I
expected
.
I'd
like
to
be
able
to
roll
this
into
reporting
-
there
are
over
200
tests
that
this
generates
in
production
and
I
have
to
trace
through
stack
traces
currently
to
figure
out
exactly
what
was
being
tested
for
each
failure
.
I'm
not
really
sure
where
I'm
making
my
mistakes
here
","
should
I
modify
my
implementation
of
pytest_generate_tests
","
or
the
way
I'm
creating
my
TestClass
","
or
set
up
the
cases
in
a
different
way
?
Ideally
","
I
want
something
that
can
be
mapped
back
via
an
ORM
to
include
test
metadata
as
well
.
If
you'd
like
","
you
can
hook
up
a
callback
to
the
legend
that
will
show
/
hide
lines
when
they're
clicked
.
There's
a
simple
example
here
:
http://matplotlib.org/examples/event_handling/legend_picking.html
Here's
a
""""
fancier
""""
example
that
should
work
without
needing
to
manually
specify
the
relationship
of
the
lines
and
legend
markers
(
Also
has
a
few
more
features
)
.
This
allows
you
to
click
on
legend
items
to
toggle
their
corresponding
artists
on
/
off
.
For
example
","
you
can
go
from
this
:
To
this
:
I'm
using
pyplot
to
display
a
line
graph
of
up
to
30
lines
.
I
would
like
to
add
a
way
to
quickly
show
and
hide
individual
lines
on
the
graph
.
Pyplot
does
have
a
menu
where
you
can
edit
line
properties
to
change
the
color
or
style
","
but
its
rather
clunky
when
you
want
to
hide
lines
to
isolate
the
one
you're
interested
in
.
Ideally
","
I'd
like
to
use
checkboxes
on
the
legend
to
show
and
hide
lines
.
(
Similar
to
showing
and
hiding
layers
in
image
editors
like
Paint.Net
)
I'm
not
sure
if
this
is
possible
with
pyplot
","
so
I
am
open
to
other
modules
as
long
as
they're
somewhat
easy
to
distribute
.
If
you're
splitting
the
text
file
into
words
based
on
whitespace
","
just
use
split()
on
the
whole
thing
.
There's
nothing
to
be
gained
by
reading
each
line
and
stripping
it
","
because
split()
already
handles
all
that
.
So
to
get
the
initial
list
of
words
","
all
you
need
is
this
:
Then
to
remove
duplicates
","
convert
the
word
list
to
a
set
:
And
finally
sort
it
:
This
can
all
be
simplified
to
three
lines
","
like
so
:
(
NB
:
the
with
statement
will
automatically
close
the
file
for
you
)
Some
thoughts
:
Consider
using
a
set
for
the
words
.
Adding
another
word
only
really
adds
it
when
its
not
in
there
already
.
Open
the
file
with
'
r
'
to
indicate
read-only
mode
.
Sorting
is
really
easy
(
also
for
sets
)
","
just
use
sorted()
.
Something
like
this
works
:
Inside
the
for
loop
for
for
word
in
words
:
when
you
do
-
lst.append(words)
-
it
appends
the
whole
words
list
into
lst
","
I
believe
you
intended
to
use
-
lst.append(word)
.
Also
","
the
for
loop
-
for
word
in
words
:
should
be
indented
inside
for
line
in
openedfile
:
","
so
that
you
run
the
loop
for
each
line
.
And
lastly
","
if
you
want
to
lexicographically
sort
the
words
","
you
should
call
-
lst.sort()
at
the
end
.
Also
","
it
would
be
better
to
use
with
statement
to
open
the
file
","
so
that
it
can
handle
closing
the
file
after
everything
is
finish
automatically
.
Also
","
it
may
be
easier
to
use
set()
as
the
initial
data
type
to
store
the
elements
as
the
not
in
operator
for
list
is
O(n)
","
and
as
the
list
gets
bigger
","
that
would
take
more
time
.
Whereas
with
a
set
data
type
","
you
do
not
need
to
worry
about
checking
if
the
set
already
has
the
word
","
since
set
does
not
allow
duplicates
.
At
the
end
","
you
can
use
list(..)
to
convert
the
set
back
to
list
and
then
sort
it
.
Example
-
First
","
I
think
you
want
lst.append(word)
to
only
append
a
word
if
not
in
the
list
already
.
You
have
lst.append(words)
.
That
is
wrong
.
Second
","
to
sort
just
use
lst.sort()
I'm
trying
to
read
lines
in
a
file
","
split
the
lines
into
words
","
and
add
the
individual
words
to
a
list
if
they
are
not
already
in
the
list
.
Lastly
","
the
words
have
to
be
sorted
.
I've
been
trying
to
get
this
right
for
a
while
","
and
I
understand
the
concepts
","
but
I'm
not
sure
how
to
get
the
exact
language
and
placement
right
.
Here's
what
I
have
:
So
","
as
far
as
I
understand
","
the
ManyToManyField
in
the
Section
class
doesn't
actually
save
any
DocumentType
data
in
the
Section
database
table
","
rather
it
saves
the
relationship
to
DocumentType
.
Therefore
if
I
want
to
access
the
document_type
of
a
section
I
have
to
look
up
the
Section
objects
via
the
DocumentType
object
","
and
vice
versa
.
For
instance
:
or
:
or
:
Although
this
:
raises
ValueError
:
invalid
literal
for
int()
with
base
10
:
'
WAN
'
because
there
is
no
data
under
the
section_document_type
m2m
field
","
only
the
relationship
.
So
this
is
still
the
case
:
Further
info
here
.
models.py
When
I
create
a
Section
in
the
Admin
it
doesn't
appear
to
save
the
document_type_s
.
In
the
django
shell
I
can
see
that
DocumentType
is
saving
properly
:
When
I
check
the
Sections
I
get
:
And
when
I
check
Documents
:
And
if
I
try
to
create
a
new
section
:
Django
appears
to
have
created
the
field
in
the
database
","
looking
at
my
migrations
file
:
I
don't
know
what
is
causing
this
?
How
do
you
think
this
is
going
to
work
?
document_type_s
is
a
ManyToMany
field
but
you
are
passing
in
a
string
value
Read
the
docs
for
ManyToMany
:
https://docs.djangoproject.com/en/1.8/topics/db/examples/many_to_many/
For
many
to
many
relation
to
be
created
the
Section
instance
already
has
to
exist
(
because
in
the
db
a
m2m
relation
is
an
extra
table
with
two
foreign
keys
)
...
so
you
need
to
:
The
problem
you
have
is
not
that
Python
thinks
that
debian
/
control
does
not
exist
","
but
rather
that
it
seems
like
deb_pkg_tools.control
does
not
exist
.
I
would
use
the
python-debian
package
from
Debian
to
parse
the
control
file
if
I
were
you
.
Here
is
the
code
that
will
parse
the
control
file
to
get
the
dependencies
.
It
should
work
even
for
packages
with
multiple
binary
packages
.
Each
item
in
the
above
example
is
a
tuple
that
pairs
the
""""
key
""""
with
the
""""
value
""""
","
so
item
[0]
gives
us
the
""""
key
""""
and
item
[1]
gives
us
the
""""
value
""""
.
Obviously
the
above
sample
just
prints
out
the
dependencies
as
they
are
in
the
control
file
","
so
the
dependencies
aren't
in
a
format
that
is
suitable
to
directly
plug
into
apt-get
install
.
Also
","
by
parsing
the
control
file
","
I
got
stuff
like
${python:Depends
}
in
addition
to
actual
package
names
","
so
that
is
something
you
will
have
to
consider
.
Here
is
an
example
of
the
output
I
got
from
the
above
example
:
I
found
this
bug
report
and
the
python-debian
source
code
to
be
quite
useful
resources
when
answering
your
question
.
You
might
want
to
have
a
look
into
mk-build-deps
(
from
the
devscripts
package
)
that
is
a
standard
script
that
already
does
what
you
want
to
achieve
.
I
am
in
the
process
of
porting
a
Ruby
file
used
in
our
build
system
to
Python
.
The
file
looks
for
Depends
lines
in
a
debian
/
control
file
in
our
repository
","
checks
every
dependency
","
and
apt-get
installs
everything
that
isn't
installed
.
I
am
trying
to
reproduce
this
functionality
.
As
part
of
porting
this
to
Python
","
I
looked
at
the
deb_pkg_tools
module
.
I
pip
installed
it
and
created
a
simple
script
","
install-dep2.py
.
However
","
when
I
run
this
script
","
I
get
the
following
error
:
The
debian
/
control
file
exists
:
How
can
I
process
this
debian
/
control
file
?
I
don't
need
to
use
deb_pkg_tools
if
there
is
a
better
way
.
I
want
to
move
files
of
certain
type
from
an
existing
directory
structure
to
a
new
dynamic
directory
structure.for
e.g
:
if
the
file
is
I
need
to
place
the
file
in
Should
I
copy
each
and
everyfile
and
then
paste
them
in
the
directory
or
can
I
zip
them
and
unzip
them
dynamically
?
for
now
I
am
using
the
below
code
to
create
the
directory
.
Please
help
me
if
there
is
any
better
way
of
doing
this
.
Thank
you
for
you
help
.
Use
shutil.copytree
to
copy
the
entire
directory
to
a
new
location
.
I'm
trying
to
learn
SGDRegressor
.
I
generate
my
own
data
but
I
don't
know
how
to
fit
that
into
the
algorithm
.
I
get
this
error
.
Found
arrays
with
inconsistent
numbers
of
samples
:
[
1
1000
]
I'm
new
to
python
and
Machine
Learning
.
What
do
I
miss
?
will
give
you
a
1
x
1000
array
.
Your
features
and
target
variables
need
to
be
in
a
column
.
Try
On
my
system
both
Python
and
Javascript
produce
the
same
result
(
modulo
sign
)
:
And
new
"Date(2000, 6, 1)"
.
getTimezoneOffset()
returns
-
240
(
different
sign
","
same
value
)
.
Python
uses
:
local
time
=
utc
time
+
utc
offset
definition
.
While
Javascript
uses
a
different
definition
:
utc
offset
=
utc
time
-
local
time
i.e.
","
both
results
are
correct
and
have
correct
signs
for
the
corresponding
definitions
.
For
a
portable
Javascript
solution
","
you
could
use
momentjs
library
that
provides
access
to
the
same
tz
database
as
pytz
Python
module
:
If
you
print
the
result
for
the
following
-
You
will
notice
that
it
gives
a
datetime.timedelta()
object
which
has
both
days
as
well
as
second
.
So
for
timezones
that
are
UTC
-
<
something
>
","
this
actually
gives
days
as
-
1
and
then
the
remaining
in
seconds
.
Example
-
To
get
the
info
about
the
actual
offset
","
you
need
to
use
both
days
as
well
as
seconds
","
using
a
code
like
(
For
the
above
timezone
-
America
/
Los_Angeles
)
-
Also
","
I
believe
when
you
are
doing
-
new
"Date(2000, 5, 1)"
.
getTimezoneOffset()
;
in
javascript
","
I
think
it
is
giving
you
the
timezone
offset
from
UTC
for
today's
date
","
rather
than
the
date
2000
/
05
/
01
(
Because
for
the
date
-
2000
/
05
/
01
the
correct
offset
is
what
you
are
getting
from
python
-
240
)
.
You
may
checkout
TimezoneJS
for
getting
the
timezone
specific
as
well
as
date
specific
offsets
","
etc.
I
have
this
code
","
that
returns
UTC
offset
from
given
date
:
Ok
","
do
it
in
JS
using
this
code
(
http://jsfiddle.net/nvn1fef0/
)
Maybe
i
doing
something
wrong
?
And
how
i
can
get
plus-minus
before
offset
(
like
in
JS
result
)
?
I've
just
thought
of
a
way
to
solve
this
-
by
combining
my
two
methods
:
First
","
focus
on
the
individual
chromosomes
","
and
then
loop
through
the
genes
in
these
smaller
dataframes
.
This
also
doesn't
have
to
make
use
of
any
SQL
queries
either
.
I've
also
included
a
section
to
immediately
identify
any
redundant
genes
that
don't
have
any
SNPs
that
fall
within
their
range
.
This
makes
use
of
a
double
for-loop
which
I
normally
try
to
avoid
-
but
in
this
case
it
works
quite
well
.
While
this
doesn't
run
spectacularly
quickly
-
it
does
run
so
that
I
can
actually
get
some
answers
.
I'd
still
like
to
know
if
anyone
has
any
tips
to
make
it
run
more
efficiently
though
.
Firstly
","
sorry
if
this
is
a
bit
lengthy
","
but
I
wanted
to
fully
describe
what
I
have
having
problems
with
and
what
I
have
tried
already
.
I
am
trying
to
join
(
merge
)
together
two
dataframe
objects
on
multiple
conditions
.
I
know
how
to
do
this
if
the
conditions
to
be
met
are
all
'
equals
'
operators
","
however
","
I
need
to
make
use
of
LESS
THAN
and
MORE
THAN
.
The
dataframes
represent
genetic
information
:
one
is
a
list
of
mutations
in
the
genome
(
referred
to
as
SNPs
)
and
the
other
provides
information
on
the
locations
of
the
genes
on
the
human
genome
.
Performing
df.head()
on
these
returns
the
following
:
SNP
DataFrame
(
snp_df
)
:
This
shows
the
SNP
reference
ID
and
their
locations
.
'
BP
'
stands
for
the
'
Base-Pair
'
position
.
Gene
DataFrame
(
gene_df
)
:
This
dataframe
shows
the
locations
of
all
the
genes
of
interest
.
What
I
want
to
find
out
is
all
of
the
SNPs
which
fall
within
the
gene
regions
in
the
genome
","
and
discard
those
that
are
outside
of
these
regions
.
If
I
wanted
to
merge
together
two
dataframes
based
on
multiple
(
equals
)
conditions
","
I
would
do
something
like
the
following
:
However
","
in
this
instance
-
I
need
to
find
the
SNPs
where
the
chromosome
values
match
those
in
the
Gene
dataframe
","
and
the
BP
value
falls
between
'
chr_start
'
and
'
chr_stop
'
.
What
makes
this
challenging
is
that
these
dataframes
are
quite
large
.
In
this
current
dataset
the
snp_df
has
6795021
rows
","
and
the
gene_df
has
34362
.
I
have
tried
to
tackle
this
by
either
looking
at
chromosomes
or
genes
seperately
.
There
are
22
different
chromosome
values
(
ints
1-22
)
as
the
sex
chromosomes
are
not
used
.
Both
methods
are
taking
an
extremely
long
time
.
One
uses
the
pandasql
module
","
while
the
other
approach
is
to
loop
through
the
separate
genes
.
SQL
method
Gene
iteration
method
Can
anyone
give
any
suggestions
of
a
more
effective
way
of
doing
this
?
You
can
use
the
following
to
accomplish
what
you're
looking
for
:
Note
:
your
example
dataframes
do
not
meet
your
join
criteria
.
Here
is
an
example
using
modified
dataframes
:
Double
check
that
you're
not
mixing
spaces
and
tabs
.
Python
thinks
the
second
for
loop
is
indented
to
the
same
level
as
the
print
statement
above
it
.
Thank
you
@Kevin
","
I
had
spaces
for
the
first
loops
","
and
indents
for
the
second
;
didn't
realize
python
differentiated
these
.
I'm
doing
some
experimenting
with
Python
args
and
kwargs
and
ran
into
an
unexpected
issue
.
My
code
is
:
The
output
is
:
If
the
args
and
kwargs
loops
are
separate
","
I
would
expect
to
see
:
Why
is
python
behaving
this
way
?
It
looks
like
it's
running
one
iteration
at
a
time
from
each
loop
","
instead
of
all
iterations
from
each
loop
before
moving
to
the
next
.
So
while
Jianxun's
answer
is
probably
completely
correct
","
it
won't
work
on
my
system
.
I
am
on
Pandas
.
17
and
the
newest
matplotlib
.
I'm
also
on
a
Macbook
.
Basically
if
I
try
to
share
an
X
axis
with
two
graphs
","
a
line
and
bar
graph
","
the
first
instantiated
graph
disappears
.
The
best
way
I
can
think
to
fix
this
is
simply
by
doing
two
graphs
on
one
(
subplots
)
and
hiding
the
X
axis
of
the
top
graph
.
Here's
what
I
did
:
I
often
see
this
in
the
context
of
matplotlib
and
open-high-low-close
","
but
I'm
wondering
if
you
can
add
a
volume
overlay
within
the
pandas
framework
.
The
final
graph
we
would
want
would
be
close
to
the
first
one
here
:
(
Matplotlib
-
Finance
volume
overlay
)
Say
we
have
a
DataFrame
like
such
:
How
can
we
get
the
[
'
num
'
","
'
rolling_30
'
","
'
rolling_10
'
","
'
rolling_60
'
]
line
chart
with
the
bottom
of
the
chart
listing
the
daily
volume
?
I
can
do
a
secondary_y
to
get
volume
on
the
right
","
but
honestly
that
looks
terrible
.
Need
it
to
be
the
traditional
volume
bar-graph
at
the
bottom
of
the
chart
.
The
basic
idea
is
to
use
.
twinx
to
create
a
secondary
y
axis
.
Below
is
a
short
sample
to
do
it
.
From
the
graph
","
you
see
that
the
left
y
axis
is
for
price
and
moving
averages
","
whereas
the
right
y
axis
is
for
volumn
.
If
you
are
using
Python
3.x
","
then
to
reload
the
names
that
have
been
imported
using
from
module
import name
","
you
would
need
to
do
-
For
Python
2.x
","
you
can
simply
do
-
I
have
a
script
that
computes
some
stuff
.
It
uses
inputs
from
a
separate
file
'
inputs.py
'
.
In
'
inputs.py
'
are
only
a
few
variables
:
In
the
main
file
I
import them
with
If
I
now
change
something
in
'
inputs.py
'
and
execute
the
script
again
it
still
uses
the
old
values
instead
of
the
new
ones
.
How
can
I
reload
the
file
?
does
not
work
.
Many
thanks
in
advance
!
Let's
quote
docs
:
reload(module)
Reload
a
previously
imported
module
.
The
argument
must
be
a
module
object
","
so
it
must
have
been
successfully
imported
before
.
This
is
useful
if
you
have
edited
the
module
source
file
using
an
external
editor
and
want
to
try
out
the
new
version
without
leaving
the
Python
interpreter
.
The
return
value
is
the
module
object
(
the
same
as
the
module
argument
)
.
The
argument
must
be
a
module
object
","
so
it
must
have
been
successfully
imported
before
.
When
you
do
from
inputs
import
*
you
actually
has
no
module
object
in
your
namespace
.
Only
module
members
.
When
reload(module)
is
executed
:
Python
modules
’
code
is
recompiled
and
the
module-level
code
reexecuted
","
defining
a
new
set
of
objects
which
are
bound
to
names
in
the
module’s
dictionary
.
The
init
function
of
extension
modules
is
not
called
a
second
time
.
As
with
all
other
objects
in
Python
the
old
objects
are
only
reclaimed
after
their
reference
counts
drop
to
zero
.
The
names
in
the
module
namespace
are
updated
to
point
to
any
new
or
changed
objects
.
Other
references
to
the
old
objects
(
such
as
names
external
to
the
module
)
are
not
rebound
to
refer
to
the
new
objects
and
must
be
updated
in
each
namespace
where
they
occur
if
that
is
desired
.
Other
references
to
the
old
objects
(
such
as
names
external
to
the
module
)
are
not
rebound
to
refer
to
the
new
objects
and
must
be
updated
in
each
namespace
where
they
occur
if
that
is
desired
.
You
star-imported
A
","
B
and
C
are
precisely
other
references
.
To
sum
up
","
an
example
code
would
be
:
does
something
like
this
pseudo
code
:
The
module
inputs
is
cached
in
sys.modules
.
If
you
do
a
reload(inputs)
","
the
cached
module
is
reloaded
","
but
the
assignment
process
which
loads
the
data
from
the
imported
module
to
the
local
name
space
is
not
repeated
.
You
have
to
do
so
by
hand
","
as
the
other
answer
already
states
.
You
could
try
using
a
class
property
of
type
bool
Above
your
init
method
","
you
could
define
a
property
like
this
:
bool
stopFlag
=
False
Then
inside
your
for
loop
","
add
an
if(!self.stopFlag)
:
break
.
This
will
check
the
stopFlag
every
iteration
","
and
negate
it
.
So
it
will
default
to
passing
the
if
check
at
first
.
Then
inside
your
stop
method
","
just
set
your
self.stopFlag
=
True
So
when
stop
is
called
","
it
will
flip
the
flag
","
and
if
the
start
method
is
running
in
it's
for
loop
","
it
will
catch
this
change
in
the
flag
and
break
out
.
I'm
using
Python
2.7
","
and
I'm
trying
to
write
a
GUI
","
but
am
having
some
issues
with
my
buttons
.
I
currently
have
everything
running
properly
","
but
assuming
I've
made
a
mistake
with
my
inputs
or
something
","
I'd
like
a
way
to
stop
a
running
function
after
hitting
the
""""
GO
""""
button
.
My
code
is
way
too
long
to
post
here
","
but
a
simple
example
is
below
.
How
do
I
make
the
""""
Stop
""""
button
break
the
start
function
","
but
not
quit
the
window
entirely
?
Maybe
something
to
do
with
threading
?
I'm
sort
of
new
with
writing
GUIs
","
and
I'm
not
really
a
programmer
","
so
this
isn't
really
my
area
of
expertise
.
The
GUI
is
totally
unresponsive
while
the
main
function
is
running
.
There
must
be
a
way
to
simultaneously
run
my
function
while
also
allowing
me
to
change
things
in
the
GUI
and
hit
buttons
","
but
I'm
not
sure
how
that
works
.
The
updates
don't
have
to
be
implemented
until
the
next
time
the
""""
GO
""""
button
is
hit
though
.
With
Tkinter
","
such
things
are
commonly
done
using
the
universal
widget
after()
method
.
You
should
generally
not
use
time.sleep()
in
a
Tkinter
program
because
it
prevents
the
mainloop()
from
running
(
which
is
what
is
making
the
GUI
unresponsive
in
your
code
)
.
A
successful
after()
call
will
return
an
integer
""""
cancel
id
""""
which
can
used
to
stop
the
callback
just
scheduled
.
This
is
what's
needed
by
the
Stop()
method
of
your
Example
class
to
stop
the
method
doing
the
counting
.
I
am
trying
to
generate
a
PDF
via
Python
using
""""
ReportLab
""""
and
I
would
like
to
add
an
image
to
it
.
The
image
that
I
have
to
use
is
a
PNG
but
it
has
this
format
:
I
don't
know
very
well
what
to
do
","
first
I
think
that
I
need
to
transform
that
URI
to
an
image
but
i
don't
know
how
to
do
it
and
then
","
use
something
like
:
In
order
to
convert
the
.
PNG
to
.
JPG
.
Can
someone
help
me
?
Try
this
(
edit
:
Thanks
to
njzk2
for
pointing
out
to
cut
the
header
)
:
Added
a
real
base64
string
for
testing
from
here
","
this
should
result
in
an
image
of
a
little
red
dot
:
I
was
wondering
if
there
is
an
easy
way
to
create
a
grid
of
checkboxes
using
Tkinter
.
I
am
trying
to
make
a
grid
of
10
rows
and
columns
(
so
100
checkboxes
)
so
that
only
two
checkboxes
can
be
selected
per
row
.
Edit
:
I'm
using
python
2.7
with
spyder
What
I
have
so
far
:
I'm
trying
to
use
state='Disabled
'
to
grey
out
a
row
once
two
checkboxes
have
been
selected
.
Here's
an
example
using
your
provided
10x10
grid
.
It
should
give
you
the
basic
idea
of
how
to
implement
this
.
Just
make
sure
you
keep
a
reference
to
every
Checkbutton
(
boxes
in
the
example
)
as
well
as
every
IntVar
(
boxVars
in
the
example
)
.
Here's
why
:
-
Checkbuttons
are
needed
to
call
config(state = DISABLED/NORMAL)
.
-
IntVars
are
needed
to
determine
the
value
of
each
Checkbutton
.
Aside
from
those
crucial
elements
its
basically
just
some
2D
array
processing
.
Here's
my
example
code
(
now
based
off
of
your
provided
code
)
.
Here's
a
version
that
puts
everything
into
a
class
so
we
don't
need
to
use
global
variables
.
It
also
avoids
the
import
*
construction
which
is
generally
considered
bad
style
in
Python
.
True
","
lots
of
example
code
uses
import
*
but
it's
not
a
good
practice
because
it
clutters
up
the
global
namespace
with
all
the
names
from
the
imported
module
.
So
those
names
can
clash
with
the
names
of
your
own
variables
","
and
they
can
also
clash
with
the
names
of
other
modules
you
import using
import
*
.
The
program
prints
lists
of
the
selected
Groups
for
each
Test
row
when
the
window
closes
.
You
can
parse
your
bashrc
file
in
the
ipython
config
and
add
any
custom
aliases
you
have
defined
:
This
should
be
placed
in
~
/
.
ipython
/
profile_default
/
ipython_config.py
.
%
rehashx
makes
system
commands
available
in
the
alias
table
so
is
also
very
useful
if
you
want
to
use
ipython
as
a
shell
.
I
need
to
use
my
aliases
from
~
/
.
bashrc
on
IPython
.
First
I've
tried
but
it
didn't
work
According
to
this
post
we
should
do
It
takes
20
sec
to
run
on
Jupiter
and
I
get
:
bash
:
line
2
:
f2py3
:
command
not
found
My
~
/
.
bashrc
file
looks
like
bash
:
line
2
:
type
:
f2py3
:
not
found
Neither
alias
","
source
","
nor
%
rehashx
%
work
I
actually
found
that
the
problem
is
Python
","
who
can't
execute
alias
command
neither
with
sh
nor
bash
.
Can
I
use
alias
with
IPython
magics
?
I
am
having
following
issue
when
installing
mitmproxy
through
pip
.
I
have
tried
other
fixed
related
to
egg
error
.
Here
on
stack
overflow
.
Can't
install
via
pip
because
of
egg_info
error
pip
install
matplotlib
fails
:
'
cannot
build
package
freetype
;
""""
python
setup.py
egg_info
""""
failed
with
error
code
1
'
Updated
after
first
response
for
libffi
:
After
Installing
libffi
","
it
started
breaking
on
libxml
.
I
found
the
lxml
on
pip
.
and
its
break
again
and
looking
for
libxml
:
(
If
you
read
through
your
log
carefully
you
might
spot
this
line
:
The
""""
fatal
error
""""
part
is
especially
important
.
:
)
This
means
that
the
ffi
headers
couldn't
be
located
by
your
compiler
.
I'm
not
sure
how
to
do
it
since
I'm
not
a
Mac
user
but
maybe
homebrew
could
help
you
","
or
Google
.
To
me
it
seems
like
you
should
install
homebrew
and
then
just
run
:
Then
try
pip
again
.
Edit
The
full
list
of
dependencies
are
:
python
libffi
libssl
libxml2
libxslt1
So
you'll
need
all
those
","
and
their
headers
","
if
you
want
to
continue
down
this
path
.
An
easier
solution
is
to
download
pre-built
binaries
for
your
Mac
","
from
mitmproxy.org
(
OSX
Mountain
Lion
and
later
)
.
I
found
this
info
in
the
installation
docs
.
I
have
just
had
this
issue
.
It
arises
when
you
run
pytest
from
the
wrong
directory
","
e.g
.
wherever
you
happen
to
be
after
you
installed
it
.
To
solve
","
create
a
new
directory
and
cd
into
that
directory
create
the
file
test_sample.py
in
your
directory
with
content
as
per
the
pytest
tutorial
now
run
the
command
pytest
and
you
will
see
the
result
that
is
shown
in
the
tutorial
.
I'm
new
to
this
myself
but
I
believe
pytest
searches
the
directory
structure
from
where
it
is
run
looking
for
tests
.
Run
it
from
the
wrong
place
and
you'll
get
some
strange
results
if
it
sees
something
that
it
thinks
might
be
a
test
.
I
read
that
unit-test
is
a
brilliant
feature
to
write
better
code
and
assert
that
the
features
of
some
target
code
stay
the
same
.
So
I
wanted
to
use
it
...
I
am
using
Anaconda
on
my
Linux
machine
.
I
started
using
pytest
by
working
through
the
manual's
starter
guide
on
their
homepage
.
After
a
successful
installation
there
appears
a
first
(
unintended
)
error
:
I
would
like
to
understand
where
this
stupid
error
comes
from
and
how
I
can
resolve
it
.
Is
the
problem
that
I
execute
the
program
py.test
without
any
filename
and
that
there
is
no
file
called
__init.py__
?
OK
I
am
feeling
really
stupid
by
asking
this
question
but
please
take
the
question
serious
since
I
found
no
hint
in
the
world
wide
web
.
Based
on
the
question
","
comments
","
and
sample
data
exactly
as
it
is
posted
(
all
spaces
","
no
tabs
)
","
my
assumption
is
that
the
real
question
is
how
to
handle
the
fact
that
each
line
in
the
data
contains
a
different
number
of
spaces
between
each
field
.
Assuming
that
spaces
can
appear
within
a
field
","
but
only
one
at
a
time
","
and
that
two
or
more
spaces
delimit
a
field
","
the
following
regex
would
easily
find
the
breaks
between
each
field
(
two
or
more
spaces
)
:
Then
use
a
substitution
to
replace
all
the
deliminators
:
Of
course
","
that
doesn't
handle
the
line
endings
or
the
heading
.
But
for
the
heading
","
you
will
probably
want
to
split
on
lines
anyway
","
so
you
can
handle
the
end
of
the
line
also
:
Now
","
just
add
an
if
statement
or
two
within
the
loop
to
detect
the
heading
and
format
it
appropriately
and
you
have
a
working
solution
.
As
that
wasn't
specifically
asked
about
","
I'll
leave
that
as
an
exercise
for
the
reader
.
In
Python
","
how
could
I
convert
a
whitespace-delimited
log
output
to
a
Markdown-formatted
table
?
I
have
an
output
log
that
contains
printouts
of
the
following
form
:
I
want
to
convert
this
to
a
form
like
the
following
(
Markdown
tables
)
:
One
way
to
do
this
could
be
to
replace
two
or
more
consecutive
spaces
with
a
vertical
bar
character
.
How
could
I
do
this
in
an
efficient
way
?
Given
that
the
table
is
always
to
have
three
columns
","
is
there
a
better
","
more
robust
way
to
do
this
?
If
you're
writing
""""
public
""""
function
","
I
believe
that
pythonic
way
would
be
to
check
arguments
(
using
if
","
not
assert
)
","
and
","
if
they're
invalid
","
raise
exception
with
type
and
info
/
message
designed
to
provide
as
much
info
as
possible
.
If
you're
writing
function
designed
for
internal
use
","
then
using
assert
to
check
the
arguments
seems
to
be
good
idea
for
me
.
I'm
confused
as
when
to
exactly
use
assertion
testing
in
my
python
functions
.
If
I
specify
assumptions
regarding
the
input
arguments
to
a
function
","
should
I
assume
that
correct
input
is
being
fed
to
the
function
or
should
I
use
assertions
to
check
the
conditions
on
the
input
args
enumerated
in
the
specification
?
New
to
the
python
library
PULP
and
I'm
finding
the
documentation
somewhat
unhelpful
","
as
it
does
not
include
examples
using
lists
of
variables
.
I've
tried
to
create
an
absolutely
minimalist
example
below
to
illustrate
my
confusion
.
The
output
is
:
The
optimizer
is
not
recognizing
the
constraint
that
we
only
add
two
values
.
I'll
leave
this
here
in
case
anyone
else
is
just
as
silly
","
but
actually
the
above
example
works
fine
.
I
had
merely
failed
to
examine
the
results
correctly
.
Instead
:
reveals
that
the
solution
is
correct
.
gethostbyaddr
will
also
query
the
hosts
file
-
/
etc
/
hosts
on
Linux
;
Windows
has
an
equivalent
at
%
SystemRoot%\System32\drivers\etc\hosts
(
location
may
vary
with
older
versions
of
Windows
)
From
the
remote
computer
","
gethostbyaddr
will
be
doing
a
DNS
lookup
and
getting
mycomputer.com
.
On
your
local
computer
it
is
probably
getting
the
answer
from
your
hosts
file
","
which
contains
an
upper
case
version
of
your
hostname
.
I'm
using
Python
2.7.9
and
while
trying
to
use
socket.gethostbyaddr
","
I
found
some
perplexing
behavior
.
My
local
computer
has
a
public
ip
","
let's
say
111.111.111.111
.
If
I
use
on
a
remote
computer
:
I
get
'
mycomputer.com
'
.
However
","
using
the
same
command
on
my
local
computer
(
or
using
127.0.0.1
instead
of
my
public
ip
)
","
I
instead
get
'
MYCOMPUTER.com
'
.
Why
is
there
a
difference
in
capitalization
?
One
of
my
folders
has
mostly
json
files
","
and
I'm
reading
the
data
they
contain
to
do
some
classification
for
an
SVM
.
A
question
I
had
was
based
on
this
code
:
Each
time
I
pipe
the
output
","
I
find
that
the
filenames
always
get
printed
in
the
same
order
","
like
so
:
There
are
a
few
text
files
","
and
some
python
files
in
this
directory
.
My
question
here
is
:
what
determines
the
order
in
which
these
files
are
printed
?
Does
the
for
loop
have
a
way
of
looking
for
files
?
I
tried
examining
whether
this
order
is
based
on
size
(
max
to
min
or
min
to
max
)
or
last
"modified(I had no reason for these tests,I just tried them since I can't think of any other insight)"
.
I
tried
this
snippet
4
times
","
and
the
order
is
the
same
each
time
.
I
have
a
labelled
classes
in
different
folders
","
so
if
I
can
be
assured
of
the
order
it
would
be
helpful
in
the
labeling
for
my
training
set(I don't know how good an idea this is)
.
The
order
is
not
defined
","
and
depends
on
the
filesystem
.
I
remember
reading
","
many
years
ago
","
that
one
of
the
improvements
of
ext3
over
ext2
is
keeping
the
pointer
in
the
directory
listing
and
beginning
the
next
operation
on
that
entry
.
Often
a
program
will
stat()
then
open()
an
entry
","
so
scanning
from
the
beginning
of
the
(
internal
)
list
would
occur
twice
for
ext2
;
with
ext3
the
second
operation
would
already
be
on
the
desired
entry
making
the
search
for
it
very
fast
.
This
is
significant
with
many
files
in
a
directory
.
The
point
being
that
listing
the
directory
will
begin
the
list
of
entries
whereever
that
pointer
happened
to
be
.
Also
","
the
order
in
which
entries
are
created
may
affect
the
order
.
The
ls
program
performs
a
sort
operation
before
producing
output
so
that
it
is
visually
consistent
and
usable
.
When
I
tried
to
run
this
code
","
I
got
an
overflow
error
.
I
suspect
you're
having
the
same
problem
","
but
for
some
reason
","
it's
not
throwing
an
error
.
If
you
scale
down
the
features
","
everything
works
as
expected
.
Using
scipy.stats.linregress
:
Using
linear_model.SGDRegressor
:
The
value
for
slope
is
a
little
lower
","
but
I'd
guess
that's
because
of
the
regularization
.
I'm
trying
to
compare
Linear
Regression
(
Normal
Equation
)
with
SGD
but
it
looks
like
SGD
is
far
off
.
Am
I
doing
something
wrong
?
Here's
my
code
And
here's
my
SGD
I
would
have
thought
that
the
coef
and
intercept
would
be
almost
the
same
as
the
data
is
linear
.
If
you
only
want
a
single
line
from
the
top
to
the
bottom
of
the
left
scatter
","
you
can
do
like
so
:
I'm
creating
a
program
to
determine
a
minimum
","
maximum
","
and
percentiles
of
winds
at
certain
heights
.
The
file
is
split
into
5
columns
.
My
code
thus
far
looks
like
this
:
plt.show()
As
you
can
see
it's
plotting
each
min
and
max
at
the
specific
altitude
with
just
a
dot
.
How
can
I
adjust
that
so
it
makes
it
a
line
instead
.
edited
answer
due
to
comment
To
create
a
line
that
connects
all
min
values
:
Store
all
min
values
in
a
list
(
using
append
)
Plot
the
list
The
code
:
Let's
say
a
input
text
file
""""
input_msg.txt
""""
file
contains
follwing
records
.
.
Jan
1
02:32:40
hello
welcome
to
python
world
Jan
1
02:32:40
hello
welcome
to
python
world
Mar
31
23:31:55
learn
python
Mar
31
23:31:55
learn
python
be
smart
Mar
31
23:31:56
python
is
good
scripting
language
Jan
1
00:00:01
hello
welcome
to
python
world
Jan
1
00:00:02
hello
welcome
to
python
world
Mar
31
23:31:55
learn
python
Mar
31
23:31:56
python
is
good
scripting
language
The
expected
output
file
(
Let's
say
outputfile.txt
)
should
contain
below
records
...
Jan
1
02:32:40
hello
welcome
to
python
world
Jan
1
02:32:40
hello
welcome
to
python
world
Mar
31
23:31:55
learn
python
Mar
31
23:31:55
learn
python
be
smart
Mar
31
23:31:56
python
is
good
scripting
language
Jan
1
00:00:01
hello
welcome
to
python
world
Jan
1
00:00:02
hello
welcome
to
python
world
Note
:
I
need
all
the
records
(
including
duplicate
)
which
are
starting
with
""""
Jan
1()
""""
and
also
I
don't
need
Duplicate
records
not
starting
with
""""
Jan
1()
""""
I
have
tried
the
following
program
where
all
the
duplicate
records
are
getting
deleted
.
Oputput
of
my
program
are
below
:
Jan
1
02:32:40
hello
welcome
to
python
world
Mar
31
23:31:55
learn
python
Mar
31
23:31:55
learn
python
be
smart
Mar
31
23:31:56
python
is
good
scripting
language
Jan
1
00:00:01
hello
welcome
to
python
world
Your
help
would
be
appreciated
!
!
!
Try
this
.
Perhaps
make
sure
that
you
are
using
the
same
parser
and
maybe
specify
which
html
parser
you
are
using
in
your
code
that
way
when
you
run
it
on
different
machine
it
will
know
which
one
to
use
.
There
is
more
info
about
this
in
the
documentation
.
I
can
run
these
code
correctly
on
my
two
computers
","
IOS
","
WINDOWS
","
but
it
can
just
find
98
names
on
my
company's
computer
","
Linux
.
And
they
all
use
python
2.7
Try
using
timedelta
instead
of
DateOffset
Have
you
tried
being
more
explicit
with
what
pd.DateOffset
is
acting
on
?
For
example
:
Then
substitute
month
and
days
values
.
Have
you
consider
using
Unix
Epoch
Time
instead
of
a
date
formatted
in
a
lesser
manner
?
There
is
a
well
documented
answer
for
converting
to
Unix
Time
","
and
dealing
with
the
sort
of
offset
in
the
question
seems
like
it
would
be
a
lot
easier
as
sliding
ranges
are
simpler
to
implement
with
a
more
or
less
continuous
sequence
of
real
numeric
values
.
You
can
use
the
offset
family
from
pd.tseries.offsets
.
Below
is
the
sample
code
.
I
am
pulling
a
chunk
of
data
within
a
range
of
time
.
It
is
pulling
date
and
times
from
column
recvd_dttm
.
It
takes
all
the
data
starting
from
a
year
ago
.
I
want
to
modify
it
so
that
it
can
pull
a
month
or
a
day
","
but
pd.DateOffset(months=1)
is
giving
a
KeyError:1
error
.
I
get
the
same
error
if
I
change
it
to
days=7
.
But
it
works
just
fine
with
years=1
.
What
is
going
on
here
?
EDIT
:
The
problem
was
coming
from
elsewhere
in
the
code
!
I
want
to
serve
some
Polymer-code
via
Django
in
a
Google
App
Engine
.
The
problem
is
","
that
Polymer
uses
double
curly
braces
","
as
does
Django
.
In
newer
Django
versions
","
one
can
use
the
verbatim-tag
","
but
in
the
version
used
in
Google
App
Engine
","
this
tag
is
not
implemented
.
Is
there
an
alternative
?
I
ended
up
using
Jinja2
instead
As
@Foon
pointed
out
","
the
canonical
way
to
do
this
is
to
subtract
a
column
.
However
","
on
a
side
note
","
as
your
problem
is
overdetermined
","
you
have
to
use
a
method
such
as
least
squares
.
By
definition
","
if
it's
an
overdetermined
problem
","
there
is
no
""""
unique
","
exact
solution
""""
.
(
Otherwise
it
would
be
even-determined
-
A
square
matrix
.
)
That
aside
","
here's
how
you'd
go
about
it
:
Let's
take
your
example
equation
:
As
you
noted
","
this
is
overdetermined
.
If
we
know
one
of
our
""""
model
""""
variables
(
let's
say
n1
in
this
case
)
","
it
will
be
even
more
overdetermined
.
It's
not
a
problem
","
but
it
means
we'll
need
to
use
least
squares
","
and
there
isn't
a
completely
unique
solution
.
So
","
let's
say
we
know
what
n1
should
be
.
In
that
case
","
we'd
re-state
the
problem
by
subtracting
n1
multiplied
by
the
first
column
in
the
solution
matrix
from
our
vector
of
observations
(
This
is
what
@Foon
suggested
)
:
Let's
use
a
more
concrete
example
in
numpy
terms
.
Let's
solve
the
equation
y
=
Ax^2
+
Bx
+
C
.
To
start
with
","
let's
generate
our
data
and
""""
true
""""
model
parameters
:
First
","
we'll
solve
it
_without
)
the
knowledge
that
B
=
1
.
We
could
use
np.polyfit
for
this
","
but
to
lead
into
the
next
bit
","
we'll
use
a
lower-level
approach
:
As
you
can
see
","
we'll
get
something
close
to
","
but
not
quite
1
.
In
this
case
(
I
didn't
set
the
seed
","
so
this
will
vary
)
","
the
model
parameters
returned
are
While
the
true
parameters
are
:
Now
let's
take
the
fact
that
we
know
b
exactly
into
account
.
We'll
make
a
new
G
with
one
less
column
and
subtract
that
column
times
b
from
our
observations
(
d
/
y
)
:
Now
m
is
"[a, c]"
and
we've
solved
for
those
two
variables
using
our
knowledge
of
b
.
Thanks
everyone
","
the
remove-a-column
trick
is
exactly
what
I
needed
.
The
particular
structure
of
my
example
","
particularly
it
not
having
full
rank
","
is
representative
of
the
class
of
problems
I'm
working
on
.
Knowing
one
of
the
variables
makes
these
problems
analytically
solvable
with
a
unique
solution
","
the
remove-a-column
trick
lets
me
successfully
find
that
solution
using
numpy.linalg.solve
","
so
my
question
was
answered
.
I'm
trying
to
solve
an
overdetermined
system
in
Python
","
using
the
numpy.solve
function
.
I
know
the
value
of
one
of
the
variables
and
I
know
that
in
theory
I
can
find
a
unique
solution
for
the
system
if
I
can
somehow
plug
in
that
known
value
.
My
system
is
of
the
form
AxC=B
.
The
variables
are
split
into
two
groups
","
one
group
of
N
variables
and
one
of
T
variables
(
although
this
does
not
matter
for
the
math
)
.
A
is
a
(
T*N
x
T+N
)
matrix
","
C
is
the
variables
vector
","
of
length
(
T+N
)
","
and
B
is
a
vector
of
length
(
T*N
)
.
How
do
I
tell
numpy.solve
(
or
another
function
in
Python
","
but
please
don't
recommend
least
squares
","
I
need
the
unique
","
exact
solution
","
which
I
know
exists
)
to
use
the
known
value
of
one
of
the
variables
?
A
simple
example
of
my
system
would
be
:
The
values
of
the
elements
of
B
would
of
course
be
known
","
as
well
as
the
value
of
one
of
the
variables
","
let's
say
I
know
that
t1=1
.
The
dots
don't
mean
anything
I
just
put
them
there
so
the
characters
wouldn't
bunch
up
.
Say
you
need
to
solve
and
you
know
t1
.
Then
you
need
to
solve
so
that
basically
you
:
remove
the
4th
column
from
the
matrix
subtract
the
right-hand-side
by
this
4th
column
multipled
by
t1
remove
t1
as
a
variable
Once
you
have
the
appropriate
matrices
","
just
call
numpy.linalg.solve
(
or
something
similar
)
.
I
suggest
that
you
don't
concern
yourself
with
whether
you're
""""
doing
least
squares
""""
","
or
whether
it's
unique
or
not
.
Let
linalg.solve
find
the
optimal
solution
(
in
the
L2
sense
)
;
if
the
solution
is
unique
","
then
it
is
unique
in
the
L2
sense
as
well
.
Python
modules
have
been
installed
but
the
site-packages
may
not
be
in
your
Python
sys.path
","
so
you
will
not
be
able
to
import the
modules
this
formula
installed
.
If
you
plan
to
develop
with
these
modules
","
please
run
like
this
:
In
my
case
it's
the
homebrew
site-packages
","
but
may
not
yours
Using
Python
2.7
I
try
to
import graph
-
tool
:
Each
time
I
execute
the
above
command
the
following
error
is
returned
and
Python
crashes
.
dyld
:
lazy
symbol
binding
failed
:
Symbol
not
found
:
__ZN5boost6python6detail11init_moduleEPKcPFvvE
Referenced
from
:
/
usr
/
local
/
lib
/
python2.7
/
site-packages
/
graph_tool
/
libgraph_tool_core.so
Expected
in
:
flat
namespace
dyld
:
Symbol
not
found
:
__ZN5boost6python6detail11init_moduleEPKcPFvvE
Referenced
from
:
/
usr
/
local
/
lib
/
python2.7
/
site-packages
/
graph_tool
/
libgraph_tool_core.so
Expected
in
:
flat
namespace
Trace
/
BPT
trap
:
5
I
installed
graph-tool
with
homebrew
on
Mac
OSX
10.10
.
Does
anybody
know
how
to
fix
this
issue
?
There
is
probably
a
mismatch
between
the
python
version
you
are
using
","
and
the
one
used
to
compile
boost::python
and
graph-tool
.
For
example
","
you
might
be
using
the
system's
python
","
whereas
graph-tool
/
python
were
compiled
with
a
version
installed
via
homebrew
.
Use
zip
to
group
"[1, 4, 7, 10]"
....
"[2,5,8,11]"
:
Use
map
to
generate
the
new
list
:
If
you
are
using
python3.x
","
you
need
list(map...)
to
get
the
list
.
I
have
a
data
structure
.
A
list
of
4
dictionaries
each
with
4
keys
and
3
values
in
a
list
.
I
want
to
sum
each
sub
column
"[1, 4, 7, 10]"
....
"[2,5,8,11]"
etc.
into
the
form
So
I'm
basically
cascading
each
column
within
each
dictionary
.
I
have
a
VERY
explicit
and
long
way
to
do
it
so
far
(
checking
the
math
is
correct
)
","
is
there
any
way
to
use
list
comprehensions
for
something
this
long-winded
or
is
it
not
worth
the
effort
in
the
end
?
Original
version
using
list
comprehensions
:
The
output
looks
like
this
:
You
can
use
zip
and
sum
function
:
dict.iterkeys()
will
returns
an
iterator
of
dictionary
keys
which
you
can
get
the
first
key
using
next
function.but
note
that
it
wouldn't
return
the
first
key
in
the
order
that
you
have
defined
your
dictionaries.since
dictionary
items
are
not
ordered
you
can
use
collections.OrderedDict
to
get
a
ordered
dictionary
.
Note
that
you
shouldn't
have
same
key
in
your
dictionaries!since
you
can
not
use
same
key
in
the
last
combined
dictionary
!
example
:
I
am
trying
to
read
a
paragraph
and
capture
all
the
sentences
in
it
with
words
matching
a
dynamic
list
of
words
.
The
python
pre-processing
steps
will
identify
the
list
of
words
.
I
want
to
use
this
list
of
words
and
identify
sentences
in
the
paragraph
that
has
at
least
one
of
the
words
from
the
list
.
All
those
identified
sentences
will
be
appended
to
a
new
variable
.
Input
:
""""
Machine
learning
is
the
science
of
getting
computers
to
act
without
being
explicitly
programmed
.
Machine
learning
is
so
pervasive
today
that
you
probably
use
it
dozens
of
times
a
day
without
knowing
it
.
Many
researchers
also
think
it
is
the
best
way
to
make
progress
towards
human-level
AI
.
""""
list
of
words
:
computer
","
researcher
Output
:
Machine
learning
is
the
science
of
getting
computers
to
act
without
being
explicitly
programmed.Many
researchers
also
think
it
is
the
best
way
to
make
progress
towards
human-level
AI
.
What
is
the
best
way
to
accomplish
this
?
Don't
use
regex
for
non
regular
patterns
","
they
are
difficult
to
understand
","
often
verbose
and
inefficient
.
What
you
want
can
easily
be
done
with
the
following
:
output
:
Give
a
try
to
his
","
This
would
fail
if
decimal
numbers
present
.
If
you
also
want
to
deal
with
those
","
then
try
this
","
Based
partially
on
this
answer
:
You
need
to
run
nltk.download()
beforehand
and
download
punkt
in
the
Models
tab
.
You
could
separate
the
content
.
I
guess
you
have
something
like
this
:
But
you
could
put
the
graph
html
in
a
separate
","
let's
say
graph_template.html
template
and
include
it
:
And
then
in
summary.html
you
can
include
graph_template.html
.
I
am
trying
to
create
a
single
page
with
different
content
from
different
template
in
django
so
i
can
print
it
.
Its
kind
of
summary
of
different
page
base.html
-
mainContent
block
is
rendered
inside
this
template
main.html
-
Need
mainContent
block
from
here
graph.html
-
Need
mainContent
block
from
here
charts.html
-
Need
mainContent
block
from
here
summary.html
-
Need
content
from
main
","
graph
","
charts
all
together
here
(
REQUIRE
)
I
have
a
base
template
which
is
extended
on
every
page
like
this
(
It
has
navbar
","
footer
and
sidebar
)
There
is
a
block
inside
base
template
where
graph
","
main
","
chart
content
is
displayed
.
What
i
am
trying
to
accomplish
is
to
get
the
mainContent
from
those
page
and
add
it
to
new
template
called
summary.html
Since
every
page
is
extending
from
base
i
am
not
sure
how
to
do
that
?
I
tried
using
include
but
it
will
also
include
base
for
every
page
.
EDIT
:
I
know
i
can
separate
the
mainContent
into
its
own
separate
files
but
i
have
lot
of
templates
and
was
looking
for
any
other
solutions
.
is
simple
indexing
","
in
effect
saying
""""
take
the
0th
element
of
the
first
coordinate
","
all
of
the
second
coordinate
","
and
"[1,3,4]"
of
the
third
coordinate
""""
.
Or
more
precisely
","
take
coordinates
(
0
","
whatever
","
1
)
and
make
it
our
first
row
","
(
0
","
whatever
","
2
)
and
make
it
our
second
row
","
and
(
0
","
whatever
","
3
)
and
make
it
our
third
row
.
There
are
5
whatevers
","
so
you
end
up
with
(
3
","
5
)
.
The
second
example
you
gave
is
like
this
:
In
this
case
you're
looking
at
a
(
5
","
8
)
array
","
and
then
taking
the
1st
","
3rd
and
4th
elements
","
which
are
the
1st
","
3rd
and
4th
rows
","
so
you
end
up
with
a
(
5
","
3
)
array
.
Edit
to
discuss
2D
case
:
In
the
2D
case
","
where
:
the
behaviour
is
similar
.
Case
1
:
is
simply
selecting
columns
1
","
3
","
and
4
.
Case
2
:
is
taking
the
1st
","
3rd
and
4th
element
of
the
array
","
which
are
the
rows
.
http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#combining-advanced-and-basic-indexing
The
docs
talk
about
the
complexity
of
combining
advanced
and
basic
indexing
.
The
indexing
coords
comes
first
","
with
the
[
0
","
:
]
after
","
producing
the
the
(
3
","
5
)
.
The
easiest
way
to
understand
the
situation
may
be
to
think
in
terms
of
the
result
shape
.
There
are
two
parts
to
the
indexing
operation
","
the
subspace
defined
by
the
basic
indexing
(
excluding
integers
)
and
the
subspace
from
the
advanced
indexing
part
.
[
in
the
case
where
]
The
advanced
indexes
are
separated
by
a
slice
","
ellipsis
or
newaxis
.
For
example
x
[
arr1
","
:
","
arr2
]
.
....
the
dimensions
resulting
from
the
advanced
indexing
operation
come
first
in
the
result
array
","
and
the
subspace
dimensions
after
that
.
I
recall
discussing
this
kind
of
indexing
in
a
previous
SO
question
","
but
it
would
take
some
digging
to
find
it
.
https://stackoverflow.com/a/28353446/901925
Why
does
the
order
of
dimensions
change
with
boolean
indexing
?
How
does
numpy
order
array
slice
indices
?
The
[
:
]
in
test
[0]
[
:
]
[coords]
does
nothing
.
test
[0]
[
:
","
coords
]
produces
the
desired
(
5
","
3
)
result
.
If
I
slice
a
2d
array
with
a
set
of
coordinates
then
my
slice
has
the
shape
that
I
would
expect
But
if
I
repeat
this
with
a
3d
array
then
the
shape
is
now
Is
there
a
reason
that
these
are
different
?
Separating
the
indices
returns
the
shape
that
I
would
expect
Why
would
these
views
have
different
shapes
?
According
to
the
source
code
","
if
there
is
a
CloseSpider
exception
being
raised
","
engine.close_spider()
method
would
be
executed
:
engine.close_spider()
itself
would
close
the
spider
and
clear
all
outstanding
requests
:
It
would
also
schedule
close_spider()
calls
for
different
components
of
Scrapy's
architecture
:
downloader
","
scraper
","
scheduler
etc.
I
want
to
know
what
impact
of
raising
CloseSpider
.
In
documentation
http://doc.scrapy.org/en/latest/topics/exceptions.html#closespider
there
is
no
information
about
it
.
As
you
know
scrapy
process
a
few
requests
at
the
same
time
.
What
if
this
exception
will
be
raised
before
last
request
will
be
handled
?
Will
it
wait
for
handling
of
rest
requests
that
was
prduced
before
?
Example
:
Thanks
for
your
responses
.
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
Possible
solution
:
I
am
currently
profiling
some
python
code
like
so
:
The
code
runs
successfuly
","
outputs
no
exception
.
However
my
restats.txt
looks
like
this
:
SomeFunc
(
i
i
gÇZ3a
/
­
?
gÇZ3a
/
­
?
0
(
s
;
C:\SomeFolder\bar.pyiL
t
Basically
it
is
150
lines
of
garbage
characters
with
random
paths
and
function
names
mixed
in
.
What
could
be
the
issue
?
Am
I
using
this
correctly
?
The
python
file
is
being
loaded
in
through
Maya
if
that
makes
a
differance
.
Please
read
the
documentation
for
what
to
do
after
you
have
collected
the
trace
information
.
I
believe
you
will
need
to
use
pstats.Stats
to
extract
the
information
you
need
.
The
file
is
not
meant
to
be
human-readable
.
As
described
in
the
documentation
","
you
can
use
the
pstats
module
to
load
the
file
and
explore
the
profiling
data
.
You
can
try
building
a
model
by
uploading
your
training
data
(
Defective
","
Not
Defective
)
to
demo.nanonets.ai
(
free
to
use
)
1
)
Upload
your
training
data
here
:
demo.nanonets.ai
2
)
Then
query
the
API
using
the
following
(
Python
Code
)
:
3
)
the
response
looks
like
:
I
have
worked
on
the
problem
of
image
classification
using
Bag
of
features
(
BoF)and
SVM
.
I
did
it
using
C
+
+
and
OpenCV
but
I
am
sure
that
you
can
get
similar
methods
for
python
too
.
Concept
:
Create
BoF
Dictionary
:
Take
one
image
from
your
training
samples
.
Extract
SIFT
keypoints
Extract
SIFT
descriptors
Use
k-means
clustering
to
cluster
the
descriptors
Create
BoF
dictionary
(
Refer
the
link
which
I
have
mentioned
below
)
Training
:
Load
your
BoF
dictionary
Initialize
your
BoF
instance
with
the
above
BoF
dictionary
Take
one
image
from
your
training
samples
.
Extract
SIFT
keypoints
Find
BoF
descriptors
for
the
extracted
keypoints
.
Use
this
BoF
descriptor
for
the
SVM
learning
Do
the
above
steps
for
all
the
training
images
You
will
get
a
SVM
classifier
files
in
.
xml
form...save
it
Testing
:
Load
your
SVM
classifier
Initialize
your
SVM
classifier
with
the
.
xml
file
which
your
created
above
Capture
image
Find
BoF
descriptor
for
the
captured
image
using
your
dictionary
Use
those
BoF
descriptor
to
classify
it
using
SVM
You
can
refer
this
article
This
is
more
of
general
""""
where
do
I
find
good
resources
to
do
something
""""
question
.
I
am
attempting
to
use
Python
(
OpenCV
or
otherwise
)
to
classify
images
based
on
a
training
set
.
My
training
set
:
this
is
made
up
of
numerous
images
of
product
defects
.
Each
image
can
be
taken
in
1
of
3
locations
on
the
product
and
each
image
will
contain
1
of
5
types
of
product
defects
.
The
defects
have
been
manually
classified
and
validated
by
a
human
.
Images
to
classify
:
These
are
made
up
of
similar
images
","
taken
in
the
same
3
locations
but
the
type
of
defect
is
not
classified
(
although
the
defective
area
IS
recognized
by
the
tool
taking
the
picture
","
it's
just
that
the
tool
does
not
CORRECTLY
classify
them
and
I
can't
change
the
tool
)
.
I
have
attempted
to
do
this
classification
following
recommendations
in
the
book
Programming
Computer
Vision
with
Python
:
Tools
and
algorithms
for
analyzing
images
.
In
this
case
I
use
SIFT
descriptors
stored
in
a
mySQL
database
(
training
data
)
in
a
Bag
of
Words
approach
.
So
far
I
am
not
having
too
much
luck
(
I
continue
to
troubleshoot
)
and
thought
I'd
seek
advice
from
any
OpenCV
experts
out
there
.
Any
references
or
advice
would
be
much
appreciated
.
So
","
coming
back
to
the
question
I
thought
that
it
would
be
worth
sharing
what
I've
learned
.
I
don't
know
that
it's
""""
the
answer
""""
but
this
is
where
I've
ended
up
.
A
work
in
progress
","
you
can
always
get
better
.
My
solution
right
now
has
been
to
combine
3
different
approaches
.
All
are
searchable
on
the
internet
so
I
won't
go
to
great
detail
on
the
how
.
First
","
I
used
the
SIFT
approach
","
generating
SIFT
histograms
using
a
command
line
call
to
VLFeat
.
This
may
be
an
option
elsewhere
in
Python
","
it's
just
what
I
used
.
I
used
k-means
clustering
to
do
the
visual
bag
of
words
vocabulary
thing
and
built
a
database
tying
the
centroid's
to
word
histograms
associated
with
the
training
images
.
I
improved
results
a
bit
by
adding
a
Root
SIFT
step
.
Then
I
created
a
separate
database
using
Dense
SIFT
(
but
no
Root
SIFT
adjustments
)
.
Lastly
","
I
created
a
database
of
color
histograms
based
on
the
RGB
components
of
the
training
images
.
I
don't
use
all
256
bins
of
RGB
but
went
with
summing
individual
R
","
G
and
B
values
over
8
bins
and
then
flattening
the
values
to
a
24
bin
histogram
.
This
same
process
is
followed
with
the
unknown
images
","
and
then
histogram
vectors
are
compared
using
Euclidean
distance
.
I
tried
Chi
Squared
comparison
as
well
but
","
in
my
case
Euclidean
provided
better
results
.
I
take
the
top
3
results
from
each
process
","
with
image
classification
based
on
5
of
9
vote
.
If
a
majority
isn't
reached
then
the
analysis
is
indeterminate
.
For
my
closed
population
of
images
I
am
at
3.1
%
misclassification
rate
","
3.1
%
indeterminate
.
Implementing
a
CNN
in
Theano
will
probably
give
you
better
results
than
anything
in
OpenCV
.
If
you
search
on
Google
scholar
there
are
a
huge
number
of
papers
on
image
classification
using
CNNs
-
most
of
these
approaches
should
not
be
difficult
to
implement
using
Theano
.
*
In
my
application
","
I
have
two
panels
.
.
class
One
represents
the
sidebar
panel
which
contains
buttons
.
Class
Two
represents
the
main
panel
which
contains
the
listbox
.
How
do
I
call
a
function
via
a
button
(
in
this
case
to
delete
items
from
a
listbox
)
whose
parent
belongs
to
another
class
(
Two
)
?
*
one
way
you
could
do
this
is
with
pub
sub
that
said
there
are
many
many
ways
to
accomplish
this
You
need
to
implement
a
__str__
method
in
your
Genres
model
.
I've
an
application
which
contains
two
different
models
","
related
by
a
Many-to-Many
relationship
as
follows
:
serializers.py
This
is
how
my
API
endpoint
form
at
http://localhost:8000/movies/
looks
like
:
Now
","
instead
of
different
genres
appearing
as
instances
of
Genres
object
numbers
","
how
can
I
make
them
appear
as
the
actual
string
corresponding
to
each
object
number
?
For
example
","
Genres
object-1
corresponds
to
Comedy
","
and
that
is
what
I
want
to
be
displayed
in
the
API
endpoint
form
to
make
the
genre
choices
more
human-readable
.
What's
the
way
to
do
something
like
that
?
Assuming
the
output
is
in
stdout
","
something
like
this
might
work
:
I
have
to
admit
though
","
I'm
not
sure
this
is
what
you
are
asking
for
.
Please
clarify
:
)
I
need
to
create
a
shell
script
(
run.sh
)
file
that
executes
my
python
code
present
in
Git
repository
and
append
it
to
the
repo
.
Since
this
is
my
first
time
","
I
am
confused
about
how
to
create
the
run.sh
file
and
in
what
language
.
You
can
either
:
Whatever
the
reason
you
ask
this
(
other
writers
will
tell
you
it
is
usually
a
better
idea
to
use
optparse
or
argparse
","
...
like
they
are
doing
in
the
stdlib
[
timeit
or
json
]
)
","
it
is
possible
.
So
let's
say
I
have
the
following
example
script
:
If
I
call
this
script
from
the
command
line
python
example.pyhow
can
I
access
either
hello()
and
/
or
bye()
.
I
just
know
how
to
call
a
function
like
:
Answer
to
comments
:
So
there
is
no
way
of
choosing
which
function
to
use
out
of
the
command
line
?
I
simply
want
a
script
containing
like
two
differing
table
comparison
functions
and
be
able
to
choose
out
of
one
script
which
to
use
You
can't
directly
call
a
function
in
a
script
with
arguments
to
the
script
","
you
have
to
handle
the
command
line
arguments
yourself
and
then
call
the
function
:
Of
course
you
might
want
to
extend
this
in
several
ways
:
to
only
process
the
first
argument
(
this
example
processes
all
of
them
)
","
to
lowercase
the
argument
so
that
HELLO
and
BYE
also
work
","
or
maybe
to
display
some
help
if
no
arguments
are
given
.
As
@bufh
mentioned
","
if
your
command
line
arguments
are
going
to
get
much
more
complicated
than
this
then
you
might
want
to
look
into
a
library
that
handles
it
for
you
","
such
as
argparse
.
A
quick
and
dirty
way
to
do
this
would
be
:
Then
you
can
run
myscript.py
hello
or
myscript.py
goodbye
","
and
when
you
add
other
global
/
top
level
functions
they
will
work
immediately
.
A
better
way
to
do
this
though
would
be
to
use
something
like
argparse
and
maybe
even
https://docs.python.org/dev/library/argparse.html#sub-commands
You
have
to
create
a
thread
in
the
background
.
Different
threads
may
run
at
one
time
.
Dialog
box
stops
current
thread
and
waits
while
user
make
a
choice
.
This
is
normal
behavior
.
Python
has
very
easy
threading
API
.
Thread
is
function
that
is
passed
as
an
argument
to
a
thread
class
constructor
.
Thread
executes
this
function
then
","
and
you
may
start
several
threads
","
they
will
not
lock
the
current
thread
.
Move
your
dialog
or
background
code
to
a
separate
thread
.
Samples
:
Simple
threading
event
example
More
info
:
https://docs.python.org/3/library/threading.html
Making
program
automatically
select
OK
in
that
dialog
box
is
not
possible
for
a
Python
program
.
Well
","
there
are
several
techniques
","
but
they
require
C
+
+
and
other
extended
skills
.
So
I've
had
a
good
look
around
","
and
this
is
the
closest
I
could
find
to
what
I
need
to
do
:
http://bytes.com/topic/python/answers/23100-windows-dialog-box-removal
But
it's
only
part
of
it
.
So
I
have
a
script
that
needs
to
run
fully
automated
.
Currently
it's
being
stopped
by
a
Windows
dialog
box
prompting
the
user
to
click
'
OK
'
.
Is
it
possible
to
register
some
sort
of
handler
that
when
a
dialog
box
opens
is
fired
and
selects
the
'
OK
'
option
for
it
?
Any
ideas
how
to
do
this
/
where
I
can
get
futher
info
on
it
?
NOTE
:
My
goal
isn't
just
to
keep
the
code
going
","
a
requirement
is
to
select
'
OK
'
on
the
dialog
box
before
continuing
.
from
django.conf.urls.static
import static
urlpatterns
=
[
"url(r'^admin/', include(admin.site.urls)"
)
","
"url(r'', include('blog.urls')"
)
","
urlpatterns
+
=
"static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)"
urlpatterns
+
=
"static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)"
I
found
the
problem
was
in
my
settings
code
.
Here
is
the
fixed
code
:
now
it
is
uploading
to
the
correct
folder
","
but
It
is
not
showing
it
on
webpage
","
cause
it
is
looking
in
a
wrong
location
.
-
_
-
Anyways
thx
for
help
!
When
I
am
trying
to
check
the
image
it
does
:
but
I
don't
get
why
is
it
looking
for
image
in
Vote
/
id-num
/
voting
/
images
/
vote
/
rather
than
the
media
root
.
Or
is
this
just
url
and
I
am
wrong
somewhere
else
?
would
appretiate
comments
on
that
.
Thanks
You
are
confused
between
media
and
static
.
Hopefully
this
will
clarify
the
matter
:
Static
Files
Static
files
are
files
that
are
shipped
as
part
of
your
application
.
STATIC_ROOT
is
where
the
collectstatic
command
will
dump
all
static
files
","
so
you
can
move
them
to
the
production
web
server
.
You
don't
need
to
mess
with
STATIC_ROOT
during
development
.
In
your
case
","
you
have
an
application
Vote
","
and
inside
it
you
have
a
static
folder
.
That's
all
you
need
to
serve
static
files
during
development
.
All
you
have
to
do
is
{
%
static
'
folder
/
filename.ext
'
%
}
where
folder
is
the
folder
inside
static
that
contains
your
file
.
So
if
your
directory
structure
is
:
You
need
<
img
src
=
""""
{
%
static
'
vote
/
images
/
foo.jpg
'
%
}
""""
>
Media
Files
Media
files
are
what
django
calls
files
that
are
uploaded
by
your
application's
users
For
uploading
files
","
you
need
to
set
a
location
where
all
uploaded
files
are
stored
.
This
location
is
the
parent
directory
and
its
universal
across
your
entire
project
.
The
setting
for
this
is
MEDIA_ROOT
.
This
should
be
a
full
system
file
path
.
Suppose
you
set
MEDIA_ROOT
=
'
/
home
/
foo
/
bar
/
proj
/
media
'
","
and
in
your
models
you
have
:
Files
for
the
model
will
be
uploaded
to
/
home
/
foo
/
bar
/
proj
/
media
/
logos
/
.
To
display
these
files
","
you
need
to
tell
django
the
name
of
a
link
that
points
to
the
MEDIA_ROOT
directory
","
this
setting
is
MEDIA_URL
.
For
development
","
you
need
to
do
three
things
:
Set
MEDIA_URL
to
/
media
/
Configure
urls.py
to
serve
media
files
:
Add
'
django.template.context_processors.media
'
to
context_processors
in
TEMPLATES
.
This
will
activate
{
{
MEDIA_URL
}
}
in
your
templates
.
I
am
new
to
django
and
tried
to
fix
this
problem
","
but
couldn't
find
whats
wrong
with
it
.
All
the
validations
of
forms
are
passed
perfectly
","
but
the
image
doesn't
actually
get
uploaded
to
the
media
root
.
I
tried
to
show
images
and
I
only
get
image
icon
instead
of
Image
.
settings
root
part
:
models
code
:
views.py
create(view)
code
:
and
there
also
is
template
code
(
which
is
pretty
huge
and
not
sure
if
it
is
required
)
:
When
I
create
a
file
with
create
view
","
it
doesn't
upload
an
image
to
the
media
/
images
/
vote
.
Why
is
this
happening
?
EDIT
1:added
some
code
Someone
also
told
me
that
I
should
add
this
to
my
code
The
Problem
now
is
that
the
When
I
try
to
get
Image
","
the
program
looks
for
images
at
:
Which
I
think
he
takes
from
my
urls.py
file
.
But
I
dont
get
why
is
the
program
looking
in
/
Vote
/
ID
/
voting
...
instead
of
MEDIA_ROOT
.
Or
does
GET
not
mean
that
it
is
looking
at
that
place
?
This
is
my
Vote
/
urls.py
:
Change
your
MEDIA_ROOT
according
to
the
path
of
your
media
files
:
For
eg
.
C:\Users\Vato\PycharmProjects\Project3\Vote\media
change
property
"name(if you need property, here no need of it to define.Because you have an instance variable username)"
","
Because
you
have
created
a
instance
variable
with
same
name
.
With
your
property
you
have
created
an
infinite
recursion
.
self.username
in
the
return
statement
calls
the
property
method
again
","
and
againâ
€
¦
Remove
the
whole
property
from
your
code
and
it
will
work
.
Python
does
not
need
explicid
accessors
.
Your
are
causing
a
race
condition
by
returning
a
call
to
the
method
username()
You
call
the
method
","
it
returns
the
method
which
is
then
called
again
ad
infinitum
First
of
all
","
I
saw
many
threads
regarding
to
""""
maximum
recursion
depth
exceeded
""""
couldn't
find
in
any
of
them
a
solution
for
my
issue
.
I'm
learning
Python
these
days
and
trying
to
understand
the
following
issue
:
assume
we
have
two
classes
:
and
a
child
class
:
While
instantiating
MongoDbReporter
instance
the
following
exception
is
raised
:
Probably
I
created
some
recursion
here
but
I
can't
find
it
","
What
am
I
doing
wrong
?
Maybe
a
little
late
","
but
this
might
interest
other
users
of
Angular
:
Simply
call
a
modal
to
get
a
pop-up
of
the
Pyramid
HTML
error
:
(
do
not
forget
to
include
$uibModal
)
.
Very
useful
for
debugging
purposes
","
but
not
recommended
for
production
.
Your
render
is
not
considered
if
you
do
not
return
a
value
for
renderer
but
return
a
response
or
exception
-
so
do
not
expect
json
on
the
output
unless
your
error
handler
supports
that
.
You
can
do
something
like
this
instead
:
And
then
you
can
retrieve
the
JSON
in
your
error
handler
and
use
angular.fromJson()
to
read
it
.
I'm
sure
there
must
be
an
easy
solution
for
this
","
but
I
have
been
unable
to
find
it
.
I'm
using
Python
Pyramid
in
my
server
and
handling
requests
/
responses
with
AngularJS
in
Javascript
.
I'm
using
the
Pyramid
HTTP
errors
to
handle
some
cases
","
and
I'm
able
to
catch
them
using
the
Angular
http.error()
statement
.
But
this
is
where
I'm
stuck
","
I
want
to
do
something
with
the
error
messages
but
if
I
log
the
response
I
just
see
html
code
.
In
Pyramid
this
is
happening
:
And
in
Angular
:
How
would
I
get
that
specific
error
message
from
the
HTTP
error
?
(
I'm
using
JSON
as
my
renderer
btw
)
My
approach
is
:
I
just
started
developing
with
cherrypy
","
so
I
am
struggling
a
little
bit
.
In
client
side
I
am
selecting
some
data
","
converting
it
to
json
and
sending
to
server
side
via
post
method
.
Then
I
am
doing
a
few
operations
with
json
and
finally
I
want
to
send
it
back
to
client
side
.
So
the
question
is
how
to
return
modified
json
to
the
client
side
(
browser
)
.
Server
side
:
Client
side
:
Just
change
to
this
line
instead
of
what
you
have
...
EDIT
:
Oh
-
and
change
your
handler
to
this
:
Hope
this
helps
!
Use
CherryPy
JSON
tools
.
Ok
","
I
found
the
solution
!
Maybe
it
will
be
useful
for
someone
else
","
so
I
paste
it
here
:
You
can
find
the
function
in
http://www.daterangepicker.com/#ex5
Use
AJAX
requests
to
send
this
data
to
server(view)
.
Something
like
I'm
coming
back
with
a
very
simple
questions
","
yet
I
am
not
able
to
solve
it
on
my
own
...
I
found
a
powerful
Daterangepicker
in
Javascript
:
http://www.daterangepicker.com/#ex5
.
As
I
have
never
learned
js
","
I
would
like
to
know
how
to
pass
the
start
and
end
dates
out
of
the
widget
to
the
views.py
file
...
The
few
hints
I
got
came
from
this
topic
:
Django
and
date
range
picker
component
for
Twitter
Bootstrap
which
only
provides
the
code
for
Django
and
not
the
js
.
Many
thanks
!
I
was
getting
this
error
when
I
was
trying
to
install
graph-tool
using
outdated
an
autoconf
/
automake
/
pkg-config
combination
(
installed
using
yum
in
CentOS
5.10
)
.
Installing
those
packages
from
source
fixed
the
problem
...
although
I'm
not
sure
how
this
related
to
my
python
installation
....
I
tried
to
install
graph-tool
on
Mac
OSX
10.10
using
homebrew
.
The
brew
build
process
works
fine
","
but
when
I
try
to
import graph
-
tool
I
get
the
error
described
in
this
question
.
Another
problem
with
homebrew
is
that
I
always
builds
graph-tool
for
python2.7
and
it
installs
the
packages
in
the
Python
2.7
sit-packages
folder
.
But
I
want
to
use
it
with
Python
3.4
.
These
are
the
reasons
why
I
tried
to
build
graph-tool
from
source
.
The
.
/
configure
command
automatically
uses
Python
2.7
","
too
.
So
I
passed
it
the
desired
Python
version
with
.
/
configure
PYTHON=python3.4
It
then
detects
the
correct
version
as
well
as
the
related
paths
but
crash
with
the
following
error
:
configure
:
error
:
Could
not
link
test
program
to
Python
.
Maybe
the
main
Python
library
has
been
installed
in
some
non-standard
library
path
.
If
so
","
pass
it
to
configure
","
via
the
LDFLAGS
environment
variable
.
Example
:
.
/
configure
"LDFLAGS=""-L"
/
usr
/
non-standard-path
/
python
/
lib
""""
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
ERROR
!
You
probably
have
to
install
the
development
version
of
the
Python
package
for
your
distribution
.
The
exact
name
of
this
package
varies
among
them
.
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
The
error
occurs
with
and
without
PYTHON
variable
set
.
From
the
output
of
.
/
configure
I
can
see
that
everything
works
fine
except
for
the
last
line
","
which
says
:
checking
consistency
of
all
components
of
python
development
environment
...
no
Whats
does
the
above
line
mean
and
how
do
I
properly
install
graph-tool
on
my
maschine
?
The
error
message
is
explaining
exactly
what
needs
to
be
done
.
Since
python
was
installed
in
a
non-standard
path
","
you
need
to
pass
the
flag
"LDFLAGS=""-L"
/
usr
/
non-standard-path
/
python
/
lib
""""
pointing
to
the
directory
where
the
python
libraries
are
located
.
This
is
most
likely
""""
/
usr
/
local
/
lib
""""
","
if
you
are
using
homebrew
.
by
here
","
Total
Emp
","
which
just
show
Employee.empCount
remains
0
.
this
inherits
and
defines
the
Employee
","
and
adds
self.empcount
.
Which
as
a
result
","
adds
1
to
empCount
.
emp1
and
emp2
is
separate
object
and
affected
by
init
individually
","
which
makes
empCount
1
.
I
am
a
beginner
in
programming
and
have
started
working
on
Python.I
am
running
following
code.Please
explain
me
the
sequence
flow
for
the
output
and
I
get
following
output
:
You
are
using
augmented
assignment
.
For
an
immutable
type
like
an
integer
","
you
are
essentially
doing
the
same
thing
as
this
:
The
self.empCount
retrieves
either
an
instance
attribute
(
if
it
exists
)
or
a
class
attribute
.
Because
there
is
no
such
instance
attribute
","
it
is
the
class
attribute
(
set
to
0
)
that
is
retrieved
and
1
is
assigned
to
self.empCount
.
Assignment
to
an
instance
attribute
always
sets
an
instance
attribute
","
never
the
class
attribute
.
Each
of
your
instances
thus
ends
up
with
self.empCount
set
to
1
and
the
class
attribute
never
changes
.
Note
that
even
if
empCount
was
a
mutable
type
","
such
as
a
list
","
the
assignment
would
give
you
an
instance
attribute
anyway
","
even
if
the
augmented
assignment
also
mutated
the
class
attribute
.
See
Why
does
+
=
behave
unexpectedly
on
lists
?
Augmented
assignment
on
non-existing
instance
attributes
is
rarely
a
good
idea
.
","
because
the
augmented
assignment
does
this
:
Note
the
explicit
assignment
to
the
self.empCount
attribute
.
If
you
wanted
to
increment
the
class
attribute
","
you
need
to
do
so
explicitly
:
So
I
granted
all
privileges
to
root
in
MySQL
So
after
running
this
i
get
:
But
after
running
my
code
on
Pycharm
which
accesses
a
local
DB
","
I'm
getting
this
:
I'm
pretty
new
to
Python
and
MySQL
","
so
I'm
gonna
need
pretty
simple
yet
detailed
advice
if
anyone
is
willing
to
help
.
Thanks
!
download
the
MySQL
python
connector
from
here
http://dev.mysql.com/downloads/connector/python/
then
run
this
within
a
file
:
After
running
your
query
make
sure
to
commit
any
changes
and
close
connections
:
I'm
playing
around
with
server
sent
events
and
flask
.
The
setup
is
very
basic
.
On
the
client
:
And
on
the
server
I
have
:
Everything
works
","
except
it
is
really
slow
.
Given
my
delay
of
time.sleep(.1)
I
would
have
expected
to
see
roughly
10
events
per
second
on
the
client
.
It
is
however
only
one
event
every
~
3
seconds
.
When
I
increase
my
delay
to
say
time.sleep(5)
","
I
get
an
event
every
8
seconds
and
so
forth
.
So
it
looks
like
I
have
a
delay
of
~
3
seconds
between
server
and
client
","
which
is
odd
given
that
they
run
on
the
same
machine
.
The
browser
I
use
for
testing
is
chromium
","
I
run
the
development
server
from
flask
.
But
the
same
happens
with
gunicorn
.
Do
I
anything
wrong
?
Is
there
a
way
to
speed
up
SSEs
?
I
found
it
out
myself
now
.
I'm
missing
the
loop
in
the
events()
function
.
Changing
it
to
solved
the
problem
.
I
have
some
data
in
pandas
which
I'm
trying
to
save
as
32-bit
float
but
instead
I'm
always
getting
64-bit
float
.
My
best
attempt
was
this
:
but
it's
not
working
.
.
any
ideas
?
Just
extending
on
the
accepted
answer
.
Note
that
if
memory
is
constrained
or
you
want
more
space
","
you
can
choose
df
[
'
a
'
]
.
astype(np.float32)
as
the
answer
gives
or
equally
substitute
np.float16
","
or
np.float64
for
numbers
","
np.int16
","
np.int32
","
np.int64
for
integers
","
Many
applications
you
can
take
down
to
int16
/
float16
and
shrink
your
data
footprint
if
accuracy
is
fine
for
your
application
.
Use
numpy.float32
:
You
can
see
that
the
dtype
is
now
float32
I
am
posting
this
duplicate
link
as
answer
","
because
title
of
your
question
is
very
readable
and
there
is
answer
for
all
your
question
Django
lazy
QuerySet
and
pagination
So
I
was
reading
about
pagination
","
I
have
done
it
quite
a
few
times
writing
this
app
but
I
was
wondering
how
does
pagination
in
django
work
at
sql
level
.
Does
Contacts.objects.all()
get
called
evertime
I
am
hitting
the
view
?
Or
does
Paginator
maintains
a
state
somehow
?
What
would
the
sql
query
look
like
for
paginator
=
"Paginator(contact_list, 25)"
Are
there
generators
being
used
behind
the
scenes
and
things
are
lazily
evaluated
?
Sorry
if
this
is
a
dumb
question
","
there
is
a
lot
of
abstraction
in
Django
and
I
seem
to
miss
all
the
action
behind
.
Thanks
in
advance
May
be
it
will
help
you
bit
more
to
handle
.
I
am
writing
a
web
service
using
Django
that
will
be
consumed
from
a
MS
SharePoint
workflow
.
In
the
SP
workflow
","
I
created
a
dictionary
with
2
items
(
id:1
","
text:'foo
'
)
","
and
used
this
dictionary
as
the
request
content
.
However
","
instead
of
using
the
dictionary
to
format
a
traditional
POST
parameter
list
","
it
sends
it
as
a
JSON
object
in
the
body
of
the
POST
request
","
so
instead
of
the
expected
:
in
the
body
of
the
request
","
there
is
this
:
which
of
course
","
in
turn
","
does
not
get
parsed
correctly
by
Python
/
Django
(
I
am
not
sure
who
exactly
does
the
parsing
)
.
How
can
I
either
get
it
to
parse
JSON
","
or
get
SharePoint
to
send
traditionally
encoded
POST
parameters
?
EDIT
I
saw
other
posts
that
explain
how
to
get
the
raw
body
and
parse
the
JSON
.
I
was
looking
for
a
solution
that
would
either
:
Make
SharePoint
send
normal
data
","
or
Get
Django
to
respect
the
Content-type
header
that
states
the
data
is
JSON
There
is
no
need
for
any
parsing
at
the
framework
level
.
The
body
of
the
post
request
is
always
available
in
request.body
","
so
you
can
access
it
directly
:
Worker
will
reserve
tasks
for
each
worker's
tread
.
If
you
want
to
limit
the
number
of
tasks
worker
can
execute
the
same
time
","
you
should
configure
your
concurrency
(
e.g
.
to
limit
1
task
at
the
time
","
you
need
worker
with
1
process
-
c
1
)
.
You
can
also
check
prefetch
configuration
","
but
it
only
defines
the
number
of
tasks
reserved
for
each
process
of
the
worker
.
Here
is
Celery
documentation
where
prefetch
configuration
explained
:
http://celery.readthedocs.org/en/latest/userguide/optimizing.html
As
I
see
in
celery
","
It
can
get
number
of
tasks
for
a
worker
","
that
can
run
them
at
a
same
time
.
I
need
run
a
task
and
set
number
of
tasks
can
run
simultaneously
with
this
task
.
Therefore
","
If
I
set
this
number
to
2
and
this
task
send
to
worker
with
10
threads
","
worker
can
run
just
one
another
task
.
Consider
this
:
Same
thing
with
slicing
done
before
Now
why
is
the
second
method
faster
than
first
method
?
What
exactly
is
taking
place
here
which
is
causing
the
second
method
to
be
more
efficient
than
first
?
EDIT
:
This
is
a
part
of
a
big
program
which
I
was
attempting
at
one
of
the
sites
.
I
was
getting
timeout
error
with
method
1
with
large
inputs
.
Whereas
the
second
method
did
it
.
It
was
at
least
2
secs
faster.The
string
length
was
10000
for
this
scenario.Here
are
the
two
submissions
:
http://hackerearth.com/submission/2129789
#
Method
1
http://hackerearth.com/submission/2130107
#
Method
2
Your
diagnosis
appears
to
be
flawed
.
Using
the
timeit
module
you'll
find
that
using
addional
locals
is
in
fact
slower
","
but
only
by
the
smallest
of
margins
.
I
reduced
your
tests
to
the
essentials
to
reduce
overhead
:
The
difference
is
so
small
as
to
be
negligible
.
If
you
run
this
often
enough
f2
can
win
by
the
same
margin
.
If
you
reverse
the
tests
(
run
f2
first
","
then
f1
)
","
the
outcomes
change
again
.
That's
because
there
is
practically
no
difference
.
The
two
functions
execute
the
exact
same
bytecodes
","
with
the
addition
of
two
STORE_FAST
and
LOAD_FAST
opcodes
for
f2
at
indices
22
","
47
","
50
and
53
.
The
STORE_FAST
and
LOAD_FAST
copy
across
a
pointer
between
the
stack
and
the
locals
array
","
both
pre-allocated
structures
.
So
f2
is
doing
the
teensiest
more
work
than
f1
is
doing
.
Changing
the
sizes
of
the
inputs
is
not
making
a
difference
here
","
a
pointer
doesn't
get
'
heavier
'
if
the
referenced
object
is
bigger
.
If
you
examine
your
results
a
little
more
closely
on
the
linked
hackerearth
site
you
will
see
that
","
individually
","
the
first
method
is
quicker
on
most
of
the
inputs
.
What
happened
is
that
the
last
two
results
(
for
inputs
7
and
8
)
didn't
complete
in
a
timely
fashion
","
which
resulted
in
an
overall
longer
runtime
.
Why
is
anyone's
guess
","
maybe
the
server
was
under
heavy
load
at
that
time
and
your
process
had
a
low
priority
.
So
today
in
science
class
I
thought
of
making
a
python
script
for
basic
perimeter
of
a
quadrilateral
.
Later
in
future
I
want
to
expand
into
circle
and
other
shape
but
I
got
stuck
in
error
.
Please
help
.
My
code
:
https://repl.it/xHG
And
the
output
comes
funky
.
If
l=2
and
b=1
then
output
comes
as
2211
.
Also
","
how
do
you
expand
it
into
different
shapes
?
I
was
thinking
of
using
if
and
else
options
so
if
choice
=
circle
then
execute
circle
code
elif
if
choice
=
triangle
then
execute
triangle
code
.
Does
anyone
have
a
better
idea
?
Remember
to
convert
data
types
You
need
to
convert
you
input
to
an
int
or
float
.
In
your
code
len
is
a
string
","
and
therefor
when
you
perform
len
+
len
you
are
performing
String
concatenation
In
Python
3
","
input
returns
a
string
(
this
is
different
in
Python
2.x
","
which
may
be
part
of
the
confusion
)
.
That
means
that
length
=
len
+
len
is
actually
performing
string
concatenation
","
ie
.
'
2
'
+
'
2
'
=
'
22
'
.
Using
either
"int(input(""..."")"
)
or
"float(input(""..."")"
)
will
turn
them
into
numbers
.
(
note
that
both
functions
will
create
errors
if
the
user
puts
in
strings
that
can't
be
converted
to
numbers
.
Ok
-
with
a
bit
more
time
spent
I've
managed
to
find
a
solution
that
worked
for
me
:
for
every
key
in
the
error
dictionary
submit
the
form
and
loop
through
and
check
for
errors
present
and
then
return
""""
Fail
""""
if
there
are
1
or
more
errors
and
""""
Pass
""""
if
no
errors
present
errorCount
is
defined
outside
of
the
whole
form
loop
and
I
also
reset
the
errorCount
after
each
form
loop
is
completed
.
The
exception
clause
is
in
there
just
so
my
script
doesn't
fail
over
when
it
can't
find
an
error
from
the
error
dictionary
.
This
may
have
been
covered
by
something
else
but
I've
been
unable
to
find
something
that
matches
up
to
my
scenario...I'm
pretty
new
to
selenium
webdriver
and
Python
so
please
excuse
any
'
best
practice
'
issues
in
my
code
.
I'm
trying
to
find
a
solution
that
counts
the
instance
of
field
errors
on
a
web
form
and
if
it
is
greater
than
or
equal
to
1
then
the
result
should
be
a
fail
.
If
zero
then
a
pass
.
In
addition
I
would
like
to
print
to
the
console
which
field
errors
being
presented
.
I'm
using
xpaths
stored
in
a
dictionary
for
maintainability
and
then
returning
the
string
from
the
label
(
which
is
also
my
xpath
identifier
)
.
In
addition
the
error
messages
are
only
presented
on
the
fly
so
need
to
handle
the
exception
'
NoSuchElementException
'
I
know
what
I
want
to
achieve
but
can't
seem
to
get
the
syntax
right
so
every
time
the
result
is
'
Pass
'
-
I
believe
this
is
because
1
or
more
of
my
xpaths
are
returning
the
exception
.
Below
is
a
snippet
of
the
code
block
I
want
to
do
the
count
and
return
the
result
:
Here
is
a
snippet
of
my
dictionary
:
In
addition
the
'
for
key
in
formErrors
'
sits
inside
a
for
loop
that
sends
inputs
from
a
csv
into
the
fields...and
submits
the
form
for
each
time
the
csv
has
a
value...I
don't
think
this
is
needed
to
answer
my
question
but
thought
best
to
provide
for
insight
.
I'm
using
a
platform
that
","
when
you
upload
pdf's
to
it
","
converts
the
pdf
with
the
base64
encode
in
Python
.
Then
it
stores
the
binary
string
in
the
database
.
Now
I
want
to
decode
the
strings
and
write
them
to
a
local
folder
","
so
I
thought
to
use
the
'
with
open
'
structure
and
pass
it
the
b
parameter
for
binary
and
then
it
should
create
test.pdf
based
of
my
decoded
string
and
write
it
to
my
desktop
?
Yet
this
yields
no
results
","
what
am
I
doing
wrong
here
?
EDIT
:
Example
binary
string
in
database
:
""""
65
/
658e9014babd33786821f3130c5f3a1cc1322ddf
""""
So
I'm
assuming
it
starts
after
the
'
/
'
mark
?
"base64.decode(,)"
takes
files
as
args
.
you
want
to
try
though
your
code
example
is
not
encoded
.
Here's
a
working
example
:
I
have
a
string
like
this
:
Original
was
encoded
in
""""
utf-8
""""
My
question
is
:
how
do
i
transform
this
byte-like
into
string
""""
as
is
""""
?
I
mean
","
keeping
the
b
'
","
\
xc2
","
\
n
...
If
I
use
"str.decode(""utf-8"")"
I
loose
most
of
the
stuff
.
You
can
use
repr(string)
-
that's
exactly
how
the
string
with
'
b
'
prefix
and
escape
sequences
is
created
.
some
change
output
","
t3
is
python
file
t3.py
Although
you
delete
the
h
","
the
object
was
still
referenced
by
g
and
f
.
gc
Python
use
reference
count
to
reclaim
memory
.
That
is
because
deleting
an
object
does
not
automatically
delete
objects
that
it
references
.
Meaning
lets
say
we
have
-
When
we
do
del
l
","
it
deletes
the
name
l
","
not
the
object
it
references
.
Example
-
It
just
decreases
the
reference
for
the
object
by
1
","
when
the
references
for
an
object
reaches
0
it
can
get
garbage
collected
.
So
in
your
binary
search
tree
case
","
when
you
do
-
You
are
just
deleting
the
local
variable
/
name
min_right
","
not
the
object
it
references
.
To
delete
the
attribute
:
It
will
be
gone
and
not
None
.
To
set
it
to
None
:
Don't
use
an
intermediate
variable
!
You
have
a
situation
that
looks
like
this
:
i.e.
three
references
to
the
same
underlying
object
.
All
that
del
h
does
is
remove
the
name
h
","
like
this
:
The
underlying
object
doesn't
change
","
and
the
two
other
names
still
provide
a
reference
to
it
","
all
you've
done
is
remove
one
reference
to
the
Foo
instance
.
h
is
now
a
NameError
","
but
g
and
f.attr
will
work
just
fine
.
I
don't
know
why
you
would
conclude
that
-
if
you
did
succeed
in
breaking
the
reference
from
f.attr
to
the
Foo
instance
","
you
would
get
an
AttributeError
","
not
None
.
Recommended
reading
on
Python
names
:
http://nedbatchelder.com/text/names.html
I
want
to
delete
a
class
instance
or
make
it
None
in
python
.
But
I'm
not
able
to
do
it
.
In
general
my
code
does
something
as
follows
:
-
More
specifically
this
problem
came
while
implementing
binary
search
trees
in
python
.
My
delete
function
does
something
like
this
:
-
Am
I
doing
something
wrong
or
is
it
expected
behavior
?
If
it
is
expected
","
what
is
the
best
way
by
OOP
standards
to
achieve
what
I
am
trying
to
do
?
You
can
try
this
If
there
are
more
then
3
tables
just
add
tables
after
FROM
T1
","
T2
","
T3
","
T4
","
.
.
TN
And
Add
Comparision
for
not
equality
by
adding
this
in
where
clause
T1.val
<
>
T4.val
AND
T2.val
<
>
T4.val
AND
T3.val
<
>
T4.val
for
every
table
/
/
this
is
added
for
T4
First
","
SQL
tables
represent
unordered
sets
.
So
","
there
is
no
correspondence
between
two
tables
based
on
position
.
Instead
","
let
me
assume
that
each
has
a
name
column
.
Second
","
SQL
doesn't
really
do
optimization
","
so
you
need
to
do
this
in
a
brute
force
way
.
That
is
","
generate
all
the
combinations
and
then
choose
the
minimum
value
.
That
uses
essentially
a
cross
join
:
Or
","
if
you
want
the
names
:
I
have
an
SQLite
database
with
3
tables
.
Each
of
these
tables
have
two
columns
:
name
and
value
such
that
all
the
tables
have
the
same
records
(
name
)
but
different
values
.
How
do
I
select
the
records
that
are
a
part
of
least
sum
of
values
spanning
across
these
tables
such
that
the
record
names
are
unique
?
Names
:
Steven
","
Jamie
","
Michael
","
Jordan
","
Gary
Values
(
in
the
order
of
names
)
Table
1
:
1
","
2
","
3
","
4
","
5
Table
2
:
2
","
3
","
1
","
5
","
6
Table
3
:
9
","
0
","
2
","
11
","
3
The
output
should
return
(
Steven
","
Michael
","
Jamie
)
because
the
sum
of
the
values
in
this
case
would
equal
to
2
which
would
be
the
least
possible
.
You
program
is
probably
failing
in
trying
to
load
the
entire
dataset
into
RAM
.
32
bits
per
float32
×
1
","
000
","
000
×
1000
is
3.7
GiB
.
That
can
be
a
problem
on
machines
with
only
4
GiB
RAM
.
To
check
that
it's
actually
the
problem
","
try
creating
an
array
of
this
size
alone
:
If
you
see
a
MemoryError
","
you
either
need
more
RAM
","
or
you
need
to
process
your
dataset
one
chunk
at
a
time
.
With
h5py
datasets
we
just
should
avoid
passing
the
entire
dataset
to
our
methods
","
and
pass
slices
of
the
dataset
instead
.
One
at
a
time
.
As
I
don't
have
your
data
","
let
me
start
from
creating
a
random
dataset
of
the
same
size
:
It
creates
a
nice
3.8
GiB
file
.
Now
","
if
we
are
in
Linux
","
we
can
limit
how
much
memory
is
available
to
our
program
:
Now
if
we
try
to
run
your
code
","
we'll
get
the
MemoryError
.
(
press
Ctrl-D
to
quit
the
new
bash
session
and
reset
the
limit
later
)
Let's
try
to
solve
the
problem
.
We'll
create
an
IncrementalPCA
object
","
and
will
call
its
.
partial_fit()
method
many
times
","
providing
a
different
slice
of
the
dataset
each
time
.
It
seems
to
be
working
for
me
","
and
if
I
look
at
what
top
reports
","
the
memory
allocation
stays
below
200M
.
I
just
tried
using
the
IncrementalPCA
from
sklearn.decomposition
","
but
it
threw
a
MemoryError
just
like
the
PCA
and
RandomizedPCA
before
.
My
problem
is
","
that
the
matrix
I
am
trying
to
load
is
too
big
to
fit
into
RAM
.
Right
now
it
is
stored
in
an
hdf5
database
as
dataset
of
shape
~
(
1000000
","
1000
)
","
so
I
have
1.000.000.000
float32
values
.
I
thought
IncrementalPCA
loads
the
data
in
batches
","
but
apparently
it
tries
to
load
the
entire
dataset
","
which
does
not
help
.
How
is
this
library
meant
to
be
used
?
Is
the
hdf5
format
the
problem
?
Thanks
for
help
Since
you
tagged
this
as
revitpythonshell
:
close
streamreader
function
Modifying
is
forbidden
because
the
document
has
no
open
transaction
.
The
document
has
no
open
transaction
.
In
Revit
Document
.
You
have
2
options
:
change
the
TransactionMode
to
Automatic
at
the
class
attribute
[
Transaction(TransactionMode.Automatic)
]
open
a
transaction
within
your
command
Transaction
tr
=
new
Transaction(commandData.Application.ActiveUIDocument.Document)
;
"tr.Start(""Command name here"")"
;
/
/
your
code
tr.Commit()
;
I've
noticed
weird
behavior
with
zmq.PUB
","
it
requires
to
have
while
loop
to
send
message
.
Example
:
Imagine
I
have
a
sub
waiting
for
a
pub
:
And
I
want
to
send
a
message
from
pub
:
This
message
won't
reach
the
sub
for
some
reason
.
You
can
fix
this
by
adding
while
or
for
loop
before
sending
a
message
publisher.send(''.join(sys.argv[1])
)
Why
is
that
?
Do
I
always
have
to
have
a
loop
with
publisher
to
distribute
messages
to
multiple
workers
?
Actually
","
there
are
a
few
problems
here
.
First
","
the
code
you've
posted
has
a
number
of
errors
in
it
(
for
example
","
you
can't
connect
to
tcp
:
/
/
*
:
5555
)
","
but
I'll
assume
that
it's
representative
of
what
you're
trying
to
do
.
Update
I'm
going
to
leave
my
original
answer
below
","
because
I
think
it's
still
useful
although
not
directly
relevant
to
your
question
(
possibly
due
to
a
morning
caffeine
deficit
","
who
knows
?
)
.
Since
your
publisher
is
calling
send
with
a
single
message
and
then
exiting
immediately
","
the
subscribers
probably
never
have
time
to
connect()
.
You
would
typically
expect
the
code
calling
bind
to
be
the
long-running
process
.
If
you
insert
a
sleep
between
the
bind
and
send
call
","
things
will
work
as
expected
:
This
works
fine
","
assuming
the
following
subscriber
:
However
","
this
isn't
a
great
solution...anything
using
sleep
for
synchronization
is
bound
to
fail
.
A
different
way
of
solving
this
would
be
to
have
your
publisher
open
both
a
PUB
socket
and
a
REP
socket
.
To
send
a
message
to
all
subscribers
","
your
tool
would
open
a
REQ
socket
to
the
broker
","
which
would
then
publish
it
to
all
subscribers
.
Original
answer
follows
The
problem
you're
hitting
is
probably
this
.
Since
you
are
having
your
subscriber
call
bind
and
your
sender
call
connect
","
it
is
likely
that
your
are
simply
losing
the
message
because
there
is
a
delay
between
a
successful
connection
and
the
subscriber
successfully
subscribing
to
messages
.
Also
see
the
question
""""
Why
do
I
see
different
behavior
when
I
bind
a
socket
versus
connect
a
socket
?
""""
in
the
FAQ
.
Demonstrating
this
with
code
","
if
we
have
this
subscriber
:
And
the
following
publisher
:
The
subscriber
will
probably
always
end
up
printing
:
This
is
because
the
first
to
send
operations
happen
to
quickly
.
This
is
generally
not
considered
a
problem
","
since
pub
/
sub
is
explicitly
not
a
reliable
transport
mechanism
.
In
the
typical
use
case
","
there
is
a
single
publisher
calling
bind
and
multiple
subscribers
calling
connect
that
may
not
all
be
present
initially
or
continuously
.
There
is
no
guarantee
that
every
subscriber
will
receive
every
message
.
If
you
need
to
reliably
transport
a
single
message
","
consider
a
REQ
/
REP
socket
pair
instead
.
You
can
use
list
comprehensions
:
I
have
a
nested
list
.
For
example
:
Now
I
want
to
sort
the
numbers
within
the
sub-lists
into
descending
order
and
finally
arrange
the
nested
list
into
alphabetical
order
in
Python
3
.
So
output
data
should
be
:
where
numbers
in
Rhys
","
Sam
","
Tom
and
Tommo
have
been
reordered
into
descending
order
.
Make
it
an
integer
with
int()
:
Ok
so
I
am
making
a
python
random
number
generator
with
the
parameters
defined
by
the
user
.
I
am
writing
this
on
python
3.4
.
this
is
the
code
:
I
am
having
an
issue
with
the
line
:
I
cannot
use
the
variable
'
numOfNums
'
as
a
parameter
.
Do
you
have
any
ideas
?
You
can
use
get_loc
to
get
the
index
position
of
the
column
of
interest
and
then
use
this
to
index
your
df
:
I
have
a
python
DataFrame
from
Pandas
Every
time
I
construct
this
DataFrame
","
the
number
and
name
of
columns
after
the
snapDate
are
different
.
I
would
need
to
construct
a
new
DataFrame
by
selecting
snapDate
as
the
indexcolumn
but
the
remaining
columns
should
be
dynamically
selected
for
plotting
any
ideas
.
How
can
I
achieve
this
?
for
the
df
above
","
I
should
always
select
the
snapDate
column
and
all
the
columns
after
the
snapDate
column
The
number
and
name
of
the
columns
after
the
snapDate
column
will
vary
.
My
objective
is
to
do
I
want
to
create
a
plot
from
the
DataFrame
by
always
picking
the
SnapDate
column
and
the
remaining
columns
which
depending
on
the
DataFrame
could
be
2
or
3
or
4
etc.
The
user
story
:
An
user
clicks
somewhere
","
and
we
are
recording
his
action
.
Can
we
","
somehow
","
get
access
to
the
original
request
object
so
we
will
be
able
to
extract
some
required
information
from
it
?
The
catch
:
We
cannot
change
the
StudentActionModel
code
","
we're
writing
a
plugin
to
the
original
Django
application
and
can't
change
any
of
the
original
code
.
We
are
just
defining
a
listener
for
the
'
post_save
'
signal
and
we
need
a
piece
of
data
from
the
original
request
object
.
You
cannot
assume
that
only
view
code
will
call
StudentActionModel.save()
-
it
could
be
called
by
a
management
command
or
just
any
script
-
which
is
why
neither
Model.save()
norpost_save()
nor
any
of
thedjango.db
`
signals
get
the
request
.
To
make
a
long
story
short
:
you'll
have
to
handle
this
in
the
views
(
or
in
a
custom
middleware
)
","
not
at
the
orm
level
.
How
can
i
evaluate
if
a
user
pressed
the
Close
Window
button
in
a
wx.fileDialog
while
opening
a
file
?
i
have
2
textboxes
...
one
is
for
showing
the
input
filepath
and
the
other
is
to
show
the
output
filepath
the
output
depends
on
the
input
filepath
","
since
the
output
filepath
is
gonna
be
the
same
directory
of
the
input
","
but
with
a
different
name
.
sometimes
the
user
opens
the
filepath
and
press
the
close
button
...
and
that
action
generates
a
outfile
equals
to
_edited.txt
but
that
shouldn't
happen
.
i
wanna
something
like
how
can
i
fix
that
?
I'm
using
:
Py
2.7.9
and
wx
3.0.2.0
the
openFile
function
You
need
to
check
the
return
from
wxFileDialog::ShowModal
.
It
will
be
wxID_OK
if
the
user
pressed
OK
","
and
wxID_CANCEL
otherwise
.
http://docs.wxwidgets.org/trunk/classwx_file_dialog.html#a04943e4abb27a197a110898d40ddb4f0
as
of
wxPython
2.8.11
you
can
use
the
context
manager
.
This
will
ensure
that
the
FileDialog
will
be
destroyed
without
you
having
to
worry
about
","
by
checking
the
return
code
""""
wx.ID_OK
""""
and
by
using
the
style
""""
wx.FD_FILE_MUST_EXIST
""""
you
know
that
a
file
was
selected
.
If
you
need
a
special
handling
of
the
cancel
dialog
you
can
do
that
with
an
'
elif
'
.
This
is
almost
for
sure
that
list2
has
11606
-
11584
=
22
repeated
elements
.
(
And
because
of
list2
is
a
subset
of
list1
then
they
exist
also
in
list1
)
I
have
two
lists
","
let's
call
them
list1
and
list2
.
list1
is
the
main
list
which
contains
all
the
values
of
my
data
.
list2
contains
certain
values
that
have
to
be
removed
from
list1
.
(
You
can
say
it
is
sort
of
like
a
sublist
of
list1
)
So
to
remove
values
of
list2
from
list1
I
do
:
So
what
you
would
expect
is
for
new_list
to
have
a
length
:
However
my
new_list
has
a
length
=
11584
!
!
!
How
is
this
possible
?
?
?
?
!
!
!
!
!
EDIT
:
I
obtained
my
list2
from
list1
using
a
calculation
.
I
also
checked
for
similar
values
using
set(list1)
&
set(list2)
list2
is
actually
a
quadrant
of
list1
","
it
is
a
follow
up
of
THIS
QUESTION
I
had
asked
previously
.
So
from
the
above
link
you
can
see
that
I
have
RA
and
DEC
co-ordinates
of
my
data
.
In
the
example
","
assume
list1
is
my
RA
.
So
once
I
have
my
quandrants
","
I
apply
:
I
know
this
is
a
follow-up
of
my
previous
question
","
so
please
bare
with
the
complications
!
!
Quite
easy
when
using
pandas
.
Also
for
more
than
3
files
.
You
can
do
this
with
csv.reader
and
csv.writer
.
https://docs.python.org/3.5/library/csv.html
Code
for
your
first
example
:
for
example
:
input
csv1
:
input
csv2
:
input
csv3
:
the
'
id's
are
not
equally
distributed
in
all
3
csv
files
","
some
are
shown
in
1
file
and
some
are
not
.
The
output
csv
file
I
want
is
look
like
this
:
output.csv
(
sum
up
the
count
values
by
id
)
OR
output.csv
(
list
all
count
values
by
id
","
value
0
for
the
item
does
exists
in
the
corresponding
csv
file
but
exists
in
other
csv
files
)
Thanks
very
much
for
any
helps
.
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
updates
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
Thanks
for
@Lilith
'
answer
","
it
worked
for
problem
1
.
I
have
modified
the
code
to
work
extra
:
the
code
can
process
all
csv
file
in
a
given
folder
","
and
the
rows
in
result
csv
are
sorted
by
'
count
'
value
.
Cannot
get
a
solution
for
solution
","
may
try
it
later
.
You
were
not
far
:
-
)
you
just
need
:
I
have
a
simple
list
of
lists
.
4
lists
each
with
3
values
in
each
errors
occur
with
""""
for
idx
","
lists
in
final_list
:
""""
and
it
says
:
I
of
course
don't
know
what
you're
doing
exactly
","
but
it
sounds
to
me
like
you're
trying
to
do
too
much
inside
of
one
function
.
Your
problem
might
be
better
solved
by
simply
splitting
up
the
contents
of
external_func
.
The
goals
here
","
as
I
understand
them
","
are
you
don't
know
ahead
of
time
what
the
object
/
method
pair
will
be
","
and
want
to
reduce
code
repetition
.
Perhaps
something
like
this
would
be
better
:
I
understand
this
is
more
code
","
but
I
think
in
the
end
it's
a
lot
clearer
what
is
happening
","
and
also
might
force
you
to
think
through
your
problem
a
bit
more
(
and
potentially
come
up
with
an
even
better
solution
)
.
Depending
on
what
you're
doing
.
Because
functions
are
also
objects
in
Python
it
is
possible
to
do
so
.
But
is
it
a
good
solution
?
It
seems
though
that
you're
trying
to
handle
a
problem
which
maybe
could
be
better
solved
with
more
of
an
object
oriented
approach
:
Output
:
In
my
view
this
would
be
a
better
approach
(
if
this
is
possible
/
reasonable
in
your
case
)
:
You
just
call
some_method()
on
your
class
","
maybe
without
even
knowing
what
exact
type
of
object
you're
dealing
with
(
regarding
inheritance
)
.
The
class
itself
knows
what
to
do
and
reacts
accordingly
when
its
method
has
been
called
.
This
of
course
doesn't
work
when
you
work
with
external
libraries
which
you
have
no
influence
on
.
Your
code
is
""""
technically
correct
""""
(
it
does
what
you
ask
for
)
but
-
at
least
in
your
example
-
pretty
useless
:
is
the
same
as
:
which
FWIW
is
the
same
as
:
but
I
assume
you
understood
this
already
.
Now
you
probably
have
a
reason
to
try
to
pass
both
the
method
AND
instance
to
external_func()
","
or
there
might
be
a
better
way
to
solve
your
real
problem
...
This
is
how
it
works
for
me
:
This
appears
to
work
correctly
.
Is
this
the
right
way
to
do
this
?
You
could
pass
SomeName()
.
some_method
or
make
some_metod
staticmethod
or
classmethod
if
there
is
no
instance
data
used
in
your
method
.
Check
documentation
to
know
more
about
staticmethod
and
classmethod
:
https://docs.python.org/3/library/functions.html#staticmethod
https://docs.python.org/3/library/functions.html#classmethod
I
got
a
weird
problem
that
I
wasn't
been
able
to
find
answer
all
over
the
internet
(
or
I
don't
know
how
to
ask
)
.
I
have
module
AAA.py
and
module
BBB.py
then
when
I
call
everything
works
as
expected
and
I
see
output
BUT
when
I
try
to
import and
use
class
AAA
from
module
BBB.py
I
get
following
error
Any
suggestions
?
I
cant
create
circular
dependencies
in
Python
?
I
am
using
version
Python
2.7.6
on
Ubuntu
When
you
want
to
use
a
module
in
another
one
","
you
have
to
import it
and
so
use
import your_module
.
You
have
to
type
your_module.foo()
if
you
want
to
use
a
method
inside
it
.
With
the
instruction
from
your_module
import attr1
","
foo1
","
[
...
]
you
are
modifying
the
module's
global
variables
","
so
that
you
can
use
attr1
or
the
method
foo1
as
if
they
were
in
your
module
.
A
concrete
example
is
:
if
you
want
to
use
the
math
module
","
you
type
import math
and
when
you
want
to
use
the
constant
pi
you
type
math.pi
","
but
if
you
are
sure
there
will
be
no
clash
with
the
other
names
","
you'll
type
from
math
import pi
and
you
will
use
the
constant
pi
as
if
you
declared
it
in
your
module
.
Indeed
-
if
AAA.py
imports
something
from
BBB.py
at
top
level
and
vice
versa
","
it
doesn't
work
as
intended
.
There
are
two
ways
you
can
solve
it
:
Import
the
modules
from
each
other
.
This
way
they
are
both
present
as
their
namespace
and
will
be
filled
during
the
import process
.
So
just
do
import BBB
and
use
BBB.BBB()
for
instantiating
the
class
:
Do
the
import where
you
need
it
:
This
way
the
link
between
the
two
modules
is
""""
looser
""""
and
not
so
tight
.
Not
sure
whether
it's
any
faster
","
but
you
could
rewrite
that
code
using
itertools.combinations
and
get
the
min
using
a
key
function
calculating
the
""""
distance
""""
.
If
this
does
not
help
","
you
might
consider
temporarily
turning
the
dictionaries
in
_list
into
lists
","
holding
just
the
values
of
the
relevant
fields
.
Instead
of
using
dictionary
lookup
","
you
can
then
just
zip
those
lists
together
with
the
weights
.
Afterwards
","
turm
them
back
into
dicts
.
This
should
be
a
bit
faster
","
but
it
will
only
work
if
the
dicts
do
not
have
any
other
fields
than
those
in
self.__fields
","
otherwise
the
dicts
can
not
be
reconstructed
from
the
lists
(
at
least
not
as
easily
)
.
Alternatively
","
you
might
use
tuples
instead
of
lists
and
use
another
dictionary
to
map
those
tuples
to
the
original
dictionaries
...
Or
try
this
","
using
the
indices
of
the
elements
instead
of
the
elements
themselves
(
not
tested
)
:
In
my
python
application
I
have
a
big
list
(
now
with
almost
9000
indexes
)
.
I
need
to
find
the
two
most
similar
items
in
this
list
.
So
","
what
I
have
now
is
something
like
:
In
the
code
","
weights
is
a
dict
","
obj1
and
obj2
are
both
objects
in
which
the
__getitem__
is
implemented
and
the
return
value
also
comes
from
a
dict
.
And
self.__fields
is
a
list
with
the
selected
fields
(
it
has
now
9
items
)
.
My
problem
is
","
this
loop
is
taking
too
much
time
to
complete
.
Even
after
5
hours
","
the
i
variable
still
in
the
first
100th
list
items
.
With
this
next
silly
code
","
I
come
to
the
conclusion
that
the
problem
is
not
the
size
of
the
list
(
the
silly
code
finishes
with
5
minutes
of
difference
)
.
Therefore
","
the
problem
seems
to
be
in
the
most
internal
loop
of
my
code
:
I
know
Python
","
but
I'm
not
a
Python
specialist
.
I
conclude
that
the
access
to
the
values
of
three
objects
through
their
key
is
a
slow
operation
.
Some
time
ago
","
I
saw
in
some
blog
that
list
comprehensions
and
/
or
lambda
operations
can
be
faster
.
So
","
my
question
is
:
how
do
I
make
this
most
internal
loop
faster
using
list
comprehensions
and
/
or
lambda
?
Feel
free
to
give
any
other
advice
if
you
want
.
You
can
try
this
-
Assume
that
I
have
the
following
data
How
do
I
find
all
indices
where
","
say
","
date_list
=
=
"datetime.datetime(2015, 3, 31)"
?
This
should
be
an
easy
problem
","
but
I
can't
seem
to
figure
it
out
.
Thanks
","
Tingis
Edit
If
we
take
a
simple
example
and
set
numids
to
1
and
the
date
of
interest
to
be
"datetime.datetime(2015, 1, 1)"
","
I
would
like
to
get
the
index
1
.
Or
a
list
[
True
","
False
","
False
","
...
","
False
]
.
Edit
2
Now
","
I
try
to
just
loop
through
the
array
and
calculate
the
difference
in
days
","
such
as
However
","
this
gives
me
an
error
that
I've
never
seen
You
can
first
count
the
number
of
visits
per
visitor
/
per
page
(
with
groupby
)
:
From
this
","
you
can
take
the
mean
for
all
pages
(
second
level
of
the
index
)
per
visitor
:
I'm
trying
to
aggregate
the
mean
visits
per
page
made
by
visitors
to
a
website
grouped
by
their
visitor
id's
and
pages
they
visited
.
In
the
example
data
below
unique_visit
is
the
individual
visits
","
visitor_id
is
who
visited
","
and
page
is
the
page
they
visited
.
I
want
to
compute
the
following
:
visitor
009903
visited
page
3ghtr
three
times
","
page
4ifac
once
and
page
9fgvb
once
.
The
mean
page
visits
of
visitor
009903
is
therefore
x
.
And
do
this
for
each
visitor
.
I'm
aiming
to
return
an
aggregated
DataFrame
/
Series
where
column
1
would
be
the
visitor_id
and
column
2
would
be
a
mean
number
as
an
'
int64
'
.
Given
that
you
can
use
whatever
format
you
like
","
I'd
say
the
simplest
approach
is
JSON
:
You
get
arbitrary
nested
and
basic
types
for
free
","
and
very
easy
loading
using
the
json
module
.
I
am
using
ConfigParser
to
get
a
large
number
of
variables
that
are
set
in
a
configuration
file
","
CONFIG.txt
.
I
actually
create
several
dictionaries
from
the
different
config
file
sections
and
have
stored
them
in
a
nested
format
.
I
want
to
keep
the
variables
stored
in
dictionary
format
as
it
makes
it
easy
to
pass
around
many
at
a
time
to
various
functions
.
ConfigParser
stores
all
the
text
from
the
file
as
strings
.
Is
there
a
quick
way
to
typecast
the
variables
properly
.
Some
of
the
variables
are
lists
of
strings
","
some
strings
","
some
integers
and
some
floats
.
I
can
format
the
config
file
how
I
like
and
don't
even
really
need
to
use
ConfigParser
if
someone
can
suggest
a
more
suitable
alternative
.
Thanks
.
The
http://127.0.0.1:8000/song/299d8fe1-7d9f-434a-ba64-94fb7a16b1a6/gives
me
the
desired
page
based
on
the
uuid
of
the
song
and
works
fine
.
(
It's
the
second
url
.
)
Here
is
the
urls.py
The
third
url
","
the
one
the
arrow
is
pointing
at
","
gives
me
a
404
error
","
with
the
message
""""
No
Song
matches
the
given
query
.
""""
I'm
guessing
there's
an
error
with
the
regex
","
but
I
can't
figure
it
out
.
EDIT
Here
are
the
views
for
both
the
song
and
song
info
:
views.py
you
should
use
this
url
Here
","
[
^
/
]
represents
any
type
of
character
but
/
","
and
the
+
right
after
matches
this
class
of
character
one
or
more
times
.
You
do
not
want
the
final
/
to
be
in
the
uuid
var
","
so
put
it
outside
the
parentheses
\
S
+
matches
any
sequence
of
characters
","
including
'
/
'
.
So
your
second
regex
is
matching
the
entire
string
of
""""
299d8fe1-7d9f-434a-ba64-94fb7a16b1a6
/
song_info
/
""""
as
the
UUID
parameter
","
and
sending
it
to
views.song
","
which
unsurprisingly
does
not
find
a
matching
song
.
You
should
make
your
regexes
more
discriminating
:
Also
","
you
should
usually
put
your
regexes
in
order
of
more
specific
to
more
general
;
Django
matches
in
order
","
so
if
you
had
put
song_info
first
that
would
also
have
solved
your
problem
.
If
you
want
to
run
a
Python
code
in
a
separate
process
","
you
could
use
multiprocessing
module
:
I
wondered
whether
using
a
subprocess
module
and
multiprocess
module
in
the
same
function
is
prohibited
?
No
.
You
can
use
both
subprocess
and
multiprocessing
in
the
same
function
(
moreover
","
multiprocessing
may
use
subprocess
to
start
its
worker
processes
internally
)
.
Reason
I
want
this
is
that
I
have
two
things
to
run
:
first
an
exe
file
","
and
second
a
function
","
each
needs
it
own
process
.
You
don't
need
multprocessing
to
run
an
external
command
without
blocking
(
obviously
","
in
its
own
process
)
;
subprocess.Popen()
is
enough
:
Subprocess.Popen
is
definitely
what
you
want
if
the
""""
worker
""""
process
is
an
executable
.
Threading
is
what
you
need
when
you
need
things
to
happen
asynchronously
","
and
multiprocessing
is
what
you
need
if
you
want
to
take
advantage
of
multiple
cores
for
the
improved
performance
(
although
you
will
likely
find
yourself
also
using
threads
at
the
same
time
as
they
handle
asynchronous
output
of
multiple
parallel
processes
)
.
The
main
limitation
of
multiprocessing
is
passing
information
.
When
a
new
process
is
spawned
","
an
entire
separate
instance
of
the
python
interpreter
is
started
with
it's
own
independent
memory
allocation
.
The
result
of
this
is
variables
changed
by
one
process
won't
be
changed
for
other
processes
.
For
this
functionality
you
need
shared
memory
objects
(
also
provided
by
multiprocessing
module
)
.
One
implementation
I
have
done
was
a
parent
process
that
started
several
worker
processes
and
passed
them
both
an
input
queue
","
and
an
output
queue
.
The
function
given
to
the
child
processes
was
a
loop
designed
to
do
some
calculations
on
the
inputs
pulled
from
the
input
queue
and
then
spit
them
out
to
the
output
queue
.
I
then
designated
a
special
input
that
the
child
would
recognize
to
end
the
loop
and
terminate
the
process
.
On
your
edit
-
Popen
will
start
the
other
process
in
parallel
","
as
will
multiprocessing
.
If
you
need
the
child
process
to
communicate
with
the
executable
","
be
sure
to
pass
the
file
stream
handles
to
the
child
process
somehow
.
New
to
multiprocessing
in
python
","
consider
that
you
have
the
following
function
:
Now
the
point
is
that
I
want
the
doit.main
to
run
in
another
process
and
to
be
non
blocking
","
so
the
code
in
do_something_else
will
run
immediately
after
the
first
has
been
launched
in
another
process
.
How
can
I
do
it
using
python
subprocess
module
?
Is
there
a
difference
between
subprocessing
and
creating
new
process
aside
to
another
one
","
why
would
we
need
a
child
processes
of
other
process
?
Note
:
I
do
not
want
to
use
multithreaded
approach
here
.
.
EDIT
:
I
wondered
whether
using
a
subprocess
module
and
multiprocess
module
in
the
same
function
is
prohibited
?
Reason
I
want
this
is
that
I
have
two
things
to
run
:
first
an
exe
file
","
and
second
a
function
","
each
needs
it
own
process
.
Update
:
After
playing
around
with
this
","
I
think
I
found
the
solution
for
my
question
along
similar
lines
as
what
JAB
suggests
.
I
will
first
do
a
groupby
using
the
column
id
:
After
this
","
I
will
apply
the
resample
method
","
and
then
use
the
reset_index()
method
to
get
back
a
DataFrame
that
looks
more
or
less
what
I
have
before
:
The
pattern
I
use
for
this
problem
is
to
pivot
the
table
so
you
only
have
the
date
as
an
index
.
This
will
allow
the
resample
function
to
work
.
You
can
then
stack
the
dataframe
and
reset
the
index
to
put
the
field
id
back
as
a
column
.
I
am
a
newbie
to
Python
and
I
have
a
dataframe
which
I
have
created
from
a
query
against
Postgresql
database
via
the
read_sql
function
in
Pandas
like
this
:
The
data
that
comes
back
has
three
columns
and
they
are
daily
data
like
this
:
Now
","
I
am
trying
to
resample
this
data
so
that
only
the
business
monthend
values
come
back
","
and
if
I
do
this
:
the
ID(non-numeric)
column
drops
off
automatically
","
and
if
I
restack
the
id
column
back
as
some
other
questions
posted
suggest
","
it
gives
me
an
error
.
I
have
been
trying
different
combinations
as
suggested
by
various
posts
and
still
no
luck
.
I
will
appreciate
it
if
someone
can
shed
some
light
on
this
.
Thanks
so
much
.
I
want
to
change
Ubuntu
user
password
using
ssh
and
Python
.
Why
this
doesn't
work
?
I
can't
find
solution
(
by
default
","
sudo
don't
read
password
from
stdin
","
but
from
terminal
.
man
sudo
give
:
You
should
use
this
option
in
sudo
.
best
regards
You
can
use
a
list
comprehension
and
unpacking
operation
:
Or
if
you
want
the
result
as
float
you
can
use
/
instead
of
/
/
:
I
am
trying
to
find
the
averages
of
some
elements
in
a
nested
list
.
I
need
to
calculate
the
average
of
the
numbers
in
each
sub-list
and
order
the
sub-lists
in
descending
order
of
the
calculated
averages
in
Python
3
.
How
do
I
output
the
list
with
the
names
and
their
averages
next
to
them
?
I
have
tried
the
avg
function
but
got
stuck
.
First
this
find
the
average
of
the
data
","
and
adds
it
to
the
list
datam
","
as
well
as
the
name
.
Then
it
sorts
this
by
the
second
item
(
index
1
","
the
mean
)
.
It
also
reverses
it
","
to
go
from
Hight
to
Low
.
printing
this
gives
the
following
:
Because
it
means
to
match
the
previous
token
","
the
*
","
as
few
times
as
possible
","
which
is
0
times
.
If
you
would
it
to
extend
to
the
end
of
the
string
","
add
a
$
","
which
matches
the
end
of
string
.
If
you
would
like
it
to
match
at
least
one
","
use
+
instead
of
*
.
The
reason
the
first
group
.
*
?
matches
24036
is
because
you
have
the
\
s
token
after
it
","
so
the
fewest
amount
of
characters
the
.
*
?
could
match
and
be
followed
by
a
\
s
is
24036
.
@iobender
has
pointed
out
the
answer
to
your
question
.
But
I
think
it's
worth
mentioning
that
if
the
numbers
are
separated
by
space
","
you
can
just
use
split
:
This
is
simpler
","
easier
to
understand
and
often
faster
than
regex
.
I'm
trying
to
parse
a
text
document
with
data
in
the
following
format
:
24036
-
977
.
I
need
to
separate
the
numbers
into
separate
values
","
and
the
way
I've
done
that
is
with
the
following
steps
.
This
does
the
job
","
however
I
was
curious
about
why
using
(
.
*
?
)
in
the
second
group
causes
the
regex
to
fail
?
I
tested
it
in
the
online
regex
tester(https://regex101.com/r/bM2nK1/1)
","
and
adding
the
?
in
causes
the
second
group
to
return
nothing
.
Now
as
far
as
I
know
.
*
?
means
to
take
any
value
unlimited
times
","
as
few
times
as
possible
","
and
the
.
*
is
just
the
greedy
version
of
that
.
What
I'm
confused
about
is
why
the
non
greedy
version
.
*
?
takes
that
definition
to
mean
capturing
nothing
?
I
am
using
a
3rd
party
python
library
that
creates
.
svg's
(
specifically
for
evolutionary
trees
)
which
has
a
render
function
for
tree
objects
.
What
I
want
is
the
svg
in
string
form
that
I
can
edit
.
Currently
I
save
the
svg
and
read
the
file
as
follows
:
This
works
","
but
is
it
possible
to
use
a
tempfile
instead
?
So
far
I
have
:
Can
anyone
explain
why
this
doesn't
work
and
/
or
how
I
could
do
this
without
creating
a
file
(
which
I
just
have
to
delete
later
)
.
The
svg_string
I
go
on
to
edit
for
use
in
a
django
application
.
EDIT
:
Importantly
","
the
render
function
can
also
be
used
to
create
other
filetypes
e.g
.
.
png
-
so
the
.
svg
extension
needs
to
be
specified
.
You
should
not
define
yourself
the
name
of
your
temporary
file
.
When
you
create
it
","
the
name
will
be
randomly
generated
.
You
can
use
it
directly
.
First
","
there's
a
duplication
in
your
code
(
reading
target
file
twice
)
.
And
you
can
use
shutil.copyfileobj
and
hashlib.update
for
memory-efficient
routine
.
(
Note
:
I
didn't
test
this
code
.
If
you
have
problem
please
notice
me
)
I
am
writing
a
backup
script
for
a
sqlite
database
that
changes
very
intermittently
.
Here's
how
it
is
now
:
Currently
the
database
weighs
just
shy
of
20MB
","
so
it's
not
that
taxing
when
this
runs
and
reads
the
whole
file
into
memory
(
and
do
it
twice
when
changes
are
detected
)
","
but
I
don't
want
to
wait
until
this
becomes
a
problem
.
What
is
the
proper
way
to
do
this
sort
of
(
to
use
Bashscript
terminology
)
stream
piping
?
I
have
data
from
a
csv
file
that
looks
like
this
:
I
want
to
plot
the
exit_status
count
(
i.e
the
amount
of
times
exit_status
=
=
1
or
exit_status
=
=
-
11
)
versus
start_time
in
bins
of
1
hour
.
Since
there
are
several
distinct
exit_status
codes
","
I
need
to
plot
it
in
the
form
of
a
stacked
bar
chart
where
each
distinct
exit
status
is
given
a
different
color
.
Can
anyone
please
help
me
?
I've
been
stuck
on
this
for
2
days
!
!
Thanks
!
Here's
how
I
would
solve
it
:
Read
the
csv-file
.
This
can
be
done
using
the
csv
module
for
python
Read
and
/
or
convert
the
date
stamps
according
to
your
bin
size
","
and
iterate
through
each
line
","
adding
to
the
correct
hour
bin
.
I
just
do
it
the
dirty
way
and
cut
the
minutes
and
seconds
:
row
[0]
[
:
-
5
]
returns
15
/
07
/
2015
11
","
a
date
and
hour
to
work
with
.
You'll
end
up
with
a
list
status_records
which
consists
of
two
dicts
","
representing
the
two
status
options
","
which
then
contain
the
hour
bins
:
""""
1
""""
:
{
""""
15
/
07
/
2015
11
""""
:
3
","
...
}
""""
-
11
""""
:
{
""""
15
/
07
/
2015
11
""""
:
0
","
...
}
Here's
a
sample
data.csv
with
some
more
data
(
so
that
you
can
actually
see
something
","
which
is
difficult
with
just
your
2
entries
-
I'm
using
the
same
date
format
and
the
status
codes
you
mentioned
)
:
And
here's
my
code
(
you'll
have
to
change
row
[0]
etc.
to
the
according
rows
to
work
with
your
csv
)
:
Output
:
Improvements
:
You
may
want
to
add
some
useful
labels
","
and
decide
wether
to
show
time
in
which
nothing
happens
(
which
will
maybe
clutter
the
chart
with
gaps
)
.
Also
","
be
aware
that
as-is
the
dates
should
be
sorted
in
the
csv
","
else
you
have
to
sort
them
yourself
in
the
code
.
Anyway
","
this
should
give
you
something
to
start
with
.
I
have
a
list
i
want
split
after
keyword
""""
boulevard
","
rue
","
chemin
""""
like
in
output
Thanks
for
your
time
It's
not
working
because
you
are
only
extracting
one
word
after
you
split
:
due
to
[3]
.
Try
[
3
:
]
instead
.
Ah
","
but
that
still
won't
be
enough
","
because
you'll
get
a
list
of
lists
","
when
you
want
a
list
of
strings
.
So
you
also
need
to
use
join
.
Now
your
only
difficulty
is
dealing
with
irregular
street
addresses
","
e.g
.
people
who
don't
have
a
house
number
","
or
several
words
in
the
street
name
.
I
have
no
solution
for
that
!
Ok
i
found
out
","
my
mpg
file
were
not
good
","
i
tried
some
built
converter
but
this
is
not
correct
you
need
to
install
ffmpeg
and
then
convert
your
video
file
with
this
command
:
ffmpeg
-
i
infile
-
vcodec
mpeg1video
-
acodec
libmp3lame
-
intra
outfile.mpg
infile
=
nameofyourfile.format
I
try
to
read
a
video
file
with
pygame.movie
module
but
everytime
i
tried
i
got
a
black
screen
with
no
errors
even
the
sound
works
.
I'm
using
a
mpeg
file
.
My
version
of
python
is
3.3.0x64
pygame
3.3x64
.
I
tried
with
python
2.7.1
x32
and
2.7
x32
pygame
but
same
problem
.
Here
is
an
exemple
of
code
that
i
used
:
I
really
need
to
be
able
to
play
videos
","
i
don't
know
how
to
do
to
solve
this
problem
if
someone
knows
the
solution
or
has
python
setup
that
works
with
pygame.movie
module
please
tell
me
I'm
using
the
Windows
version
of
Python
3.4.3
(
can't
use
Cygwin's
Python
because
I
require
pywin32
)
","
Cygwin
2.0.4(0.287/5/3)
","
and
Django
1.7.8
(
can't
use
1.8.x
for
dependency
reasons
)
.
My
problem
is
that
about
half
the
time
I
do
anything
Django
related
in
the
Cygwin
shell
","
I
get
error
-
In
this
case
","
I'm
attempting
to
run
a
simple
unit
test
using
nose
.
Strange
thing
is
that
it
intermittently
occurs
about
50
%
of
the
time
.
I
can
confirm
that
my
PYTHONPATH
is
set
properly
with
this
small
test
.
Which
outputs
.
What
would
cause
such
discrepancy
?
Here
is
what
ended
up
being
the
true
cause
.
I
had
environment
variables
DJANGO_SETTINGS_MODULE
and
django_settings_module
-
same
name
","
different
case
.
DJANGO_SETTINGS_MODULE
was
set
appropriately
to
config.settings.local
","
while
django_settings_module
was
set
to
asd
.
I
had
no
idea
that
1
)
both
of
these
were
defined
and
2
)
the
shell
(
Cygwin
shell
","
anyways
)
randomly
returns
a
value
despite
case
differences
.
Try
this
I've
tried
pretty
much
everything
on
stackoverflow
and
other
forums
to
get
the
/
usr
/
include
/
folder
on
my
mac
(
currently
using
OS
X
10.9.5
)
Re-installed
Xcode
and
command
line
tools
(
actually
","
command
line
tool
wasn't
one
of
the
downloads
available
-
so
I'm
guessing
it's
was
already
downloaded
)
tried
/
Applications
/
Install
Xcode.app
command
line
on
terminal
I
haven't
tested
if
there
is
no
standard
library
on
Xcode
","
but
I'm
only
trying
to
build
cloudera
/
hue
from
github
and
it
won't
install
because
there
is
no
/
usr
/
include
/
python2.7
(
and
couldn't
really
ask
their
forum
because
the
error
isn't
coming
from
cloudera
/
hue
)
.
How
do
I
get
the
/
usr
/
include
folder
?
Output
:
I
have
this
text
file
:
How
can
I
parse
each
line
and
when
I
see
group
","
store
the
group
and
save
it
in
the
list
and
save
it
into
a
dictionary
This
is
what
I
tried
:
the
net
object
is
expecting
a
list
of
images
as
input
","
you
supply
only
one
image
.
Try
Note
the
square
brackets
(
[]
)
around
im_proc
converting
it
into
a
list
containing
a
single
image
.
The
solution
to
this
was
to
switch
out
the
prototext
.
The
C
+
+
code
PyCaffe
wraps
around
causes
this
behavior
.
This
thread
has
the
solution
:
Prediction
in
Caffe
-
Exception
:
Input
blob
arguments
do
not
match
net
inputs
I
have
found
this
line
of
code
to
send
inputs
to
the
network
in
caffe
:
I
tried
adapting
this
code
for
my
work
as
follows
:
where
im_proc
=
"np.zeros((100,9)"
)
(
just
for
testing
)
but
I
get
the
following
error
:
I
am
unable
to
understand
why
I
cant
run
the
forward
method
this
way
.
I
am
able
to
call
net.forward()
regularly
though
.
From
the
comments
I
understand
that
I
am
supposed
to
initialize
the
input
array
with
possibly
the
caffe.io.Transformer
function
.
I
tried
the
following
loop
:
But
this
still
causes
the
same
error
.
Did
u
add
cgi
handler
in
apache2
AddHandler
cgi-script
.
py
https://www.linux.com/community/blogs/129-servers/757148-configuring-apache2-to-run-python-scripts
A
script
I
wrote
long
ago
(
getWords.py
)
used
to
be
executable
at
my
localhost
(
http://local.example.com/getWords.py?query-string
)
My
python
script
starts
like
this
:
I
have
enabled
ExecCGI
cat
/
etc
/
apache2
/
sites-available
/
example
But
still
the
script
force
downloads
(
FireFox
)
or
shows
the
script
code
(
Chrome
)
.
-
rwxr-xr-x
1
username
username
4794
Jul
14
17:15
getWords.py
Any
idea
what
is
wrong
on
my
localhost
?
On
my
host
(
Webfaction
)
","
its
running
at
http://example.com/getWords.py?query-string
From
OP's
comment
I
think
the
minimal
solution
a
set
of
helper
functions
instead
of
classes
.
the
xmltodict
library
makes
it
easy
to
turn
the
XML
data
into
nested
dictionaries
","
more
or
less
like
JSON
.
A
set
of
helpers
that
parse
the
contents
and
generate
appropriate
C-struct
strings
is
all
that's
really
needed
.
If
you
can
work
with
dictionaries
:
You
can
do
something
like
:
Which
will
produce
something
like
:
I'd
try
to
get
it
working
with
basic
functions
first
before
trying
to
build
up
a
class
scaffold
.
It
would
be
pretty
easy
to
hide
the
details
in
a
class
using
property
descriptors
:
I
need
some
advice
.
Two
questions
","
does
something
already
exist
for
this
","
what
modules
should
I
use
to
develop
this
.
I
have
some
structures
that
come
from
an
XML
file
.
I
want
to
represent
them
in
Python
Classes
(
maybe
using
a
factory
to
create
a
class
per
structure
)
.
But
I
want
these
classes
to
have
a
function
that
will
emit
the
structure
as
a
C
Struct
.
From
my
research
ctypes
seems
like
the
recommended
thing
to
use
to
represent
the
structures
in
Python
classes
","
but
I
don't
see
any
methods
for
anything
that
will
emit
C
Stucts
for
the
creation
of
a
header
file
.
A
third
","
unrelated
detail
was
the
problem
:
pyenv
.
There's
a
bug
report
here
","
but
long
story
short
is
that
pyenv
uses
shims
to
intercept
package
imports
and
route
them
correctly
.
This
means
that
pyenv
has
to
muddle
with
the
path
.
When
I
run
python
directly
","
the
shims
are
apparent
in
the
python
path
:
However
","
in
the
dynamically
imported
package
","
the
same
code
results
in
an
output
of
:
So
","
the
issue
appears
to
be
that
pyenv
isn't
doing
its
shim
magic
during
dynamic
imports
.
To
solve
the
problem
","
I
used
pip
install
-
-
user
to
force
pip
to
install
to
the
place
where
the
dynamic
import is
looking
","
rather
than
where
pyenv
wants
the
install
to
go
.
(
also
add
a
-
I
if
you've
previously
installed
the
package
to
force
a
reinstall
)
It
can
also
be
solved
by
appending
the
install
location
(
in
my
case
","
/
usr
/
lib
/
python2.7
/
site-packages
)
to
the
python
path
using
sys.path.append
","
but
that
smells
awful
and
could
cause
problems
down
the
line
for
other
people
.
Virtualenvs
are
notorious
.
They
tinker
around
with
paths
lots
of
times
and
mess
up
quite
a
lot
of
things
.
You
need
to
check
the
PATH
variable
inside
b.py
in
both
scenarios
.
This
will
most
probably
not
be
the
same
.
You'd
need
to
set
the
path
to
include
your
c.py's
directory
.
You
can
check
the
PATH
in
main.py
and
if
it
is
correct
there
","
it
would
mean
that
one
of
your
other
imports
is
changing
your
sys.path
ina
way
to
remove
that
.
So
","
I
have
a
directory
structure
:
In
main.py
","
modules
are
dynamically
loaded
at
runtime
","
depending
on
which
modules
are
specified
.
(
This
allows
for
a
hypothetical
c.py
to
be
added
","
main.py
to
be
rerun
","
and
the
program
to
detect
the
addition
of
c.py
and
run
it
.
)
The
problem
is
that
b.py
imports
a
module
installed
via
pip
(
in
a
virtualenv
)
.
(
I'm
going
to
refer
to
it
as
a
library
to
help
avoid
confusion
.
)
When
b.py
is
run
directly
(
python
b.py
)
the
library
imports
just
fine
.
When
the
shell
is
opened
and
the
library
is
imported
by
hand
","
it
works
.
But
","
when
main.py
is
run
and
b.py
is
dynamically
imported
(
using
pkgutil.iter_modules
to
detect
the
modules
and
then
importlib.import_module
to
import the
required
ones
)
","
the
library
that
b.py
imports
isn't
found
-
an
ImportError
:
No
module
is
thrown
.
To
recap
:
a
module
imports
an
installed
library
","
and
this
works
when
the
module
is
run
directly
or
when
the
library
in
question
is
imported
manually
in
the
python
interpreter
","
but
when
the
module
is
dynamically
imported
","
the
library
isn't
found
.
What
gives
?
I
have
a
model
:
and
I
have
a
form
:
the
min_value
option
could
not
be
specified
in
the
accounts
class
but
I
didn't
want
to
remove
the
ksize
field
completely
because
...
I
needed
it
.
It
seems
to
be
working
","
I
have
a
default
value
in
the
form
field
when
displayed
as
expected
and
it
has
a
minimum
allowed
value
.
But
I
am
mixing
PositiveIntegerField
and
IntegerField
and
I
haven't
seen
any
documentation
about
using
this
method
either
.
Can
someone
please
tell
me
if
this
is
safe
/
sensible
?
Thanks
.
You
don't
want
worry
about
mixin
types
.
Because
PositiveIntegerField
is
like
an
IntegerField
.
It
has
only
positive
values
from
0
to
2147483647
I
am
learning
Artificial
Neural
Network
(
ANN
)
recently
and
have
got
a
code
working
and
running
in
Python
for
the
same
based
on
mini-batch
training
.
I
followed
the
book
of
Michael
Nilson's
Neural
Networks
and
Deep
Learning
where
there
is
step
by
step
explanation
of
each
and
every
algorithm
for
the
beginners
.
There
is
also
a
fully
working
code
for
handwritten
digit
recognition
which
works
fine
for
me
too
.
However
","
I
am
trying
to
tweak
the
code
a
bit
by
means
of
passing
the
whole
mini-batch
together
to
train
by
backpropagation
in
the
matrix
form
.
I
have
also
developed
a
working
code
for
that
","
but
the
code
performs
real
slow
when
run
.
Is
there
any
way
I
can
implement
a
full
matrix
based
approach
to
mini-batch
learning
of
the
network
based
on
back
propagation
algorithm
?
Here
is
my
code
.
The
time
taken
to
iterate
30
epochs
reduces
from
800
+
seconds
to
200
+
seconds
on
my
machine
.
As
I
am
new
to
python
","
I
use
what
is
readily
available
.
This
snippet
only
requires
numpy
to
run
.
Give
it
a
try
.
What
is
the
most
efficient
way
to
find
the
intersection
of
2
lists
when
both
lists
contain
non-hashables
?
Basically
","
let's
say
I
have
the
following
lists
(
which
I
completely
made
up
)
:
We
can
see
that
the
first
element
of
A
is
the
same
as
the
first
element
of
B
.
I
can
do
the
simple
thing
by
creating
a
for
loop
:
but
I'm
just
wondering
if
there
is
a
more
efficient
","
cooler
","
or
simpler
way
.
For
example
","
there
is
:
...
but
obviously
I
can't
use
set
or
frozenset
for
non-hashables
because
I
get
Is
there
anything
like
this
for
non-hashables
?
Hashing
is
what
makes
the
set
efficient
.
If
you
can't
hash
","
then
you
can't
take
advantage
of
that
efficiency
-
you
have
to
compare
every
object
with
every
other
object
and
you
get
an
O(nÂ²)
algorithm
instead
of
an
amortized
O(n)
one
.
However
","
if
you
care
only
about
object
identity
and
not
equality
","
then
you
can
make
dicts
mapping
the
object
ids
to
the
objects
","
and
take
the
intersection
of
the
ids
:
If
you
want
a
more
code-efficient
solution
or
you
do
care
about
object
equality
","
then
@Neil's
answer
should
suffice
.
A
simple
solution
using
list
comprehension
:
intersection
=
[
element
for
element
in
A
if
element
in
B
checks
if
two
elements
of
the
same
value
are
there
.
For
example
","
for
"[1, 2, 3]"
and
"[0, 1, 5]"
","
it
will
return
[1]
.
Well
","
here's
another
way
.
(
I
assume
x
is
your
original
2d
array
.
)
Still
uses
a
Python
list
generator
expression
","
however
.
I
can't
think
of
any
reason
you
would
need
Instead
of
Which
you
can
get
simply
with
a.T
If
you
really
need
a
list
","
then
you
can
use
list(a.T)
Here
is
one
way
:
Here
is
another
one
:
What
would
be
the
best
way
of
converting
a
2D
numpy
array
into
a
list
of
1D
columns
?
For
instance
","
for
an
array
:
I
would
like
to
get
:
This
works
:
but
I
was
wondering
if
there
is
a
better
solution
using
pure
Numpy
functions
?
I
think
the
error
you
report
can
only
happen
if
nums
is
an
empty
list
.
In
that
situation
","
nums
[
len(nums)
-
1
]
is
not
a
valid
index
(
as
there
are
no
valid
indexes
into
an
empty
list
)
.
For
this
problem
","
there's
really
not
much
point
to
special
casing
the
last
two
items
in
the
list
.
You
can
make
your
code
much
simpler
by
handling
all
the
cases
with
one
loop
:
As
the
comment
says
","
the
loop
body
where
I
use
list
indexes
won't
run
if
the
length
of
the
list
is
less
than
2
.
That's
because
the
range
will
be
empty
","
and
iterating
on
an
empty
sequence
does
nothing
.
A
slightly
fancier
approach
would
be
to
use
zip
on
two
iterators
of
num
that
are
offset
by
one
place
.
This
is
more
advanced
Python
stuff
","
so
if
you
don't
understand
it
yet
","
don't
worry
too
much
about
it
:
This
approach
to
iterating
over
pairs
using
zip
is
inspired
by
the
itertools
documentation
","
where
it's
given
in
the
pairwise
recipe
:
I
need
to
return
True
if
the
array
contains
a
2
next
to
a
2
somewhere
.
But
it
gives
me
an
error
that
"says:""list"
index
out
of
range
""""
.
What
should
i
change
?
I'm
pretty
new
to
the
stuff
","
so
probably
my
code
is
one
of
the
longest
ways
to
solve
it
","
but
i
appreciate
any
help
.
Thank
you
!
Easier
to
read
and
understand
.
Wrap
this
in
a
function
and
it
adds
an
item
and
the
count
gets
updated
along
with
the
newly
entered
string
Using
python
3.5.2
If
you
like
my
answer
please
click
the
green
arrow
.
Thanks
.
I
need
some
major
improvements
in
my
programming
/
coding
and
it's
already
been
a
month
of
this
computer
language
field
.
Right
now
I'm
trying
to
create
a
class
with
3
functions
(
lunch
","
breakfast
","
and
dinner
)
and
let's
say
I
want
to
call
the
function
lunch
and
add
'
Strawberry
'
to
the
lunch
list
;
it's
supposed
to
add
1
to
the
list
count
(
list_count
)
for
amount
of
foods
entered
in
the
list
count
so
far
","
and
adds
'
strawberry
'
to
the
dictionary
.
So
what
I'm
trying
to
do
is
I
created
a
blank
dictionary
list
(
lunch_list
)
and
created
a
starting
count
of
food
items
(
lunch_count
)
So
if
I
call
lunch
in
the
Food
class
","
I'm
trying
to
make
the
result
like
this
:
I
was
ready
to
write
this
script
but
after
writing
this
I
confused
myself
a
lot
more
.
I
feel
lost
.
This
is
going
to
be
embarassing
for
me
but
here
is
my
code
:
It
doesn't
seem
like
your
'
Strawberry
'
parameter
matches
the
lunch_count
parameter
in
your
lunch
function
.
In
the
lunch
method
","
you
then
can
increment
it
by
one
every
time
you
call
lunch
","
rather
than
adding
it
as
a
parameter
.
If
you
are
new
to
Python
","
I
would
recommend
Learn
Python
the
Hard
Way
.
Well
this
is
a
bit
silly
","
but
I
tried
a
different
example
and
my
connection
is
working
fine
now
.
Only
difference
I
can
see
is
that
the
second
one
used
HTTPHandler
instead
of
Proxy
","
as
I
have
an
apparently
solution
I'm
not
too
worried
","
but
woudl
still
be
interested
to
know
why
I
had
this
issue
in
the
first
place
.
Your
question
sets
the
proxy
URL
to
http://user:password@http://newyork.wonderproxy.com:11001
which
isn't
valid
.
If
you
changed
http_proxy_server
to
newyork.wonderproxy.com
then
your
first
solution
might
work
better
.
I've
looked
through
some
of
the
other
posts
on
this
and
I
hope
I'm
not
duplicating
","
but
I'm
stuck
on
a
real
headscratcher
with
setting
a
proxy
server
for
urllib2
.
I'm
running
the
below
:
I'm
running
this
against
an
IP
info
site
","
but
try
as
I
might
the
response
always
comes
out
with
my
non-proxy
IP
address
.
When
I
manually
set
my
proxy
through
system
settings
I
do
get
a
different
response
","
so
I've
confirmed
it's
not
an
issue
with
the
proxy
criteria
itself
.
Any
help
that
could
be
offered
would
be
much
appreciated
!
I'm
having
trouble
styling
my
django
form
with
a
ModelMultipleChoiceField
.
Heres
my
form
:
And
I
used
this
for
html
:
This
requires
me
to
put
my
HTML
in
my
forms
file
","
which
is
not
a
very
mvc
way
.
Also
it
restricts
me
heavily
in
how
i
can
style
the
list
(
i.e
making
table
of
the
model
instance
fields
)
Is
there
anyway
to
make
this
smarter
?
I'd
like
to
access
{
{
skript.name
}
}
","
{
{
skript.checkbox
}
}
or
something
in
my
{
%
for
skript
in
result_form.skripten
%
}
loop
","
but
thats
sadly
not
possible
...
You
can
use
render_to_string
.
It
loads
a
template
and
renders
it
with
a
context
.
Returns
a
string
.
For
various
variants
you
can
make
various
StyledModelMultipleChoiceField
subclasses
OR
pass
the
desired
template_name
when
initialising
the
class
.
Eg
:
Use
self.template_name
where
appropriate
:
If
you
want
to
display
multiple
instances
","
use
a
formset
.
Using
Pandas
data
frame
group
by
feature
and
I
want
to
group
by
column
c_b
and
calculate
unique
count
for
column
c_a
and
column
c_c
.
My
expected
results
are
","
Expected
results
","
Met
with
strange
error
about
unhashable
type
","
does
anyone
have
any
ideas
?
Thanks
.
Input
file
","
Source
code
","
Error
message
","
For
the
number
of
unique
elements
in
each
group
","
you
can
use
:
You
are
not
returning
those
two
lines
.
You
are
returning
the
values
that
those
two
lines
evaluate
to
.
In
your
cases
","
those
are
boolean
expressions
that
will
evaluate
to
either
true
or
false
.
That
final
boolean
value
is
what
your
function
returns
.
Python
evaluates
whatever
expression
follows
the
return
statement
","
and
the
result
of
the
evaluation
is
what
is
returned
.
You
have
used
two
boolean
expressions
:
This
evaluates
to
True
if
both
a
<
0
and
b
<
0
;
otherwise
it
evaluates
to
False
This
evaluates
to
True
if
either
of
the
parentheses
evaluate
to
True
;
otherwise
it
evaluates
to
False
which
is
what
you
want
.
So
pos_neg
automatically
returns
either
True
or
False
depending
on
the
specified
inputs
.
Perhaps
you
were
thinking
of
testing
each
condition
for
True
/
False
","
then
using
an
if
statement
to
return
True
if
the
condition
is
True
and
to
return
False
if
the
condition
is
False
.
That
would
work
","
but
would
be
unnecessarily
long
and
complex
.
Here
","
you
are
just
returning
with
whatever
the
boolean
value
of
each
expression
works
out
to
be
","
whether
it's
True
or
False
.
Given
2
int
values
","
return
True
if
one
is
negative
and
one
is
positive
.
Except
if
the
parameter
""""
negative
""""
is
True
","
then
return
True
only
if
both
are
negative
.
I
finally
figured
it
out
.
But
...
why
do
I
return
those
two
lines
instead
of
True
?
How
do
I
know
to
do
this
in
a
similar
situation
?
Got
there
in
the
end
:
)
I
used
the
following
:
I
can
then
use
test2
as
the
variable
.
I
have
the
following
form
which
is
working
perfectly
:
However
","
as
you
can
see
the
campaignno
is
hard
written
into
the
query
.
Ideally
I
would
like
to
select
the
maximum
campaignno
from
a
mysql
table
and
use
that
as
the
campaigno
instead
of
the
102501349
as
above
.
The
mysql
table
and
model
is
as
follows
:
mySQL
table
:
Django
Model
:
I
hope
this
makes
sense
but
if
you
require
any
more
information
then
just
ask
:
)
I
appreciate
any
assistance
in
advance
","
many
thanks
","
Alan
.
Hi
I
want
to
generalize
the
strategy
from
1
to
10
possible
investments
using
an
array
(
""""
instruments
""""
)
to
simplify
the
tasks
of
loading
the
10
feeds
","
creating
the
10
SMAs
and
then
","
each
day
","
checking
if
a
signal
crossing
happened
in
one
(
or
more
)
of
the
instruments
.
I
am
stuck
in
this
.
Also
plotter
is
plotting
graphs
separately
but
i
want
plot
all
intruments
result
in
one
graph
.
This
is
my
code
:
I
have
just
started
dealing
with
pyalgotrade
","
but
I
think
you're
doing
a
rather
simple
mistake
(
as
indicated
by
gzc
)
:
an
instance
of
class
Bars
is
a
collection
of
bars
from
different
instruments
that
all
have
the
same
timestamp
.
So
","
when
your
onBars
event
is
called
","
you
actually
have
to
loop
through
all
the
instruments
in
the
dictionary
.
The
question
you
are
asking
-
-
about
what
happens
when
-
-
relates
to
events
that
occur
upstream
of
Flask...like
when
gunicorn
starts
up
.
Flask
is
a
Web
Server
Gateway
Interface
(
WSGI
)
application
framework
.
WSGI
is
a
Python-specific
framework
that
defines
how
a
web
server
will
interface
with
an
application
.
(
The
Java
equivalent
is
a
Java
servlet
.
)
The
server
will
use
Python's
WSGI
protocol
to
call
Flask
when
appropriate
.
The
Flask
documentation
on
deploying
a
standalone
WSGI
application
shows
how
to
set
this
up
manually
","
and
gives
you
a
little
insight
into
what's
going
on
upstream
of
Flask
.
The
section
on
Gevent
is
useful
because
it
explicitly
shows
the
import statement
for
yourapplication
and
its
relationship
to
the
HTTP
server
:
The
source
code
for
gunicorn
shows
a
similar
approach
","
loading
the
WSGI
application
once
per
worker
","
and
then
running
indefinitely
.
...
Flask's
application
context
But
you
may
be
searching
for
a
way
to
create
some
new
variable
or
instantiate
a
new
database
connection
or
establish
a
unique
session
cookie
-
-
things
that
should
occur
every
time
a
request
is
processed
","
and
not
just
once
when
the
application
code
is
loaded
.
For
this
","
you
should
look
into
Flask's
application
context
.
From
the
cited
page
:
The
application
context
is
created
and
destroyed
as
necessary
.
It
never
moves
between
threads
and
it
will
not
be
shared
between
requests
.
As
such
it
is
the
perfect
place
to
store
database
connection
information
and
other
things
.
I
would
like
some
information
about
how
the
code
gets
executed
when
a
Flask
application
is
deployed
(
say
","
with
gunicorn
)
.
For
example
","
suppose
I
have
the
following
in
views.py
:
When
does
foo
get
sorted
?
Only
once
when
the
server
starts
?
Every
time
there's
a
request
?
Something
else
?
The
presence
of
before_first_request
and
before_request
in
the
documentation
seem
to
offer
clues
","
but
I
can't
understand
the
problems
they're
meant
to
solve
until
I
understand
how
exactly
the
code
is
run
on
a
server
.
Can
you
explain
step-by-step
(
e.g
.
First
A
happens
","
and
then
B
","
then
C
)
or
point
to
an
on-line
resource
","
please
?
The
error
you
are
getting
is
because
you
are
trying
to
index
a
list
using
a
tensor
.
One
way
to
solve
this
will
be
to
turn
the
list
into
a
tensor
.
Alternatively
you
can
use
pmc.switch()
to
choose
between
priors
and
pseudopriors
","
something
like
:
I
did
not
check
your
code
thoroughly
","
but
notice
you
have
Instead
","
you
should
write
or
may
be
:
Since
0
evaluates
as
False
and
1
evaluates
as
True
.
Also
you
have
And
you
should
have
Your
question
made
me
realize
I
forgot
to
port
a
pseudoprior
example
.
I
will
do
it
ASAP
.
EDITED
Here
if
the
full
model
Notice
that
your
code
had
several
issues
","
like
missing
parenthesis
in
the
definition
of
the
variable
b
and
the
order
of
the
prior
and
pseudopriors
was
inverted
.
Additionally
I
change
the
code
in
ordet
to
let
aBeta
","
bBeta
and
thetahave
shape=nCond
","
and
then
in
the
likellihood
define
p
as
p=theta
[cond_idx]
.
I
did
not
check
the
results
against
Kruschke's
book
","
but
the
trace
look
reasonable
.
I
am
new
to
PyMC3
and
am
trying
to
implement
the
hierarchical
model
from
Kruschke
(
2015
)
section
12.2.2
(
model
comparison
)
.
I
succeeded
in
defining
the
full
model
and
then
looking
at
the
differences
of
posterior
parameter
values
(
determine
whether
difference
can
credibly
be
said
to
be
zero
)
.
I
also
tried
to
explicitly
do
the
comparison
in
the
model
as
shown
in
the
book
(
defining
a
full
model
and
a
restricted
model
and
sampling
these
using
a
categorical
distribution
)
.
Basically
I
try
to
implement
the
below
JAGS
model
definition
in
PyMC3
.
http://nbviewer.jupyter.org/github/JWarmenhoven/DBDA-python/blob/master/Notebooks/Chapter%2012.ipynb
But
I
don't
know
how
I
can
use
the
model
index
to
select
the
(
pseudo
)
priors
.
Any
pointers
?
JAGS
:
PyMC3
:
Output
:
UPDATED
After
correcting
the
pseudopriors
(
missing
parenthesis
)
the
results
look
much
better
.
However
","
I
am
not
sure
whether
the
pmc.Beta()
function
works
well
with
arrays
as
arguments
for
a
and
b
.
http://nbviewer.jupyter.org/github/JWarmenhoven/DBDA-python/blob/master/Notebooks/Chapter%2012.ipynb
This
web.session
module
gives
you
the
session
support
","
initializer
-
You
can
set
the
initial
session
which
is
not
mandatory
.
eg
:
initializer={'room
'
:
None
}
sets
the
initial
session
of
room
to
None
.
DiskStore
makes
the
session
to
save
in
disk
whereas
DBStore
stores
session
in
databases
.
app
gives
the
application
instance
created
as
"web.application(urls, globals()"
)
with
URLs
which
mapped
to
the
relevant
classes
.
Refer
this
.
I'm
currently
learning
Python
and
can't
work
out
what
the
web.session.Session
is
in
the
follow
game
program
:
What
are
app
","
Diskstore
","
and
initializer
?
Django
is
not
able
to
reverse
the
use
of
a
|
character
outside
of
a
capturing
group
.
However
","
I
highly
doubt
you
need
it
here
.
Django
always
matches
the
first
slash
of
the
url
","
so
there's
no
need
to
match
the
starting
slash
in
your
regex
.
Adding
a
starting
slash
would
only
match
a
second
slash
at
the
start
of
your
url
.
Unless
you
want
the
url
path
to
be
example.com
/
/
vieworder
/
1
/
rather
than
example.com
/
vieworder
/
1
/
","
you
should
just
remove
the
slash
from
your
pattern
.
Since
the
first
slash
is
already
matched
by
Django
","
and
there's
nothing
else
between
the
first
slash
and
the
vieworder
/
1
/
part
","
you
can
just
leave
the
include
pattern
empty
:
This
will
match
the
url
example.com
/
vieworder
/
1
/
","
and
allow
Django
to
reverse
the
url
.
As
for
your
second
problem
:
You
need
to
make
the
outer
group
a
non-capturing
group
with
?
:
:
Django
will
substitute
the
outermost
capturing
group
","
which
in
this
case
should
contain
/
1
instead
of
1
.
By
making
it
a
non-capturing
group
","
Django
will
substitute
the
argument
1
into
the
inner
group
instead
of
the
outer
group
.
Okay
","
to
be
clear
I've
searched
and
read
","
followed
the
official
docs
","
tried
multiple
solutions
from
SOF
","
nothing
seems
to
be
working
so
I
have
to
resort
to
shamefully
asking
for
help
.
I'm
simply
trying
to
generate
urls
the
proper
way
.
root
urls.py
:
urls.py
:
template
file
:
also
tried
:
Error
:
Reverse
for
'
vieworder
'
with
arguments
'
(
1
","
)
'
and
keyword
arguments
'
{
}
'
not
found
.
1
pattern(s)
tried
:
[
'
|
/
vieworder
/
(
?
P<order_id>d
+
)
/
$
'
]
I
don't
understand
why
this
is
not
working
.
I
tested
the
regex
against
vieworder
/
1
/
in
a
regex
tester
and
it
works
fine
.
Django
even
tells
me
in
the
error
that
it
tried
the
correct
url
pattern
","
however
the
error
really
isn't
very
clear
on
what
is
actually
wrong
.
I
think
spider
arguments
would
be
the
solution
in
your
case
.
When
invoking
scrapy
like
scrapy
crawl
some_spider
","
you
could
add
arguments
like
scrapy
crawl
some_spider
-
a
foo=bar
","
and
the
spider
would
receive
the
values
via
its
constructor
","
e.g
.
:
What's
more
","
as
scrapy.Spider
actually
sets
all
additional
arguments
as
instance
attributes
","
you
don't
even
need
to
explicitly
override
the
__init__
method
but
just
access
the
.
foo
attribute
.
:
)
I
am
running
a
CrawlSpider
and
I
want
to
implement
some
logic
to
stop
following
some
of
the
links
in
mid-run
","
by
passing
a
function
to
process_request
.
This
function
uses
the
spider's
class
variables
in
order
to
keep
track
of
the
current
state
","
and
depending
on
it
(
and
on
the
referrer
URL
)
","
links
get
dropped
or
continue
to
be
processed
:
I
think
that
if
I
were
to
run
several
spiders
on
the
same
machine
","
they
would
all
use
the
same
class
variables
which
is
not
my
intention
.
Is
there
a
way
to
add
instance
variables
to
CrawlSpiders
?
Is
only
a
single
instance
of
the
spider
created
when
I
run
Scrapy
?
I
could
probably
work
around
it
with
a
dictionary
with
values
per
process
ID
","
but
that
will
be
ugly
...
I
have
some
data
in
a
PostgreSQL
table
.
I
am
pulling
the
data
back
to
a
notebook
via
code
like
the
following
:
In
the
PostgreSQL
table
all
columns
were
typed
accurately
but
result_df
is
as
follows
:
Converting
the
date
column
was
fine
:
As
was
ensuring
all
None
values
are
now
NaN
values
:
But
to
convert
the
columns
score
&
cost
to
numeric
I
need
to
execute
the
following
3
lines
of
code
:
If
I
use
only
lines
1
and
2
then
the
typing
is
still
object
-
if
I
use
only
lines
1
and
3
then
all
the
data
is
converted
to
NaN
as
if
all
the
data
has
not
coerced
.
Why
do
I
have
to
use
this
code
and
is
there
a
more
elegant
solution
?
you
can
use
the
following
solution
to
parse
to
numeric
:
let
me
know
if
this
works
After
sometime
I
found
out
the
problem
was
wrong
indentation
in
a
one
hot
vector
encoding
code
.
Also
I
reduced
the
size
of
my
dataset
Size
to
make
it
comply
faster
.
below
is
the
corrected
code
I
get
this
error
when
I
run
my
code
.
Exception
:
Input
arrays
should
have
the
same
number
of
samples
as
target
arrays
.
Found
12196
input
samples
and
1
target
samples
.
Below
is
the
model
I
train
.
I
also
encoded
a
one
hot
vector
of
my
train
data
.
below
is
the
code
and
this
is
my
output
for
the
one
hot
vector
and
finally
I
am
new
to
this
","
plese
pardon
me
.
Thank
you
Yes
","
it's
possible
:
All
you
need
to
do
is
using
the
most
proper
geometry
manager
(
pack
in
this
case
)
for
the
label
widget
that
is
intended
to
show
a
status
(
ststlabel
)
;
then
adding
some
options
to
the
geometry
manager
:
fill
makes
it
extended
as
you
stretch
the
window
horizontally
;
and
side
puts
it
at
the
very
bottom
of
the
screen
which
means
it
will
always
stay
there
(
until
you
make
the
geometry
of
any
of
preceding
widgets
to
bottom
.
Otherwise
","
it'll
be
replaced
with
that
)
.
Now
we
got
it
!
But
there
is
a
problem
that
it
doesn't
show
the
beginning
of
the
status
text
.
To
solve
this
add
an
anchor
option
to
the
status
label
and
change
its
value
to
W
since
we
want
to
see
the
beginning
(
the
most
left
side
)
of
the
text
.
And
W
which
stands
for
""""
West
""""
","
does
it
.
I
was
working
on
a
Tkinter
GUI
with
a
status
bar
at
the
bottom
to
display
instructions
or
the
file
paths
on
hover
over
a
specific
widget
","
especially
if
the
file
path
was
too
long
to
write
in
the
given
section
.
Is
there
any
way
that
the
status
bar
could
extends
past
the
window
in
the
case
of
a
long
name
?
(
Like
maybe
a
window
without
the
top
bar
?
)
The
status
bar
is
cut
off
at
the
end
:
What
I
want
the
status
bar
to
look
like
:
I've
seen
other
applications
where
the
status
bar
can
extend
past
the
window
but
I
was
wondering
if
that
was
possible
in
Tkinter
.
Any
help
would
be
appreciated
!
I
get
a
weird
error
when
I
try
to
query
an
item
by
id
.
I
have
tried
all
suggestion
I
have
found
and
only
when
doing
raw
query
I
get
a
proper
result
.
Part
of
the
Traceback
:
The
query
is
result
=
User.query.filter_by(id=idd)
.
first()
with
idd
of
type
int
.
The
type
of
the
ID
field
in
MySQL
db
is
INT
and
the
model
is
like
this
The
database
is
initialized
as
such
:
What
could
be
wrong
?
Any
suggestions
?
Change
your
id
type
as
such
:
Turns
out
it
was
the
fact
that
in
the
MySQL
schema
for
some
reason
deleted
was
defined
as
a
BIT
type
.
All
type
combinations
in
SQLAlchemy
failed
.
I
had
to
change
the
schema
to
make
it
into
a
TINYINT(1)
First
of
all
","
the
#
character
in
your
file
will
make
numpy
think
everything
after
""""
ADIDGoogle
""""
in
each
line
is
a
comment
.
It
appears
you
can
change
the
comment
character
using
the
comments
kwarg
in
np.loadtxt
.
This
will
solve
the
IndexError
","
leaving
the
delimiter
problem
.
I
want
to
read
the
file
of
below
format
into
numpy
array
in
python
.
it
is
having
three
columns
separated
by
'
\
t
'
.
i
want
to
read
this
into
numpy
array
with
two
columns
where
the
date
and
time
in
to
one
column
and
id
in
another
column
.
I
tried
using
but
it
is
giving
me
error
as
:
You
can
read
via
genfromtxt
line-by-line
https://www.python.org/downloads/release/python-352/
Download
latest
version
of
python
","
then
run
Pycharm
and
a
new
selection
will
populate
.
When
I
opened
Pycharm
:
Community
Edition
","
I
pressed
'
Create
new
project
'
and
on
the
bottom
it
said
'
No
python
interpreter
selected
'
.
What
do
I
select
for
the
python
interpreter
?
Plus
","
I
could
not
find
where
python
was
installed
.
I
created
a
GtkComboBox
with
dozens
of
items
.
When
I
perform
I
see
that
the
pop-up
menu
containing
the
items
is
very
vertically
large
.
How
do
I
set
a
maximum
size
?
I
checked
the
documentation
and
I
not
found
a
method
to
define
it
.
Example
:
As
far
as
I
know
(
and
can
find
)
","
it
is
impossible
to
set
the
number
of
rows
in
a
GtkComboBox
drop
down
menu
.
If
you
insist
on
using
Gtk.ComboBox()
","
you
can
however
reduce
the
height
in
case
of
large
amounts
of
entries
by
using
:
Which
would
show
:
I
want
to
create
a
proxy
class
that
wraps
an
int
for
thread-safe
access
.
In
contrast
to
the
built-in
type
","
the
proxy
class
is
mutable
","
so
that
it
can
be
incremented
in-place
.
Now
","
I
want
to
use
that
class
just
as
a
normal
integer
from
the
outside
.
Usually
","
Python's
__getattr__
makes
it
very
easy
to
forward
attribute
access
to
the
inner
object
:
However
","
__getattr__
does
not
get
triggered
for
magic
methods
like
__add__
","
__rtruediv__
","
etc
that
I
need
for
the
proxy
to
behave
like
an
integer
.
Is
there
a
way
to
generate
those
methods
automatically
","
or
otherwise
forward
them
to
the
wrapped
integer
object
?
The
blog
post
linked
by
@VPfB
in
the
comments
has
a
more
generic
and
thorough
solution
to
proxying
dunder
methods
for
builtin
types
","
but
here's
a
simplified
and
rather
brutish
example
for
the
same
.
I
hope
it
helps
in
understanding
how
to
create
such
forwarding
methods
.
After
your
Dialog
is
closed
","
and
the
popup
variable
referencing
it
goes
out
of
scope
","
Python
will
garbage-collect
it
.
This
causes
the
entire
underlying
C
+
+
object
","
including
all
of
its
sub-widgets
and
layouts
","
to
be
deleted
.
However
","
you're
keeping
a
reference
to
a
layout
used
by
the
dialog
","
and
hence
the
layout
will
have
been
deleted
by
the
second
time
you
try
to
open
the
dialog
.
I
find
it
odd
that
you're
doing
all
of
the
initialization
of
your
Dialog
class
outside
of
the
Dialog
class
.
Instead
","
I
would
recommend
moving
the
creation
of
popLayout
","
and
all
of
the
creation
and
setup
of
table
","
inside
your
Dialog
class
.
This
way
the
layout
gets
created
each
time
the
dialog
is
opened
.
You'll
need
to
add
gameResults
as
a
parameter
to
the
__init__
method
of
Dialog
","
and
you
can
also
remove
the
layout
parameter
you
have
there
at
the
moment
because
it
isn't
used
.
After
doing
this
","
your
Dialog
class
should
look
like
the
following
:
and
your
doStuff()
method
should
look
like
the
following
:
I
made
these
changes
to
your
code
and
I
was
able
to
open
the
dialog
multiple
times
.
I'll
leave
it
up
to
you
to
move
your
main
window
set-up
code
inside
your
Window
class
in
the
same
way
.
Finally
","
please
note
that
I
have
only
tested
this
using
PyQt
.
However
","
I
would
expect
that
my
changes
would
also
work
for
PySide
.
The
aim
of
this
program
is
to
show
the
tradeWindow
as
a
QWidget
and
then
show
a
QDialog
each
time
doStuff
is
called
(
via
button
)
if
there
are
results
.
The
code
works
first
time
","
but
second
time
i
get
error
messages
:
It
seems
my
layout
gets
deleted
when
i
close
QDialog
the
first
time
.
Moving
popLayout
=
QHBoxLayout()
to
start
of
doStuff
which
I
thought
would
fix
the
problem
gives
me
this
error
instead
:
That
doesn't
make
much
sense
to
me
at
all
since
it
should
always
be
getting
defined
before
being
referenced
?
I
can't
find
the
problem
anyway
.
I'm
sure
a
lot
of
my
code
could
be
improved
as
well
as
I
am
very
new
to
classes
etc.
If
you
have
any
tips
on
how
to
open
the
QDialog
each
time
that
is
better
than
what
I'm
currently
trying
or
other
helpful
tips
","
please
don't
hesitate
to
mention
that
as
well
.
(
Try
to
ignore
the
crappy
naming
convention
","
I
will
fix
that
in
the
future
.
)
Thank
you
for
any
help
!
I
have
the
same
problem
.
If
you
call
os.path.abspath()
on
your
relative
path
you'll
see
that
the
absolute
path
is
wrong
.
The
only
workaround
I
found
was
to
change
the
relative
path
to
absolute
to
the
test
file
path
by
using
__file__
and
then
moving
up
one
level
to
exclude
the
file
name
:
I
am
trying
to
write
a
unit
test
for
a
piece
of
code
that
includes
using
pandas
to
read
a
CSV
file
from
a
relative
path
.
The
directory
structure
is
:
In
main.py
","
I
have
:
In
test_main.py
","
I
have
Things
work
fine
if
I
run
main.py
","
but
when
I
ask
Anaconda
to
""""
run
project
tests
""""
","
I
get
an
IOError
complaining
that
'
dat
/
file.csv
'
does
not
exist
.
It's
related
to
the
fact
that
it's
a
relative
path
","
since
when
I
change
it
to
/
home
/
user
/
...
/
thing1
/
dat
/
file.csv
","
it
works
.
Is
there
a
way
that
I
can
make
the
unit
test
work
while
keeping
the
relative
path
?
I
wanted
to
run
a
python
script
with
sbatch
","
however
","
it
seems
that
the
only
way
to
run
a
python
script
with
sbatch
is
to
have
a
bash
script
that
then
run
the
python
script
.
As
in
having
batch_main.sh
:
then
running
:
The
issue
with
this
is
that
I'd
wish
to
have
a
separate
config
file
for
the
arguments
(
since
its
usually
not
a
single
number
or
argument
)
and
also
be
able
to
use
the
array
option
.
Also
","
I
usually
run
multiple
different
sbatch
jobs
simultaneously
(
with
different
configurations
)
","
thus
it
would
be
nice
if
changing
the
configuration
file
does
not
make
the
different
sbatch
runs
get
in
the
way
with
one
another
(
since
if
the
job
gets
queued
and
then
the
config
file
changes
later
","
it
will
run
the
most
recent
config
file
rather
than
the
copy
of
the
config
when
I
ran
sbatch
)
.
To
get
around
this
issue
I
discovered
that
when
I
run
a
sbatch
script
","
SLURM
actually
copies
the
submission
script
to
its
internal
database
(
I
disovered
it
after
asking
:
Changing
the
bash
script
sent
to
sbatch
in
slurm
during
run
a
bad
idea
?
)
.
Therefore
","
I
actually
decided
to
hard
code
the
configuration
into
the
bash
submission
script
(
making
the
submission
script
the
config
file
essentially
)
.
That
way
I
just
edit
the
submission
script
and
then
run
the
file
.
However
","
I'd
like
to
stop
this
since
this
makes
me
write
in
bash
which
I
want
to
avoid
at
all
cost
.
Ideally
","
I'd
like
to
run
a
sbatch
that
directly
runs
python
.
Since
this
might
not
be
possible
","
I
wanted
to
know
what
other
options
did
exist
to
solve
this
issue
.
For
example
","
is
it
possible
to
have
slurm
copy
a
different
file
(
like
a
python
config
file
)
to
its
internal
database
so
that
when
it
queues
the
job
it
runs
the
job
that
I
exactly
want
to
run
?
(
notice
that
running
a
sbatch
job
and
then
changing
the
config
file
is
not
the
way
to
do
this
","
since
this
might
cause
issue
when
one
changes
the
config
file
","
slrum
will
read
the
most
recent
copy
of
the
config
rather
than
the
copy
of
the
config
when
the
job
was
ran
)
.
Or
what
other
options
do
I
have
?
Am
I
really
stuck
with
writing
bash
or
can
I
do
something
else
to
deal
with
configurations
in
python
rather
than
some
other
weird
hack
?
In
general
I
also
wanted
to
know
what
people
did
for
this
in
the
real
world
or
what
a
good
practice
/
standard
was
for
this
.
Python
scripts
are
valid
submission
files
provided
that
they
start
with
the
python
shebang
(
typically
#
!
/
usr
/
bin
/
env
python
)
.
For
instance
:
Note
that
if
your
script
imports
custom
modules
","
you
will
need
to
set
the
PYTHONPATH
even
if
they
lie
in
the
current
directory
.
After
reading
the
post
that
you
linked
","
your
answer
is
there
.
I
quote
:
The
easiest
way
to
do
this
is
to
write
a
main
function
per
module
and
run
all
the
main
functions
serially
.
If
an
experienced
Python
developer
tells
you
that
the
easiest
way
to
do
it
is
the
way
that
you
are
currently
doing
it
(
with
the
example
you've
shown
)
then
it's
pretty
safe
to
say
that
he's
right
and
thats
how
to
do
it
.
Stick
with
what
you're
doing
.
It's
cumbersome
and
repetitive
","
but
also
simplistic
and
it
actually
works
.
If
you
are
unconvinced
","
i'm
sure
there
is
a
module
somewhere
on
PyPI
that
can
help
you
out
","
all
though
the
complexity
may
be
unneccessary
.
I've
written
a
bunch
of
.
py
files
","
in
the
standard
package
structure
","
with
scripts
and
__init__.py
in
each
subdirectory
.
Many
of
these
.
py
modules
import from
each
other
using
'
absolute
imports
'
","
such
as
Each
one
has
a
run
function
and
has
I
want
to
run
a
single
command-line
command
that
will
a
series
of
these
modules
.
I'm
currently
using
a
shell
script
","
of
the
form
Currently
","
if
there
is
an
error
in
run_second
","
it'll
just
skip
to
run_third
","
but
I'd
like
it
to
stop
and
alert
me
to
the
exception
.
One
approach
","
mentioned
in
this
answer
","
is
but
this
seems
a
bit
cumbersome
and
repetitive
.
Is
there
a
nice
way
to
do
this
in
Python
?
Using
Pandas
data
frame
group
by
feature
and
I
want
to
group
by
column
c_b
and
(
1
)
calculate
unique
count
for
column
c_a
and
column
c_c
","
(
2
)
and
get
the
max
value
of
column
c_d
.
Wondering
if
there
is
any
solution
to
write
one
line
of
group
by
code
to
achieve
both
goals
?
I
tried
the
following
line
of
code
","
but
it
seems
not
correct
.
My
expected
results
are
","
Expected
results
","
Thanks
.
Input
file
","
Source
code
","
You
can
pass
a
dict
to
agg()
:
If
you
don't
want
c_b
as
index
","
you
can
pass
as_index=False
to
groupby
:
I'm
successfully
able
to
create
import hooks
to
load
files
directly
from
memory
in
python2.7
.
The
example
I
used
was
the
accepted
response
to
this
question
:
python:Import
module
from
memory
However
;
when
applying
this
code
on
pypy
;
i
get
an
import error
.
I
have
also
tried
other
import hook
examples
that
work
with
regular
python
but
not
with
pypy
","
such
as
this
:
python
load
zip
with
modules
from
memory
Does
anyone
know
why
import hooks
do
not
work
in
pypy
?
Is
there
something
I
am
missing
?
The
problem
is
that
in
both
of
the
examples
you
point
to
","
load_module()
does
not
add
the
loaded
module
to
sys.modules
.
Normally
","
it
should
do
so
(
and
then
PyPy
works
like
CPython
)
.
If
load_module()
does
not
add
the
module
to
sys.modules
","
then
every
single
import a
will
call
load_module()
again
and
return
a
new
copy
of
the
module
.
For
example
","
in
the
example
from
python:Import
module
from
memory
:
This
is
documented
in
https://www.python.org/dev/peps/pep-0302/#id27.
The
load_module()
method
is
responsible
for
doing
more
checks
than
these
simple
examples
show
.
In
particular
","
note
this
line
(
emphasis
in
the
original
)
:
Note
that
the
module
object
must
be
in
sys.modules
before
the
loader
executes
the
module
code
.
So
","
the
fact
that
PyPy
behaves
differently
than
CPython
in
this
case
could
be
understood
as
a
behavior
difference
that
follows
from
code
that
fails
to
respect
the
docs
.
But
anyway
","
my
opinion
is
that
it
should
be
fixed
.
I've
created
an
issue
at
https://bitbucket.org/pypy/pypy/issues/2382/sysmeta_path-not-working-like-cpythons.
You
can
use
a
list
comprehension
or
generator
expression
.
All
the
answers
I
found
to
this
seem
to
make
you
have
to
specify
which
key
you
want
to
find
the
max
/
min
for
e.g
.
Finding
minimum
values
in
a
list
of
dicts
.
I
have
a
list
of
dicts
like
:
So
here
it
would
pull
out
12
and
8991
.
And
I
want
to
find
the
max
and
min
of
all
the
values
in
this
list
of
dicts
.
How
do
I
do
this
without
having
to
specify
each
key
separately
(
e.g
.
1
","
2
","
3
)
-
in
the
real
version
I
have
20
+
.
You
can
flatten
the
values
","
then
find
the
min
and
max
:
The
rule
is
quite
simple
:
to
place
a
widget
in
another
widget
","
you
need
a
reference
to
the
other
widget
.
In
your
case
","
the
simple
thing
to
do
is
create
a
dictionary
to
hold
references
to
your
frames
:
By
the
way
","
you
can
make
your
loop
more
readable
(
and
more
""""
pythonic
""""
)
by
directly
iterating
over
the
list
of
tab
names
rather
than
iterating
over
the
index
values
:
I
found
a
way
makeing
it
easy
to
automatically
generate
multiple
Tabs
with
the
ttk
notebook
.
As
a
""""
solution
""""
to
:
http://stackoverflow.com/questions/39175898/automatic-multiple-tab-generation-with-tkinter-notebook?noredirect=1#comment65695477_39175898
But
now
I
have
a
problem
to
fill
the
Tabs
with
individual
content
namely
widgets
.
What
do
I
need
for
""""
?
?
?
""""
in
the
commented
out
part
?
I
would
appreciate
it
very
much
if
you
show
me
how
to
solve
this
.
Fyi
","
instead
of
executing
self.logger.info(msg)
directly
","
i
just
wrapped
it
around
the
following
code
which
opens
filehandler
and
closes
it
each
time
im
writing
to
a
log
.
.
rewrite
self.logger.info(msg)
to
self.write_to_log(msg)
where
:
I
have
a
class
called
Job
which
has
a
logger
and
jobs
go
off
do
something
and
logs
via
job.logger.info()
but
when
i
have
multiple
jobs
i.e.
thousands
","
it's
throwing
error
I
thought
every
time
I
logged
something
","
it
would
open
then
close
the
file
as
I
have
overwritten
emit()
Is
there
a
pattern
/
ways
to
have
thousands
of
loggers
?
Guess
your
operating
system
is
running
out
of
file
handles
.
Cygwin
setup
can
install
~
4000
packages.GDB
is
one
of
them
.
Why
do
you
think
GDB
should
be
installed
by
default
?
Please
read
:
https://cygwin.com/cygwin-ug-net/setup-net.html#setup-packages
(
I
wrote
this
before
I
saw
matzeri's
response
.
I
still
may
try
extending
python
to
run
code
from
the
C
library
","
but
getting
the
C
code
I've
already
written
to
work
would
be
vastly
preferable
.
)
I
have
no
idea
what's
going
on
with
Cygwin's
gdb
","
but
after
getting
over
my
frustration
I
answered
my
alternative
question
.
Turns
out
you
can
indeed
call
a
C
function
from
python
.
Extending
Python
with
C
or
C
+
+
I'll
try
it
tomorrow
and
I
see
caveats
","
but
if
python.org
has
a
section
on
it
","
I'm
betting
it
can
be
done
.
I'm
trying
to
use
NetBeans
8.1
with
Cygwin
to
write
","
compile
and
debug
a
C
program
.
I
knew
nothing
about
C
when
I
started
this
","
and
somehow
found
my
way
to
fixing
all
the
compiler
errors
.
But
when
it
came
time
to
debug
there
was
no
debugger
!
Long
story
short
","
there's
no
gdb.exe
in
the
Cygwin
/
bin
directory
and
even
a
fresh
install
of
Cygwin
didn't
produce
one
.
I
tried
another
gcc
compiler
that
did
have
gdb
","
but
Netbeans
won't
use
it
.
I
really
don't
know
anything
about
debugging
C
in
Netbeans
with
Cygwin
.
All
I
wanted
to
do
was
just
bash
my
way
through
this
one
C
program
because
I
need
to
access
a
C
library
.
Alternatively
","
does
anyone
know
if
and
how
to
run
a
C
subroutine
in
python
?
(
A
vastly
superior
language
to
C
/
C
+
+
","
in
my
opinion
.
)
I
would
be
delighted
to
get
access
to
this
C
library
from
either
NetBeans
or
rewrite
my
access
code
in
python
.
I
disagree
with
@rvinas
answer
","
you
don't
need
to
create
a
Variable
to
hold
the
value
of
a
tensor
you
want
to
retrieve
.
You
can
just
use
graph.get_tensor_by_name
with
the
correct
name
to
retrieve
your
tensor
:
Now
you
want
to
recreate
the
same
scope
and
get
back
a
and
b
.
For
b
","
you
don't
even
need
to
be
in
the
scope
","
you
just
need
the
exact
name
of
b
.
tf.get_variable()
won't
work
to
get
an
operation
.
Therefore
","
I
would
define
a
new
variable
storing
"tf.maximum(1,2)"
to
retrieve
it
later
:
Note
that
you
need
to
define
b
using
tf.get_variable()
in
order
to
retrieve
it
later
.
My
question
is
related
to
this
Tensorflow
:
How
to
get
a
tensor
by
name
?
I
can
give
names
to
operations
.
But
actually
they
named
differently
.
For
example
:
tf.get_variable
creates
variable
with
exactly
the
same
name
as
I
ask
.
Operations
add
prefixes
to
scope
.
I
want
to
name
my
operation
so
that
I
can
get
it
.
In
my
case
I
want
to
get
b
with
tf.get_variable('b')
in
my
scope
.
How
can
I
do
it
?
I
can't
do
it
with
tf.Variable
because
of
this
issue
https://github.com/tensorflow/tensorflow/issues/1325
May
be
I
need
to
set
addition
parameters
to
variable
scope
","
or
to
operation
","
or
somehow
use
tf.get_variable
?
I
want
to
force
a
line
break
after
every
10
numbers
or
9
nine
splits
on
a
txt
file
in
python
?
How
would
I
go
about
this
?
So
say
i
have
int
a
txt.file
and
output
should
be
so
essentially
break
after
every
10
numbers
","
or
9
line
splits
I
have
tried
:
But
this
doesn't
quite
solve
it
.
Also
note
that
the
line
lengths
can
vary
.
So
the
first
line
could
have
5
numbers
and
the
second
line
have
9
and
the
third
10
and
the
fourth
10
How
about
using
[count]
to
count
the
occurrences
of
an
item
in
the
list
?
It
looks
like
everything
below
the
with
statement
should
be
indented
one
more
level
.
Your
method
seems
like
a
good
start
","
but
it
will
not
work
if
there
are
more
than
2
groups
on
one
line
.
The
following
code
takes
care
of
that
and
simplifies
things
a
bit
:
I'm
not
able
understand
what
to
do
here
.
Can
someone
help
.
I've
a
few
lists
:
slice
is
a
list
of
indices
I
need
to
get
from
slicing
array
.
interval
is
a
list
of
indices
used
to
stop
the
slicing
when
slice
[i]
is
interval
[j]
where
i
and
j
are
looping
variables
.
I
need
to
form
a
list
of
lists
from
array
based
on
slice
and
intervals
based
on
the
condition
that
when
slice
[i]
is
not
interval
[j]
here
in
my
case
:
when
slice
[i]
and
interval
[j]
are
equal
for
value
12
.
So
I
need
to
form
a
list
of
lists
from
array
which
is
and
when
slice
[i]
is
interval
[j]
output
=
output
+
intermediate
and
the
slicing
is
continued
.
which
is
now
the
next
value
in
interval
is
17
so
till
we
have
17
in
slice
we
form
another
list
from
array
[
slice
[6]
:
slice
[
6+1
]
+
1
]
and
add
this
to
the
output
.
This
continues
.
The
final
output
should
be
:
which
is
Here
is
a
recursive
solution
which
goes
through
the
index
once
and
dynamically
check
if
the
index
is
within
the
intervals
and
append
the
sliced
results
to
a
list
accordingly
:
Data
:
A
straightforward
solution
:
I
have
changed
some
variable
names
to
avoid
conflicts
.
On
large
data
","
you
may
convert
intervals
to
a
set
.
I
want
to
use
the
.
.
include
:
:
function
inline
","
but
I
can
only
get
it
to
actually
include
the
file
I
want
if
I
separate
it
with
two
new
lines
from
the
previous
text
.
Before
anyone
asks
","
the
file
I
want
to
include
is
a
protocol
number
","
so
no
","
it
doesn't
benefit
from
a
new
line
","
at
all
.
I
want
to
be
able
to
change
it
easily
so
I
can
use
it
on
multiple
places
of
my
documentation
.
I
guess
that
an
example
would
be
""""
We
currently
use
the
protocol
(
proto.txt
)
.
""""
I'm
new
to
Sphinx
and
rst
","
so
maybe
there
is
a
very
obvious
solution
I
haven't
found
.
Inline
includes
are
not
possible
with
Sphinx
.
However
","
you
can
define
global
aliases
in
the
rst_epilog
variable
of
your
build
configuration
file
.
For
example
","
you
can
add
the
following
lines
to
your
conf.pyfile
:
Now
","
you
can
access
the
variables
|
version
|
and
|
protocol
|
from
any
.
rst
file
within
your
project
","
for
example
like
this
:
Version
|
version
|
uses
the
|
protocol
|
protocol
.
becomes
Version
4.1
uses
the
httpx
protocol
.
If
other
parts
of
your
software
require
protocol
(
or
other
variables
)
to
be
specified
in
a
specific
file
or
format
","
you
can
write
a
script
to
read
it
from
there
as
a
variable
into
the
Sphinx
configuration
file
.
While
going
through
the
documentation
of
django
to
muster
the
detailed
knowledge
","
i
endured
the
word
'
table
level
operation
'
and
'
record
level
operation
'
.
What
is
the
difference
in
between
them
?
Could
anyone
please
explain
me
this
2
word
with
example
?
Does
they
have
other
name
too
?
P.S
I
am
not
asking
their
difference
just
because
i
feel
they
are
alike
but
i
feel
it
can
be
more
clear
to
comprehend
this
way
.
In
the
context
of
Django
","
record
level
operations
are
those
that
on
a
single
records
.
An
example
is
when
you
define
custom
methods
in
a
model
:
Table
level
operations
are
those
that
act
on
a
set
of
records
and
an
example
of
these
are
when
you
define
a
ModelManager
for
a
class
:
PS
:
I
took
these
examples
from
django
documentation
.
I
do
not
know
specifically
how
Django
people
use
the
terms
","
but
'
record-level
operation
'
should
mean
an
operation
on
1
or
more
records
while
a
'
table-level
operation
'
should
mean
an
operation
of
the
table
as
a
whole
.
I
am
not
quite
sure
what
an
operation
on
all
rows
should
be
-
-
perhaps
both
","
perhaps
it
depends
on
the
result
.
In
Python
","
the
usual
term
for
'
record-level
'
would
be
'
element-wise
'
.
For
Python
builtins
","
bool
operates
on
collections
:
"bool([0, 1, 0, 3])"
=
True
.
For
numpy
arrays
","
bool
operates
(
at
least
usually
)
on
elements
:
`
"bool([0, 1, 0, 2])"
=
"[False, True, False, True]"
.
Also
compare
"[1,2,3]"
*
2
=
"[1,2,3,1,2,3]"
versus
"[1,2,3]"
*
2
=
"[2,4,6]"
.
I
hope
this
helps
.
See
if
it
makes
sense
in
context
.
Here
both
B
and
C
are
derived
from
A
","
but
with
different
__init__()
parameters
.
My
question
is
how
to
write
the
correct
/
elegant
code
here
to
initialize
self.a
","
self.b
","
self.c1
","
self.c2
in
the
following
example
?
Maybe
another
question
is--is
it
a
good
coding
practice
to
do
this
variable
setting
in
__init()
__
function
or
it
is
better
to
use
simpler
__init__()
function
","
and
do
set()
function
for
each
class
later
","
which
seems
not
as
simple
as
to
just
do
it
in
__init()
__
?
Multiple
inheritance
in
Python
requires
that
all
the
classes
cooperate
to
make
it
work
.
In
this
case
","
you
can
make
them
cooperate
by
having
the
__init__
method
in
each
class
accept
arbitrary
*
*
kwargs
and
pass
them
on
when
they
call
super()
.
__init__
.
For
your
example
class
hierarchy
","
you
could
do
something
like
this
:
Note
that
if
you
wanted
D
to
use
the
argument
values
directly
(
rather
than
using
self.a
","
etc.
)
","
you
could
both
take
them
as
named
arguments
and
still
pass
them
on
in
the
super()
call
:
Accepting
and
passing
on
some
args
is
important
if
some
of
the
parent
classes
don't
save
the
arguments
(
in
their
original
form
)
as
attributes
","
but
you
need
those
values
.
You
can
also
use
this
style
of
code
to
pass
on
modified
values
for
some
of
the
arguments
(
e.g
.
with
"super(D, self)"
.
"__init__(a=a, b=b, c1=2*c1, c2=5*c2, **kwargs)"
)
.
This
kind
of
collaborative
multiple
inheritance
with
varying
arguments
is
almost
impossible
to
make
work
using
positional
arguments
.
With
keyword
arguments
though
","
the
order
of
the
names
and
values
in
a
call
doesn't
matter
","
so
it's
easy
to
pass
on
named
arguments
and
*
*
kwargs
at
the
same
time
without
anything
breaking
.
Using
*
args
doesn't
work
as
well
(
though
recent
versions
of
Python
3
are
more
flexible
about
how
you
can
call
functions
with
*
args
","
such
as
allowing
multiple
unpackings
in
a
single
call
:
"f(*foo, bar, *baz)"
)
.
If
you
were
using
Python
3
(
I'm
assuming
not
","
since
you're
explicitly
passing
arguments
to
super
)
","
you
could
make
the
arguments
to
your
collaborative
functions
""""
keyword-only
""""
","
which
would
prevent
users
from
getting
very
mixed
up
and
trying
to
call
your
methods
with
positional
arguments
.
Just
put
a
bare
*
in
the
argument
list
before
the
other
named
arguments
:
"def __init__(self, *, c1, c2, **kwargs):"
.
Gives
an
error
Tried
changing
covert
function
in
functools.py
","
still
the
same
issue
.
The
function
generate
","
as
its
docstring
states
","
""""
Generates
an
iterator
of
all
sentences
from
a
CFG
.
""""
Clearly
it
does
so
by
choosing
alternative
expansions
in
the
order
they
are
listed
in
the
grammar
.
So
","
the
first
time
is
sees
an
NP
","
it
expands
it
with
the
rule
NP
->
NP
PP
.
It
now
has
another
NP
to
expand
","
which
it
also
expands
with
the
same
rule
...
and
so
on
ad
infinitum
","
or
rather
until
python's
limits
are
exceeded
.
To
fix
the
problem
with
the
grammar
you
provide
","
simply
reorder
your
first
two
NP
rules
so
that
the
recursive
rule
is
not
the
first
one
encountered
:
Do
it
like
this
and
the
generator
will
produce
lots
of
complete
sentences
for
you
to
examine
.
Note
that
the
corrected
grammar
is
still
recursive
","
hence
infinite
;
if
you
generate
a
large
enough
number
of
sentences
","
you
will
eventually
reach
the
same
recursion
depth
limit
.
I
tried
to
number
the
repeating
occurrence
of
NP
NN
DT
etc.
It
seems
to
solve
the
problem
due
to
unique
identification( I presume)
.
What
wonders
me
is
it
should
have
been
like
this
at
first
place
i.e
the
tree
production
thrown
out
should
have
serialized
the
parts
of
speech
.
Brute-force
but
straightforward
is
to
emit
your
assignments
as
a
batch
script
on
stdout
","
and
execute
that
script
in
the
existing
interpreter
(
akin
to
source
in
bash
)
:
I've
been
stumped
for
far
too
long
.
Hoping
someone
can
assist
.
I
am
writing
a
Python
CLI
application
that
needs
to
set
a
temporary
environment
variable
(
PATH
)
for
the
current
command
prompt
session
(
Windows
)
.
The
application
already
sets
the
environment
variables
permanently
for
all
future
sessions
","
using
the
method
seen
here
.
To
attempt
to
set
the
temporary
env
vars
for
the
current
session
","
I
attempted
the
following
:
using
os.environ
to
set
the
environment
variables
using
the
answer
here
which
makes
use
of
a
temporary
file
.
Unfortunately
this
works
when
run
directly
from
the
Cmd
Prompt
","
but
not
from
Python
.
calling
SET
using
subprocess.call
","
subprocess.check_call
The
users
of
the
tool
will
need
this
so
they
do
not
have
to
close
the
command
prompt
in
order
to
leverage
the
environment
variables
I've
set
permanently
.
I've
see
other
applications
do
this
","
but
I'm
not
sure
how
to
accomplish
this
with
Python
.
I
am
trying
to
do
something
similar
to
GridSearch
in
sklearn
:
I
want
to
get
a
list
of
three
models
","
where
all
parameters
are
fixed
except
for
C
corresponding
to
the
1
","
10
","
and
100
in
each
model
.
I
have
the
following
two
functions
.
I
then
build
a
model
and
specify
a
dictionary
of
parameters
.
And
generate
the
models
using
the
functions
I
just
defined
.
However
","
the
outcome
is
the
same
model
(
using
the
last
parameter
","
i.e.
100
)
being
repeated
3
times
.
It
seems
there
is
some
aliasing
going
on
.
model
refers
to
the
same
object
throughout
each
iteration
of
the
list
comprehension
in
model_GridSearch
","
so
you're
just
assigning
a
C
value
3
times
to
the
same
object
.
You
can
do
a
few
different
things
to
fix
this
:
you
could
a
copy
of
the
object
using
the
copy
module
","
or
pass
in
the
class
into
the
models_GridSearch
function
instead
of
an
instance
","
and
instantiate
an
object
on
each
iteration
.
You
could
also
refactor
your
code
in
various
ways
to
fix
things
.
It
all
depends
on
your
goals
.
Copy
method
:
Pass
in
class
:
Unfortunately
you
don't
tell
us
:
Which
TDS
version
you
wish
to
actually
use
.
Which
pymssql
version
you
are
using
and
how
you've
installed
it
If
you
are
trying
to
use
TDS
8.0
and
see
pymssql+FreeTDS
is
using
7.1
then
you
a
)
Don't
need
to
worry
as
they
are
the
same
thing
.
See
http://www.freetds.org/userguide/choosingtdsprotocol.htm#AEN910
b
)
Actually
use
""""
7.1
""""
as
FreeTDS
1.0
deprecates
usage
of
""""
8.0
""""
.
See
https://github.com/FreeTDS/freetds/blob/1855d0f72aadd998ab133208fcd3f4d168074ab5/NEWS#L6-L7
IMPORTANT
CHANGE
:
The
following
command
also
works
and
gets
me
the
correct
prompt
.
There
must
then
be
an
issue
with
pymssql
.
So
I'm
fighting
with
my
pymssql
and
freetds
drivers
.
Platform
Versions
Etc
:
Ubuntu
16.04
FreeTDS
v0.91
(
used
by
working
tsql
)
FreeTDS
v0.95
(
used
by
pymssql
)
pymssql
v2.1.3
Target
Database
:
SQL
Azure
(
latest
)
Instructions
for
install
:
https://azure.microsoft.com/en-us/documentation/articles/sql-database-develop-python-simple/
I've
gone
into
every
freetds.conf
file
I
can
find
:
/
etc
/
freetds
/
freetds.conf
;
/
root
/
.
freetds.conf
I
have
set
the
global
TDS
version
to
8.0
.
I
have
overwritten
from
the
python
perspective
in
my
pymssql.connect
to
overwrite
the
version
to
8.0
I
run
the
diagnostics
tools
:
tsql
-
C
and
get
back
4.2
as
the
version
I
run
the
code
dumping
the
logs
to
stdout
","
and
notice
","
the
version
is
7.1
.
net.c:202:Connecting
to
191.238.6.43
port
1433
(
TDS
version
7.1
)
The
following
tsql
command
works
for
me
...
Notice
the
version
number
.
Its
8.0
.
I
can
validate
I
get
data
back
and
can
do
all
I
want
with
this
.
So
there
is
an
obvious
issue
here
with
how
pymssql
is
hooking
up
with
freetds
.
Here
is
all
the
output
from
the
log
dump
in
case
somebody
sees
something
I
am
failing
to
...
It
appears
that
freetds
v0.95
is
not
compatible
with
Ubuntu
16.04
and
neither
is
the
latest
version
of
pymssql
since
it
ships
with
v0.95
and
it
appears
there
isn't
much
you
can
do
to
swap
the
version
it
uses
out
.
I
was
able
to
get
this
to
work
by
doing
the
following
:
Also
note
that
it
will
not
work
with
the
Anaconda
Interpreter
.
I
have
only
tested
the
Anaconda
Interpreter
and
the
standard
CPython
interpreter
.
I've
made
the
decision
to
enter
the
world
of
mobile
app
development
.
I've
gotten
pretty
familiar
with
Kivy
in
order
to
do
this
.
I'm
running
into
wall
after
wall
trying
to
deploy
anything
I
make
.
First
off
","
I
am
absolutely
unable
to
create
a
working
VM
on
my
machine
at
home
.
I
attempted
to
Enable
virtualization
in
my
machine's
BIOS
so
I
could
use
the
64-bit
version
of
Ubuntu
","
and
I
can't
find
the
option
in
the
menus
.
The
32-bit
option
throws
a
fatal
""""
kernel
panic
""""
error
at
installation
.
I
installed
the
new
Bash
on
Ubuntu
on
Windows
utility
in
an
attempt
to
use
that
to
run
the
Buildozer
tool
to
package
my
.
apk
file
.
However
","
Buildozer
cannot
be
run
as
root
.
I
get
a
security
issue
when
trying
to
access
the
app
directory
(
which
lives
on
my
Windows
host's
home
drive
)
as
a
non-root
user
.
On
top
of
that
","
I've
tried
to
move
the
files
into
the
lxss
folder
","
where
the
Ubuntu
files
seem
to
live
.
I'm
running
out
of
options
entirely
.
My
entire
process
is
being
stymied
because
of
this
one
tool
simply
not
cooperating
.
What
can
I
do
to
get
Buildozer
to
run
properly
knowing
that
I'm
almost
exclusively
limited
to
a
Windows
machine
?
Edit
:
The
VM
image
at
https://kivy.org/#download
will
not
run
for
me
either
.
It
gets
hung
up
while
booting
.
To
start
off
with
","
the
Windows
Subsystem
for
Linux
(
WSL
)
is
not
a
full-fledged
Linux
distribution
.
A
lot
of
things
do
not
work
on
it
","
and
unless
you
are
testing
software
or
doing
very
simple
stuff
","
it
is
best
to
leave
it
disabled
.
Next
","
to
run
virtualization
on
your
machine
","
you
most
likely
have
to
disable
WSL
and
reboot
your
machine
.
To
enable
hardware
virtualization
(
a
requirement
for
64bit
guests
)
","
you
have
to
enable
it
in
your
BIOS
.
This
is
labeled
as
VT-x
or
AMD-V
depending
on
your
processor
type
.
Enable
this
","
save
the
BIOS
configuration
and
then
restart
the
machine
.
Next
","
download
virtualbox
from
virtualbox.org
and
install
it
;
make
sure
you
check
for
updates
as
well
.
Reboot
the
machine
","
because
virtualbox
installs
some
networking
drivers
.
Next
","
download
the
virtualbox
image
for
kivy
this
is
just
a
hard
drive
image
","
you
still
have
to
configure
it
.
Download
it
","
and
extract
the
archive
.
Next
","
open
the
virtualbox
manager
on
your
computer
and
create
a
new
virtual
machine
.
Select
""""
linux
""""
and
""""
Ubuntu
64bit
""""
.
Next
","
under
the
Hard
Drive
section
","
select
""""
use
existing
drive
""""
","
and
select
the
.
vdi
file
from
the
archive
you
downloaded
.
Next
","
go
to
Setting
and
increase
the
video
RAM
to
more
than
32MB
and
enable
3D
acceleration
.
Finally
","
start
the
virtual
machine
.
Once
the
machine
boots
","
there
should
be
a
readme
file
on
the
desktop
which
has
further
instructions
.
The
numpy.where()
version
is
fast
enough
","
you
can
speedup
it
a
little
by
method3()
.
If
the
>
condition
can
change
to
>
=
","
you
can
also
use
method4()
.
the
%
timeit
result
:
I've
read
a
lot
about
different
techniques
for
iterating
over
numpy
arrays
recently
and
it
seems
that
consensus
is
not
to
iterate
at
all
(
for
instance
","
see
a
comment
here
)
.
There
are
several
similar
questions
on
SO
","
but
my
case
is
a
bit
different
as
I
have
to
combine
""""
iterating
""""
(
or
not
iterating
)
and
accessing
previous
values
.
Let's
say
there
are
N
(
N
is
small
","
usually
4
","
might
be
up
to
7
)
1-D
numpy
arrays
of
float128
in
a
list
X
","
all
arrays
are
of
the
same
size
.
To
give
you
a
little
insight
","
these
are
data
from
PDE
integration
","
each
array
stands
for
one
function
","
and
I
would
like
to
apply
a
Poincare
section
.
Unfortunately
","
the
algorithm
should
be
both
memory
-
and
time-efficient
since
these
arrays
are
sometimes
~
1Gb
each
","
and
there
are
only
4Gb
of
RAM
on
board
(
I've
just
learnt
about
memmap'ing
of
numpy
arrays
and
now
consider
using
them
instead
of
regular
ones
)
.
One
of
these
arrays
is
used
for
""""
filtering
""""
the
others
","
so
I
start
with
secaxis
=
X.pop(idx)
.
Now
I
have
to
locate
pairs
of
indices
where
(
secaxis
[
i-1
]
>
0
and
secaxis
[i]
<
0
)
or
(
secaxis
[
i-1
]
<
0
and
secaxis
[i]
>
0
)
and
then
apply
simple
algebraic
transformations
to
remaining
arrays
","
X
(
and
save
results
)
.
Worth
mentioning
","
data
shouldn't
be
wasted
during
this
operation
.
There
are
multiple
ways
for
doing
that
","
but
none
of
them
seem
efficient
(
and
elegant
enough
)
to
me
.
One
is
a
C-like
approach
","
where
you
just
iterate
in
a
for-loop
:
This
is
clearly
very
inefficient
and
besides
not
a
Pythonic
way
.
Another
way
is
to
use
numpy.nditer
","
but
I
haven't
figured
out
yet
how
one
accesses
the
previous
value
","
though
it
allows
iterating
over
several
arrays
at
once
:
Third
possibility
is
to
first
find
sought
indices
with
efficient
numpy
slices
","
and
then
use
them
for
bulk
multiplication
/
addition
.
I
prefer
this
one
for
now
:
But
this
is
seemingly
done
in
7
+
2*(N
-
1
)
passes
","
moreover
","
I'm
not
sure
about
secaxis
[inds]
type
of
addressing
(
it
is
not
slicing
and
generally
it
has
to
find
all
elements
by
indices
just
like
in
the
first
method
","
doesn't
it
?
)
.
Finally
","
I've
also
tried
using
itertools
and
it
resulted
in
monstrous
and
obscure
structures
","
which
might
stem
from
the
fact
that
I'm
not
very
familiar
with
functional
programming
:
Not
only
this
looks
very
ugly
","
it
also
takes
an
awful
lot
of
time
to
complete
.
So
","
I
have
following
questions
:
Of
all
these
methods
is
the
third
one
indeed
the
best
?
If
so
","
what
can
be
done
to
impove
the
last
one
?
Are
there
any
other
","
better
ones
yet
?
Out
of
sheer
curiosity
","
is
there
a
way
to
solve
the
problem
using
nditer
?
Finally
","
will
I
be
better
off
using
memmap
versions
of
numpy
arrays
","
or
will
it
probably
slow
things
down
a
lot
?
Maybe
I
should
only
load
secaxis
array
into
RAM
","
keep
others
on
disk
and
use
third
method
?
(
bonus
question
)
List
of
equal
in
length
1-D
numpy
arrays
comes
from
loading
N
.
npy
files
whose
sizes
aren't
known
beforehand
(
but
N
is
)
.
Would
it
be
more
efficient
to
read
one
array
","
then
allocate
memory
for
one
2-D
numpy
array
(
slight
memory
overhead
here
)
and
read
remaining
into
that
2-D
array
?
I'll
need
to
read
your
post
in
more
detail
","
but
will
start
with
some
general
observations
(
from
previous
iteration
questions
)
.
There
isn't
an
efficient
way
of
iterating
over
arrays
in
Python
","
though
there
are
things
that
slow
things
down
.
I
like
to
distinguish
between
the
iteration
mechanism
(
nditer
","
for
x
in
A
:
)
and
the
action
(
alist.append(...)
","
x
[
i+1
]
+
=
1
)
.
The
big
time
consumer
is
usually
the
action
","
done
many
times
","
not
the
iteration
mechanism
itself
.
Letting
numpy
do
the
iteration
in
compiled
code
is
the
fastest
.
is
much
faster
than
The
np.nditer
isn't
any
faster
.
nditer
is
recommended
as
a
general
iteration
tool
in
compiled
code
.
But
its
main
value
lies
in
handling
broadcasting
and
coordinating
the
iteration
over
several
arrays
(
input
/
output
)
.
And
you
need
to
use
buffering
and
c
like
code
to
get
the
best
speed
from
nditer
(
I'll
look
up
a
recent
SO
question
)
.
https://stackoverflow.com/a/39058906/901925
Don't
use
nditer
without
studying
the
relevant
iteration
tutorial
page
(
the
one
that
ends
with
a
cython
example
)
.
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
Just
judging
from
experience
","
this
approach
will
be
fastest
.
Yes
it's
going
to
iterate
over
secaxis
a
number
of
times
","
but
those
are
all
done
in
compiled
code
","
and
will
be
much
faster
than
any
iteration
in
Python
.
And
the
for
f
in
X
:
iteration
is
just
a
few
times
.
@HYRY
has
explored
alternatives
for
making
the
where
step
faster
.
But
as
you
can
see
the
differences
aren't
that
big
.
Other
possible
tweaks
If
X
was
an
array
","
res
could
be
an
array
as
well
.
But
for
small
N
I
suspect
the
list
res
is
just
as
good
.
Don't
need
to
make
the
arrays
any
bigger
than
necessary
.
The
tweaks
are
minor
","
just
trying
to
avoid
recalculating
things
.
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
This
use
of
np.where
is
just
np.nonzero
.
That
actually
makes
two
passes
of
the
array
","
once
with
np.count_nonzero
to
determine
how
many
values
it
will
return
","
and
create
the
return
structure
(
list
of
arrays
of
now
known
length
)
.
And
a
second
loop
to
fill
in
those
indices
.
So
multiple
iterations
are
fine
if
it
keeps
action
simple
.
You
can
use
csv
for
reading
&
writing
and
itertools.combinations
for
generating
the
pairs
:
Output
:
How
is
it
possible
to
take
a
CSV
file
with
rows
of
varying
lengths
(
up
to
12
in
practice
)
as
input
and
then
output
a
new
CSV
file
where
new
rows
consist
of
the
0th
element
of
each
row
+
each
unique
pair
of
elements
(
>
0th
)
in
alphabetical
order
?
The
original
row
elements
are
not
in
alphabetical
order
.
Please
see
image
for
example
input
and
output
.
enter
image
description
here
In
Sphinx
projects
","
the
presentation
of
the
HTML
page
is
controlled
using
a
template
language
(
Jinja2
by
default
)
.
So
you
can
make
your
pages
more
interactive
by
adding
Javascript
to
the
HTML
template
file(s)
and
it
will
get
inserted
when
Sphinx
uses
that
particular
template
file
to
render
a
page
.
Locate
your
templates
directory
by
searching
for
templates_path
in
your
conf.py
Jinja
templates
can
extend
one
another
","
so
you
probably
want
the
file
that
begins
with
{
%
extends
""""
basic
/
layout.html
""""
%
}
.
Once
you
track
down
where
in
the
sequence
of
extension
you
want
to
make
your
change
","
you
need
to
combine
:
The
section
of
the
page
where
you
want
this
to
take
effect
(
typically
the
main
block
)
the
CSS
with
a
class
for
code
blocks
(
eg
.
in
mine
it's
class=highlight
)
.
The
.
CSS
file
might
well
be
in
docs
/
source
/
_static
/
A
javascript
snippet
to
create
the
button
and
write
to
the
clipboard
(
eg
.
https://clipboardjs.com/
)
Next
","
build
the
Sphinx
project
locally
(
make
html
)
until
you
have
it
dialed
in
and
the
import to
readthedocs
.
I
have
been
working
on
a
personal
""""
how-to
""""
guide
","
chronicling
and
keeping
a
journal
of
my
studies
as
I
go
along
.
I
now
have
a
","
almost
too
long
block
of
code
that
","
when
I've
encountered
this
length
of
code
myself
","
its
always
frustrating
trying
to
highlight
JUST
the
block
without
it
highlighting
the
whole
page
","
or
not
enough
.
So
","
my
question
is
","
for
rst
(
reStructuredText
)
.
.
code-block::'s
","
is
there
an
add-on
or
a
way
to
add
in
a
copy
button
","
for
automatic
highlighting
or
automatically
adding
the
text
to
the
users
clipboard
?
Or
would
this
be
a
more
html-literal
type
of
code
I'd
have
to
include
in
the
build
and
reference
it
in
the
code
block
?
If
so
","
what
would
something
like
that
look
like
as
well
?
Since
I'm
a
Python
beginner
","
I'm
trying
to
study
some
codes
from
some
websites
.
I
found
in
GitHub
an
algorithm
which
does
a
Bruteforce
search
for
arithmetic
expressions
.
The
code
is
:
it
simply
ends
trying
any
arithmetic
expression
using
my
numbers
","
then
prints
the
ones
which
get
the
target
as
result
(
the
result
I
have
)
.
I'm
wondering
","
how
can
I
make
it
print
any
solution
the
script
tried
when
the
expression
doesn't
have
as
result
the
target
I've
set
?
edit
:
This
is
the
code
I
tried
:
I
get
the
actual
product
as
result
","
but
I
get
results
for
small
operations
in
expressions
:
While
I'm
actually
trying
to
get
only
42
at
the
start
of
the
string
.
The
recursion
terminates
by
yielding
str(num[0])
if
num
[0]
equals
target
and
nothing
otherwise
.
If
something
is
yielded
","
the
string
expression
is
built
on
successive
yields
.
To
get
all
expressions
","
something
must
always
be
yielded
.
I
choose
to
also
yield
whether
the
target
was
reached
.
Instead
","
the
expression
could
be
evaluated
before
printing
.
There
is
a
glitch
in
the
original
.
The
product
is
appended
to
the
end
","
but
the
first
number
matching
the
product
from
the
front
is
replaced
.
I
believe
that
the
result
could
be
omission
of
expressions
","
in
which
case
the
algorithm
is
not
complete
.
Since
replace
cannot
be
done
starting
at
the
end
","
the
product
should
be
placed
at
the
front
(
"subnumbers.insert(0, product)"
)
so
that
it
is
the
product
that
gets
replaced
.
I
will
let
you
experiment
with
what
difference
this
makes
.
But
I
believe
the
code
would
have
been
slightly
easier
to
understand
if
written
correctly
.
Put
the
my_list
=
[]
inside
the
outer
loop
to
reinitialize
it
each
time
.
Also
","
if
you
don't
want
the
newline
","
don't
append
it
to
my_list
.
If
you
want
all
the
output
on
the
same
line
use
the
optional
arg
end
to
the
print
function
.
I
agree
with
@Neil
It
looks
like
you
want
a
2d
array
.
Here's
another
way
to
do
it
.
This
will
create
a
0
-
9
number
object
5
to
9
times
.
e.g
[
[
2
","
3
","
4
","
5
...
]
","
[
1
","
2
","
2
","
3
...
]
...
]
Then
you
just
iterate
over
the
object
to
print
it
out
the
way
you
want
UPDATE
Given
the
comment
you
replied
with
here
is
example
of
a
random
number
file
generator
.
If
you
want
more
numbers
on
each
line
format
the
text
inside
the
write
method
.
E.G
This
code
creates
sublists
inside
my_list
","
and
prints
out
each
sublist
every
iteration
.
So
your
output
might
be
something
like
this
:
but
my_list
would
store
all
the
sublists
","
like
this
:
[
"[0, 4, 8]"
","
"[3, 1, 5]"
","
...
","
"[9, 2, 5]"
]
Currently
getting
this
output
looking
for
this
output
Code
:
I'm
port
a
Python
project
over
to
C
#
.
So
far
I've
run
into
this
problem
","
is
there
any
way
I
could
port
this
to
C
#
?
I've
tried
this
","
So
now
I
am
getting
very
confused
","
lot's
of
bugs
","
here
any
there
.
I
seems
over
complicated
and
impractical
.
I
hope
someone
can
help
me
:
)
If
you
need
any
more
information
","
just
leave
a
comment
","
If
it's
hard
to
read
","
again
just
leave
a
comment
.
~
Coolq
This
is
one
way
you
could
go
about
doing
it
...
Thanks
@Aison
for
giving
me
an
inspiration
.
This
solution
is
not
exactly
what
I
want
.
But
it
save
my
life
for
now
.
From
SerializerMethodField
.
I
decided
to
modify
ProfileSerializer
to
And
this
what
I
got
from
it
.
This
is
enough
for
now
.
I
have
2
existing
tables
in
database
and
i
haven't
permission
to
alter
them
.
Show
as
models
below
.
If
sql
i
have
to
Things
I've
already
try
:
Things
I've
got
:
Things
I
expected
:
I'm
using
django-rest-framework
3.2.5
and
django
1.6
how
do
i
serialize
them
?
Try
this
:
To
solve
this
","
I
did
the
following
:
(
note
that
it
is
not
entirely
clear
to
me
which
of
these
solved
the
problem
","
since
I
didn't
test
thoroughly
)
.
1
)
Installed
python
at
Python.org
instead
of
Mac's
stupid
version
2
)
re-installed
all
of
the
modules
like
numpy
","
scipy
","
matplotlib
","
sklearn
and
ran
this:hash
-
r
python
according
to
this
source
:
Symbol
not
found
:
__PyCodecInfo_GetIncrementalDecoder
","
because
it
doesn't
make
python
use
the
cached
versions
of
the
modules
.
3
)
Then
","
I
realized
that
I
had
this
issue
:
https://github.com/scipy/scipy/issues/5093.
To
solve
it
","
I
had
to
make
sure
I
installed
the
scipy
module
using
python
-
m
pip
install
scipy='0.15.0
'
instead
of
just
pip
install
scipy='0.15.0
'
","
because
this
solved
the
issue
based
on
this
source
:
Can't
load
Python
modules
installed
via
pip
from
site-packages
directory
.
So
","
in
conclusion
it
turns
out
there
really
is
a
big
different
between
what
is
installed
by
pip
","
and
what
is
imported
when
python
is
executed
from
the
terminal
.
So
","
to
ensure
that
you
are
using
the
pip
to
install
the
modules
into
a
particular
python
","
you
can
use
python
-
m
pip
install
<
package
name
>
.
I
guess
you
are
using
MAC
OS
.
I
did
a
workaround
to
ignore
the
existing
version
of
numpy
(
which
MAC
won't
let
you
uninstall
)
","
and
install
an
upgraded
version
.
Command
:
Worked
fine
for
me
.
If
you're
using
the
brew
version
of
python
If
you're
using
the
mac
version
of
python
:
python
2.7
python
3
I'm
on
a
Mac
","
and
I
installed
numpy
and
sklearn
in
that
order
.
Now
","
I'm
faced
with
these
errors
that
have
already
been
mentioned
on
SO
several
times
:
sklearn
""""
numpy.dtype
has
the
wrong
size
","
try
recompiling
""""
in
both
pycharm
and
terminal
ValueError
:
numpy.dtype
has
the
wrong
size
","
try
recompiling
ImportError
in
importing
from
sklearn
:
cannot
import name
check_build
So
","
I
try
to
remediate
this
error
by
uninstalling
numpy
","
and
reinstalling
a
previous
version
.
1
)
sudo
pip
install
-
-
upgrade
numpy..gives
permission
error
...
OSError
:
[
Errno
1
]
Operation
not
permitted
:
'
/
tmp
/
pip-OVY0Vq-uninstall
/
System
/
Library
/
Frameworks
/
Python.framework
/
Versions
/
2.7
/
Extras
/
lib
/
python
/
numpy-1.8.0rc1-py2.7.egg-info
'
...
2
)
I
tried
brew
uninstall
numpy
","
but
import numpy
still
works
even
after
a
shell
restart
.
The
only
thing
left
I
can
think
of
is
to
manually
delete
all
of
the
numpy
files
","
which
","
on
a
Mac
seeem
to
be
found
under
sudo
rm
-
rf
/
System
/
Library
/
Frameworks
/
Python.framework
/
Versions
/
2.7
/
Extras
/
lib
/
python
/
numpy
....
but
even
that
gives
me
a
permission
error
.
what
gives
?
I
have
a
pretty
simple
Django
app
","
that
I
am
trying
to
run
unit
tests
on
.
In
my
tests.py
file
I
am
trying
to
import the
parent
apps
views
file
.
I
tried
'
from
.
import views
'
but
got
an
error
:
I
read
that
when
a
relative
path
does
not
work
","
you
can
try
using
an
absolute
path
so
I
tried
'
from
menu
import views
'
but
than
got
another
error
:
When
I
run
a
local
server
for
the
application
it
works
just
fine
.
Its
only
when
I
run
'
coverage
run
'
coverage
run
menu
/
tests.py
'
.
Since
it
is
running
fine
","
and
the
module
is
in
my
setting's
installed
apps
","
Im
not
entirely
sure
why
this
is
happening
.
menu
/
tests.py
settings.py
Traceback
It's
not
much
information
you
gave
us
.
.
but
when
I
take
a
look
at
the
Traceback
it
says
File
'
menu
/
tests.py
'
.
So
if
the
views.py
is
also
inside
the
menu
Folder
you
just
can
write
:
If
the
views.py
is
in
the
main
folder
you
could
write
:
I
think
your
best
bet
would
be
to
install
fabric
and
rsync_project
on
'
bastion
'
and
use
fabrics
""""
run
""""
to
call
rsync_project
on
bastion
.
Another
option
would
be
to
use
""""
get
""""
to
receive
a
copy
from
bastion
and
run
rsync_project
locally
.
But
this
approach
will
not
match
your
picture
.
Here
is
my
scenario
:
I
want
to
execute
local
fab
command
to
rsync
code
in
the
bastion
server
to
the
web
server
parallel
","
but
I
find
rsync_project
can
only
run
as
a
local
command
","
fail
to
find
code
base
path
in
my
local
machine
.
How
to
slove
this
issue
","
and
is
there
a
way
to
set
local
host
string
as
the
bastion
server
to
let
rsync_project
run
on
the
bastion
server
properly
?
Thank
you
for
you
time
.
views.py
urls.py
:
The
following
error
is
raised
during
runserver
(
without
basename
kwarg
)
:
AssertionError:base_nameargument
not
specified
","
and
could
not
automatically
determine
the
name
from
the
viewset
","
as
it
does
not
have
a.querysetattribute
.
However
","
when
I
add
the
wished
basename
","
the
error
changes
to
:
TypeError
:
as_view()
takes
1
positional
argument
but
2
were
given
I
suspect
that
it
might
be
related
to
combining
a
ViewSet
and
a
ListAPIView
in
one
router
.
Solution
:
You
must
inherit
from
viewsets.ViewSetMixin
to
implement
the
methods
that
are
required
by
a
ViewSetClass
to
be
registered
with
the
DRF
router
","
otherwise
it
is
possible
to
use
the
simple
Django
urlconf
notation
.
views.py
:
urls.py
:
Ok
so
","
I'm
16
and
am
new
to
Python
.
I
need
projects
to
do
to
help
me
learn
","
so
I
came
up
with
a
PlayStation
Plus
code
generator
as
a
project
and
so
far
it
does
:
-
Generates
a
code
-
Logs
in
to
Account
Management
at
Sony's
site
-
Enters
code
at
redeem
section
-
Checks
if
error
occurred
It
works
completely
fine
if
only
done
ONCE
but
if
for
example
","
I
set
amount
to
2
","
I
get
my
error
message
""""
Error
found
""""
and
then
it
crashes
and
gives
me
this
:
Any
help
or
advice
would
be
appreciated
","
I
apologize
before
hand
if
this
is
not
detailed
enough
.
This
is
an
idea
of
what
I
mentioned
in
the
comment
You
can
also
go
as
far
as
to
do
something
like
html.find(ID)
>
-
1
:
do
something
else
the
ID
wasn't
present
on
the
page
so
quit
or
reload
.
Also
more
then
anything
else
it's
probably
better
to
move
inside
the
while
loop
because
so
that
the
object
is
the
correct
object
through
everyloop
.
What
could
be
happening
is
that
you're
using
elements
that
belonged
to
a
different
dom
.
I
haven't
run
this
yet
","
so
I
could
be
wrong
","
but
fetch()
is
an
asynchronous
method
that
takes
a
callback
","
and
the
callback
is
what
handles
the
response
from
the
server
when
it
comes
.
In
other
words
","
I'm
pretty
sure
all
you're
doing
here
in
this
~
1
second
is
creating
the
representations
of
the
requests
—
you're
definitely
not
waiting
for
responses
","
and
I'm
not
sure
you're
necessarily
even
sending
the
requests
.
I
don't
know
the
internal
implementation
here
","
but
you
might
be
buffering
them
in
some
sense
","
or
you
could
be
making
attempts
to
send
them
as
fast
as
possible
but
some
/
many
are
failing
—
and
you
don't
really
know
what's
actually
happening
","
because
these
asynchronous
requests
are
""""
fire
and
forget
""""
.
If
you
want
to
know
how
many
USEFUL
requests
you
can
send
","
as
in
ones
that
will
reach
the
server
and
get
a
response
","
you'd
need
to
do
the
timing
in
a
callback
","
not
in
the
thread
that's
creating
the
requests
.
Well
","
yes
","
you
spun
off
1000
asynchronous
requests
to
Google
and
timed
that
.
You
did
not
","
however
","
time
the
overhead
of
actually
making
the
HTTP
call
.
That
would
require
a
callback-type
of
handler
.
As
the
other
answers
already
pointed
out
","
tornado
works
asynchronously
.
You
have
to
run
the
requests
in
an
IOLoop
.
Please
note
that
this
has
nothing
to
do
with
multithreading
.
The
requests
are
executed
in
parallel
","
but
within
a
single
thread
(
the
core
feature
of
async
IO
)
.
This
is
how
it
could
be
done
:
Your
code
only
creates
1000
request
objects
(
Future
objects
to
be
precise
)
","
that
are
actually
never
send
to
the
server
.
Result
1.11500000954
seconds
I
wrote
this
to
see
how
fast
I
can
send
1000
requests
to
any
site
(
I
chose
google
)
and
I
don't
know
why
","
but
I
feel
like
I
did
something
wrong
and
its
not
actually
going
this
quick
","
if
I
did
do
something
wrong
","
could
someone
point
out
my
error
?
Thank
you
!
I'm
trying
to
figure
out
how
to
return
a
function
in
Python
.
However
","
I
don't
know
if
the
issue
is
with
reduce
or
with
my
code
.
What
I
currently
have
is
this
:
Code
:
However
","
I
get
the
error
message
:
TypeError
:
'
int
'
object
is
not
subscriptable
What's
causing
this
?
As
it
is
defined
in
your
question
","
fifth(x)
is
the
same
as
:
Now
","
if
x
=
"[1,2,3,4,5,6,7,8,9]"
","
then
But
that
means
that
But
as
the
error
message
tells
us
","
1
is
not
subscriptable
.
I
assume
what's
really
causing
this
is
a
misconception
about
the
order
reduce
works
.
It
works
from
left
to
right
","
so
the
leftmost
function
given
is
applied
first
(
i.e.
","
innermost
)
:
If
you
instead
wanted
a(b(c(x)
)
)
","
you
would
have
to
either
pass
them
to
combine
in
the
reverse
order
","
or
modify
your
defintion
of
combine
so
that
it
reverses
the
order
of
the
arguments
for
you
:
I'm
trying
to
access
an
asp
page
to
get
data
from
a
website
but
it
always
redirects
my
to
its
main
page
.
I've
tried
setting
allow_redirect
to
false
but
that
throws
an
error
","
saying
""""
the
object
has
moved
and
can
be
found
at
href=main.htm
""""
.
The
website
requires
basic
auth
.
-
The
url
I
want
the
data
from
is
:
However
","
whenever
I
run
the
requests.get
command
","
it
redirects
to
example.com.au
/
blah
/
main.htm
.
my
code
:
output
:
I
was
able
to
access
the
.
asp
page
by
first
loading
a
primary
asp
page
then
loading
the
asp
page
I
wanted
under
the
same
session
.
I
am
trying
to
get
the
user
input
and
based
off
of
user
input
","
it
runs
the
gameloop
function
.
However
","
when
the
user
presses
the
right
key
","
it
does
not
run
the
function
or
at
least
I
do
not
think
so
.
No
errors
are
thrown
.
I
am
confused
on
why
it
will
not
work
right
now
.
I
am
running
python
3.4.3
.
Also
","
I
noticed
that
before
the
user
input
","
I
can't
move
the
pygame
window
until
after
the
user
input
.
That
tells
me
the
function
is
working
though
.
However
","
when
I
change
the
background
of
the
gameloop
function
to
blue
","
it
does
nothing
either
.
So
that
tells
me
it
is
not
working
.
I
have
no
idea
what
is
going
on
.
Here
is
my
code
.
Thanks
in
advance
.
The
reason
that
gameloop
is
not
being
run
is
that
you
have
missed
the
brackets
from
instructionread=instructionread.lower
(
)
This
means
that
instead
instructionread
being
given
the
value
""""
y
""""
it
gets
the
value
of
a
<
built-in
method
lower
of
str
object
>
","
so
the
gameloop
doesn't
run
.
Note
that
gameloop
has
no
loop
(
except
the
one
that
goes
ones
through
the
events
)
.
The
procedure
is
run
once
","
and
then
the
program
quits
.
So
","
if
I
input
a
word
""""
tkinter
""""
then
it'll
give
me
the
letter
frequency
.
However
","
if
I
input
""""
t
k
i
n
t
e
r
""""
it'll
give
me
totally
different
data
.
How
do
I
remove
the
whitespace
from
any
input
","
and
then
how
do
I
append
key
","
value
/
float(len(user_input)
)
*
100
","
'
%
'
in
a
Tkinter
Label
?
I
found
how
to
replace
whitespace
","
and
with
suggestions
from
helpful
commentors
","
can
successfully
add
text
to
a
label
that
displays
the
data
with
the
corresponding
key
.
However
","
I
have
another
question
on
how
to
remove
the
previous
label
each
time
you
click
the
button
?
code
:
Use
.
"replace("" "","""")"
to
remove
the
whitespace
.
Below
code
might
help
to
append
result
to
tkinter
label
You
can
use
str.format
to
achieve
your
goal
.
You
can
use
printf
style
formatting
:
I
have
a
function
which
takes
a
count
and
a
string
as
input
.
It
should
return
a
list
of
all
words
in
that
string
of
length
count
","
and
greater
.
Python
however
doesn't
recognise
my
variable
and
returns
an
empty
list
.
How
do
I
pass
in
the
variable
'
count
'
so
that
the
function
returns
words
of
count
and
longer
?
You're
bound
to
get
some
level
of
opinion
on
a
question
like
this
-
-
My
answer
is
to
use
inheritance
iff
your
classes
are
designed
to
be
inherited
.
Use
composition
in
all
the
other
cases
.
In
practice
","
this
usually
means
that
if
the
code
that
you
are
extending
exists
in
a
realm
out
of
your
control
","
don't
subclass
it
unless
their
documentation
talks
about
how
you
can
subclass
it
effectively
.
In
this
case
","
since
it
seems
like
you
are
the
author
of
the
class
hierarchy
","
inheritance
seems
to
make
sense
(
though
some
of
your
assertions
are
incorrect
-
-
D
is
not
so
easy
to
implement
as
you
have
supposed
since
none
of
the
superclass
__init__
will
get
called
)
.
You
probably
should
familiarize
yourself
with
the
design
pattern
in
Raymond
Hettinger's
""""
super
considered
super
""""
article
now
the
implementation
of
D
is
trivially
simple
:
Note
that
we
don't
even
need
to
define
__init__
because
all
of
the
constructors
of
the
super
classes
can
accept
arbitrary
keyword
arguments
.
Instantiating
a
D
looks
like
:
Assume
I
have
a
series
of
classes
as
follows
:
If
now
D
is
a
both
B
and
C
","
it
can
be
implemented
as
Or
D
can
be
understood
as
a
combination
of
B
and
C
which
seems
a
bit
more
complicated
than
the
multi-inheritance
case
.
Well
","
Now
let's
imaging
class
D2
is
a
combination
of
5
pieces
of
B
and
3
pieces
of
C
","
and
class
D3
is
made
of
D2
plus
an
additional
C(with different parameter than the ones in D)
","
and
D4
is
made
of
D2
and
2
additional
pieces
of
C(these 2 additional C has same parameter but different from the ones in D2)
.
It
seems
D2
is
good
to
use
composition
","
but
D3
is
good
to
use
inheritance
of
D2
and
C
","
and
D4
is
good
to
use
inheritance
of
D2
and
C
.
Use
combinations
","
to
me
","
has
the
disadvantage
that
I
have
to
write
d.c.c1
rather
than
d.c1(as in inheritance case)
to
get
the
parameter
c
or
I
need
to
store
the
parameter
c
in
D
directly(besides the one as d.c.c1)
.
Is
there
any
philosophical
consistent
way
to
deal
with
these
classes
?
Thanks
!
I'm
trying
to
run
a
tensorflow
graph
to
train
a
model
and
periodically
evaluate
using
a
separate
evaluation
dataset
.
Both
training
and
evaluation
data
is
implemented
using
queue
runners
.
My
current
solution
is
to
create
both
inputs
in
the
same
graph
and
use
a
tf.cond
dependent
on
an
is_training
placeholder
.
My
issue
is
highlighted
by
the
following
code
:
I
also
had
to
comment
out
the
image_summary
line
133
of
tensorflow
/
models
/
image
/
cifar10
/
cifar10_inputs.py
.
This
yielded
the
following
results
:
It
would
seem
in
the
mixed
case
both
inputs
are
being
read
/
parsed
","
even
though
only
1
is
used
.
Is
there
a
way
of
avoiding
this
redundant
computation
?
Or
is
there
a
nicer
way
of
switching
between
training
/
evaluation
data
that
still
leverages
the
queue-runner
setup
?
After
some
experimentation
","
my
current
best
solution
is
to
have
a
main
graph
featuring
training
inputs
and
a
separate
graph
with
just
evaluation
data
operations
.
I
open
a
separate
session
to
get
evaluation
data
and
feed
this
to
the
training
graph
when
I
want
to
evaluate
.
Highly
inelegant
(
and
evaluation
runs
take
longer
than
they
ideally
would
as
they
have
to
come
ot
of
one
session
only
to
be
fed
to
another
)
","
but
assuming
evaluation
runs
are
rare
compared
to
training
runs
","
this
seems
preferable
to
the
original
version
...
Results
:
Update
:
when
using
this
approach
in
training
problems
with
tf.contrib.layers
and
regularization
","
I
find
the
regularization
losses
go
to
infinity
if
the
DataSupplier
graph
is
on
the
same
device
as
the
training
graph
.
I
cannot
for
the
life
of
me
explain
why
this
is
the
case
","
but
explicitly
setting
the
device
of
the
DataSupplier
to
the
CPU
(
given
the
training
graph
is
on
my
GPU
)
seems
to
work
...
Have
you
read
the
last
section
of
this
link
about
multi
inputs
?
I
think
you
can
add
a
is_training
argument
to
your
input
function
to
distinguish
training
data
from
eval
data
.
Then
you
can
reuse
sharing
variables
to
get
the
logits
for
eval
data
and
build
a
op
for
eval
.
Then
in
your
graph
","
run
valudation_accuracy=sess.run(eval_op
)
to
get
eval
accuracy
.
Update
:
Hi
","
from
my
understanding
","
if
you
want
to
train
for
n
batches
","
evaluate
","
train
","
evaluate
","
you
can
keep
there
two
ops
in
the
same
graph
","
no
need
to
build
a
new
one
.
Assume
you
have
already
build
all
the
needed
function
","
then
the
code
should
like
this
:
Instead
of
"default=""None"
""""
","
specify
null=True
to
allow
null
for
foreign
key
:
I
am
new
to
Django
","
I
am
tring
to
make
a
Blog
here
is
my
models
but
when
i
run
the
command
:
python
manage.py
migrate
i
get
the
ValueError
:
invalid
literal
for
int()
with
base
10
:
'
None
'
I
tried
to
delete
the
old
migration
file
and
migrate
again
but
I
got
the
same
error
","
what
should
I
do
?
I'm
using
sphinx-autodoc-annotation
to
read
the
function
annotations
in
my
Python
code
and
use
that
to
generate
the
appropriate
expected
argument
types
and
return
types
.
It's
working
great
on
my
local
machine
","
but
I
had
to
pip
install
sphinx-autodoc-annotation
of
course
.
I'm
trying
to
generate
the
same
documentation
using
Read
the
Docs
","
but
it
gives
me
an
error
:
Is
it
possible
to
configure
Read
the
Docs
to
work
with
sphinx-autodoc-annotation
","
and
if
so
","
how
do
I
make
it
work
?
Activate
the
Install
Project
option
for
your
Read
the
Docs
project
.
If
the
option
is
activated
","
Read
the
Docs
will
try
to
execute
setup.py
install
on
your
package
(
see
:
RtD
docs
)
.
In
setup.py
","
you
can
install
packages
as
specified
in
your
requirements
file
.
Have
a
look
at
the
source
code
of
the
Flask-MongoRest
project
for
an
example
.
Add
sphinx-autodoc-annotation
as
the
only
requirement
to
your
requirements.txt
file
.
I
have
a
datframe
with
4
columns
of
strings
and
others
as
integers
.
Now
I
need
to
find
out
those
rows
of
data
where
at
least
one
of
the
column
is
a
non-zero
value
(
or
>
0
)
.
My
output
should
be
I
have
tried
the
following
to
obtain
the
answer
.
The
string
values
are
in
colums
0
","
1
","
2
and
-
1
(
last
column
)
.
What
I
am
receiving
as
output
is
How
to
obtain
the
desired
output
You
were
close
:
assume
your
dataframe
is
df
Here
is
an
alternative
solution
which
uses
select_dtypes()
method
:
Explanation
:
Seems
has
been
updated
your
default
anaconda
and
python
folder
to
python
3
.
Easy
way
is
to
remove
anaconda
folder
listed
and
download
python
2.7.11
from
anaconda
and
reinstall
.
In
case
you
have
anaconda
and
anaconda3
in
your
system
than
you
can
update
the
path
according
with
your
needs
:
export
PATH=$HOME
/
anaconda
/
bin:$PATH
or
export
PATH=$HOME
/
anaconda3
/
bin:$PATH
.
Hope
it
helps
.
I
was
playing
around
with
this
some
time
ago
and
seem
to
remember
that
in
my
PATH
variable
in
Windows
(
assuming
Windows
)
-
the
PATH
variable
will
be
read
such
that
the
first
python
key
is
taken
as
the
default
.
So
in
Win
10
","
I
pushed
the
one
I
want
to
the
very
top
of
the
list
and
that
worked
for
me
.
I
recently
installed
the
anaconda
package
while
following
a
tutorial
on
data
science
from
the
guys
at
dataquest.io
and
after
my
install
when
I
type
""""
python
""""
on
the
terminal
I
start
Python
3
instead
of
2.7
.
How
do
I
get
""""
python
""""
to
open
up
python
2.7
again
?
When
I
type
and
I
get
the
same
path
:
/
Users
/
me
/
anaconda
/
bin
/
Using
Mac
OS
X
El
Capitan
You
can
create
a
soft
link
from
python
to
python2.7
","
namely
:
Since
you
skipped
the
code
of
your
function
it's
hard
to
say
what's
wrong
.
But
I
suspect
that
you
just
don't
catch
GET
parameters
correctly
.
To
do
this
you
can
either
use
variable
rules
with
dynamic
name
component
in
your
route
;
or
access
parameters
submitted
in
the
URL
with
request.args.get
.
Here's
a
minimal
example
showing
boths
methods
:
My
route
method
is
:
And
my
template
is
:
When
I
click
the
link
","
the
address
bar
shows
""""
127.0.0.1:5000
/
movie
/
?
page_num=5
""""
","
but
the
pagination.page
shows
it
is
still
page
1
.
Why
the
parameter
was
ignored
and
how
can
I
fix
it
?
The
error
at
the
end
of
the
stacktrace
is
the
key
to
understand
what's
going
on
here
.
ValueError
:
pos_label=1
is
not
a
valid
label
:
"array(['ham', 'spam'], 
        dtype='|S4')"
You're
trying
to
score
your
model
with
precision
and
recall
.
Recall
that
these
scoring
methods
are
formulated
in
terms
of
true
positives
","
false
positives
","
and
false
negatives
.
But
how
does
sklearn
know
what
is
positive
and
what
is
negative
?
Is
it
'
ham
'
or
'
spam
'
?
We
need
a
way
to
tell
sklearn
that
we
consider
'
spam
'
the
positive
label
and
'
ham
'
the
negative
label
.
According
to
the
sklearn
documentation
","
the
precision
and
recall
scorers
by
default
expect
a
positive
label
of
1
","
hence
the
pos_label=1
part
of
the
error
message
.
There
are
at
least
3
ways
to
go
about
fixing
this
.
1
.
Encode
'
ham
'
and
'
spam
'
values
as
0
and
1
directly
from
the
data
source
in
order
to
accommodate
the
precision
/
recall
scorers
:
2
.
Use
sklearn's
built-in
function
(
label_binarize
)
to
transform
the
categorical
data
into
encoded
integers
in
order
to
accommodate
precision
/
recall
scorers
:
This
will
transform
your
categorical
data
to
integers
.
3
.
Create
scorer
objects
with
custom
arguments
for
pos_label
:
As
the
documentation
says
","
the
precision
and
recall
scores
by
default
have
a
pos_label
argument
of
1
","
but
this
can
be
changed
to
inform
the
scorer
which
string
represents
the
positive
label
.
You
can
construct
scorer
objects
that
have
different
arguments
with
make_scorer
.
After
making
any
of
these
changes
to
your
code
","
I'm
getting
average
precision
and
recall
scores
of
around
0.990
and
0.704
","
consistent
with
the
book's
numbers
.
Of
all
the
3
options
","
I
recommend
#
3
the
most
because
it
is
harder
to
get
wrong
.
I'm
working
through
a
tutorial
that
has
this
section
:
Which
I
then
copied
with
few
modifications
:
However
","
despite
there
being
next
to
no
differences
in
the
code
","
the
results
in
the
book
are
far
better
than
mine
:
Book
:
Precision
0.992137651822
[
0.98717949
0.98666667
1
.
0.98684211
1
.
]
Recall
0.677114261885
[
0.7
0.67272727
0.6
0.68807339
0.72477064
]
Mine
:
Precision
0.108435683974
[
2.33542342e-06
1.22271611e-03
1.68918919e-02
1.97530864e-01
3.26530612e-01
]
Recalls
0.235220281632
[
0.00152053
0.03370787
0.125
0.44444444
0.57142857
]
Going
back
over
the
script
to
see
what
went
wrong
","
I
thought
that
line
18
:
was
the
culprit
","
and
changed
(
df
[
'
label
'
]
","
df
[
'
message
'
]
)
to
(
df
[
'
message
'
]
","
df
[
'
label
'
]
)
.
But
that
gave
me
an
error
:
What
could
be
the
problem
here
?
The
data
is
here
:
http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection
in
case
anyone
wants
to
try
.
subprocess.run
was
added
in
Python
3.5
as
a
simplification
over
subprocess.Popen
when
you
just
want
to
execute
a
command
and
wait
until
it
finishes
","
but
you
don't
want
to
do
anything
else
meanwhile
.
For
other
cases
","
you
still
need
to
use
subprocess.Popen
.
The
main
difference
is
that
subprocess.run
executes
a
command
and
waits
for
it
to
finish
","
while
with
subprocess.Popen
you
can
continue
doing
your
stuff
while
the
process
finishes
and
then
just
repeatedly
call
subprocess.communicate
yourself
to
pass
and
receive
data
to
your
process
.
Note
that
","
what
subprocess.run
is
actually
doing
is
invoking
for
you
the
Popen
and
communicate
","
so
you
don't
need
to
make
a
loop
to
pass
/
receive
data
nor
wait
for
the
process
to
finish
.
Check
this
site
for
the
information
of
which
parameters
of
subprocess.run
are
passed
to
Popen
and
which
to
communicate
.
I'm
new
to
the
subprocess
module
and
the
ducumentation
leaves
me
wondering
what
the
difference
is
between
subprocess.popen
and
subprocess.run
.
Is
there
a
difference
in
what
the
command
does
?
Is
one
just
newer
?
Which
is
better
to
use
?
There
are
couple
of
issues
with
the
code
.
First
since
you're
using
long
I
guess
you're
running
it
on
Python
2
.
On
Python
2
range
will
generate
a
list
and
you'll
run
out
of
memory
since
problem
description
states
that
max
N
is
10^9
.
You
could
fix
that
problem
by
switching
to
xrange
that
returns
xrange
object
instead
.
If
you
make
the
change
described
above
the
second
issue
would
be
speed
.
Since
max
N
is
10^5
you'd
potentially
have
to
iterate
over
10^14
numbers
which
takes
far
too
long
.
In
order
to
fix
the
used
algorithm
needs
to
be
changed
.
You
can
use
formula
n
*
(
n
+
1
)
*
mul
/
2
to
calculate
the
sum
of
all
multiples
of
mul
in
range
0...n
.
Then
you
solve
a
case
by
just
adding
the
sum
of
multiples
of
3
and
5
and
subtract
multiples
of
15
:
What
is
wrong
with
this
code
?
It
is
not
passing
case
2
&
3
on
HackerRank
.
I'm
new
in
programming
and
can't
figure
out
what
I
did
wrong
.
https://www.hackerrank.com/contests/projecteuler/challenges/euler001
You
have
to
use
:
instead
of
>
in
tuples
.
like
the
answer
ahsanul
haque
provided
.
thumbs
up
for
him
.
How
do
you
select
python
Tuple
/
Dictionary
values
where
the
index
is
greater
than
some
number
.
I
would
think
the
code
should
look
similar
to
the
following
assuming
we
create
a
Tuple
:
dt
=
(
100
","
200
","
300
","
400
)
dt
[
dt.index
>
1
]
You
could
just
slice
the
tuple
.
recently
I
have
been
trying
to
make
a
spin-off
of
Collatz
conjecture
using
python
3.0
.
The
program
works
as
it
should
with
positive
integers
","
but
it
will
not
work
with
negative
integers
.
In
the
program
I
check
if
the
number
is
negative
and
if
so
I
square
it
","
and
then
proceed
with
collatz
","
rules
.
Unfortuanitly
it
gives
off
no
error
messages
.
Code
below
:
feel
free
to
try
out
my
code
","
it
only
uses
python
3
and
sys
!
-
thanks
!
In
the
mapping
used
by
the
Collatz
conjecture
","
a
positive
value
always
maps
to
another
positive
value
.
So
you
only
need
to
check
for
negative
values
as
part
of
your
initialisation
:
The
loop
while
not
number
!
=
1
:
can
be
deleted
.
It
will
only
run
if
number
is
equal
to
1
(
there
is
a
double
negative
)
.
If
you
enter
1
","
this
loop
will
run
once
","
assign
3*1+1
to
number
","
and
then
the
second
loop
will
start
.
In
other
words
","
this
loop
is
pointless
:
delete
it
.
Note
the
line
number
*
*
2
will
not
change
the
value
of
number
","
since
there
is
no
assignment
.
The
problem
is
that
x
is
not
an
integer
","
it
is
a
string
.
In
this
case
you
are
asking
Python
to
count
up
to
something
which
","
as
far
as
Python
is
concerned
","
has
no
numerical
value
.
Adding
an
int()
statement
and
printing
the
correct
thing
should
do
the
trick
.
Sorry
","
but
everything
.
For
the
sake
of
time
here's
the
code
they
want
:
EDIT
:
Problems
with
your
code
:
You
don't
convert
x
to
an
integer
You
never
use
the
square
function
Said
function
is
unnecessary
considering
*
*
2
does
exactly
that
Above
code
in
english
:
I
am
super
advanced
in
Python
and
trying
to
get
this
right
.
.
""""
Implement
a
program
that
requests
an
integer
n
from
the
user
and
prints
on
the
screen
the
squares
of
all
numbers
from
0
up
to
","
but
not
including
","
n
""""
What's
the
problem
?
Reset
flag
to
false
!
I
am
currently
making
a
web
builder
where
it
will
generate
a
random
gif
based
on
the
GIPHY
API
.
I
am
having
a
problem
where
I
test
a
case
on
when
the
api
returns
with
0
results
.
while
not
flag
:
get_image_link(get_random_query()
)
Essentially
","
if
the
results
come
back
with
no
data
in
the
results
","
i
want
it
to
retry
the
function
to
grab
another
word
.
The
program
works
when
a
word
with
results
comes
back
but
when
it
comes
back
with
0
results
I
get
a
TypeError
and
it
doesn't
go
back
into
the
loop
.
I
am
sure
it
does
this
because
it
doesn't
break
out
of
the
function
and
instead
returns
a
[]
type
.
How
can
I
break
out
of
the
function
and
get
back
into
the
while
loop
so
I
can
generate
another
result
?
Thank
you
.
You
can
move
catching
exception
outside
the
function
i.e
:
hasattr
and
setattr
can
be
very
useful
to
solve
your
problem
.
(
docs
)
I
have
a
huge
array
of
JSON
objects
in
a
json
file
.
some
objects
have
more
keys
than
others
","
but
they
are
all
keys
that
are
fields
in
a
model
class
that
I
have
.
I
am
wondering
","
what
is
the
best
way
to
iterate
over
each
JSON
object
to
create
a
model
instance
with
the
data
from
that
object
","
creating
null
values
for
any
field
that
the
object
does
not
include
?
minerals.json
(
snippet
)
models.py
You
have
to
pass
a
Flask
instance
to
gunicorn
","
not
a
function
name
like
you
did
with
index
.
So
if
your
app
saved
as
app.py
","
then
you
have
to
run
gunicorn
as
follows
:
I'm
trying
to
run
a
Flask
app
with
Gunicorn
.
When
I
run
gunicorn
app:index
","
I
get
the
error
TypeError
:
index()
takes
0
positional
arguments
but
2
were
given
.
None
of
the
examples
I've
seen
show
index
needing
two
parameters
.
What
does
this
error
mean
?
How
do
I
run
Flask
with
Gunicorn
?
I
changed
index
to
take
two
arguments
","
but
got
a
different
error
.
@Jose
Yeah
Thank
you
.
I
seen
work
in
local
for
me
.
But
we
plan
to
use
Python
on
Google
App
Engine
and
It
not
allow
use
extend
library
.
But
I
find
how
to
do
it
use
standard
date
","
time
modules
Although
I
didn't
change
it
to
America
/
Los
Angeles
time
zone
yet
","
but
it
should
be
easy
and
I
already
get
weekend
timestamp
I
have
to
convert
get
weekend
timestamp
from
php
to
python
.
I
google
many
times
but
couldn't
find
the
answer
.
in
php
I
can
do
Do
you
guys
know
how
to
do
the
thing
like
that
in
python
pls
help
me
out
You
can
use
the
dateutil
package
.
Output
:
2016-09-02
02:19:21.899007-05:00
Can
someone
tell
me
how
to
rotate
only
part
of
an
image
like
this
:
How
to
find
coordinate
/
center
of
this
image
:
i
can
rotate
all
pict
using
this
You
can
solve
this
problem
as
such
.
Say
you
have
img
=
"Image.open(""nime1.png"")"
Create
a
copy
of
the
image
using
img2
=
img.copy()
Create
a
crop
of
img2
at
the
desired
location
using
img2.crop()
.
You
can
read
how
to
do
this
here
Paste
img2
back
onto
img
at
the
appropriate
location
using
img.paste()
Notes
:
To
find
the
center
coordinate
","
you
can
divide
the
width
and
height
by
2
:
)
You
can
crop
an
area
of
the
picture
as
a
new
variable
.
In
this
case
","
I
cropped
a
120x120
pixel
box
out
of
the
original
image
.
It
is
rotated
by
90
and
then
pasted
back
on
the
original
.
So
I
thought
about
this
a
bit
more
and
crafted
a
function
that
applies
a
circular
mask
to
the
cropped
image
before
rotations
.
This
allows
an
arbitrary
angle
without
weird
effects
.
Another
way
to
get
list
of
"[long, lat]"
without
list
comprehension
:
If
you
would
like
list
of
tuples
instead
:
If
you
want
to
convert
to
floats
too
:
The
direct
translation
of
your
code
into
a
list
comprehension
is
:
The
obSet
[
'
total_results
'
]
is
informative
but
not
needed
","
you
could
just
loop
over
obSet
[
'
results
'
]
directly
and
use
each
resulting
dictionary
:
Now
you
have
a
list
of
strings
however
","
as
each
'
location
'
is
still
the
long
","
lat
formatted
string
you
printed
before
.
Split
that
string
and
convert
the
result
into
a
sequence
of
floats
:
Now
you
have
a
list
of
lists
with
floating
point
values
:
If
you
must
have
tuples
rather
than
lists
","
add
a
tuple()
call
:
The
latter
also
makes
sure
the
expression
works
in
Python
3
(
where
map()
returns
an
iterator
","
not
a
list
)
;
you'd
otherwise
have
to
use
a
nested
list
comprehension
:
You
can
iterate
over
the
list
of
results
directly
:
This
will
return
a
list
of
tuple
","
elements
of
the
tuples
are
unicode
.
If
you
want
that
the
elements
of
tuples
as
floats
","
do
the
following
:
To
correct
way
to
get
a
list
of
tuples
using
list
comprehensions
would
be
:
You
can
of
course
replace
to_tuple()
with
a
lambda
function
","
I
just
wanted
to
make
the
example
clear
.
Moreover
","
you
could
use
map()
to
have
a
tuple
with
floats
instead
of
string
:
return
"tuple(map(float,coords_str.split(',')"
)
)
.
obSet
[
'
results
'
]
is
a
list
","
no
need
to
use
range
to
iterate
over
it
:
To
make
this
into
list
comprehension
you
can
write
:
But
","
each
location
is
coded
as
a
string
","
instead
of
list
or
tuple
of
floats
.
To
get
it
to
the
proper
format
","
use
That
is
","
split
the
item
[
'
location
'
]
string
into
parts
using
","
as
the
delimiter
","
then
convert
each
part
into
a
float
","
and
make
a
tuple
of
these
float
coordinates
.
Let's
try
to
give
this
a
shot
","
starting
with
just
1
location
:
>
>
>
(
long
","
lat
)
=
obSet
[
'
results
'
]
[0]
[
'
location
'
]
Alright
","
so
that
didn't
work
","
but
why
?
It's
because
the
longitude
and
latitude
coordinates
are
just
1
string
","
so
you
can't
unpack
it
immediately
as
a
tuple
.
We
must
first
separate
it
into
two
different
strings
.
>
>
>
(
long
","
lat
)
=
obSet
[
'
results
'
]
[0]
[
'
location
'
]
.
"split("","")"
From
here
we
will
want
to
iterate
through
the
whole
set
of
results
","
which
we
know
are
indexed
from
0
to
n
.
"tuple(obSet['results'][i]['location'].split("","")"
)
will
give
us
the
tuple
of
longitude
","
latitude
for
the
result
at
index
i
","
so
:
>
>
>
[
"tuple(obSet['results'][i]['location'].split("","")"
)
for
i
in
range(n)
]
ought
to
give
us
the
set
of
tuples
we
want
.
I'm
new
to
Python
and
trying
to
figure
out
the
best
way
to
parse
the
values
of
a
JSON
object
into
an
array
","
using
a
list
comprehension
.
Here
is
my
code
-
I'm
querying
the
publicly
available
iNaturalist
API
and
would
like
to
take
the
JSON
object
that
it
returns
","
so
that
I
take
specific
parts
of
the
JSON
object
into
a
bumpy
array
:
This
all
works
fine
and
gives
the
following
output
:
What
I'd
like
to
do
next
is
replace
the
for
loop
with
a
list
comprehension
","
and
store
the
location
value
in
a
tuple
.
I'm
struggling
with
the
syntax
in
that
I'm
guessing
it's
something
like
this
:
But
this
doesn't
work...thanks
for
any
help
.
Input
File
-
input.csv
Output
result
I
am
trying
to
solve
one
problem
according
to
above
input
","
output
.
So
far
I
can
read
only
first
block
Text
.
I
want
to
more
generic
function
so
possibly
I
will
only
initialise
the
variable
that
need
to
read
within
the
block
","
not
hard
coding
(
e.g
.
#
#
#
#
#
#
#
A
Result
:
#
#
#
#
#
#
#
#
#
)
and
furthermore
pass
the
block
info
to
another
function
that
will
sum
up
the
value
.
Any
suggestion
will
be
greatly
appreciate
.
Thanks
:
)
Throw
in
a
little
bit
of
regex
:
Regex
Explanation
:
Debuggex
Demo
Is
this
Flask
?
If
so
","
you
need
to
return
the
string
you
want
to
send
to
the
user
.
A
print
statement
writes
to
the
web
server
log
.
You
should
replace
the
last
line
of
your
get_title
function
with
this
:
I
was
trying
to
get
the
title
of
the
websites
.
So
","
I
used
this
snippet
to
do
this
This
works
perfectly
and
gives
Google
as
the
title
and
flushes
the
output
to
test_data.txt
.
But
when
I
try
to
run
the
same
code
as
a
web
service
","
it
doesn't
work
.
I
get
an
empty
text
file
.
I
am
hitting
this
URL
to
run
this
web
service
on
my
local
http://0.0.0.0:8881/get_title
Another
thing
which
has
made
me
even
more
anxious
is
when
I
run
the
web
service
for
msn.com
","
it
works
well
for
both
snippets(even the web service)
.
Any
help
would
be
thankful
!
!
I
am
creating
a
histogram
in
Seaborn
of
my
data
in
a
pretty
standard
way
","
ie
:
This
produces
the
following
output
:
I
also
want
to
visualize
the
mean
+
standard
deviation
of
this
dataset
on
the
same
plot
","
ideally
by
having
a
point
for
the
mean
on
the
x-axis
(
or
right
above
the
x-axis
)
and
notched
error
bars
showing
the
standard
deviation
.
Another
option
is
a
boxplot
hugging
the
x-axis
.
I
tried
just
adding
the
line
which
is
commented
out
(
sns.boxplot()
)
","
but
it
looks
super
ugly
and
not
at
all
what
I'm
looking
for
.
Any
suggestions
?
The
boxplot
is
drawn
on
a
categorical
axis
and
won't
coexist
nicely
with
the
density
axis
of
the
histogram
","
but
it's
possible
to
do
it
with
a
twin
x
axis
plot
:
I'm
trying
to
read
log
lines
by
forming
key-value
pairs
but
I
get
an
error
.
This
is
my
code
:
Error
:
line_collect
=
lines.collect()
......
InvalidInputException
:
Input
path
does
not
exist
:
file
:
/
C
:
/
TestLogs
esting.log
I
don't
know
how
to
correct
this
.
I'm
new
to
python
and
spark
.
When
the
character
sequence
\
t
is
found
in
a
string
","
it
will
be
replaced
by
a
TAB
character
.
You
can
actually
see
this
in
the
error
message
.
I'd
recommend
always
using
the
forward
slash
/
as
the
directory
separator
","
even
on
Windows
.
Alternatively
prefix
the
string
with
an
r
like
this
:
"r""does"
not
replace
\
t
with
<
tab
>
.
""""
.
You
might
want
to
read
up
on
string
literals
:
https://docs.python.org/2.0/ref/strings.html.
Try
replace
"logLine=sc.textFile(""C:\TestLogs\testing.log"").cache"
(
)
with
"logLine=sc.textFile(""C:\\TestLogs\\testing.log"").cache"
(
)
The
backslash
character
is
not
'
\
'
in
a
string
but
rather
""""
\
\
""""
I'm
newbie
in
python
and
trying
to
parse
data
in
my
application
using
these
lines
of
codes
But
I'm
getting
this
error
on
json.loads
Expecting
value
:
line
1
column
1
(
char
0
)
this
is
json
formatted
data
that
I
send
from
angular
app
(
Updated
)
Object
{
ClientTypeId
:
6
","
ClientName
:
""""
asdasd
""""
","
ClientId
:
0
","
PhoneNo
:
""""
123
""""
","
FaxNo
:
""""
123
""""
","
NTN
:
""""
1238
""""
","
GSTNumber
:
""""
1982
""""
","
OfficialAddress
:
""""
sads
""""
","
MailingAddress
:
""""
asdasd
""""
","
RegStartDate
:
""""
17-Aug-2016
""""
","
15
moreâ
€
¦
}
these
are
the
values
that
I
get
in
json_str
ClientTypeId=5&ClientName=asdasd&ClientId=0&PhoneNo=123&FaxNo=123&NTN=123&GSTNumber=12&OfficialAddress=adkjh&MailingAddress=adjh&RegStartDate=09-Aug-2016&RegEndDate=16-Aug-2016&Status=1&CanCreateUser=true&UserQuotaFor=11&UserQuotaType=9&MaxUsers=132123&ApplyUserCharges=true&ApplyReportCharges=true&EmailInvoice=true&BillingType=1&UserCharges=132&ReportCharges=123&MonthlyCharges=123&BillingDate=16-Aug-2016&UserSessionId=324
I
don't
know
what's
wrong
in
it
.
.
can
anyone
mention
what's
the
mistake
is
?
?
Your
data
is
not
JSON-formatted
","
not
even
the
one
you
included
in
your
updated
answer
.
Your
data
is
a
JavaScript-object
","
not
an
encoded
string
.
Please
note
the
""""
N
""""
in
JSON
:
Notation
-
-
it
is
a
format
inspired
from
how
data
is
written
in
JavaScript
code
","
but
runtime
JavaScript
data
is
not
represented
in
JSON
.
The
""""
JSON
""""
you
pasted
is
how
your
browser
represents
the
object
to
you
","
it
is
not
proper
JSON
(
that
would
be
{
""""
ClientTypeId
""""
:
6
","
...
}
-
-
note
the
quotes
around
the
property
name
)
.
When
sending
this
data
to
the
server
","
you
have
to
encode
it
.
You
think
you
are
sending
it
JSON-encoded
","
but
you
aren't
.
You
are
sending
it
""""
web
form
encoded
""""
(
data
of
type
application
/
x-www-form-urlencoded
)
.
Now
either
you
have
to
learn
how
to
send
the
data
in
JSON
format
from
Angular
","
or
use
the
correct
parsing
routine
in
Python
:
urllib.parse.parse_qs
.
Depending
on
the
library
you
are
using
","
there
might
be
a
convenience
method
to
access
the
data
as
well
","
as
this
is
a
common
use
case
.
Say
you
had
a
list
","
for
example
:
And
you
had
the
following
list
:
How
would
you
create
a
function
that
simply
re-orders
the
second
list
based
on
the
total
number
of
times
an
element
from
the
entire
first
list
matches
any
part
of
an
individual
string
from
the
second
list
?
For
further
clarity
:
in
the
second
list
the
'
a
'
string
would
contain
1
match
the
'
a
b
c
'
string
would
contain
3
matches
the
second
example
list
would
essentially
end
up
in
reverse
order
once
the
function
has
finished
My
attempt
:
Here's
one
unstable
way
to
do
it
:
If
stable
sort
is
required
you
could
leverage
enumerate
:
Above
generates
tuples
where
second
item
is
a
string
from
l2
and
first
item
is
the
count
of
matches
from
l1
:
Then
those
tuples
are
sorted
to
descending
order
:
And
finally
only
the
strings
are
taken
:
In
the
second
version
tuples
have
reverse
index
to
ensure
stability
:
Similar
answer
:
The
most
straightforward
approach
would
be
to
use
the
key
parameter
of
the
sorted
built-in
function
:
The
key
function
is
called
once
per
item
in
the
list
to
be
sorted
.
Still
this
is
inefficient
since
count
takes
linear
time
.
I
have
multiple
CSV
files
in
a
directory
.
Some
contain
more
columns
(
which
would
be
OK
to
drop
)
.
Is
there
an
elegant
way
to
deduplicate
records
between
these
CSV
files
and
reduce
columns
to
a
common
set
of
columns
?
Currently
","
I
will
use
python
/
pandas
to
accomplish
this
.
I
will
load
all
the
files
into
a
single
data
frame
","
note
in
an
additional
column
where
the
records
originated
from
(
filename
)
","
drop
the
additional
columns
and
finally
have
pandas
deduplicate
via
http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.duplicated.html
In
the
last
step
","
I
write
the
deduplicated
files
back
to
disk
based
on
the
filename-identifier
column
.
Is
there
a
simpler
approach
to
this
?
I
do
not
known
how
to
make
it
simpler
.
I
have
an
equal
script
done
for
some
data
of
mine
.
It
just
runs
twice
","
first
to
determine
the
min
/
max
cols
in
all
documents
and
finally
to
rewrite
the
csv
files
in
an
new
folder
","
to
keep
the
original
data
.
I
am
just
using
the
csv
lib
from
python
.
https://docs.python.org/2/library/csv.html
There
are
no
checks
in
this
script
","
as
it's
just
a
quick
and
dirty
script
.
The
deduplication
is
not
done
.
It
just
cuts
all
data
to
the
same
length
","
but
you
can
replace
the
last
line
with
your
deduplication
code
.
No
","
it's
not
a
bug
.
When
you
use
[
:
]
you
are
using
slicing
notation
and
it
takes
all
the
list
:
and
in
your
case
:
What
you
realy
wish
is
using
numpy
indexing
notation
:
This
is
not
a
bug
.
x
[1]
[
:
]
[0]
is
not
a
multiple
index
(
""""
give
me
the
elements
where
first
dimension
is
1
","
second
is
any
","
third
is
0
""""
)
.
Instead
","
you
are
indexing
three
times
","
three
objects
.
To
use
multiple
index
","
you
want
to
do
it
in
a
single
slice
:
I
seems
found
a
bug
when
I'm
using
python
2.7
with
numpy
module
:
Here
I
got
the
full
'
x
'
array
as
follows
:
Then
I
try
to
indexing
single
row
values
in
sheet
[1]
:
Result
:
But
something
wrong
while
I
was
try
to
indexing
single
column
in
sheet
[1]
:
Result
still
be
the
same
as
previous
:
Should
it
be
"array([20, 25, 30, 35])"
?
?
It
seems
something
wrong
while
indexing
the
middle
index
with
range
?
I
have
an
app
on
heroku
named
as
floating-river-39482
.
And
I'm
trying
to
deploy
the
app
by
giving
the
command
git
push
heroku
master
.
But
I
got
an
error
from
terminal
like
this
In
this
error
message
the
name
of
the
app
is
sleepy-inlet-36834
.
But
I'm
trying
to
push
the
app
floating-river-39482
.
I
don't
have
an
app
named
sleepy-inlet-36834
.
How
can
I
correct
this
error
?
It
looks
like
you've
somehow
got
your
Git
repository
referencing
an
old
(
or
deleted
)
Heroku
application
.
What
you
can
do
most
easily
is
open
up
your
.
git
/
config
file
in
your
project
","
and
switch
out
the
https://git.heroku.com/sleepy-inlet-36834.git/
URL
with
the
correct
one
for
your
Heroku
application
:
https://git.heroku.com/floating-river-39482.git.
Then
give
it
another
go
=
)
To
explain
further
:
Heroku
works
by
using
Git
to
setup
a
'
remote
'
.
Remotes
are
just
places
you
can
push
(
or
pull
)
code
from
.
Git
creates
remotes
by
listing
them
in
your
project's
.
git
/
config
file
.
Try
to
change
your
views.py
:
I
have
a
html
form
that
looks
like
this
.
and
url.py
and
views.py
The
only
thing
I
get
back
is
How
do
I
get
back
the
options
held
in
the
select
tag
?
I
would
use
the
model
/
view
form
here
but
I'm
doing
some
very
crazy
things
with
JS
to
constantly
update
the
options
available
.
UPDATE
I
have
used
:
But
it
will
only
return
[
'
Test
'
]
If
I
highlight
it
with
my
mouse
.
I
simply
want
all
the
options
under
the
select
tag
.
ex
.
and
Because
you
say
you're
doing
weird
things
with
Js
","
you
can
first
check
the
POST
request
from
your
browser
and
what
parameters
you
send
with
it
.
so
i'm
making
alittle
project
as
i
am
a
beginner
and
i'm
doing
some
webscraping
.
I
wanted
to
print
the
lyrics
of
a
song
each
on
it's
line
using
beautifulsoup
in
python
but
instead
it's
printing
like
this
:
I
looked
out
this
morning
and
the
sun
was
goneTurned
on
some
music
to
start
my
dayI
lost
myself
in
a
familiar
songI
closed
my
eyes
and
I
slipped
awayIt's
more
than
a
feeling
(
more
than
a
feeling)When
I
hear
that
old
song
they
used
to
play
(
more
than
a
feeling)And
I
begin
dreaming
(
more
than
a
feeling)Till
I
see
Marianne
walk
awayI
see
my
Marianne
walkin
'
awaySo
many
people
have
come
and
goneTheir
faces
fade
as
the
years
go
byYet
I
still
recall
as
I
wander
onAs
clear
as
the
sun
in
the
summer
skyIt's
more
than
a
feeling
(
more
than
a
feeling)When
I
hear
that
old
song
they
used
to
play
(
more
than
a
feeling)And
I
begin
dreaming
(
more
than
a
feeling)Till
I
see
Marianne
walk
awayI
see
my
Marianne
walkin
'
awayWhen
I'm
tired
and
thinking
coldI
hide
in
my
music
","
forget
the
dayAnd
dream
of
a
girl
I
used
to
knowI
closed
my
eyes
and
she
slipped
awayShe
slipped
awayIt's
more
than
a
feeling
(
more
than
a
feeling)When
I
hear
that
old
song
they
used
to
play
(
more
than
a
feeling)And
I
begin
dreaming
(
more
than
a
feeling)Till
I
see
Marianne
walk
away
This
is
my
code
:
Try
writing
it
into
a
simple
for
loop
You
need
to
remove
the
strip
=
True
parameter
to
get_text
.
That
strips
the
string
resulting
in
the
joined
output
you
see
.
By
removing
it
:
It
prints
fine
.
Having
some
spare
time
","
I
have
modified
your
code
.
The
code
you
supplied
is
riddled
with
small
coding
and
logical
errors
.
You
would
be
best
running
the
diff
command
between
your
original
code
and
the
code
below
","
to
see
where
the
many
differences
are
and
there
is
no
guarantee
that
is
bug
free
now
either
.
I
hope
you're
not
planning
on
fleecing
your
fellow
pupils
","
the
rebalance
routine
is
the
twisted
work
of
a
future
loan-shark
.
;
)
I'm
programming
a
slot
machine
for
school
and
cannot
get
the
machine
to
re-run
once
it
is
finished
.
I
am
relatively
new
and
would
like
some
honest
feedback
.
How
can
I
get
my
program
to
re-run
?
This
is
the
code
I'm
trying
to
do
this
with
.
I've
just
modified
my
code
to
look
like
this
.
This
can
be
treated
as
a
binary
text
classification
problem
.
You
should
collect
documents
that
contain
'
adult-content
'
as
well
as
documents
that
do
not
contain
adult
content
(
'
universal
'
)
.
It
may
so
happen
that
a
word
/
phrase
that
you
have
included
in
the
list
arrBad
may
be
present
in
the
'
universal
'
document
","
for
example
","
'
girl
on
top
'
in
the
sentence
'
She
wanted
to
be
the
first
girl
on
top
of
Mt
.
Everest
.
'
You
need
to
get
a
count
vector
of
the
number
of
times
each
word
/
phrase
occurs
in
a
'
adult-content
'
document
and
a
'
universal
'
document
.
I'd
suggest
you
consider
using
algorithms
like
Naive
Bayes
(
which
should
work
fairly
well
in
your
case
)
.
However
","
if
you
want
to
capture
the
context
in
which
each
phrase
is
used
","
you
could
consider
the
Support
Vector
Machines
algorithm
as
well
(
but
that
would
involve
tweaking
a
lot
of
complex
parameters
)
.
I
would
approach
this
as
a
Text
Classification
problem
","
because
using
blacklists
of
words
typically
does
not
work
very
well
to
classify
full
texts
.
The
main
reason
why
blacklists
don't
work
is
that
you
will
have
a
lot
of
false
positives
(
one
example
:
your
list
contains
the
word
'
sexy
'
","
which
alone
isn't
enough
to
flag
a
document
as
being
for
adults
)
.
To
do
so
you
need
a
training
set
with
documents
tagged
as
being
""""
adult
content
""""
and
others
""""
safe
for
work
""""
.
So
here
is
what
I
would
do
:
check
whether
an
existing
labelled
dataset
can
be
used
.
You
need
several
thousands
of
documents
of
each
class
.
If
you
don't
find
any
","
create
one
.
For
instance
you
can
create
a
scraper
and
download
Reddit
content
.
Read
for
instance
Text
Classification
of
NSFW
Reddit
Posts
Build
a
text
classifier
with
NLTK
.
If
you
don't
know
how
","
read
:
Learning
to
Classify
Text
I
think
you
more
need
to
explore
on
filtering
algorithms
","
studying
their
usage
","
how
multi
pattern
searching
works
and
how
you
can
use
some
of
those
algorithms
(
their
implementations
are
free
online
","
so
it
is
not
hard
to
find
an
existing
implementation
and
customize
for
your
needs
)
.
Some
pointers
can
be
.
Check
how
grep
family
of
algorithm
works
","
especially
the
bitap
algorithm
and
Wu-Manber
implementation
for
fgrep..Depending
upon
how
accurate
you
want
to
be
","
it
may
require
adding
some
fuzzy
logic
handling
(
think
why
people
use
fukc
instead
of
fuck..right
?
)
.
You
may
find
Bloom
Filter
interesting
","
since
it
wont
have
any
false
negatives
(
your
data
set
)
","
downside
is
that
it
may
contain
false
positives
.
.
You
may
be
interested
in
something
like
TextRazor
.
By
using
their
API
you
could
be
able
to
classify
the
input
text
.
And
for
example
you
can
choose
to
remove
all
input
texts
thats
comes
with
some
of
the
categories
or
keywords
you
don't
want
.
I
want
to
filter
out
adult
content
from
tweets
(
or
any
text
for
that
matter
)
.
For
spam
detection
","
we
have
datasets
that
check
whether
a
particular
text
is
spam
or
ham
.
For
adult
content
","
I
found
a
dataset
I
want
to
use
(
extract
below
)
:
Question
How
can
I
use
that
dataset
to
filter
text
instances
?
Your
input
not
defined
here
","
I
assumed
as
a
list
of
list
like
this
.
Then
you
can
do
like
this
","
Result
Not
sure
if
my
problem
sound
a
bit
tricky..my
requirement
is
like
this
:
I
have
three
columns
of
data
in
txt
file
as
below
:
col1
can
be
viewed
as
dict
keys
which
repeat
for
<
=
5
times
","
col3
can
also
viewed
as
nested
dict
keys
with
values
in
col2
","
i.e.
each
key
in
col1
has
<
=
5
pairs
of
(
col2
:
col3
)
.
I
would
like
to
sort
the
nested
dictionary
by
col2
and
replace
the
col2
values
with
highest
ranking
","
i.e.
:
I
don't
care
about
the
values
in
col2
","
i
only
care
about
the
ranking
of
col3
for
each
col1
value
:
I
tried
turning
the
data
into
nested
dictionaries
like
:
I
have
searched
around
and
found
some
solutions
like
sort
nested
dict
etc.
","
but
I
cannot
replace
the
values
with
rankings
either...can
someone
please
help
?
Well
","
here
is
a
way
to
do
it
in
basic
Python
:
Let's
create
data
consisting
of
items
from
each
columns
:
In
[163]
:
data
=
[
*
"zip(col1, col2, col3)"
]
Let's
use
itertools
module
to
group
them
up
:
Now
","
groups
is
a
generator
.
If
we
want
to
see
what
it
looks
like
we
will
need
to
iterate
it
:
and
we
get
:
But
we
exhausted
the
iterator
.
So
let's
create
it
again
","
and
now
that
we
know
what
it
contains
","
we
can
perform
the
desired
sorting
:
OK
","
one
more
thing
","
and
I
do
that
now
in
the
editor
:
And
we
get
:
If
you
want
to
use
id
attribute
in
GML
for
labeling
nodes
","
you
can
designate
the
label
attribute
for
the
nx.read_gml
argument
as
follows
.
I
have
a
long
GML
file
(
Graph
Modelling
Language
)
that
i
am
trying
to
read
with
Networkx
in
Python
.
In
the
GML
file
","
nodes
don't
have
label
","
like
this
:
I
get
an
error
when
reading
the
file
:
G
=
nx.read_gml('simple_graph.gml')
I
see
that
it
complains
because
the
nodes
don't
have
labels
.
From
the
documentation
of
GML
i
thought
that
labels
were
not
obligatory
(
maybe
i'm
wrong
?
)
.
Would
there
be
a
way
to
read
such
a
file
without
labels
?
Or
should
i
change
my
gml
file
?
Thank
you
for
your
help
!
I
have
data
in
the
form
x-y-z
and
want
to
create
a
power
spectrum
along
x-y
.
Here
is
a
basic
example
I
am
posting
to
check
where
I
might
be
going
wrong
with
my
actual
data
:
This
results
in
the
following
function
&
frequency
figures
with
the
incorrect
frequency
.
Thanks
.
One
of
your
problems
is
that
matplotlib's
imshow
using
a
different
coordinate
system
to
what
you
expect
.
Provide
a
origin='lower
'
argument
","
and
the
peaks
now
appear
at
y=0
","
as
expected
.
Another
problem
that
you
have
is
that
fftfreq
needs
to
be
told
your
timestep
","
which
in
your
case
is
8
/
(
N
-
1
)
You
may
say
""""
but
the
frequency
is
10
","
not
0.5
!
""""
However
","
if
you
want
to
sample
a
frequency
of
10
","
you
need
to
sample
a
lot
faster
than
8
/
19
!
Nyquist's
theorem
says
you
need
to
exceed
a
sampling
rate
of
20
to
have
any
hope
at
all
For
a
square
matrix
","
each
cell
is
either
black
(
1
)
or
white
(
0
)
","
trying
to
find
the
max
sub-square
whose
border
is
black
.
Here
is
my
code
with
Python
2.7
","
wondering
if
logically
correct
?
And
any
performance
improvements
?
Thanks
.
My
major
ideas
is
","
keep
track
of
how
many
black
nodes
are
on
top
(
continuously
)
and
on
left
(
continuously
)
","
which
is
left
and
top
matrix
represents
.
Then
","
based
on
the
left
and
top
tracking
","
for
any
node
","
I
will
try
to
find
minimal
value
of
top
and
left
continuous
black
node
","
then
I
will
base
the
minimal
value
to
see
if
a
square
could
be
formed
.
Edit
1
","
fix
an
issue
","
which
is
pointed
by
j_random_hacker
.
Edit
2
","
address
the
issue
by
j_random_hacker
.
Edit
3
","
new
fix
This
isn't
logically
correct
","
as
the
following
simple
counterexample
shows
:
This
array
contains
no
bordered
square
","
but
your
code
reports
that
it
contains
a
square
of
edge
length
3
.
You
need
either
a
third
nested
loop
through
all
candidate
square
sizes
down
from
x
to
1
(
making
the
solution
O(n^3)
-
time
)
or
","
possibly
","
some
more
sophisticated
data
structure
that
enables
an
equivalent
check
to
be
performed
faster
.
[
EDIT
to
address
updated
algorithm
]
The
new
algorithm
thinks
there
is
no
bordered
square
in
either
despite
there
being
a
3x3
one
in
each
.
Stop
guessing
at
fixes
and
think
about
how
to
break
down
the
set
of
all
possible
locations
of
bordered
squares
into
sets
that
you
can
efficiently
(
or
even
not-so-efficiently
)
check
.
As
I
tried
to
say
at
the
top
:
For
each
possible
bottom-right
corner
of
a
bordered
square
","
your
code
is
currently
only
checking
one
rectangle
.
But
many
more
rectangles
could
have
(
i
","
j
)
as
a
bottom-right
corner
-
-
where
are
the
tests
for
them
?
(
Answer
rewrite
)
*
*
NOTE
*
*
","
the
paginator
contains
a
bug
that
doesn't
tally
with
the
documentation
(
or
vice
versa
)
.
MaxItems
doesn't
return
the
Marker
or
NextToken
when
total
items
exceed
MaxItems
number
.
Indeed
PageSize
is
the
one
that
controlling
return
of
Marker
/
NextToken
indictator
.
It
is
not
your
mistake
that
your
code
doesn't
works
.
MaxItems
in
the
paginator
seems
become
a
""""
threshold
""""
indicator
.
Ironically
","
the
MaxItems
inside
original
boto3.iam.list_users
still
works
as
mentioned
.
If
you
check
boto3.iam.list_users
","
you
will
notice
either
you
omit
Marker
","
otherwise
you
must
put
a
value
.
Apparently
","
paginator
is
NOT
a
wrapper
for
all
boto3
class
list_
*
method
.
You
can
follow
up
the
issue
I
filed
in
boto3
github
.
According
to
the
member
","
you
can
call
build_full_result
after
paginate()
","
that
will
show
the
desire
behavior
.
BACKGROUND
:
The
AWS
operation
to
list
IAM
users
returns
a
max
of
50
by
default
.
Reading
the
docs
(
links
)
below
I
ran
following
code
and
returned
a
complete
set
data
by
setting
the
""""
MaxItems
""""
to
1000
.
http://boto3.readthedocs.io/en/latest/guide/paginators.html
https://boto3.readthedocs.io/en/latest/reference/services/iam.html#IAM.Paginator.ListUsers
QUESTION
:
If
the
""""
MaxItems
""""
was
set
to
10
","
for
example
","
what
would
be
the
best
method
to
loop
through
the
results
?
the
I
tested
with
the
following
but
it
only
loops
2
iterations
before
'
IsTruncated
'
=
=
False
and
results
in
""""
KeyError
:
'
Marker
'
""""
.
Not
sure
why
this
is
happening
because
I
know
there
are
over
200
results
.
THanks
.
I
see
you've
solved
the
indentation
issue
","
here's
another
version
of
your
example
:
You
need
to
adjust
the
paths
of
the
images
(
""""
background.jpg
""""
","
""""
player.jpg
""""
)
","
with
this
version
you're
not
loading
over
and
over
your
sprite
in
the
game
event
loop
and
the
Player
movement
will
be
smooth
.
PROBLEM
SOLVED
I
have
solved
the
problem
by
just
unindenting
this
part
from
the
FOR
loop
while
not
exitgame
:
I
have
added
an
Object
/
Image
on
the
main_screen
","
the
object
is
called
cancer_cell
.
What
I'm
trying
to
do
here
is
that
I
want
the
object
move
smoothly
.
I
have
to
repeatedle
press
on
the
arrow
keys
to
keep
it
moving
.
How
do
I
make
it
move
while
arrow
keys
are
pressed
?
here's
the
code
:
someone
told
me
to
try
the
solution
at
How
to
use
pygame.KEYDOWN
:
but
that
didn't
work
either
.
Or
maybe
i
did
it
wrong
:
I
have
a
Windows
7
computer
that
I
use
to
host
a
Jupyter
noteboks
for
use
across
my
local
network
.
However
","
I
would
like
to
be
able
to
access
these
notebooks
remotely
over
the
internet
.
Is
this
possible
?
I've
tried
researching
ways
but
haven't
managed
to
understand
anything
.
The
closest
I've
got
was
a
guide
on
how
to
set
it
up
using
AWS
(
https://gist.github.com/iamatypeofwalrus/5183133
)
but
the
key
step
in
this
is
to
set
up
a
""""
public
DNS
""""
.
However
","
I
haven't
found
how
to
set
this
up
in
Windows
.
Thanks
","
You
can
use
some
of
free
/
paid
VPN
solutions
to
make
virtual
network
between
you
and
jupyter-notebook
host
to
workaround
need
of
public
DNS
.
Hamachi
for
example
.
I'm
talking
about
case
when
you
just
want
to
access
jupyter
notebook
remotely
","
not
when
you
want
to
share
it
with
anyone
.
Python
3.5
on
Windows
.
I
have
a
9x2
range
in
Excel
that
I
want
to
read
into
an
array
in
Python
.
Excel
range
data
:
Python
code
:
Now
how
do
I
get
the
2nd
column
in
there
?
I've
also
tried
this
way
:
Same
thing
.
Thanks
You
are
trying
to
index
an
uninitialized
array
first
initialize
it
w
","
h
=
8
","
5
.
Matrix
=
[
[
0
for
x
in
range(w)
]
for
y
in
range(h)
]
#
initializing
Matrix
[0]
[6]
=
3
#
input
value
or
values
through
a
loop
print
Matrix
[0]
[6]
#
prints
the
element
or
elements
In
your
case
initialize
DrA
","
DrB
=
50
","
50
.
physicians
=
[
'
DrA
'
","
'
DrB
'
]
Read
up
list
comprehension
in
python
Save
Link
As
will
open
the
system
dialog
which
can
not
be
controlled
through
selenium
directly
.
Having
said
that
","
download
preferences
can
be
configured
in
the
profile
","
which
can
be
used
while
launching
the
browser
and
in
that
case
","
any
click
to
download
will
save
the
file
as
per
preferences
in
the
chrome
profile
Reference
:
https://sites.google.com/a/chromium.org/chromedriver/capabilities
As
per
java
you
can
use
following
code
for
clicking
save
as
link
and
click
save
in
new
window
As
per
your
question
you
need
python
code
so
you
can
convert
this
code
in
python
by
using
this
link
https://pypi.python.org/pypi/py_robot/0.1
Newbie
:
There
are
different
files
on
a
webpage
","
which
can
be
downloaded
as
follows
:
1
.
Right
click
on
a
file
link
2
.
Select
""""
Save
link
as
""""
3
.
Click
""""
Save
""""
button
on
the
new
window
.
I
tried
the
following
code(for first 2 steps)
","
but
it
is
not
working
:
Kindly
suggest
how
to
download
the
file
using
these
3
steps
.
Thanks
By
default
","
describe
only
works
on
numeric
dtype
columns
.
Add
a
keyword-argument
include='all
'
.
From
the
documentation
:
If
include
is
the
string
‘
all
’
","
the
output
column-set
will
match
the
input
one
.
To
clarify
","
the
default
arguments
to
describe
are
include=None
","
exclude=None
.
The
behavior
that
results
is
:
None
to
both
(
default
)
.
The
result
will
include
only
numeric-typed
columns
or
","
if
none
are
","
only
categorical
columns
.
Also
","
from
the
Notes
section
:
The
output
DataFrame
index
depends
on
the
requested
dtypes
:
For
numeric
dtypes
","
it
will
include
:
count
","
mean
","
std
","
min
","
max
","
and
lower
","
50
","
and
upper
percentiles
.
For
object
dtypes
(
e.g
.
timestamps
or
strings
)
","
the
index
will
include
the
count
","
unique
","
most
common
","
and
frequency
of
the
most
common
.
Timestamps
also
include
the
first
and
last
items
.
try
the
follwing
code
For
a
certain
Kaggle
dataset
(
rules
prohibit
me
from
sharing
the
data
here
","
but
is
readily
accessible
here
)
","
I
get
:
whereas
for
the
same
dataset
df_train.columns
gives
me
:
and
df_train.dtypes
gives
me
:
Am
I
missing
some
reason
why
pandas
only
describes
one
column
in
the
dataset
?
If
I
remember
correctly
","
there
should
be
a
specific
function
available
in
opencv
for
python
called
"pyopencv_to(pyobject, cv::mat, ... )"
","
which
also
takes
strides
into
the
account
and
can
handle
both
gray
and
color
images
.
Since
showNaoImage
returns
a
numpy
array
","
you
can
use
numpy's
C
API
to
extract
values
from
the
frame
","
or
obtain
the
pointer
to
the
memory
that
holds
the
values
.
See
the
documentation
","
specifically
the
part
that
deals
with
array
data
access
.
To
convert
the
data
to
a
Mat
array
accepted
by
cv::imshow
","
use
the
Mat
constructor
with
the
data
and
its
dimensions
.
For
example
:
I
have
a
c
+
+
facerecognition
code
and
a
python
code
in
opencv
.
In
python
code
i
read
frames
from
a
robot
and
i
want
to
send
this
fram
to
my
c
+
+
code
.
I
use
this
link
tho
call
python
function
in
c
+
+
function
.
my
c
+
+
function
is
embed.cpp
:
and
my
python
code
is
alvideo2.py
:
So
","
my
question
is
how
can
i
use
returnd
fram
in
"cv::imshow(""Original_image"
""""
","
presult
)
;
?
.
in
otherwise
how
can
i
convert
pyobject
to
Mat
?
thanks
alot
.
The
idea
is
to
convert
the
list
of
lists
to
lists
of
tuples
","
which
are
hashable
and
are
","
thus
","
candidates
of
making
sets
themselves
:
And
one
more
touch
:
ADDED
In
python
2.7
the
final
touch
is
simpler
:
Consider
two
lists
A
and
B
.
I
know
that
list(set(A)
-
set(B)
)
will
give
the
difference
between
A
and
B
.
What
about
the
situation
whereby
elements
in
both
A
and
B
are
lists
.
i.e.
A
and
B
are
list
of
list
?
For
e.g
.
I
wish
to
return
the
difference
A
-
B
as
a
list
of
list
i.e.
[
"[1,2]"
","
"[5,6]"
]
Here's
a
one-liner
you
could
use
:
Or
if
you
want
to
use
filter
:
Use
str.startswith
and
boolean
indexing
:
Another
solution
with
str.contains
:
Last
solution
with
filter
:
I
have
a
large
database
in
which
I
want
to
select
all
the
columns
that
meet
a
certain
criteria
:
My
data
looks
like
the
following
:
I
want
to
select
all
the
data
whose
Name
starts
with
target
.
So
the
selected
df
would
be
:
I
am
reading
the
data
using
:
How
can
I
select
the
data
I
want
?
You
may
unwrap
the
string
in
variable
for
each
feature
as
:
Now
","
you
may
check
for
each
feature
as
:
I
guess
you
could
do
something
like
:
Where
using
:
reversed
-
because
bits
are
read
from
LSB
to
MSB
so
we
need
to
iterate
from
right
to
left
.
enumerate
-
because
we
want
to
get
the
associated
index
to
check
in
our
mapping
what
is
the
feature
name
/
attributes
.
""""
""""
.
join(bits)
-
because
we
want
to
be
able
to
join
all
the
items
on
the
list
to
one
iterable
object
we
can
go
through
.
The
solution
depends
on
your
way
to
enumerate
the
features
.
if
you
enumerate
your
features
from
0
to
63
it
would
be
something
like
this
:
However
","
you
can
easily
adjust
this
method
to
match
other
enumerations
.
I
have
a
list
of
64
bits
.
Here
each
and
every
bit
represents
a
feature
.
If
a
bit
is
1
then
that
particular
feature
is
supported
else
not
.
Is
there
any
way
were
i
can
check
the
bit
and
just
return
its
associated
feature
if
it
is
1
.
Input
:
1111111
Output
:
If
all
bits
are
one
then
I
need
to
print
all
eight
features
masked
in
it
.
You
can
make
a
list
of
features
first
.
Then
make
a
function
.
Then
use
a
loop
to
get
features
for
each
bit
string
.
Note
that
","
it
is
not
very
efficient
","
but
it
is
verbose
so
that
a
newbie
can
understand
.
I
don't
know
whether
I've
understood
exactly
:
)
You
can
redirect
the
output
of
the
solution
by
using
which
is
relative
to
the
top_dir
(
https://waf.io/apidocs/Context.html?highlight=top_dir#waflib.Context.top_dir
)
.
So
in
your
case
:
I
recently
wrote
this
simple
waf
build
script
:
but
the
problem
is
that
the
outputed
solution
file
is
at
the
root
of
the
project
(
where
wscript
is
)
;
Would
there
be
a
way
to
generate
the
ide
specific
files
into
another
directory
(
for
instance
","
ide
/
msvs
)
?
You
need
to
use
a
dictionary
","
not
an
array
:
I
read
the
name
and
the
points
of
each
person
from
a
database
and
I
need
to
have
something
like
that
:
I've
tried
this
:
but
I
got
this
error
In
python
I
have
2
three
dimensional
arrays
:
T
with
size
(
n
","
n
","
n
)
U
with
size
(
k
","
n
","
n
)
T
and
U
can
be
seen
as
many
2-D
arrays
one
next
to
the
other
.
I
need
to
multiply
all
those
matrices
","
ie
I
have
to
perform
the
following
operation
:
As
n
might
be
very
big
I
am
wondering
if
this
operation
could
be
in
some
way
speed
up
with
numpy
.
Carefully
looking
into
the
iterators
and
how
they
are
involved
in
those
dot
product
reductions
","
we
could
translate
all
of
those
into
one
np.einsum
implementation
like
so
-
I
have
a
3
tables
table
1
table
2
table
3
How
I
can
change
a
status
in
table
2
when
I
create
a
link
in
table
3
","
with
sqlalchemy
ORM
in
python
","
status
must
be
changed
when
link
in
table
3
created
and
also
must
be
changed
when
link
deleted
","
who
have
any
cool
and
simple
ideas
?
solved
problem
by
use
ORM
Events
See
the
following
other
question
for
the
resolution
to
this
problem
.
sqlplus
remote
connection
giving
ORA-21561
Effectively
the
client
requires
a
host
name
in
order
to
generate
a
unique
identifier
which
is
used
when
connecting
to
the
database
.
Ok
","
here
is
the
explanation
:
Oracle
has
a
funny
behavior
where
if
the
hostname
given
by
hostname
can't
be
resolved
","
it
will
fail
connecting
to
the
DB
.
Fortunately
","
in
Linux
","
one
can
override
the
DNS
entry
for
a
session
by
writing
an
alias
file
in
/
tmp
","
then
setting
the
environment
variable
HOSTALIASES
to
that
file
.
So
adding
this
code
to
my
function
help
to
generate
this
file
","
and
now
I
can
successfully
connect
:
Hope
it
can
help
someone
else
!
I'm
trying
to
connect
to
an
Oracle
DB
using
AWS
Lambda
Python
code
.
My
code
is
below
:
If
have
already
added
all
dependencies
in
""""
lib
""""
folder
","
thanks
to
AWS
Python
Lambda
with
Oracle
When
running
this
code
","
I'm
getting
:
DatabaseError
:
ORA-21561
:
OID
generation
failed
i've
tried
to
connect
using
IP
of
the
Oracle
server
and
the
name
:
same
error
.
Here
is
the
output
of
the
error
For
those
who
have
successfully
run
the
CX_Oracle
in
AWS
Lambda
Python
","
can
you
please
help
?
Thanks
I'm
not
sure
the
best
way
to
explain
it
so
I
created
an
example
picture
and
made
up
some
data
:
I
looked
at
this
post
and
know
I
need
to
use
some
forloop
template
stuff
:
Displaying
a
Table
in
Django
from
Database
The
parts
that
are
throwing
me
off
are
how
to
have
a
table
within
a
table
.
For
example
","
the
doctor
'
Bob
""""
has
3
different
patients
.
Bob
has
one
row
but
Bob's
patient
column
has
3
rows
within
it
.
I'm
having
trouble
finding
an
example
like
this
to
work
off
of
and
am
not
sure
how
to
approach
it
.
I
also
need
those
patients
to
link
to
the
individual
patient
pages
.
So
if
I
click
on
Bob's
patient
'
C755
'
I
will
be
directed
to
that
patient's
page
.
I
was
thinking
something
like
:
Thanks
for
any
help
.
Just
not
sure
the
best
way
to
approach
this
.
""""
A
'
related
manager
'
is
a
manager
used
in
a
one-to-many
or
many-to-many
related
context
.
""""
https://docs.djangoproject.com/en/1.10/ref/models/relations/
I
can't
see
how
your
relation
between
doctor
and
patient
is
","
but
supposing
it
is
something
like
this
:
You
can
easly
iterate
over
doctor.patient
using
:
If
you
don't
have
a
verbose_name
attribute
for
the
relation
","
then
instead
of
using
doctor.patients.all
you
use
doctor.patients_set.all
.
And
to
the
patient's
page
","
you
can
use
a
method
inside
your
class
Patient
to
easly
do
this
:
""""
with
get_absolute_url
on
your
model
it
can
make
dealing
with
them
on
a
per-object
basis
much
simpler
and
cleaner
across
your
entire
site
""""
https://godjango.com/67-understanding-get_absolute_url/
Like
Avihoo
Mamka
mentioned
in
the
comment
you
need
to
provide
some
extra
request
headers
to
not
get
rejected
by
this
website
.
In
this
case
it
seems
to
just
be
the
User-Agent
header
.
By
default
scrapy
identifies
itself
with
user
agent
""""
Scrapy
/
{
version}(+http
:
/
/
scrapy.org
)
""""
.
Some
websites
might
reject
this
for
one
reason
or
another
.
To
avoid
this
just
set
headers
parameter
of
your
Request
with
a
common
user
agent
string
:
You
can
find
a
huge
list
of
user-agents
here
","
though
you
should
stick
with
popular
web-browser
ones
like
Firefox
","
Chrome
etc.
for
the
best
results
You
can
implement
it
to
work
with
your
spiders
start_urls
too
:
I'm
new
to
scrapy
and
I
made
the
scrapy
project
to
scrap
data
.
I'm
trying
to
scrapy
the
data
from
the
website
but
I'm
getting
following
error
logs
I'm
trying
following
command
then
on
website
console
then
I
got
the
response
but
when
I'm
using
same
path
inside
python
script
then
I
got
the
error
which
I
have
described
above
.
Commands
on
web
console
:
Please
help
me
.
Thanks
Use
a
generic
type
to
indicate
that
you'll
be
returning
an
instance
of
cls
:
Any
subclass
overriding
the
class
method
but
then
returning
an
instance
of
a
parent
class
(
TrivialClass
or
a
subclass
that
is
still
an
ancestor
)
would
be
detected
as
an
error
","
because
the
factory
method
is
defined
as
returning
an
instance
of
the
type
of
cls
.
The
bound
argument
specifies
that
T
has
to
be
a
(
subclass
of
)
TrivialClass
;
because
the
class
doesn't
yet
exist
when
you
define
the
generic
","
you
need
to
use
a
forward
reference
(
a
string
with
the
name
)
.
See
the
Annotating
instance
and
class
methods
section
of
PEP
484
.
Note
:
The
first
revision
of
this
answer
advocated
using
a
forward
reference
naming
the
class
itself
as
the
return
value
","
but
issue
1212
made
it
possible
to
use
generics
instead
","
a
better
solution
.
Given
a
class
with
a
helper
method
for
initialization
:
Is
it
possible
to
annotate
the
return
type
of
the
from_int
method
?
I'v
tried
both
cls
and
TrivialClass
but
PyCharm
flags
them
as
unresolved
references
which
sounds
reasonable
at
that
point
in
time
.
I
have
this
code
which
scrapes
usernames
:
When
it
starts
putting
all
the
names
in
a
list
like
this
After
an
hour
off
running
I
get
memory
allocation
errors
","
obviously
I
know
why
this
happens
but
how
can
I
fix
it
?
I
was
thinking
just
getting
the
usernames
from
a
single
page
and
output
them
to
file
immediately
","
delete
contents
of
list
","
repeat
","
but
I
didn't
know
how
to
implement
this
.
You
can
use
Python
Resource
Library
to
increase
your
process
allocated
memory
as
threads
of
a
process
use
memory
of
their
parent
process
they
cannot
allocate
extra
memory
.
The
code
you
use
keeps
all
the
downloaded
documents
in
memory
for
two
reasons
:
you
return
a.string
","
which
is
not
just
a
str
but
a
bs4.element.NavigableString
and
as
such
keeps
a
reference
to
its
parent
and
ultimately
to
the
whole
document
tree
.
you
return
a
generator
expression
","
which
will
capture
the
local
context
(
in
this
case
the
soup
)
until
it
is
used
.
One
way
to
fix
this
would
be
to
use
:
This
way
no
references
to
the
soup
objects
are
kept
","
and
the
expression
is
executed
immediately
and
a
list
of
strs
returned
.
As
this
answer
suggests
you
have
to
use
I
want
to
do
an
automation
of
the
Internet
Explorer
.
Open
the
Internet
Explorer
","
navigate
to
login.live.com
and
set
a
value
into
the
email
textbox
.
Here's
the
simple
script
:
The
last
line
always
returns
the
following
TypeError
:
getElementById()
takes
exactly
1
argument
(
2
given
)
When
I
try
to
add
the
value
through
the
console
of
the
Internet
Explorer
it
works
.
Btw
.
the
getElementsByTagName()
method
works
without
any
Errors
.
Thanks
for
any
help
!
Okay
.
.
I
wrote
a
workaround
for
this
:
For
browser
automation
I
recommend
to
use
the
Selenium
library
.
The
traceback
does
not
show
rpy2
code
involved
","
and
the
details
of
regression_test.py
are
not
known
.
I
have
seen
reports
of
unicode-related
issues
with
Spyder
.
Can
you
try
with
Python
3
instead
of
Python
2
?
Python
3
is
making
the
handling
of
string
a
little
more
consistent
","
and
it
might
solve
the
issue
.
I've
installed
rpy2
in
Anaconda
.
This
is
the
code
I
run
:
I
encouter
error
(
I've
changed
username
to
""""
myuser
""""
)
:
My
software
versions
:
Anaconda
4.0.0
(
64-bit
)
Python
2.7.11
R
3.3.1
IPython
4.1.2
Does
anybody
know
how
to
deal
with
it
?
I
got
it
working
with
however
","
how
would
I
edit
my
code
so
it
can
read
text
with
characters
like
""""
Ã
±
""""
and
parse
/
write
them
without
getting
errors
about
decode
problems
?
I'm
currently
reading
a
config
file
getting
all
values
from
a
specific
section
and
printing
them
however
","
instead
of
printing
them
I
would
like
to
create
a
new
config
file
with
just
that
section
I
am
reading
.
How
would
I
go
about
this
?
code
config
ini
Also
while
I'm
here
","
I'm
not
sure
how
I
would
get
this
to
work
with
encoded
strings
like
""""
ñ
""""
as
it
errors
","
however
I
need
my
new
config
file
exactly
how
I'm
reading
it
.
I
am
using
pyhs2
to
query
hive
through
python
but
I
cannot
set
the
queue
inside
the
query
.
I
want
to
set
the
queue
to
adhoc
and
when
I
try
to
put
the
queue
inside
the
query
:
The
second
part
of
the
query
doesn't
get
executed
Apparently
it
is
not
possible
to
run
multiple
statements
with
pyhs2
therefore
you
are
unable
to
set
the
mapred
queue
name
.
Pyhs2
is
no
longer
maintained
and
the
readme
advises
to
use
PyHive
which
supports
these
kind
of
operations
.
Well
","
maybe
What
Angelo
says
is
not
really
true
.
You
can
set
it
in
different
context
like
this
.
I
hope
you
got
the
idea
.
Your
command
contains
a
process
substitution
","
but
Popen
runs
its
command
using
/
bin
/
sh
.
When
run
as
/
bin
/
sh
","
though
","
bash
does
not
allow
process
substitutions
.
You
can
explicitly
request
that
the
command
be
run
with
bash
using
the
executable
option
.
I'm
trying
to
execute
a
shell
command
via
Python
code
","
but
I'm
not
capable
to
understand
why
it
is
failing
.
When
printing
the
command
and
pasting
it
to
the
shell
to
try
executing
it
directly
works
perfectly
fine
","
that's
the
strange
part
.
From
Python
I'm
getting
the
following
:
Is
there
anything
I'm
missing
?
My
code
looks
like
this
","
where
'
cmd
'
is
the
string
with
the
command
.
The
OS
is
a
CentOS
with
a
bash
shell
:
How
does
string
formats
'
%
'
operator
is
implemented
in
CPython
2.7
?
Can't
find
any
reference
in
the
Python
documentation
.
Well
","
I
fact
I
found
the
topic
:
String
conversion
and
formatting
","
for
PyOS_snprintfand
PyOS_vsnprintf
.
But
not
sure
it
match
my
question
.
See
also
my
question
Optional
keys
in
string
formats
using
'
%
'
operator
?
The
implementation
is
in
the
PyString_Format
that
is
called
by
the
string_mod
function
in
Objects
/
stringobject.c
.
The
latter
in
turn
is
a
slot
method
stored
in
PyString_Type->tp_as_number->nb_remainder
.
The
functions
PyOS_*snprintf
are
not
really
related
to
the
implementation
of
str.__mod__
.
In
Django
is
it
possible
to
create
through
dictionary
similar
as
to
filtering
?
Here
is
my
models.py
:
Here
is
my
views.py
:
However
the
code
above
will
throw
an
error
.
What
I
really
am
trying
to
do
is
to
do
a
create
function
coming
from
a
""""
request.POST
""""
but
my
views
above
serves
as
a
simple
example
on
doing
it
Simply
unwrap
the
dictionary
within
create
function
like
:
I
am
using
the
below
python
code
to
read
data
from
a
url
.
The
curl
command
from
unix
works
.
But
when
i
try
to
store
the
returned
json
in
a
python
variable
","
it
is
always
blank
.
Any
pointers
?
I
do
see
the
output
on
the
Spyder
Console
","
but
never
in
the
variable
.
EDIT
:
My
environment
details
.
I
am
on
Windows
7
","
executing
the
command
from
Anaconda
Spyder
IDE
.
My
Bad
:
(
Actually
","
I
am
on
a
secure
connection
behind
firewall
.
Hence
","
i
have
to
set
the
proxy
before
i
make
the
call
.
That's
why
I
should
take
a
break
.
:
-
/
You
can
use
the
subprocess
PIPE
to
capture
the
stdout
and
stderr
","
like
so
:
As
@MartijnPieters
mentioned
","
dictionaries
lack
indices
","
but
they
do
have
keys
.
The
following
function
pred()
(
short
for
""""
predecessor
""""
)
returns
the
key
of
the
item
whose
duration
is
the
largest
duration
<
=
the
passed
duration
.
It
returns
None
(
which
can
be
tested
for
)
if
the
passed
duration
is
smaller
than
all
of
the
durations
in
the
dictionary
:
For
example
","
if
(
by
the
way
-
-
don't
use
dict
as
an
identifier
since
it
has
a
predefined
meaning
in
Python
)
then
:
Note
that
On
Edit
:
Here
is
another
variation
on
the
same
idea
","
one
that
returns
an
integer
rank
:
Then
:
Final
Edit
:
At
the
cost
of
some
readability
","
here
is
a
1-liner
:
Lets
say
I
have
a
time
dict
and
how
can
I
get
the
""""
rank
""""
value
from
the
dict
with
a
new
duration
number
","
lets
say
133.92
.
It
should
return
dict
index
1
","
since
the
top
list
is
:
auth-10|duration:132.72
auth-4|duration:144.59
auth-9|duration:154.92
and
133.92
is
bigger
than
132.72
","
but
less
than
144.59
or
154.92
I'm
sorry
if
Its
unclear
explained
","
but
I
tried
my
best
.
EDIT
:
I'm
trying
it
again
:
I
need
a
function
which
returns
the
""""
predicted
/
rank
""""
for
a
custom
duration
from
the
sorted
list
/
sorted
by
""""
duration
/
DESC
""""
.
So
a
duration
of
160
would
return
the
last
place
","
which
is
4
.
(
index+1
)
.
A
120
duration
should
return
me
the
first
position
which
is
index
0
or
1st
(
index+1
)
.
I
just
installed
Google
app
engine
for
Python
2.7
and
I
wrote
some
code
for
test
.
Just
a
simple
HTML
form
.
Here
is
the
code
:
And
then
I
tried
to
write
a
separate
procedure
that
writes
out
my
form
","
like
so
:
Well
","
the
thing
is
that
the
first
code
is
working
fine
","
but
the
second
one
is
returning
the
HTTP
error
500
.
I
tried
this
out
from
a
course
on
Udacity
and
I
simply
copied
the
code
from
there
.
I
really
don't
know
why
it's
not
working
.
PS
.
I
see
this
message
in
terminal
(
Linux
)
:
""""
IndentationError
:
unindent
does
not
match
any
outer
indentation
level
INFO
2016-08-29
12:17:37
","
155
module.py:788
]
default
:
""""
GET
/
HTTP
/
1.1
""""
500
-
""""
Later
Edit
:
I
solved
this
by
simply
writing
the
""""
write_form
""""
procedure
after
the
""""
get
""""
procedure
inside
the
MainPage
class
.
You
have
probably
mixed
up
tabs
and
spaces
.
See
this
answer
for
more
information
and
hints
on
how
to
fix
it
:
https://stackoverflow.com/a/3920674/3771575
You
could
use
curses
for
this
.
I
would
like
to
understand
how
to
reprint
multiple
lines
in
Python
3.5
.
This
is
an
example
of
a
script
where
I
would
like
to
refresh
the
printed
statement
in
place
.
What
I
am
trying
to
do
is
have
:
Write
on
top
of
/
update
/
refresh
:
The
values
of
each
line
will
change
each
time
.
This
is
effectively
giving
me
a
status
update
of
each
Line
.
I
saw
another
question
from
5
years
ago
however
with
the
addition
of
the
end
argument
in
Python
3
+
print
function
","
I
am
hoping
that
there
is
a
much
simpler
solution
.
If
you
want
to
clear
the
screen
each
time
you
call
print()
","
so
that
it
appears
the
print
is
overwritten
each
time
","
you
can
use
clear
in
unix
or
cls
in
windows
","
for
example
:
If
I've
understood
correctly
you're
looking
for
this
type
of
solution
:
This
solution
won't
work
with
some
software
like
IDLE
","
Sublime
Text
","
Eclipse
...
The
problem
with
running
it
within
this
type
of
software
is
that
clear
/
cls
uses
ANSI
escape
sequences
to
clear
the
screen
.
These
commands
write
a
string
such
as
""""
\
033
[
[
80;j
""""
to
the
output
buffer
.
The
native
command
prompt
is
able
to
interpret
this
as
a
command
to
clear
the
screen
but
these
pseudo-terminals
don't
know
how
to
interpret
it
","
so
they
just
end
up
printing
small
square
as
if
printing
an
unknown
character
.
If
you're
using
this
type
of
software
","
one
hack
around
could
be
doing
print('\n' * 100)
","
it
won't
be
the
optimal
solution
but
it's
better
than
nothing
.
[
EDIT
:
Code
is
revised
to
show
my
attempt
at
doing
Granitosaurus
'
suggestion
]
I
need
to
execute
some
predefined
searches
and
crawl
the
results
1
level
deep
","
then
parse
those
pages
.
This
has
to
be
done
sequentially
because
the
site
will
not
allow
results
to
be
crawled
unless
those
results
were
just
retrieved
pursuant
to
a
search
.
My
predefined
searches
are
in
a
list
and
I'm
trying
to
loop
through
that
list
","
but
I
cannot
get
it
to
work
.
Scrapy
either
jumps
all
around
or
loops
through
all
the
start_requests
without
parsing
each
as
it
goes
.
Can
anyone
show
me
a
way
to
do
this
?
Thanks
!
The
search
terms
are
simply
pairs
of
date
ranges
which
I've
stored
in
a
list
at
the
beginning
.
Here's
my
[
NEWLY
EDITED
]
code
with
my
non-working
loop
commented
out
:
When
this
runs
","
it
loops
through
the
loop
making
each
request
","
but
it
fails
to
call
self.parse
on
each
loop
(
along
with
all
the
callables
that
run
in
turn
","
after
self.parse
)
.
After
the
loop
","
it
parses
a
single
set
of
search
results
","
then
acts
like
it's
finished
.
Here's
the
pastebin
log
of
it
running
from
PyCharm
.
Any
ideas
?
Seems
like
you
are
being
restricted
by
the
website
session
.
What
you
could
try
is
to
spawn
multiple
sessions
in
start_requests()
and
then
for
each
session
yield
unique
search
query
.
Something
like
:
First
","
an
aside
:
the
code
you
show
contains
an
error
on
line
9
","
which
is
not
indented
legally
.
Execution
begins
with
line
19
","
which
calls
"towers(3,...)"
","
which
proceeds
up
to
line
12
","
where
it
calls
"towers(2,...)"
","
which
proceeds
up
to
line
12
","
where
it
calls
"towers(1,...)"
","
which
prints
something
and
returns
.
When
it
returns
","
execution
resumes
in
the
"towers(2,...)"
invocation
","
and
resumes
at
line
16
","
as
you
observed
.
Please
can
someone
explain
why
this
code
completes
the
recursive
call
on
line
12
with
n
decrementing
to
1
","
then
does
another
call
to
n
=
2
again
and
then
move
to
line
16
after
?
I
have
been
using
python
tutor
and
am
trying
to
understand
each
step
and
why
the
algorithm
works
.
First
of
all
","
your
code
is
not
perfectly
alright
.
Here
is
the
changed
towers
function
for
you
->
And
here
goes
the
explanation
.
Look
at
the
picture
->
By
calling
"Movetower(3,a,b,c)"
","
you
intend
to
move
all
the
3
discs
from
tower
A
to
tower
B
.
So
the
sequential
calls
are
->
Hope
it
helps
:
)
You
can
also
find
some
good
explanations
here
:
Tower
of
Hanoi
:
Recursive
Algorithm
For
Animation
:
https://www.cs.cmu.edu/~cburch/survey/recurse/hanoiex.html
I
have
a
2D
array
","
a
","
comprising
a
set
of
100
x
","
y
","
z
coordinates
:
I
would
like
to
create
a
3D
binary
image
","
b
","
with
101
pixels
in
each
of
the
x
","
y
","
z
dimensions
","
of
coordinates
ranging
between
0.00
and
1.00
.
Pixels
at
locations
defined
by
a
should
take
on
a
value
of
1
","
all
other
pixels
should
have
a
value
of
0
.
I
can
create
an
array
of
zeros
of
the
right
shape
with
b
=
"np.zeros((101,101,101)"
)
","
but
how
do
I
assign
coordinate
and
slice
into
it
to
create
the
ones
using
a
?
You
could
do
something
like
this
-
Runtime
on
the
assignment
part
-
Let's
use
a
bigger
grid
for
timing
purposes
.
First
","
start
off
by
safely
rounding
your
floats
to
ints
.
In
context
","
see
this
question
.
Next
","
assign
those
indices
in
b
to
1
.
But
be
careful
to
use
an
ordinary
list
instead
of
the
array
","
or
else
you'll
trigger
the
usage
of
index
arrays
.
It
seems
as
though
performance
of
this
method
is
comparable
to
that
of
alternatives
(
Thanks
@Divakar
!
:
-
)
I
created
a
small
example
with
size
10
instead
of
100
","
and
2
dimensions
instead
of
3
","
to
illustrate
:
Well
","
the
reason
the
list
that
find_all
returns
is
empty
is
because
that
data
is
generated
with
a
separate
call
that
isn't
completed
by
just
sending
a
GET
request
to
that
URL
.
If
you
look
through
the
Network
tab
on
Chrome
/
Firefox
and
filter
by
XHR
","
by
examining
the
requests
and
responses
of
each
network
action
","
you
can
find
what
you
URL
you
ought
to
be
sending
the
GET
request
too
.
In
this
case
","
it's
"https://query2.finance.yahoo.com/v10/finance/quoteSummary/AAPL?formatted=true&crumb=8ldhetOu7RJ&lang=en-US&region=US&modules=defaultKeyStatistics%2CfinancialData%2CcalendarEvents&corsDomain=finance.yahoo.com,"
as
we
can
see
here
:
So
","
how
do
we
recreate
this
?
Simple
!
:
This
will
return
the
JSON
response
as
a
dict
.
From
there
","
navigate
through
the
dict
until
you
find
the
data
you're
after
:
I'm
trying
to
pull
information
from
the
'
Key
Statistics
'
page
for
a
ticker
in
Yahoo
(
since
this
isn't
supported
in
the
Pandas
library
)
.
Example
for
AAPL
:
Edit
:
thanks
Andy
!
Question
:
This
is
printing
an
empty
array
.
How
do
I
change
my
findAll
to
return
598.56B
?
How
i
can
get
the
specific
column
data
from
the
csv
file
based
on
column
name
.
please
suggest
me
to
write
the
python
script
to
get
the
same
.
i
basically
suggest
you
look
around
the
sites
before
asking
questions
like
this
since
there
are
many
answers
there
to
satisfy
your
requirement
have
check
out
this
link
Read
specific
columns
from
a
csv
file
with
csv
module
?
i
would
suggest
you
to
use
pandas
for
this
questions
and
you
could
get
the
column
using
i
took
this
example
from
pandas
home
page
give
credits
to
them
for
building
such
a
fascinating
library
.
You
can
use
filter
:
For
reference
","
a
straight
forward
list
comprehension
(
loop
)
answer
:
basically
the
same
speed
as
the
filter
approach
:
But
this
is
a
toy
example
","
so
timings
aren't
meaningful
.
If
a
wasn't
already
an
array
","
these
list
approaches
would
be
faster
than
the
array
ones
","
due
to
the
overhead
of
creating
arrays
.
There
are
some
numpy
set
operations
","
but
they
work
with
1d
arrays
.
We
can
get
around
that
by
converting
2d
arrays
to
1d
structured
.
There
is
a
version
of
this
using
np.void
","
but
it's
easier
to
remember
and
play
with
this
'
i
","
i
'
dtype
.
So
this
works
:
but
it
is
much
slower
than
the
iterations
:
As
discussed
in
other
recent
union
questions
","
np.in1d
uses
several
strategies
.
One
is
based
on
broadcasting
and
where
.
The
other
uses
unique
","
concatenation
","
sorting
and
differences
.
A
broadcasting
solution
(
yes
","
it's
messy
)
-
but
faster
than
in1d
.
First
off
","
convert
the
set
to
a
NumPy
array
-
Then
","
based
on
this
post
","
you
would
have
three
approaches
.
Let's
use
the
second
approach
for
efficiency
-
Sample
run
-
A
one
line
solution
using
a
list
comprehension
:
I
have
a
numpy
array
","
for
example
:
and
I
also
have
a
set
I
want
to
find
the
index
of
vectors
that
exist
in
set
b
","
here
is
but
I
use
a
for
loop
to
implement
this
","
is
there
a
convinient
way
to
do
this
job
avoiding
for
loop
?
The
for
loop
method
I
used
:
I
am
currently
running
a
Flask
+
SQLAlchemy
+
uWSGI
+
nginx
stack
for
an
API
backend
that
I
am
developing
for
my
application
.
I
was
attempting
to
see
what
is
the
maximum
amount
of
concurrent
connection
I
can
have
on
my
server
by
using
ApacheBench
and
sending
different
amounts
of
concurrent
requests
to
an
endpoint
on
the
server
.
This
endpoint
would
take
a
JSON
request
body
","
extract
certain
values
","
run
a
query
","
then
return
a
JSON
response
based
on
the
results
of
the
query
.
I
ran
a
base
test
of
1
concurrent
request
for
10
requests
","
and
I
got
an
average
response
time
of
60
milliseconds
.
Running
another
test
with
10
concurrent
requests
for
100
requests
returned
an
average
of
150ms
","
100
concurrent
requests
for
1000
returned
1500ms
and
500
concurrent
requests
returned
around
7000-9000ms
.
The
latency
seems
to
be
linearly
increasing
which
makes
sense
","
but
it
seemed
to
increase
TOO
quickly
.
After
doing
a
lot
of
tinkering
and
profiling
","
I
found
that
the
bottleneck
was
the
queries
.
At
the
start
of
the
benchmark
","
the
queries
would
process
and
return
quickly
under
10-50ms
","
but
it
quickly
increased
and
in
some
cases
latencies
of
10000-15000
ms
were
being
seen
.
I
couldn't
figure
out
why
the
db
was
slowing
down
so
much
especially
since
it
is
empty
(
barring
test
data
)
.
I
tried
running
the
application
WITHOUT
a
connection
pool
","
and
the
results
showed
that
latency
went
down
(
7-9s
to
5-6s
)
.
I
did
not
think
this
was
possible
because
everything
I
read
suggested
having
a
connection
pool
will
always
make
things
faster
because
you
avoid
the
overhead
of
establishing
a
new
connection
every
time
you
make
a
request
.
I
also
experimented
with
INCREASING
the
connection
pool
size
(
from
the
default
5
to
50
)
","
and
that
decreased
latency
even
more
than
the
pool-less
setup(5-6s to 3-4s)
.
The
latency
is
still
extremely
high
(
3-4
seconds
for
an
API
seems
unreasonable
by
any
standard
)
","
and
I
am
trying
to
figure
out
how
I
can
decrease
it
even
more
.
Is
the
answer
just
more
connections
?
Note
:
I
am
running
4
uWSGI
processes
with
100
threads
each
on
a
server
with
4GB
ram
and
a
quad
core
processor
.
I
am
using
the
psycopg2-cffi
adapter
for
the
connection
","
and
the
application
is
running
on
PyPy
.
The
linear
increase
is
very
much
normal
","
if
the
database
has
to
process
your
queries
sequentially
.
Essentially
","
all
concurrent
requests
get
started
at
the
same
time
","
but
finish
one
after
another
","
so
","
assuming
a
pool
with
a
single
connection
","
60ms
per
request
","
and
10
concurrent
requests
","
you're
going
to
see
requests
taking
60ms
","
120ms
","
180ms
","
240ms
","
300ms
","
360ms
","
420ms
","
480ms
","
540ms
","
600ms
","
600ms
","
...
","
600ms
","
540ms
","
480ms
","
...
.
We
can
calculate
how
much
time
it
takes
for
the
average
request
","
given
n
requests
and
m
concurrent
requests
:
These
numbers
are
similar
to
what
you
are
seeing
.
Now
comes
the
big
question
.
Why
does
the
database
process
queries
almost
sequentially
?
I
can
think
of
a
few
reasons
:
Locking
:
each
started
transaction
needs
to
lock
some
resource
exclusively
so
only
one
(
or
few
)
transaction
can
run
at
a
time
CPU-bound
query
:
each
transaction
takes
significant
CPU
resources
so
other
transactions
must
wait
for
CPU
time
Large
table
scans
:
the
database
cannot
keep
the
entirety
of
the
table
in
memory
and
therefore
must
read
from
disk
for
every
transaction
How
do
you
fix
this
?
This
is
a
complicated
question
","
but
a
few
potential
solutions
:
Optimize
your
queries
;
either
optimize
it
so
that
they
don't
all
fight
for
the
same
resource
","
or
","
optimize
it
so
that
they
don't
take
as
long
Batch
your
queries
so
that
you
need
to
run
fewer
in
total
Cache
your
responses
so
that
they
don't
hit
the
database
at
all
So
currently
I
have
something
like
this
:
Model
:
Admin
:
I
create
these
objects
in
code
-
meaning
I
set
the
report
object
.
But
what
I
would
like
in
the
Django
admin
is
if
I
could
allow
a
user
to
edit
that
report
object
but
only
the
one
set
.
They
would
be
allowed
to
change
it
(
so
hopefully
the
drop
down
menu
would
no
longer
be
there
)
so
the
nice
pencil
icon
would
still
be
there
","
but
things
like
that
""""
+
""""
icon
would
be
gone
.
And
this
is
not
to
say
the
user
can't
edit
all
reports
","
it's
just
that
in
the
ConfirmEmail
Admin
they
can
only
view
that
specific
report
attached
to
it
.
I've
been
smacking
away
at
this
and
can't
seem
to
get
it
work
.
I
would
also
be
inclined
to
just
have
the
current
Report
Form
embedded
into
the
ConfirmEmail
form
-
but
don't
know
how
I
would
go
about
doing
that
.
You
should
first
introduce
a
model
admin
for
your
Report
model
","
then
override
the
has_add_permission
function
of
your
ReportAdmin
.
You
can
also
remove
/
disable
the
+
button
using
javascript
in
the
page
","
but
be
aware
that
the
user
can
cause
damage
if
he
knows
the
add
url
","
or
disables
javascript
.
According
to
the
docs
on
rect
:
The
Rect
object
has
several
virtual
attributes
which
can
be
used
to
move
and
align
the
Rect
:
x
","
y
top
","
left
","
bottom
","
right
topleft
","
bottomleft
","
topright
","
bottomright
midtop
","
midleft
","
midbottom
","
midright
center
","
centerx
","
centery
size
","
width
","
height
w
","
h
I've
bolded
center
.
I
was
looking
through
a
pygame
tutorial
and
encountered
the
following
part
of
a
script
:
The
last
line
set
the
center
of
the
rect
at
(
200.150
)
I
looked
through
the
pygame
documentation
and
there
is
no
center
attribute
in
the
Rect
class
.
But
the
script
works
...
why
?
Consider
True
is
1
and
0
is
False
.
Since
a
is
set
to
False
(
a=False
in
first
statement
of
code
)
","
the
first
part
'
a==True
'
i.e.
0
=
=
1
will
return
0
(
False
)
.
Then
remaining
will
be
False
or
True
since
'
a==True
'
is
False
.
So
it
will
be
like
0
or
1
(
False
or
True
)
.
We
know
that
0
AND
0
=
0
1
AND
0
=
0
1
AND
1
=
1
0
OR
0
=
0
0
OR
1
=
1
1
OR
1
=
1
So
in
your
case
","
0
OR
1
will
result
to
1
i.e.
True
.
Summary
:
That's
why
""""
Hell
yeah
","
I'm
genius
""""
will
be
printed
.
Here
the
code
.
Output
is
'
Hell
yeah
","
I'm
genius
'
Anything
True
","
it
will
run
that
section
...
This
one
also
returns
""""
Hell
yeah
","
I'm
genius
""""
tab_change
can
show
their
id
and
names
","
but
not
correctly
.
When
Tab1
is
clicked
","
I
clicked
Tab2
but
it
still
print
0
Tab1
","
it
needs
one
more
click
to
print
1
Tab2
.
Tab2
click
to
Tab1
is
the
same
","
it
needs
one
more
click
to
show
the
current
selected
tab
.
I
want
to
find
why
the
tabs
need
double
click
?
And
how
can
I
get
selected
tab
correctly
by
a
single
click
?
Change
:
To
:
Because
unless
you
have
released
the
pressed
button
","
the
tab
hasn't
changed
!
Putting
your
customized
templates
in
templates
/
registration
(
not
register
)
should
work
.
At
least
if
your
TEMPLATES
setting
is
correctly
configured
:
https://docs.djangoproject.com/en/1.8/ref/templates/upgrading/
And
you
probably
already
checked
this
hint
from
the
project's
FAQ
?
I
want
to
use
custom
templates
","
but
django
keeps
using
the
admin
templates
instead
of
mine
!
To
fix
this
","
make
sure
that
in
the
INSTALLED_APPS
of
your
settings.py
the
entry
for
the
registration
app
is
placed
above
django.contrib.admin
.
I've
been
struggling
with
django
registration
redux
over
the
past
two
weeks
.
.
I'm
using
the
templates
the
were
provided
in
the
documentation
but
I've
made
a
couple
of
changes
like
adding
crispy
forms
and
changing
the
button
and
some
other
stuff
but
the
problem
is
that
none
of
these
changes
are
being
shown
on
http://127.0.0.1:8000/accounts/register
or
any
other
link
.
I'm
using
djang
registration
redux
1.4
","
django
1.8
","
python
2.7.10
Any
help
appreciated
","
thanks
Basically
I'm
trying
to
take
the
print
output
for
my
code
and
make
it
into
a
variable
.
I
tried
to
do
this
by
converting
it
to
a
string
but
it
gave
me
an
error
saying
""""
TypeError
:
str()
takes
at
most
3
arguments
(
6
given
)
""""
my
code
:
Is
there
either
a
way
to
convert
the
output
to
a
string
despite
the
""""
6
argument
""""
thing
or
some
other
""""
str()
""""
-
like
thing
that
would
work
?
*
x
supplies
much
too
many
arguments
to
str()
causing
the
error
","
and
sep
isn't
an
argument
of
str()
.
str()
expects
one
object
to
be
converted
","
and
*
x
gives
5
.
Try
this
:
This
will
go
through
the
list
and
join
each
element
together
with
an
underscore
between
.
Try
it
on
IDEOne
You
can
also
use
"map(function, sequence)"
to
shorten
the
join()
.
The
function
being
applied
is
str()
to
convert
the
numbers
into
string
form
","
and
to
the
x
list
.
Here's
with
map()
:
You
can
convert
the
list
of
ints
to
a
list
of
strings
using
map
","
then
use
join
to
bind
them
together
:
Note
:
Your
code
did
not
work
because
print
does
some
work
for
you
:
All
non-keyword
arguments
are
converted
to
strings
like
str()
does
and
written
to
the
stream
.
So
","
print
did
the
string
conversion
step
for
you
.
When
you
call
str()
","
it
expects
only
an
object
to
convert
to
a
string
","
with
two
optional
keyword
arguments
(
encoding
and
errors
)
.
By
calling
"str(*x, sep='_')"
","
you
are
passing
in
5
arguments
plus
a
separator
","
which
is
not
a
valid
call
.
I'm
trying
to
code
a
dictionary
based
logging
configuration
and
have
been
stumped
by
a
ValueError
that
occurs
when
I
run
the
program
.
I've
stripped
it
down
to
the
essentials
and
the
problem
remains
.
I've
read
the
3.5
docs
","
logging
HOWTO
","
Logging
Cookbook
","
etc.
but
unfortunately
","
the
solution
has
not
presented
itself
.
Any
help
would
be
appreciated
.
Also
","
I'm
only
3
weeks
into
python
so
I
may
just
be
out
of
my
depth
at
this
point
.
Here's
the
code
...
When
run
","
I
receive
the
follow
:
or
sometimes
this
...
I
suspect
that
the
different
errors
may
have
to
do
with
the
sometimes
changing
order
of
the
dictionary
?
Change
the
loggers
section
to
The
'
'
(
empty
string
)
refers
to
the
root
logger
.
you
can
add
more
loggers
for
different
components
:
I
am
trying
to
close
all
browser
instance
opened
by
a
test
case
in
one
go
that
is
immediately
when
the
test
fails
.
I
have
opened
more
than
one
instance
of
same
kind
that
is
i
am
trying
to
automate
chat
application
so
i
need
to
open
two
instance
of
same
browser
kind
.
But
once
the
test
fails
","
both
the
instance
needs
to
be
closed
","
but
my
test
closes
the
browser
for
whom
a
particular
step
is
failed
.
how
to
close
both
the
browser
instance
when
test
fails
for
instance
alone
.
driver.quit()
is
not
working
.
As
I
have
opened
browser
instance
with
different
driver
names
i.e.
and
What
about
calling
quit()
for
both
of
them
Or
for
more
generic
way
keep
them
in
list
and
iterate
over
it
.
I
am
using
pandas
0.17.0
and
have
a
df
similar
to
this
one
:
with
following
dtypes
:
When
I
reindex
my
df
to
minute
intervals
all
the
columns
int64
change
to
float64
.
Also
","
if
I
try
to
resample
df3
=
df.resample('Min')
The
int64
will
turn
into
a
float64
and
for
some
reason
I
loose
my
object
column
.
print
(
df3.dtypes
)
Since
I
want
to
interpolate
the
columns
differently
based
on
this
distinction
in
an
subsequent
step
(
after
concatenating
the
df
with
another
df
)
","
I
need
them
to
maintain
their
original
dtype
.
My
real
df
has
far
more
columns
of
each
type
","
for
which
reason
I
am
looking
for
a
solution
that
does
not
depend
on
calling
the
columns
individually
by
their
label
.
Is
there
a
way
to
maintain
their
dtype
throughout
the
reindexing
?
Or
is
there
a
way
how
I
can
assign
them
their
dtype
afterwards
(
they
are
the
only
columns
consisiting
only
of
integers
besides
NANs
)
?
Can
anybody
help
me
?
It
is
impossible
","
because
if
you
get
at
least
one
NaN
value
in
some
column
","
int
is
converted
to
float
.
But
if
use
parameter
fill_value
in
reindex
","
dtypes
are
not
changed
:
Better
is
use
method='ffill
in
reindex
:
If
use
resample
","
you
can
get
column
A
back
by
unstack
and
stack
","
but
unfortuntely
there
is
still
problem
with
float
:
I
try
modify
previous
answer
for
casting
to
`
int
:
In
Python
2
","
it's
O(n)
","
and
it
builds
a
new
list
.
In
Python
3
","
it's
O(1)
","
but
it
doesn't
return
a
list
.
To
draw
a
random
element
from
a
dict's
keys
","
you'd
need
to
convert
it
to
a
list
.
It
sounds
like
you
were
probably
using
random.choice(d.keys()
)
for
part
3
of
that
problem
.
If
so
","
that
was
O(n)
","
and
you
got
it
wrong
.
You
need
to
either
implement
your
own
hash
table
or
maintain
a
separate
list
of
elements
","
without
sacrificing
average-case
O(1)
insertions
and
deletions
.
I
came
across
a
question
when
I
solve
this
LeetCode
problem
.
Although
my
solution
got
accepted
by
the
system
","
I
still
do
not
have
any
idea
after
searching
online
for
the
following
question
:
What
is
the
time
complexity
of
dict.keys()
operation
?
Does
it
return
a
view
of
the
keys
or
a
real
list
(
stores
in
memory
)
of
the
keys
?
I'm
new
to
python
and
scrapy
.
I
want
to
scrap
data
from
website
.
The
web
site
uses
AJAX
for
scrolling
.
The
get
request
url
is
as
below
.
Please
help
me
how
I
can
use
scrapy
or
any
other
python
libraries
Thanks
.
Seems
like
this
AJAX
request
expects
a
correct
Referer
header
","
which
is
just
a
url
of
the
current
page
.
You
can
simply
set
the
header
when
creating
the
request
:
I
have
texts
like
I
want
to
remove
only
the
text
with
braces
and
angular
brackets
and
extract
in
between
data
.
I
tried
but
since
closing
braces
are
at
the
end
of
text
","
whole
text
is
getting
deleted
when
re.sub
is
used
.
How
to
take
the
only
data
in
between
multiple
braces
(
angular
or
flower
brackets
)
Regex
explanation
:
Debuggex
Demo
Regex
explanation
:
Debuggex
Demo
Read
the
section
'
Watch
Out
for
The
Greediness
!
'
at
http://www.regular-expressions.info/repeat.html
Try
non-greedy
regular
expressions
","
i.e.
I
want
to
do
something
like
this
.
Let's
say
we
have
a
tensor
A
.
And
I
want
to
get
nonzero
values
and
their
indices
from
it
.
There
are
similar
operations
in
Numpy
.
np.flatnonzero(A)
return
indices
that
are
non-zero
in
the
flattened
A
.
x.ravel()
[
np.flatnonzero(x)
]
extract
elements
according
to
non-zero
indices
.
Here's
a
link
for
these
operations
.
How
can
I
do
somthing
like
above
Numpy
operations
in
Tensorflow
with
python
?
(
Whether
a
matrix
is
flattened
or
not
doesn't
really
matter
.
)
You
can
achieve
same
result
in
Tensorflow
using
not_equal
and
where
methods
.
where
is
a
tensor
of
the
same
shape
as
A
holding
True
or
False
","
in
the
following
case
This
would
be
sufficient
to
select
zero
or
non-zero
elements
from
A
.
If
you
want
to
obtain
indices
you
can
use
wheremethod
as
follows
:
where
tensor
has
two
True
values
so
indices
tensor
will
have
two
entries
.
where
tensor
has
rank
of
two
","
so
entries
will
have
two
indices
:
I
am
trying
to
export
to
CSV
files
from
a
Jupyter
notebook
.
Even
when
I
test
examples
copy-pasted
from
the
documentation
(
see
below
)
","
I
get
a
""""
'
str
'
object
is
not
callable
""""
error
message
.
I
have
fiddled
endlessly
with
the
parameters
.
The
same
thing
happens
with
a
Pandas
dataframe
and
I
try
to
use
to_csv
.
Basically
:
yields
I'm
new
to
coding
","
so
I
don't
really
know
how
to
troubleshoot
past
this
point...can
anyone
help
?
The
error
you
are
seeing
can
only
be
explained
if
csv.writer
is
a
string
.
This
recreates
your
error
:
I'm
guessing
you
are
either
working
in
an
environment
like
Spyder
which
by
default
keeps
a
single
session
alive
for
code
to
run
in
","
or
you
are
experimenting
in
a
REPL
like
IPython
or
a
Jupyter
notebook
.
At
some
point
you
mistakenly
assigned
a
new
value
to
csv.writer
.
The
problem
will
go
away
if
you
reset
your
environment
","
since
there
is
nothing
wrong
with
your
code
.
To
reset
","
for
IPython
or
Spyder
just
exit
and
start
again
.
For
Jupyter
notebook
select
""""
Kernel|Restart
""""
from
the
menu
.
I'm
using
the
python
compiler
in
java
and
I
tried
import os
.
My
problem
is
when
I
couldn't
input
it
to
continue
the
next
line
while
it
just
sent
back
this
message
os
is
not
yet
implemented
in
skulpt
on
line
1
.
I
already
tried
with
another
app
and
still
get
a
same
result
.
I
tried
to
search
Google
but
no
result
.
I
read
in
learning
web
and
it's
never
seem
the
existence
of
this
error
.
What
is
skulpt
?
Please
help
me
To
find
which
standard
modules
the
most
recent
version
of
skulpt
supports
and
which
it
doesn't
","
get
a
copy
of
the
source
code
:
And
","
look
through
the
src
/
lib
directory
.
Every
successfully
implemented
module
will
be
a
directory
in
/
src
/
lib
.
os
is
not
one
of
those
directories
.
Module
os
is
not
implemented
in
skulpt
.
I
have
a
written
a
DLL
in
which
I'm
getting
one
of
the
paths
:
Something
is
being
done
in
the
code
to
get
this
path
.
And
now
","
the
python
script
that
I
have
written
to
retrieve
this
path
from
the
DLL
is
as
shown
:
But
the
value
that
I'm
able
to
see
is
just
the
first
character
of
the
path
instead
of
the
whole
string
.
Can
anybody
help
me
with
this
problem
?
The
reason
you
see
only
the
first
character
is
that
by
calling
c_char()
you
create
a
single
char
value
that
Python
treats
like
a
str
(
Python
2
)
object
or
bytes
(
Python
3
)
object
of
length
1
.
You
are
probably
lucky
that
you
do
not
get
a
segmentation
fault
.
By
writing
more
than
1
byte
or
a
NULL-terminated
string
of
length
>
0
(
e.g
.
with
strcpy
)
in
the
C
code
","
you
actually
produce
an
undetected
buffer
overflow
.
ctypes
does
not
know
how
many
bytes
you
have
written
at
the
pointer's
memory
location
.
path.value
is
still
a
str
/
bytes
of
length
1
.
It
would
be
better
to
change
the
C
pathinfo
function
into
someting
like
Use
ctypes.create_string_buffer()
to
allocate
memory
in
your
Python
code
and
let
pathinfo
return
the
length
of
the
result
.
Of
course
you
have
to
check
","
whether
char
*
path
is
large
enough
using
bsize
in
your
C-Code
.
The
Python-code
would
look
like
this
:
Also
be
aware
of
NULL-termination
in
the
C
domain
","
character
encodings
when
converting
python
strings
to
char
*
and
back
","
the
changes
regarding
str
/
bytes
in
ctypes
regarding
Python
2
and
Python
3
.
Haven't
done
it
","
but
have
done
the
following
:
To
pass
an
array
of
ints
","
for
example
","
use
this
:
Create
a
type
of
","
say
","
20
ints
:
Create
an
instance
:
And
then
you
can
pass
data
.
Extract
the
numbers
from
data
by
using
the
list
function
:
So
may
be
you
can
do
the
same
with
chars
:
and
then
:
In
the
case
of
c_char
array
there
is
no
need
to
use
the
list
function
","
but
this
works
:
What
could
be
regex
which
match
anystring
followed
by
daily
but
it
must
not
match
daily
preceded
by
m
?
For
example
it
should
match
following
string
beta.daily
abcdaily
dailyabc
daily
But
it
must
not
match
mdaily
or
abcmdaily
or
mdailyabc
I
have
tried
following
and
other
regex
but
failed
each
time
:
r
'
[
^
m
]
daily
'
:
But
it
doesn't
match
with
daily
r
'
[
^
m
]
?
daily
'
:
It
match
with
string
containing
mdaily
which
is
not
intended
Just
add
a
negative
lookbehind
","
(
?
<
!
m)d
","
before
daily
:
The
zero
width
negative
lookbehind
","
(
?
<
!
m
)
","
makes
sure
daily
is
not
preceded
by
m
.
Demo
I
am
using
Google's
Earth
Engine
API
to
access
LandSat
images
.
The
program
is
as
given
below
","
Load
a
landsat
image
and
select
three
bands
.
Create
a
geometry
representing
an
export
region
.
Export
the
image
","
specifying
scale
and
region
.
it
throws
the
following
error
.
Please
Help
.
I
am
unable
to
find
the
right
function
to
download
images
.
There
is
a
typo
in
your
code
","
Export
should
start
from
the
capital
letter
.
See
documentation
.
If
you
are
using
the
python
API
","
you
have
to
use
the
'
batch
'
submodule
.
The
default
behaviour
is
to
save
to
your
google
drive
.
You
can
specify
your
bounding
box
as
a
list
of
coordinates
as
well
:
This
should
generate
a
file
called
'
exportExample.tif
'
in
your
GoogleDrive
top
folder
.
Also
note
that
the
semicolons
at
the
end
of
each
line
are
not
necessary
in
python
.
To
build
on
Ben's
answer
","
you
can
also
use
:
from
your
original
post
","
but
add
the
following
line
beneath
it
so
the
coordinates
are
in
the
correct
format
for
the
task_config-->Region
field
:
It
prevents
a
""""
task_config
""""
formatting
mismatch
when
you
get
to
here
:
This
will
allow
you
to
use
the
given
function
from
the
API
","
but
it
will
extract
the
coordinates
in
such
a
way
that
you
can
use
them
in
the
approach
suggested
by
Ben
above
.
Solution
:
you
have
__all__
set
to
empty
list
i.e.
from
package
import
*
basically
imports
nothing
set
it
to
__all__
=
[
'
submodule
'
]
in
__init__.py
What
exactly
is
__all__
?
In
simplest
words
all
help
customizing
the
from
package
import
*
i.e.
with
all
we
can
set
what
will
be
imported
and
what
not
.
From
the
docs
:
The
public
names
defined
by
a
module
are
determined
by
checking
the
module’s
namespace
for
a
variable
named
all
;
if
defined
","
it
must
be
a
sequence
of
strings
which
are
names
defined
or
imported
by
that
module
.
The
names
given
in
all
are
all
considered
public
and
are
required
to
exist
.
If
all
is
not
defined
","
the
set
of
public
names
includes
all
names
found
in
the
module’s
namespace
which
do
not
begin
with
an
underscore
character
(
'
_
'
)
.
all
should
contain
the
entire
public
API
.
It
is
intended
to
avoid
accidentally
exporting
items
that
are
not
part
of
the
API
(
such
as
library
modules
which
were
imported
and
used
within
the
module
)
.
One
important
thing
to
note
here
is
-
Imports
without
*
are
not
affected
by
__all__
i.e.
Members
that
are
not
mentioned
in
__all__
are
accessible
from
outside
the
module
using
direct
import
-
from
<
module
>
import
<
member
>
.
An
Example
:
the
following
code
in
a
module.py
explicitly
exports
the
symbols
foo
and
bar
:
These
symbols
can
then
be
imported
like
so
:
If
you
define
__all__
","
then
only
the
attributes
mentioned
there
will
be
imported
via
*
","
while
the
excluded
ones
have
to
be
imported
explicitly
.
So
either
use
or
if
you
really
want
to
use
the
(
discouraged
!
)
from
package
import
*
","
declare
in
package
.
Note
how
tedious
it
will
become
to
keep
this
up-to-date
...
If
a
module
package
defines
__all__
","
it
is
the
list
of
module
names
that
are
imported
by
from
package
import
*
So
if
you
define
__all__
as
empty
list
","
from
package
import
*
will
import nothing
.
Try
defining
it
like
this
:
Also
note
that
you
don't
have
to
do
from
package
import
*
to
use
sub_module
You
can
also
just
do
:
The
Python
documentation
says
Consider
this
code
:
In
this
example
","
the
echo
and
surround
modules
are
imported
in
the
current
namespace
because
they
are
defined
in
the
sound.effects
package
when
the
from...import
statement
is
executed
.
(
This
also
works
when
__all__
is
defined
.
)
I
try
the
following
code
When
package
/
__init__.py
is
empty
","
the
code
works
fine
.
However
","
when
package
/
__init__.py
contains
__all__
=
[]
","
print(sub_module)
will
raise
NameError
.
What
is
(
This
also
works
when
all
is
defined
.
)
from
the
documentation
means
?
The
codes
:
In
main.py
:
When
package
/
__init__.py
is
empty
","
executing
python3
main.py
gets
<
module
'
package.sub_module
'
from
'
/
path
/
to
/
package
/
sub_module.py
'
When
package
/
__init__.py
contains
__all__
=
[]
","
executing
python3
main.py
gets
I
am
trying
to
make
a
link
in
the
sidebar
of
my
Person
model
as
persons
.
For
that
I
made
a
templatetags
folder
where
my
member_template_tags.py
:
and
my
view
file
:
my
person_list.html
:
and
in
base.html
","
sidebar
code
is
:
When
I
tried
it
gives
following
traceback
:
Template
error
:
Exception
Type
:
NoReverseMatch
at
/
person
/
Edit
:
When
I
use
The
same
code
gives
'
get_person_list
'
received
too
many
positional
arguments
errors
.
The
new
traceback
:
How
do
I
fix
this
error
and
make
link
in
the
sidebar
so
that
it
gives
the
full
view
of
person_list
template
?
Any
help
will
be
much
appreciated
.
There
are
too
many
issues
with
your
code
to
correctly
identify
what
is
the
problem
.
You
have
an
extra
comma
in
your
custom
tag
:
Your
context
variable
is
called
persons
","
yet
you
are
using
object_list
in
your
template
","
this
needs
to
be
corrected
.
Your
template
has
a
for
loop
","
but
its
missing
a
endfor
You
have
{
%
url
'
member:person-list
'
%
}
this
is
a
namespaced
URL
.
Make
you
have
set
it
up
correctly
in
your
urls.py
.
Your
tag
doesn't
take
any
arguments
","
yet
you
are
passing
it
a
context
variable
in
{
%
get_person_list
persons
%
}
.
Once
you
solve
all
these
","
you
have
to
figure
out
where
the
actual
error
is
coming
from
.
Chances
are
","
it
is
in
your
base
template
which
you
are
inheriting
as
its
obvious
you
don't
have
a
registration_register
url
tag
in
the
code
you
have
posted
.
I've
found
a
solution
","
the
environment
variables
(
including
python
path
)
should
be
defined
from
pycharm
:
Run
/
Debug
configurations
->
Environment
variables
.
Pycharm
won't
use
bashrc
paths
.
I
am
trying
to
run
my
code
in
the
server
using
ssh
remote
interpreter
.
The
connection
and
the
deployment
work
but
when
I
want
to
import libraries
located
in
the
server
it
gives
an
import error
ssh
:
/
/
*
*
*
@****.com:22
/
usr
/
bin
/
python
-
u
/
home
/
/
main.py
Traceback
(
most
recent
call
last
)
:
File
""""
/
home
/
/
main.py
""""
","
line
11
","
in
from
clplibs.clp
import ContinuousLearningPlatform
as
clp
ImportError
:
No
module
named
clplibs.clp
Process
finished
with
exit
code
1
Without
knowing
much
about
the
variables
in
your
code
","
the
error
indicates
that
at
model.classes_
is
1
or
more
dimensions
","
and
the
first
is
size
2
","
in
other
words
2
rows
/
classes
","
and
possibly
many
columns
.
ypred
is
probably
quite
large
","
and
np.argmax(ypred...)
is
the
index
of
its
largest
values
(
along
axis
0
)
","
i.e.
14328
.
Maye
the
correct
use
is
model.classes_
[
:
","
np.argmax
...
]
.
You
need
to
look
at
the
shape
of
ypred
","
andmodel.classes_
`
","
and
possibly
other
variables
in
this
area
.
The
above
code
is
giving
an
Index
error
saying
that
index
14328
is
out
of
bounds
for
axis
0
and
size
2
.
The
error
is
coming
at
this
line
ypredc
=
model.classes_
[
"np.argmax(ypred, axis = 0)"
]
Can
anyone
help
me
on
this
?
you
can
use
python
copy
library
:
I
have
a
class
inheriting
from
collections.OrderedDict
with
an
initializer
that
takes
a
positional
argument
without
default
.
My
goal
is
to
create
a
shallow
copy
of
the
instance
.
However
","
my
initial
naive
approach
below
does
not
work
and
raises
a
type
error
as
the
positional
argument
must
be
supplied
when
building
the
new
instance
.
A
solution
could
be
to
add
:
But
this
becomes
unwieldy
when
the
number
of
attribute
grows
.
Is
there
a
better
","
more
direct
solution
to
this
problem
?
EDIT
:
I
am
using
Python
3.5
I've
just
started
out
Python
and
was
trying
to
make
a
countdown
timer
and
make
it
unbreakable
as
possible
","
however
when
I
enter
blank
inputs
","
the
while
loop
won't
handle
it
","
and
this
message
would
show
up
instead
:
invalid
literal
for
int()
with
base
10
:
'
'
.
It
also
pointed
the
error
occurring
at
the
line
where
it
asks
for
count-down
.
Any
help
will
be
appreciated
.
A
bad
string
that
can
not
be
converted
to
an
int
","
will
raise
a
ValueError
.
You
catch
that
exception
and
just
repeat
the
prompt
like
this
:
As
soon
as
a
proper
integer
>
0
has
been
entered
the
while
will
break
.
This
is
because
you
do
re-assign
x
every
time
.
x
=
+
1
!
=
x
+
=
1
.
x
=
+
1
assigns
x
to
1
;
x
+
=
1
","
the
augmented
assignment
statement
","
increments
x
by
one
:
The
+
sign
in
the
assignment
operator
is
on
the
wrong
side
.
It
should
be
on
the
left
like
this
:
What
you're
doing
is
simply
assigning
x
to
the
value
of
+
1
over
and
over
;
it
will
just
keep
getting
assigned
to
1
.
I
am
a
huge
beginner
","
but
I
have
a
variable
that
has
a
value
","
and
I'd
like
to
change
it
within
a
function
","
so
that
outside
of
the
function
the
variable
is
permanently
changed
.
Searching
around
I've
found
some
information
on
how
to
access
outside
variables
(
I
think
their
called
global
variables
)
","
but
not
permanently
modifying
them
in
any
way
.
Here's
a
little
bit
of
code
to
represent
what
it
is
I'm
trying
to
do
:
The
idea
is
that
it
would
have
the
console
output
...
since
it
is
changing
the
global
x
variable
by
adding
one
to
it
.
But
instead
it
is
creating
a
new
global
variable
called
x
","
and
setting
it's
value
to
1
every
time
I
run
the
variableChanger()
function
","
and
I
just
end
up
with
...
What
I
want
to
do
is
access
the
variable
x
that's
at
the
top
of
the
code
and
increment
that
","
so
that
whenever
and
wherever
I
access
it
later
","
it
has
the
incremented
value
.
Is
there
a
way
to
do
that
?
Are
you
sure
it
shouldn't
be
Otherwise
check
if
you
have
this
dependency
installed
or
if
you
have
a
""""
elementtree
""""
folder
in
your
current
project
that
screws
with
the
import lookup
.
Sounds
like
missing
or
corrupt
lxml
package
to
me
.
Workarounds
are
in
:
Try
out
Taurus
Installer
Reinstall
lxml
and
/
or
bzt
packages
via
pip
.
Check
out
:
Taurus
->
Installing
and
Upgrading
->
Windows
Taurus
:
A
New
Star
in
the
Test
Automation
Tools
Constellation
for
detailed
setup
instructions
.
If
the
problem
persists
update
your
question
with
the
bzt.log
file
contents
.
When
I
run
the
code
as
bzt
test.yml
I
am
getting
the
error
.
Before
it
was
working
fine
.
I'm
having
trouble
with
using
impyla
library
on
windows
I
installed
impyla
library
pip
install
impyla
Error
occured
when
I
tried
to
import impyla
libary
in
python
code
Traceback
(
most
recent
call
last
)
:
...
File
""""
D
:
/
test
/
test.py
""""
","
line
14
","
in
from
impala.dbapi
import connect
File
""""
C:\Anaconda3\lib\site-packages\impala\dbapi.py
""""
","
line
28
","
in
import impala
.
hiveserver2
as
hs2
File
""""
C:\Anaconda3\lib\site-packages\impala\hiveserver2.py
""""
","
line
32
","
in
from
impala._thrift_api
import
(
File
""""
C:\Anaconda3\lib\site-packages\impala_thrift_api.py
""""
","
line
73
","
in
include_dirs
=
[thrift_dir]
)
File
""""
C:\Anaconda3\lib\site-packages\thriftpy\parser__init__.py
""""
","
line
30
","
in
load
include_dir=include_dir
)
File
""""
C:\Anaconda3\lib\site-packages\thriftpy\parser\parser.py
""""
","
line
496
","
in
parse
url_scheme
)
)
thriftpy.parser.exc.ThriftParserError
:
ThriftPy
does
not
support
generating
module
with
path
in
protocol
'
c
'
when
I
tried
to
print
include_dir
","
which
was
D
:
/
test\thrift
I
just
cannot
import libray
at
all
help
me
I
was
encountering
the
same
error
with
impyla
on
an
Anaconda
Python
3.6
distribution
on
Windows
.
Instead
of
installing
using
pip
","
I
was
able
to
get
it
working
using
:
https://anaconda.org/anaconda/impyla
I
had
the
same
problem
with
thriftpy
","
the
problem
on
windows
is
an
absolute
path
is
like
C:\foo\bar.thrift
But
","
the
way
the
thrift
library
parses
the
file
","
it
detects
the
C
:
as
if
it
were
a
protocol
like
http:
or
https:
Its
pretty
easy
to
workaround
you
just
have
to
strip
the
first
two
characters
from
the
path
with
a
slice
like
path
[
2
:
]
Just
slice
when
you
call
thriftpy.load
or
in
the
library
file
OR
You
can
go
a
bit
deeper
and
make
the
same
change
that
I
already
submitted
as
a
patch
on
the
github
page
...
perhaps
it
will
be
incorporated
in
the
next
version
of
thrift
.
My
justification
of
why
this
change
is
valid
is
in
the
pull
request
.
If
its
incorporated
then
you
wont
have
to
worry
about
making
the
same
change
again
when
you
update
the
library
.
If
not
then
just
strip
the
two
characters
again
.
The
easiest
answer
to
any
Python
in-memory
search-for
question
is
""""
use
a
dict
""""
.
Dicts
give
O(ln N)
key-access
speed
","
lists
give
O(N)
.
Also
remember
that
you
can
put
a
Python
object
into
as
many
dicts
(
or
lists
)
","
and
as
many
times
into
one
dict
or
list
","
as
it
takes
.
They
are
not
copied
.
It's
just
a
reference
.
So
the
essentials
will
look
like
At
the
end
of
this
loop
","
hotelsbyphone
[
""""
123456
""""
]
will
be
a
list
of
hotel
objects
which
had
""""
123456
""""
as
one
of
their
phone_search
strings
.
The
key
coding
feature
is
the
.
"setdefault(key, [])"
method
which
initializes
an
empty
list
if
the
key
is
not
already
in
the
dict
","
so
that
you
can
then
append
to
it
.
Once
you
have
built
this
index
","
this
will
be
fast
Alternatively
to
try
...
except
","
test
if
x
in
hotelsbyphone
:
My
database
consists
of
collection
of
a
large
no
.
of
hotels
(
approx
121
","
000
)
.
This
is
how
my
collection
looks
like
:
Each
document
can
have
1
or
more
phone
numbers
separated
by
""""
|
""""
delimiter
.
I
have
to
group
together
documents
having
same
phone
number
.
By
real
time
","
I
mean
when
a
user
opens
up
a
particular
hotel
to
see
its
details
on
the
web
interface
","
I
should
be
able
to
display
all
the
hotels
linked
to
it
grouped
by
common
phone
numbers
.
While
grouping
","
if
one
hotel
links
to
another
and
that
hotels
links
to
another
","
then
all
3
should
be
grouped
together
.
Example
:
Hotel
A
has
phone
numbers
1|2
","
B
has
phone
numbers
3|4
and
C
has
phone
numbers
2|3
","
then
A
","
B
and
C
should
be
grouped
together
.
This
is
one
code
I
wrote
in
Python
.
In
this
one
","
I
tried
performing
linear
search
in
a
for
loop
.
I
am
getting
some
errors
as
of
now
but
it
should
work
when
rectified
.
I
need
an
optimized
version
of
this
as
liner
search
has
poor
time
complexity
.
I
am
pretty
new
to
this
so
any
other
suggestions
to
improve
the
code
are
welcome
.
Thanks
.
You
need
django-reversion
+
django-reversion-compare
.
I
have
been
using
django-reversion
for
at
least
5
years
and
django-reversion-compare
is
just
an
add-on
for
django-reversion
","
which
makes
it
possible
to
compare
different
revisions
of
the
same
object
/
record
.
Additionally
see
the
following
grid
.
I
need
to
implement
Django
model
history
version
&
compare
view
.
I
found
there
are
packages
like
django-reversion
","
django-simple-history
","
django-revisions
to
implement
model
history
version
and
django-reversion-compare
for
compare
view
.
My
requirement
focuses
mainly
on
implementing
version
history
&
compare
view
outside
admin
.
Can
someone
share
the
feedback
on
these
packages
?
I
found
few
references
like
"http://treyhunner.com/2011/09/django-and-model-history/,"
but
it's
still
not
clear
to
me
.
I
am
interested
to
know
the
ease
&
limitations
of
implementing
these
packages
.
I'm
trying
to
do
this
question
for
this
online
coding
course
I'm
part
of
","
and
one
of
the
questions
requires
me
to
add
together
integers
in
a
list
.
I've
tried
to
find
the
answer
(
and
visited
a
few
other
pages
on
this
site
)
","
but
I
can't
think
of
anything
.
Help
please
!
Here's
my
code
so
far
:
Your
error
is
caused
because
you
provide
sum
with
an
integer
value
(
iatt
=
int(i)
)
when
you
should
be
providing
it
with
the
contents
of
the
list
which
is
split
on
'
","
'
.
You
have
a
couple
of
options
for
this
.
Either
provide
a
comprehension
to
sum
and
cast
every
element
to
an
int
inside
the
comprehension
:
or
","
use
a
built-in
like
map
which
pretty
much
does
the
same
thing
:
in
both
cases
","
sum
expects
something
that
can
be
iterated
through
and
it
handles
the
summing
.
Of
course
","
you
could
manually
loop
over
the
contents
of
att
","
adding
int(i)
to
total
as
you
go
:
I
am
trying
to
modify
a
Python
script
I
wrote
for
Python
64bit
2.7
to
use
it
with
Python
64bit
3.4.3.6
.
This
program
is
using
from
win32com.client
import Dispatch
.
When
running
it
with
Python
64bit
2.7
it
works
perfectly
well
","
but
with
Python
64bit
3.4
I
have
the
following
error
:
So
I
checked
in
WinPython-64bit-3.4.3.6\python-3.4.3.amd64\Lib\site-packages
and
there
are
three
folders
:
\
win32
","
\
win32com
and
\
win32comext
.
I
was
able
to
find
a
file
called
win32api.pyd
in
\
win32
.
NB
:
I
found
similar
topics
but
none
of
them
really
answered
the
problem
.
the
line
works
under
WinPython-64bit-3.4.4.4Qt5
","
but
it
should
work
on
WinPython-64bit-3.4.3.6
too
.
Here
is
the
code
and
output
","
I
think
from
the
output
","
it
mean
when
fpr
is
0
","
tpr
is
0
","
this
is
correct
as
the
prediction
results
marks
everything
to
be
0
.
But
the
output
also
said
","
when
fpr
is
1
","
tpr
is
also
1
.
I
think
it
is
not
correct
","
since
the
predictor
never
predict
something
to
be
positive
(
label
to
be
1
)
","
so
how
could
the
fpr
(
=
#
of
correct
prediction
of
1
/
total
#
of
1
)
and
tpr
(
=
#
of
prediction
of
1
/
total
#
of
0
)
both
to
be
1
?
Output
","
These
two
illustrations
would
give
you
a
better
understanding
of
how
the
FPR
and
TPR
get
computed
.
Case-1
:
True
Positive
=
0
False
Positive
=
0
True
Negative
=
9
False
Negative
=
1
True
Positive
Ratio
","
(
tpr
)
=
True
Positive
/
(
True
Positive
+
False
Negative
)
Therefore
","
tpr
=
0
/
(
0+1
)
=
0
.
False
Positive
Ratio
","
(
fpr
)
=
False
Positive
/
(
False
Positive
+
True
Negative
)
Therefore
","
fpr
=
0
/
(
0+9
)
=
0
.
Case-2
:
True
Positive
=
1
False
Positive
=
0
True
Negative
=
9
False
Negative
=
0
True
Positive
Ratio
","
(
tpr
)
=
True
Positive
/
(
True
Positive
+
False
Negative
)
Therefore
","
tpr
=
1
/
(
1+0
)
=
1
.
False
Positive
Ratio
","
(
fpr
)
=
False
Positive
/
(
False
Positive
+
True
Negative
)
Therefore
","
fpr
=
0
/
(
0+9
)
=
0
.
Note
:
According
to
the
roc_curve
documentation
","
it
is
clearly
stated
that
thresholds
[0]
represents
no
instances
being
predicted
and
is
arbitrarily
set
to
max(pred)
+
1
.
[
Here
","
2
for
binary
classification
task
]
This
becomes
valid
when
the
fpr
and
tpr
when
calculated
becomes
a
fraction
and
cannot
be
quantified
to
0
or
1
.
Hence
","
the
threshold
varies
from
0
","
1
","
2
.
For
eg
","
when
the
last
2
values
of
pred
array
become
1
","
you
get
3
values
for
threshold
as
fpr
and
tpr
become
fractional
values
.
But
in
our
case
","
both
fpr
and
tpr
had
been
either
of
0
or
1
and
hence
there
wasn't
any
need
for
a
third
value
of
the
threshold
.
Also
","
the
array
elements
in
both
fpr
and
tpr
form
an
increasing
sequence
","
i.e
vary
from
0
→
1
and
must
satisfy
a
shape
>
=
2
.
Therefore
","
it
is
mandatory
to
have
both
0's
and
1's
in
the
array
as
the
starting
and
terminal
values
.
Incase
of
fractional
values
of
fpr
and
tpr
","
the
middle
column
would
contain
these
values
enclosed
by
0
and
1
on
either
side
of
the
array
.
I
am
a
beginner
in
Python
and
programming
in
general
and
I'm
trying
to
solve
a
problem
on
codingbat.com
(
the
problem
is
called
""""
array_front9
""""
under
the
section
""""
Warmup-2
""""
)
.
The
problem
is
:
""""
Given
an
array
of
ints
","
return
True
if
one
of
the
first
4
elements
in
the
array
is
a
9
.
The
array
length
may
be
less
than
4
.
""""
Here
is
my
code
which
works
if
I
create
a
list
and
then
run
it
locally
(
on
codingbat.com
it
is
necessary
to
create
a
function
but
I
do
not
create
a
function
to
test
my
code
locally
)
:
Here
is
the
code
I'm
trying
to
run
on
codingbat.com
but
I
receive
""""
Error:list
index
out
of
range
""""
error
:
Here
is
the
solution
according
to
codingbat.com
:
Here
is
the
current
URL
for
this
problem
:
http://codingbat.com/prob/p110166
Can
anyone
point
me
to
what
I'm
doing
wrong
?
Simply
test
this
expression
:
I
explain
:
As
others
have
pointed
out
","
your
array
might
be
shorter
than
4
items
","
in
which
case
indexing
at
","
say
3
","
the
fourth
number
","
will
raise
an
IndexError
.
But
let's
use
the
feature
that
slicing
won't
raise
an
exception
","
that
is
:
In
the
above
example
I
took
a
slice
from
the
beginning
of
the
array
up
until
the
10'th
item
","
and
didn't
get
an
exception
","
but
Python
returned
the
entire
list
.
Thus
","
you
could
do
the
check
as
simply
as
:
and
that's
all
!
Easier
solution
for
this
problem
would
be
:
If
you
want
to
check
for
9
","
even
if
the
list
length
is
less
than
4
","
then
below
is
the
solution
:
It
is
given
in
the
problem
that
the
array
length
may
be
less
than
4
.
Your
are
just
iterating
over
an
array
.
You
should
first
check
whether
the
array
is
at
least
of
length
4
.
If
the
array
is
less
than
4
then
only
iterate
through
the
length
of
the
array
.
You
use
range(4)
.
So
it
will
always
look
for
4
numbers
.
So
when
there
are
only
three
numbers
it
will
still
look
for
a
fourth
one
.
The
fourth
one
doesn't
exist
so
it
throws
an
error
.
Instead
you
need
to
see
what
the
lenght
of
the
list
is
and
then
iterate
over
that
lenght
.
Like
this
:
Or
even
simpler
you
can
iterate
over
a
list
like
this
:
You
have
to
get
the
minimum
checking
length
of
the
input
list
:
The
problem
is
""""
check
the
first
four
numbers
of
a
list
","
to
see
if
any
of
them
is
a
9
""""
.
Now
","
if
the
list
is
less
than
4
","
then
automatically
the
check
will
fail
:
Next
","
you
need
to
check
if
the
first
four
items
contain
nine
.
There
are
many
ways
to
do
this
","
one
of
the
easiest
is
:
Combine
the
two
:
The
issue
with
your
initial
approach
is
you
are
forcing
the
length
to
be
4
","
and
assuming
that
at
minimum
the
array
will
have
4
elements
.
If
the
array
has
3
elements
","
then
your
code
will
not
work
.
To
avoid
this
","
don't
assume
a
specific
length
.
If
you
use
slicing
","
then
you
are
guaranteed
that
you
will
get
at
most
4
items
.
Here
is
an
example
of
this
:
Even
though
i
only
has
two
items
","
the
slice
to
4
works
and
doesn't
raise
an
error
.
Once
i
has
more
than
4
items
","
it
will
limit
to
the
first
four
:
However
","
our
problem
statement
specifically
said
that
check
if
the
first
four
items
contain
9
.
If
we
just
use
the
slice
trick
","
like
this
:
It
will
return
true
even
if
we
pass
it
an
array
like
[
'
1
'
","
'
9
'
]
since
the
slice
will
always
work
.
To
catch
this
condition
-
we
first
check
if
the
length
of
the
list
is
less
than
4
","
because
if
it
is
-
it
automatically
fails
the
condition
.
Thank
you
for
all
your
answers
","
they
have
been
really
helpful
!
I
decided
to
use
the
IF
loop
instead
of
the
FOR
loop
","
like
this
:
As
much
as
I
can
see
","
this
solution
works
no
matter
what
the
length
of
the
list
is
","
and
it
seems
like
it
fits
the
definition
of
the
problem
i.e.
""""
no
matter
what
the
length
of
the
list
is
","
if
any
of
the
first
four
elements
equals
9
","
return
True
""""
.
The
problem
states
""""
The
array
length
may
be
less
than
4
.
""""
Your
code
currently
assumes
the
array
is
at
least
length
4
.
However
","
if
i
is
greater
than
the
last
index
of
the
list
","
an
IndexError
will
be
thrown
.
You
should
first
check
if
len(arr)
<
4
and
return
False
if
so
.
From
the
discussions
get
to
know
that
you
are
treating
with
list
of
tuple
instead
of
dict
.
So
list.pop
always
expect
a
integer
that's
why
you
getting
an
error
.
And
dict
expect
it's
key
.
So
here
you
have
to
convert
the
input
like
dict
or
pop
up
from
list
with
using
it's
index
.
You
can
do
it
like
so
:
and
then
:
I
have
the
following
problem
:
I
have
a
defaultdict
called
word_count
containing
words
and
the
number
how
often
they
occur
.
I
get
this
by
counting
the
reply
of
the
Google
Speech
API
.
However
","
this
API
gives
me
back
things
like
'
\
303\266
'
for
the
German
letter
'
รถ
'
.
Now
I
want
to
go
through
this
dict
","
test
if
one
of
these
things
shown
above
is
there
and
replace
it
with
the
right
thing
like
this
:
Filling
the
defaultdict
:
So
far
it
works
fine
","
I
can
print
the
dict
and
it
gets
me
the
words
with
the
number
.
Now
replacing
the
key
:
Now
this
does
not
work
","
I
guess
because
I
cannot
pop(key)
as
it
expects
an
integer
.
How
else
would
I
do
this
?
I
tried
several
approaches
","
but
nothing
seems
to
work
here
.
Any
help
would
be
greatly
appreciated
!
Solution
:
Turns
out
this
was
my
fault
","
as
I
sorted
the
dict
and
thereby
turned
it
to
a
list
of
tuples
.
Thanks
to
everyone
who
helped
me
figure
this
out
!
I
think
you
can
use
ix
for
selecting
columns
from
A
to
T
","
then
divide
by
div
with
numpy.log
.
Last
use
sum
:
Setup
I
have
a
dataframe
as
shown
below
.
The
last
column
shows
the
sum
of
values
from
all
the
columns
i.e.
A
","
B
","
D
","
K
and
T
.
Please
note
some
of
the
columns
have
NaN
as
well
.
How
can
I
calculate
the
entropy
for
each
row
?
i.e.
I
should
find
something
like
following
The
condition
is
that
whenever
the
value
inside
the
log
becomes
zero
or
NaN
","
the
whole
value
should
be
treated
as
zero
(
by
definition
","
the
log
will
return
an
error
as
log
0
is
undefined
)
.
I
am
aware
of
using
lambda
operation
to
apply
on
individual
columns
.
Here
I
am
not
able
to
think
for
a
pure
pandas
solution
where
a
fixed
column
sum
is
applied
on
different
columns
A
","
B
","
D
etc..
Though
I
can
think
of
a
simple
loopwise
iteration
over
CSV
file
with
hard-coded
column
values
.
I
can
ran
tensorflow
in
anaconda
command
line
successfully
","
but
when
I
ran
it
in
pycharm
","
it
complains
for
not
finding
tensorflow
","
but
I
do
find
it
in
the
package
of
this
python
interpreter
.
Is
there
anything
else
I
need
to
do
?
Thanks
Looking
at
the
console
","
your
.
py
file
is
also
called
tensorflow
","
so
when
you
do
import tensorflow
as
tf
","
you
end
up
trying
to
import things
from
the
same
file
.
Rename
your
own
file
to
something
else
","
such
as
tensorflow_test_1.py
or
whatnot
.
:
)
You
can
try
this
(
assuming
name
of
the
date
field
is
'
date
'
I
convert
a
string
to
date
using
pandas
.
When
I
write
the
DF
to
CSV
","
the
date
comes
like
'
2016-08-15
instead
of
plain
2016-08-15
.
Unable
to
read
it
as
date
in
ETL
tool.Same
is
the
case
for
all
date
fields
.
Any
suggestion
to
get
the
date
format
correctly
?
df.to_csv('/Users/tcssig/Documents/Sarang.csv')
I'm
trying
to
implement
convolutional
neural
network
in
Python
.
However
","
when
I
use
signal.convolve
or
np.convolve
","
it
can
not
do
convolution
on
X
","
"Y(X is 3d, Y is 2d)"
.
X
are
training
minibatches
.
Y
are
filters
.
I
don't
want
to
do
for
loop
for
every
training
vector
like
:
So
","
is
there
any
function
I
can
use
to
do
convolution
efficiently
?
Scipy
implements
standard
N-dimensional
convolutions
","
so
that
the
matrix
to
be
convolved
and
the
kernel
are
both
N-dimensional
.
A
quick
fix
would
be
to
add
an
extra
dimension
to
Y
so
that
Y
is
3-Dimensional
:
I'm
assuming
here
that
the
last
axis
corresponds
to
the
image
index
as
in
your
example
"[width, height, image_idx]"
(
or
"[height, width, image_idx]"
)
.
If
it
is
the
other
way
around
and
the
images
are
indexed
in
the
first
axis
(
as
it
is
more
common
in
C-ordering
arrays
)
you
should
replace
Y
[
...
","
None
]
with
Y
[
None
","
...
]
.
The
line
Y
[
...
","
None
]
will
add
an
extra
axis
to
Y
","
making
it
3-dimensional
"[kernel_width, kernel_height, 1]"
and
thus
","
converting
it
to
a
valid
3-Dimensional
convolution
kernel
.
NOTE
:
This
assumes
that
all
your
input
mini-batches
have
the
same
width
x
height
","
which
is
standard
in
CNN's
.
EDIT
:
Some
timings
as
@Divakar
suggested
.
The
testing
framework
is
setup
as
follows
:
Find
bellow
tests
for
different
configurations
:
Varying
image
size
S
:
Varying
number
of
images
N
:
Varying
kernel
size
K
:
So
","
in
short
","
ndimage.convolve
is
always
faster
","
except
when
the
kernel
size
is
very
large
(
as
K
=
31
in
the
last
test
)
.
If
I
have
a
path
in
a
string
such
as
:
I
want
to
put
a
\
after
the
word
Python
Such
that
it
looks
like
:
Documents
/
Programming
/
Python
\
scripts
/
How
do
I
do
that
?
You
can
use
the
replace
method
:
I
fixed
it.The
thing
was
that
the
height
was
too
big.I
changed
:
"textBox.place(x=16,y=16,height=684,width=650)"
to
"textBox.place(x=16,y=16,height=580)"
and
everything
went
fine.Just
as
expected
.
I
decided
I
want
to
do
some
GUI
programming
with
Python
Tkinter
","
and
I
wanted
to
start
with
a
Text
Editor.I've
written
some
code
and
everything
seems
to
be
looking
fine
but
I
have
trouble
inserting
text
to
the
Text
widget.It
doesn't
insert
all
lines
just
a
few.Anyone
seems
to
know
the
answer?I'm
starting
with
Tkinter
and
I
might
move
to
PyQT.I
want
to
use
Python
2.7
","
so
please
don't
post
Python
3.x
asnwers.Thanks
Some
basic
question
from
beginner
.
Is
there
a
way
to
""""
push
""""
attribute
to
a
decorated
function
not
using
function
arguments
?
Thanks
in
advance
.
If
you
set
the
attribute
on
new_func
instead
","
you
can
access
it
simply
as
decorated_function.some_attr
:
Otherwise
","
wraps
makes
the
original
function
available
as
decorated_function.__wrapped__
in
Python
3
:
In
Python
2
","
the
__wrapped__
is
not
set
by
wraps
","
so
we
need
to
set
it
up
manually
:
However
","
this
sounds
like
an
XY
problem
;
if
you
want
to
pass
a
value
to
the
decorated_function
you
should
let
decorator_
pass
it
as
an
argument
instead
.
Depending
on
whether
you
use
Python
2
or
3
you
can
inject
variables
into
the
globals
of
a
function
like
this
:
Python
2
Python
3
I
have
a
list
which
contains
words
.
I
want
to
know
the
most
frequent
words
in
my
list
.
I
tried
using
'
counter
'
from
collections
package
.
and
I
got
this
result
.
but
I
only
want
the
words
and
not
the
frequency
no
.
attached
with
it
.
like
from
HERE
Counter
is
slower
than
defaultdict
.
if
performance
is
important
try
this
:
You
can
use
the
index
and
list
comprehension
:
Or
in
one
line
:
Use
list
comprehension
to
extract
them
from
result
:
Try
this
:
I
am
new
to
python
.
I
have
a
large
header
formatted
input
file
where
header
line
starts
with
'
>
'
.
My
file
is
like
:
I
want
to
extract
each
header
and
its
corresponding
Start-End
value(s)
(
after
Predicted
binding
regions
line
)
in
a
(
output.txt
file
)
.
For
the
above
(
input.txt
)
","
the
output
will
be
:
I
have
tried
:
But
it
gives
me
:
Which
is
obviously
not
right
.
I
get
the
range
but
without
header
descriptors
and
with
some
extra
values
.
How
can
I
get
my
above
mentioned
output
?
Thanks
Ps
.
I
am
using
python
2.7
in
my
Windows7
machine
.
I
notice
there
is
a
white
space
in
URL
","
is
that
correct
?
I
tested
in
my
ipython
with
requests
.
.
that
the
response
was
:
For
HTTP
and
HTTPS
.
I
have
developed
a
desktop
client
using
PyQt4
","
it
connect
to
my
web
service
by
requests
lib
.
You
know
","
requests
maybe
one
of
the
most
useful
http
client
","
I
think
it
should
be
no
problem
.
My
desktop
client
works
all
right
until
something
strange
happened
.
I
use
the
following
code
to
send
request
to
my
server
.
where
header
only
includes
auth
token
.
I
cannot
connect
to
my
web
service
","
all
the
http
request
pop
the
same
error
""""
'
Cannot
connect
to
proxy
.
'
","
"error(10061, '')"
""""
.
For
example
:
GET
Url
:
http://
api.fangcloud.com
/
api
/
v1
/
user
/
timestamp
"HTTPSConnectionPool(host='api.fangcloud.com', port=443)"
:
Max
retries
exceeded
with
url
:
/
api
/
v1
/
user
/
timestamp
(
Caused
by
"ProxyError('Cannot connect to proxy.', error(10061, '')"
)
)
this
API
does
nothing
but
return
the
timestamp
of
my
server
.
When
I
copy
the
url
into
Chrome
in
same
machine
with
same
environment
","
it
returns
correct
response
.
However
","
my
desktop
client
can
only
returns
error
.
Is
it
anything
wrong
with
requests
lib
?
I
googled
this
problem
of
connection
error
10061
(
""""
No
connection
could
be
made
because
the
target
machine
actively
refused
it
""""
)
.
This
maybe
caused
by
TCP
connect
rejection
of
web
server
.
The
client
sends
a
SYN
packet
to
the
server
targeting
the
port
(
80
for
HTTP
)
.
A
server
that
is
running
a
service
on
port
80
will
respond
with
a
SYN
ACK
","
but
if
it
is
not
","
it
will
respond
with
a
RST
ACK
.
Your
client
reaches
the
server
","
but
not
the
intended
service
.
This
is
one
way
a
server
could
“
actively
refuse
”
a
connection
attempt
.
But
why
?
My
client
works
all
right
before
and
Chrome
still
works
.
I
use
no
proxy
on
my
machine
.
Is
there
anything
I
miss
?
From
the
example
in
this
section
","
it
looks
like
you
need
to
define
an
object
and
express
all
coordinates
in
terms
of
the
handlebox
size
","
This
seems
to
work
okay
with
PatchCollection
","
at
least
for
me
on
matplotlib
version
1.4.3
.
The
result
looks
like
","
Currently
","
I
am
trying
to
make
my
own
custom
legend
handler
by
creating
a
proxy
artist
(
?
)
patch
using
PatchCollections
and
then
following
http://matplotlib.org/users/legend_guide.html
to
make
a
custom
handler
.
However
I
am
running
into
a
roadblock
in
trying
to
implement
this
into
the
legend
.
The
arguments
for
legend
takes
in
patches
","
but
not
patchcollections
.
The
above
is
the
code
to
visualise
the
handler
.
Basically
a
rectangle
with
the
upper
triangle
as
dashed
lines
and
the
lower
as
solid
Using
","
Recreates
the
error
.
Is
there
a
workaround
?
I
am
trying
to
make
NAO
understand
the
words
in
the
vocabulary
and
return
the
same
when
read
from
its
memory
.
Below
is
the
code
.
But
somehow
the
getdata()
from
memory
object
returns
empty
.
Any
help
is
highly
appreciated
.
It's
because
you
stopped
the
asr
with
asr.pause(True)
.
You
only
need
to
write
this
asr.pause(False)
just
before
your
time.sleep
.
You
can
also
use
standard
boxes
in
choregraphe
.
We
can
use
break
statement
inside
if
statement
for
termination
.
Instead
of
break
we
can
write
os.exit
or
sys.exit
os._exit
calls
the
C
function
_exit()
which
does
an
immediate
program
termination
.
Note
the
statement
""""
can
never
return
""""
.
sys.exit()
is
identical
to
raise
SystemExit()
.
It
raises
a
Python
exception
which
may
be
caught
by
the
caller
.
You
can
put
your
code
in
function
.
A
function
you
can
return
the
different
values
.
While
Calling
the
function
you
can
check
what
function
is
returning
.
You
can
use
if
condition
like
Use
break
to
exit
the
loop
.
Use
exit
to
exit
the
program
.
I'm
very
new
to
Python
and
want
to
know
how
to
end
a
program
with
an
if
statement
that
prints
a
message
","
as
this
does
not
appear
to
be
happening
with
my
'
stand
'
variable
when
it
is
under
<
5000
.
The
if
statement
bolded
*
*
*
*
is
the
code
I'm
having
trouble
with
and
it
still
prints
the
message
I
want
","
however
","
the
program
does
not
stop
with
that
message
but
continue
going
to
the
next
code
(
countdown
variable
)
.
Ignore
'
countdown
'
.
Here
is
a
part
of
my
code
.
I
would
advise
putting
these
buttons
on
a
relative
layout
","
and
then
manipulating
their
pos_hint
properties
for
positioning
.
Screenshot
:
Code
:
I'm
new
to
Kivy
and
I've
been
trying
for
a
couple
of
days
for
a
suitable
layout
but
I
dont
seem
to
get
a
result
.
I
want
the
buttons
'
2
'
and
'
3
'
in
the
picture
to
stay
on
corners
like
buttons
'
1
'
and
'
4
'
.
what
should
I
do
?
http://i.stack.imgur.com/Y6Rjo.png
here
is
my
code
but
it
doesnt
work
as
needed
:
Initially
I
was
not
using
the
unittest
framework
","
so
to
test
that
two
objects
of
the
same
class
are
not
comparable
using
the
operators
<
and
>
=
I
did
something
like
:
after
that
","
though
","
I
decided
to
start
using
the
unittest
module
","
so
I'm
converting
my
tests
to
the
way
tests
are
written
with
the
same
module
.
I
was
trying
to
accomplish
the
equivalent
thing
as
above
with
:
but
this
does
not
quite
work
","
because
o1
<
o2
tries
to
call
the
operator
<
","
instead
of
being
a
reference
to
a
function
","
which
should
be
called
as
part
of
the
test
.
Is
there
a
way
to
accomplish
what
I
need
without
needing
to
wrap
o1
<
o2
in
a
function
?
Use
assertRaises
as
a
context
manager
:
Here
is
an
explanation
of
the
with
statement
.
Here
are
the
docs
.
TL;DR
:
It
allows
the
execution
of
a
code
block
with
a
""""
context
""""
","
i.e.
things
to
be
set
up
and
disposed
before
/
after
the
execution
","
error
handling
etc.
In
the
case
of
assertRaises
","
its
context
manager
simply
checks
whether
an
execption
of
the
required
type
has
been
raised
","
by
checking
the
exc
agrument
passed
to
its
__exit__
method
.
I
like
your
way
","
it
is
explicit
","
the
for
loop
is
understandable
by
all
and
it
isn't
all
that
slow
compared
to
other
approaches
.
Some
suggestions
I'd
make
would
be
to
change
your
condition
from
if
c
!
=
b
'
'
to
if
c
since
a
non-empty
byte
object
will
be
truthy
and
","
*
don't
name
your
list
bytes
","
you
mask
the
built-in
!
Name
it
bt
or
something
similar
:
-
)
Other
options
include
itertools.takewhile
which
will
grab
elements
from
an
iterable
as
long
as
a
predicate
holds
;
your
operation
would
look
like
:
This
is
slightly
slower
but
is
more
compact
","
if
you're
a
one-liner
lover
this
might
appeal
to
you
.
Slightly
faster
and
also
compact
is
using
index
along
with
a
slice
:
While
compact
it
also
suffers
from
readability
.
In
short
","
I'd
go
with
the
for
loop
since
readability
counts
as
very
pythonic
in
my
eyes
.
I
have
a
numpy
bytes
array
containing
characters
","
followed
by
b
'
'
","
followed
by
others
characters
(
including
weird
characters
which
raise
Unicode
errors
when
decoding
)
:
I
want
to
get
everything
before
the
first
b
'
'
.
Currently
my
code
is
:
I
suppose
there
is
a
more
efficient
and
Pythonic
way
to
do
that
.
Any
idea
?
You
are
using
a
ProcessPoolExecutor
.
This
will
fork
new
processes
for
performing
work
.
These
processes
will
not
share
memory
","
each
instead
getting
a
copy
of
the
distances
matrix
.
Thus
any
changes
to
their
copy
will
certainly
not
be
reflected
in
the
original
process
.
Try
using
a
ThreadPoolExecutor
instead
.
NOTE
:
Globals
are
generally
viewed
with
distaste
...
pass
the
array
into
the
function
instead
.
I
have
a
numpy
array(matrix)
","
which
I
want
to
fill
with
calculated
values
in
asynchronously
.
As
a
result
","
I
want
to
have
matrix
distances
with
calculated
values
","
but
at
the
end
I
receive
matrix
filled
with
default(-1)
value
.
I
understand
","
that
something
wrong
with
sharing
distances
between
threads
","
but
I
can't
figure
out
what's
exactly
wrong
.
IIUC
and
you
are
talking
about
pandas
-
you
can
use
squeeze
parameter
:
as
a
vanilla
Python
list
:
from
docs
:
squeeze
:
boolean
","
default
False
If
the
parsed
data
only
contains
one
column
then
return
a
Series
Try
"pandas.Series(data=None, index=None, dtype=None, name=None, copy=False, fastpath=False)"
after
reading
your
data
as
data
frame
with
read_csv
.
Pandas
documentation
Another
possibility
is
to
use
iloc
function
","
when
you
deal
with
a
row
","
not
a
column
.
See
here
:
Convert
pandas
data
frame
to
series
I
have
a
data
in
a
column
in
a
csv
format
and
want
to
append
it
in
series
in
python
that
it
can
be
displayed
like
:
How
can
it
be
done
?
Should
I
do
it
via
read_csv
and
then
somehow
transform
it
to
series
or
it
is
a
another
way
?
Thanks
!
The
way
in
Python
to
do
it
would
be
:
or
","
in
Python
3.6
:
However
","
this
is
wrong
!
It
will
fail
if
k
and
v
have
spaces
","
and
it
can
be
a
security
hazard
(
imagine
if
k
=
'
;
rm
-
rf
/
;
)
.
The
right
way
to
spawn
subprocesses
is
with
this
:
Here
is
a
code
i
would
like
to
","
insert
the
value
of
""""
K
""""
and
""""
v
""""
in
cmd
after
robocopy
command
","
so
that
with
in
the
for
loop
it
will
perform
robocopy
for
all
the
source
and
destination
mentioned
in
dictionary
D
=
{
}
i
would
also
like
that
the
script
checks
for
failures
in
the
robocopy
output
logs
in
error.log
file
if
two
files
have
failed
then
the
script
should
send
a
mail
to
some
email
address
.
I
guess
...
its
not
really
clear
what
you
are
asking
...
Im
sure
you
could
do
the
same
with
just
urllib
and
/
or
urllib2
I'm
learning
python
these
days
","
and
I
have
a
rather
basic
question
.
There's
a
remote
server
where
a
webserver
is
listening
in
on
incoming
traffic
.
If
I
enter
a
uri
like
the
following
in
my
browser
","
it
performs
certain
processing
on
some
files
for
me
:
My
rather
basic
question
is
:
how
can
I
achieve
sending
the
above
dynamic
parameters
(
i.e.
container
=
container_name
and
video
=
video_name
)
to
223.58.1.10:8000
/
processVideo
via
a
python
script
?
I've
seen
ways
to
ping
an
IP
","
but
my
requirements
are
more
than
that
.
Guidance
from
experts
will
be
very
helpful
.
Thanks
!
I
never
bothered
at
the
time
but
have
used
Selenium
in
a
different
project
since
and
had
a
user
request
to
provide
an
answer
so
here's
the
basics
for
getting
going
with
selenium
to
get
past
the
Forbes
splash
page
.
You'll
need
to
install
a
driver
for
selenium
","
either
firefox
driver
","
chrome
driver
or
PhantomJS
which
is
headless
.
If
you're
on
Mac
","
chromedriver
can
be
easily
installed
via
Homebrew
","
or
by
copying
the
single
PhantomJS
driver
file
to
the
path
indicated
in
the
#
comment
Is
it
possible
for
Requests
to
navigate
its
way
through
the
Forbes
welcome
page
?
I'm
trying
to
access
this
article
which
for
most
will
end
up
with
a
splash
screen
welcome
page
before
then
redirecting
to
the
actual
article
itself
.
I
note
in
Chrome
the
URL
of
the
article
is
then
appended
with
a
value
once
it
resolves
to
the
actual
article
","
though
this
seems
random
every
time
.
I
have
a
sense
this
may
involve
cookies
but
so
far
my
code
has
not
grabbed
any
html
apart
from
the
html
making
up
the
welcome
page
.
Output
I
imagine
that
as
a
browser
can
eventually
resolve
to
the
article
","
Requests
should
be
able
to
as
well
","
but
as
I
can't
work
out
what
Forbes
is
doing
","
I
can't
work
out
how
to
design
the
Requests
argument
appropriately
.
Any
ideas
?
You
can
use
None
:
The
awkwardness
arises
from
trying
to
take
advantage
of
the
:
-
n
syntax
.
Since
there's
no
difference
between
:
0
and
:
-
0
","
we
can't
use
this
syntax
to
distinguish
0
from
the
start
from
0
from
the
end
.
In
this
case
it
is
clearer
to
use
the
:
i
indexing
","
and
adjust
the
range
accordingly
:
arr
[
:
None
]
means
0
from
the
end
","
and
usually
is
sufficient
.
Keep
in
mind
the
arr
[
:
i
]
translates
to
"arr.__getitem__(slice(None,i,None)"
)
.
The
:
syntax
is
short
hand
for
a
slice
object
.
And
the
slice
signature
is
:
slice
itself
is
pretty
simple
.
It's
the
__getitem__
method
that
gives
it
meaning
.
You
could
","
in
theory
","
subclass
list
","
and
give
it
a
custom
__getitem__
that
treats
"slice(None,0,None)"
as
"slice(None,None,None)"
.
I
wish
to
write
a
for
loop
that
prints
the
tail
of
an
array
","
including
the
whole
array
(
denoted
by
i==0
)
Is
this
task
possible
without
a
branching
logic
","
ie
a
if
i==0
statement
inside
the
loop
?
This
would
be
possible
if
there
is
a
syntax
for
slicing
with
inclusive
end
index
.
output
:
wanted
output
:
Had
a
brain
freeze
","
but
this
would
do
:
Although
I
still
wish
python
had
nicer
syntax
to
do
these
kind
of
simple
tasks
.
I
have
an
RDD
called
codes
","
which
is
a
pair
","
that
has
a
string
as
its
1st
half
and
another
pair
as
its
2nd
half
:
and
I
am
trying
to
get
this
:
How
to
do
this
?
My
failed
attempt
:
I
think
what
you
want
is
the
following
:
If
it
is
python
2
","
for
legibility
you
could
:
By
this
way
you
will
be
able
to
""""
extract
""""
each
value
associated
with
the
key
and
force
that
every
row
is
a
record
of
form
(
k
","
v
)
.
Are
you
sure
the
error
wasn't
the
fact
that
you
left
out
the
quotation
marks
around
your
class
?
You've
written
<
div
class=page-content
>
instead
of
<
div
"class=""page-content"
""""
>
.
I
am
a
complete
beginner
with
BeautifulSoup
and
I
am
now
trying
to
insert
a
new
tag
into
a
children
div
of
a
parent
div
.
Basically
I
have
this
HTML
snippet
:
Here
is
my
current
code
:
I
can
fetch
the
page-content
and
his
children
content-block
DIV
","
but
the
append
function
doesn't
do
anything
","
this
is
the
output
that
I
get
:
Found
the
issue
","
I
have
to
use
findNext
instead
of
findChildren
.
now
the
append
is
working
fine
.
I
am
having
data
such
as
","
data
I
want
to
apply
a
function
for
the
dataframe
","
this
gives
an
output
","
data
I
want
to
apply
this
function
only
when
filter
=
A
and
for
the
remaining
part
I
want
it
be
NULL
.
The
output
I
want
here
is
","
data
Here
the
value
is
NULL
because
it
didnt
satisfy
the
condition
filter
=
A
.
I
want
the
function
to
be
applied
only
when
filter
=
A
.
Can
anybody
help
me
in
changing
the
code
inorder
to
get
this
output
in
pyspark
?
What
you
need
is
to
use
when
and
otherwise
.
By
the
way
you
don't
have
to
create
that
UDF
.
If
you
really
need
to
call
that
function
","
you
can
simply
replace
"col(""id"")"
+
1
by
"yourUDF(col(""id"")"
)
The
first
if-else
should
come
inside
the
the
for
loop
.
The
second
if
else
block
should
come
outside
.
I'm
currently
trying
to
teach
myself
Python
with
the
Python
Crash
Course
book
by
Eric
Matthes
and
I
seem
to
be
having
difficulties
with
excercise
5-9
regarding
using
if
tests
to
test
empty
lists
.
Here
is
the
question
:
5-9
.
No
Users
:
Add
an
if
test
to
hello_admin.py
to
make
sure
the
list
of
users
is
not
empty
.
•
If
the
list
is
empty
","
print
the
message
We
need
to
find
some
users
!
•
Remove
all
of
the
usernames
from
your
list
","
and
make
sure
the
correct
message
is
printed
.
Here
is
my
code
from
hello_admin.py
:
Now
here
is
my
code
for
5-9
which
is
not
outputting
anything
:
Does
anyone
having
any
feedback
for
why
my
code
is
not
outputting
:
""""
We
need
to
find
some
users
!
""""
Thank
you
for
your
time
.
:
)
It's
not
outputting
anything
since
your
if
and
else
blocks
are
inside
the
for
loop
which
iterates
over
usernames
.
Since
usernames
is
an
empty
list
","
it's
not
iterating
over
anything
","
hence
not
reaching
any
of
those
conditional
blocks
.
You
might
want
to
put
instead
:
That
will
print
the
last
username
in
the
usernames
list
twice
","
though
.
Nothing
","
leave
it
as
is
.
That
is
what
I
could
tell
from
their
documentation
:
https://mypy.readthedocs.io/en/latest/class_basics.html?highlight=self
If
I'm
using
mypy
on
my
project
","
what
type
should
my
methods
self
object
be
given
?
I
have
a
raw
list
sorteddict
in
the
form
of
:
and
I
want
to
generate
more
legible
data
by
making
it
a
JSON
object
that
looks
like
:
Unfortunately
so
far
","
I
have
only
found
a
manual
way
to
create
data
as
shown
above
","
and
was
wondering
if
there
was
a
much
more
eloquent
way
of
generating
the
data
rather
than
my
messy
script
.
I
got
to
the
point
where
I
needed
to
retrieve
the
last
item
in
the
list
and
ensure
that
there
was
no
comma
on
the
last
line
before
I
thought
I
should
seek
some
advice
.
Here's
what
I
had
:
I
have
used
json.JSONEncode.encode()
which
I
thought
that
was
what
I
was
looking
for
","
but
what
ended
up
happening
is
""""
Frequencies
""""
would
be
prepended
to
each
s
[0]
item
.
Any
ideas
to
clean
the
code
?
I
may
not
be
understanding
you
correctly
-
but
do
you
just
want
to
turn
a
list
of
"[word, frequency]"
lists
into
a
dictionary
?
If
you
then
want
to
write
this
to
a
file
:
You
need
to
make
a
nested
dict
out
of
your
current
one
","
and
use
json.dumps
.
Not
sure
how
sorteddict
works
","
but
:
should
work
.
Additionally
","
you
say
that
you
want
something
json
encoded
","
but
your
example
is
not
valid
json
.
So
I
will
assume
that
you
actually
want
legitimate
json
.
Here's
some
example
code
:
pandas.DataFrame.drop
takes
level
as
an
optional
argument
"df.drop('1995-96', level='start')"
As
of
v0.18.1
","
its
docstring
says
:
I
have
a
multi-index
dataframe
that
looks
like
this
:
I'd
like
to
drop
by
the
specific
values
for
the
first
level
(
level=0
)
.
In
this
case
","
I'd
like
to
drop
everything
that
has
1995-96
in
the
first
index
.
I'm
+
-
in
the
same
case
as
you
.
I
will
explain
my
solution
.
I'm
not
opening
the
pdfs
with
"PdfFileReader('filename.pdf', 'rb')"
but
I'm
passing
the
pdfs
content
in
an
array
for
the
merge
(
pdfs_content_array
)
.
Then
I'm
preparing
the
merger
and
my
output
(
don't
want
to
save
the
generated
file
locally
so
I
have
to
use
BytesIO
to
save
the
merged
content
somewhere
)
calc_page_sum
is
needed
to
compare
the
page
number
results
.
The
most
important
part
is
:
calc_page_sum
+
=
PdfFileReader(bytes_content)
.
getNumPages()
so
I
open
the
bytes
content
with
PdfFileReader
and
get
the
pages
number
.
Then
I'm
appending
the
merger
...
merger.append
","
bytes_content
I'm
writing
the
merge
into
my
bytes
output
and
compare
it
with
the
calc_page_sum
.
That's
it
.
Hope
this
will
help
!
2nd
Version
:
I
am
trying
to
use
PdfFileMerger()
in
PyPDF2
to
merge
pdf
files
(
see
code
)
.
However
","
my
merge
commands
are
subject
to
certain
conditions
and
it
could
turn
out
that
no
merged
pdf
file
is
generated
.
I
would
like
to
know
how
to
determine
the
page
count
after
performing
merges
using
PdfFileMerger()
.
If
nothing
else
","
I
would
like
to
know
if
the
number
of
pages
is
non-zero
.
Maintaining
a
counter
to
do
this
would
be
cumbersome
because
I
am
performing
the
merges
across
several
functions
and
would
prefer
a
more
elegant
solution
.
The
doctest
documentation
has
a
section
about
execution
context
.
My
reading
is
that
the
globals
in
the
module
are
shallow
copied
for
the
tests
in
each
docstring
","
but
are
not
reset
between
tests
within
a
docstring
.
Based
on
that
description
","
I
would
have
thought
the
following
doctests
would
pass
:
But
instead
the
following
tests
pass
!
It
seems
like
the
globals
are
shared
across
tests
across
docstrings
?
But
only
within
function
calls
?
Why
is
this
the
resulting
behavior
?
Does
this
have
something
to
do
with
functions
having
a
dictionary
of
globals
separate
from
the
execution
context
?
Not
exactly
.
While
it
is
true
that
the
globals
are
shallow
copied
","
what
you
are
actually
seeing
is
the
scoping
of
globals
(
using
the
keyword
global
)
and
how
it
actually
operates
at
the
module
level
in
Python
.
You
can
observe
this
by
putting
in
a
pdb.set_trace()
inside
function
f
right
after
the
assignment
(
X
=
2
)
.
Yes
","
the
value
is
indeed
2
within
the
scope
of
f
","
but
let's
take
a
look
of
its
globals
.
Let's
look
at
how
they
compare
in
the
current
frame
and
up
a
frame
.
Ah
ha
","
you
can
see
that
they
are
NOT
actually
the
same
thing
","
and
that
X
is
indeed
1
and
has
not
been
changed
","
because
the
globals
there
are
within
the
<
doctest
doc.f
>
module
created
by
doctest
for
this
reason
.
Let's
continue
.
So
what
you
actually
saw
is
that
the
globals
within
the
doctest
is
not
the
same
one
as
the
one
on
your
source
(
hence
g
will
return
2
because
X
was
really
was
changed
in
the
module
here
by
f
","
but
not
in
the
doctest
module
shallow
copy
scope
)
","
even
though
it
originally
was
copied
from
the
module
but
the
changes
are
not
reflected
back
to
the
underlying
module
since
this
is
how
the
global
keyword
operates
-
at
the
module
level
","
not
across
modules
.
Simple
enough
to
do
:
Switch
to
the
directory
where
App
Engine
is
installed
on
your
computer
.
It
is
usually
under
the
following
path
:
C:\Program
Files\Google\google_appengine
Run
the
below
command
appcfg.py
download_app
â€“A
MyAppName
-
V
1
c:\AppEngine\SourceCode
Solution
:
Example
:
I
have
some
php
source
code
working
in
an
app
engine
production
environment
that
won't
work
anywhere
else
.
Old
versions
of
that
code
don't
seem
to
work
at
all
either
so
I
need
to
get
that
source
and
see
what
the
heck
it
is
doing
differently
.
This
discussion
outlines
the
challenges
I'm
having
:
http://google-app-engine.75637.x6.nabble.com/Download-specific-Module-code-td699.html
I
was
told
to
try
using
appcfg.py
download_app
:
The
problem
is
that
this
command
does
not
allow
me
to
specify
module
/
service
so
I
can't
target
my
source
code
.
Also
I'm
not
the
App
owner
so
I
can't
download
everything
but
from
what
I
understand
that
still
wouldn't
work
because
appcfg
only
seems
to
target
the
default
service
.
The
article
I
linked
ends
with
a
suggestion
to
use
gcloud
to
download
the
app
but
I
haven't
been
able
to
find
how
to
do
that
.
Does
anyone
here
know
how
to
resolve
this
?
The
service
is
running
in
a
Flexible
instance
-
not
sure
if
that
makes
a
difference
...
From
appcfg.py
download_app
-
-
help
:
Usage
:
appcfg.py
[options]
download_app
-
A
app_id
[
-
V
version
]
...
-
M
MODULE
","
-
-
module=MODULE
Set
the
module
","
overriding
the
module
value
from
app.yaml
.
...
This
invocation
allowed
me
to
download
the
code
for
the
non-default
python
module
of
:
Of
course
","
you
need
to
meet
authentication
and
ownership
requirements
","
etc.
And
your
app
shouldn't
have
been
configured
for
prevention
of
source
code
downloads
(
ireversible
config
)
I
am
attempting
to
use
collectstatic
in
the
bash
console
to
get
my
CSS
running
on
a
django
app
on
pythonanywhere
.
Unfortunately
","
I
am
getting
an
error
:
Here
is
the
.
py
where
timezone
is
imported
If
I
am
not
mistaken
","
pythonanywhere
uses
Django
1.3.7
by
default
.
It
looks
like
Django
timezone
support
was
not
added
until
version
1.4
:
https://docs.djangoproject.com/en/1.10/releases/1.4/#what-s-new-in-django-1-4
You
should
update
Django
to
the
most
recent
version
(
or
at
least
an
earlier
version
)
and
everything
should
work
as
expected
(
at
least
with
timezones
)
.
You
can
upgrade
by
opening
a
bash
console
from
the
Consoles
tab
on
your
pythonanywhere
profile
","
and
running
the
command
:
Or
install
a
newer
version
in
a
virtualenv
:
Edit
:
I
tested
out
my
first
suggestion
","
and
was
not
able
to
get
it
to
work
on
my
pythonanywhere
account
(
I
think
it
has
to
do
with
the
permissions
that
pythonanywhere
gives
their
users
)
.
However
","
using
the
second
method
(
i.e.
","
using
a
virtualenv
)
did
work
to
install
the
latest
version
of
Django
","
which
does
include
timezone
support
in
django.utils.timezone
.
you
just
need
(
assuming
you
setup
your
database
and
installed_apps
stuff
in
settings.py
)
In
your
project's
settings.py
","
just
add
this
at
beginning
.
Then
you
can
use
django
orm
","
remember
to
delete
the
middleware
you
don't
need
in
django
settings
.
You
have
have
a
python
script
that
requires
some
celery
tasks
and
you
need
Django
ORM
too
for
the
database
interactions
.
You
can
setup
the
django
project
create
an
app
for
your
purpose
","
include
in
settings.py
and
inside
your
app
in
models.py
create
the
required
models
.
ref
:
What
minimal
files
i
need
to
use
django
ORM
Set
up
the
environment
for
executing
celery
.
Ie
","
redis
server
.
integrate
""""
djcelery
""""
with
django
project
.
for
the
celery
task
purpose
.
you
can
use
celery
beats
for
the
periodic
tasks
.
or
delay
.
ref
:
http://docs.celeryproject.org/en/latest/django/first-steps-with-django.html
You
can
import and
use
the
django
models
like
normal
inside
the
celery
tasks
.
And
The
celery
tasks
you
can
run
using
i
.
celery
-
A
tasks
worker
-
-
loglevel=info
ii
.
celery
-
A
tasks
beat
-
l
info
.
use
beats
if
you
want
the
tasks
which
are
written
for
periodic
execution
.
If
the
tasks
need
to
be
executed
asynchronously
just
immediately
or
after
a
time
interval
","
you
can
use
task_name.delay()
call
the
tasks
inside
the
python
script
using
delay()
i
think
to
use
djcelery
in
your
script
you
may
need
to
set
up
the
django
env
inside
the
script
.
just
Do
django.setup()
.
i
think
this
will
help
you
to
solve
your
problem
.
I
am
developing
a
small
independent
python
application
which
uses
Celery
.
I
have
built
this
using
django
framework
but
my
application
is
back
end
only
.
This
means
that
the
users
do
not
need
to
visit
my
site
and
my
application
is
built
only
for
the
purpose
of
receiving
tasks
queue
from
celery
and
performing
operations
on
the
database
.
In
order
to
perform
operations
on
the
database
","
I
need
to
use
Django
modules
.
What
I
am
trying
to
do
is
eliminate
the
rest
of
my
django
application
and
use
ONLY
celery
and
django
models
modules
(
including
the
dependencies
required
to
run
these
)
.
In
short
","
my
simple
celery
application
will
be
running
receiving
instructions
from
my
redis
broker
and
perform
operations
in
database
using
django
models
.
Is
is
possible
to
do
this
?
If
so
","
how
?
Here
is
my
project
structure
:
Here
is
my
settings.py
:
I
am
trying
to
send
images
from
a
Java
client
to
a
Python
server
using
gRPC
and
ProtoBuf
.
I
encode
the
image
as
a
ByteString
and
send
it
with
a
blockingStub
.
The
Python
server
receives
the
ProtoBuf
","
but
the
Java
ByteString
is
now
a
Python
str
.
I'm
not
sure
how
to
restore
the
image
from
the
str
.
When
I
call
in
the
server
:
it
raises
:
Traceback
(
most
recent
call
last
)
:
File
""""
/
usr
/
local
/
lib
/
python2.7
/
dist-packages
/
grpc
/
_server.py
""""
","
line
364
","
in
_call_behavior
return
"behavior(argument, context)"
","
True
File
""""
process_server.py
""""
","
line
57
","
in
Process
request.ParseFromString(request.png_encoded)
File
""""
/
usr
/
local
/
lib
/
python2.7
/
dist-packages
/
google
/
protobuf
/
message.py
""""
","
line
185
","
in
ParseFromString
self.MergeFromString(serialized)
File
""""
/
usr
/
local
/
lib
/
python2.7
/
dist-packages
/
google
/
protobuf
/
internal
/
python_message.py
""""
","
line
1082
","
in
MergeFromString
if
"self._InternalParse(serialized, 0, length)"
!
=
length
:
File
""""
/
usr
/
local
/
lib
/
python2.7
/
dist-packages
/
google
/
protobuf
/
internal
/
python_message.py
""""
","
line
1108
","
in
InternalParse
new_pos
=
"local_SkipField(buffer, new_pos, end, tag_bytes)"
File
""""
/
usr
/
local
/
lib
/
python2.7
/
dist-packages
/
google
/
protobuf
/
internal
/
decoder.py
""""
","
line
850
","
in
SkipField
return
WIRETYPE_TO_SKIPPER
[wire_type]
(
buffer
","
pos
","
end
)
File
""""
/
usr
/
local
/
lib
/
python2.7
/
dist-packages
/
google
/
protobuf
/
internal
/
decoder.py
""""
","
line
820
","
in
_RaiseInvalidWireType
raise
_DecodeError('Tag had invalid wire type.')
DecodeError
:
Tag
had
invalid
wire
type
.
Java
client
:
Python
server
:
I've
tried
to
google
""""
Decode
Java
Protobuf
in
Python
""""
and
""""
decode
java
protobuf
Bytestring
in
python
""""
but
had
no
luck
.
Thanks
in
advance
!
ByteString
in
Java
is
just
an
immutable
byte
[]
.
The
Python
2
equivalent
is
str
.
It
is
a
binary
representation
.
If
you
wanted
to
save
the
image
to
disk
","
you
would
:
Iterating
over
the
groups
of
partial
entries
looks
like
a
job
for
itertools.groupby
.
But
first
lets
put
relationships
into
a
format
that
is
easier
to
use
","
prehaps
a
back_populates:label
dictionary
?
Next
because
we
will
be
using
itertools.groupby
it
will
need
a
keyfunc
to
distinguish
between
the
different
groups
of
entries
.
So
given
one
entry
from
the
initial
results
","
this
function
will
return
a
dictionary
with
only
the
pairs
that
will
not
be
condensed
/
converted
Now
we
will
be
able
to
traverse
the
results
in
groups
something
like
this
:
The
only
issue
is
that
if
we
build
our
entry
from
base_info
it
will
confuse
groupby
so
we
need
to
make
an
entry
to
work
with
:
Note
that
I
am
using
sets
here
because
they
are
the
natural
container
when
all
contence
are
unique
","
however
because
it
is
not
json-compatible
we
will
need
to
change
them
into
lists
at
the
end
.
Now
that
we
have
an
entry
to
build
we
can
just
iterate
through
the
group
to
add
to
each
new
field
from
the
original
then
once
the
final
entry
is
constructed
all
that
is
left
is
to
convert
the
sets
back
into
lists
And
that
entry
is
done
","
now
we
can
either
append
it
to
a
list
called
new_results
but
since
this
process
is
essentially
generating
results
it
would
make
more
sense
to
put
it
into
a
generator
making
the
final
code
look
something
like
this
:
Then
the
new_results
can
be
gotten
like
this
:
I
have
a
database
schema
in
Postgres
that
looks
like
this
(
in
pseudo
code
)
:
Hopefully
this
makes
intuitive
sense
.
It's
a
users
table
that
has
a
many-to-many
relationship
with
a
permissions
table
as
well
as
a
many-to-many
relationship
with
an
addresses
table
.
In
Python
","
when
I
perform
the
correct
SQLAlchemy
query
incantations
","
I
get
back
results
that
look
something
like
this
(
after
converting
them
to
a
list
of
dictionaries
in
Python
)
:
So
in
this
contrived
example
","
Joe
is
both
a
user
and
and
an
admin
.
John
is
only
a
user
.
Both
Joe's
home
and
work
addresses
exist
in
the
database
.
Only
John's
home
address
exists
.
So
the
question
is
","
does
anybody
know
the
best
way
to
go
from
these
SQL
query
'
results
'
to
the
more
compact
'
desired_results
'
below
?
Additional
information
required
:
Small
list
of
dictionaries
describing
the
'
labels
'
I
would
like
to
use
in
the
desired_results
for
each
of
the
fields
that
have
many-to-many
relationships
.
Final
consideration
","
I've
put
together
a
concrete
example
for
the
purposes
of
this
question
","
but
in
general
I'm
trying
to
solve
the
problem
of
querying
SQL
databases
in
general
","
assuming
an
arbitrary
amount
of
relationships
.
SQLAlchemy
ORM
solves
this
problem
well
","
but
I'm
limited
to
using
SQLAlchemy
Core
;
so
am
trying
to
build
my
own
solution
.
Update
Here's
an
answer
","
but
I'm
not
sure
it's
the
best
/
most
efficient
solution
.
Can
anyone
come
up
with
something
better
?
If
you
check
the
implementation
of
get_context_name()
","
you'll
see
this
:
And
the
implementation
for
get_context_data()
(
from
SingleObjectMixin
)
:
So
you
can
see
that
get_context_data()
adds
to
the
dictionary
an
entry
with
the
key
context_object_name
(
from
get_context_object_name()
)
which
returns
obj._meta.model_name
when
self.context_object_name
isn't
defined
.
In
this
case
","
the
view
got
self.object
as
a
consequence
of
the
call
to
get()
which
calls
get_object()
.
get_object()
takes
the
model
that
you've
defined
and
automatically
queries
it
from
your
database
using
the
pk
you've
defined
in
your
urls.py
file
.
http://ccbv.co.uk/
is
a
very
good
website
for
seeing
all
of
the
functions
and
attributes
the
class
based
views
of
Django
has
to
offer
in
a
single
page
.
In
the
following
code
","
How
does
the
template
details.html
knows
that
album
is
passed
to
it
by
views.py
although
we
have
never
returned
or
defined
any
context_object_name
in
DetailsView
class
in
views.py
.
Please
explain
how
are
the
various
things
getting
connected
here
.
details.html
views.py
urls.py
Thanks
in
advance
!
!
here
is
my
basic
code
.
currently
type
hint
shows
the
type
of
a.to(b)
is
Optional
[State]
","
but
what
I
wanted
is
to
return
the
parameter
is
self
","
which
is
BState
.
Is
this
available
in
the
type
system
in
Python
?
If
you'd
like
to
have
a
function
always
return
exactly
the
same
type
as
its
parameter
","
you
can
use
generics
via
the
TypeVars
class
:
The
expression
TState
=
"TypeVar('TState', bound=State)"
means
""""
create
a
new
generic
parameter
named
TState
which
must
always
be
a
subclass
of
a
State
object
.
""""
However
","
since
the
State
class
isn't
defined
yet
","
we
need
to
use
a
forward
reference
and
have
the
bound
be
a
string
instead
of
the
class
name
:
TState
=
"TypeVar('TState', bound='State')"
.
You
could
also
do
TState
=
TypeVar('TState')
which
means
""""
create
a
new
generic
parameter
named
TState
which
can
be
anything
""""
","
but
that
probably
isn't
what
you
want
so
I
don't
recommend
it
.
You
can
learn
more
about
upper
bounds
in
TypeVars
here
:
http://mypy.readthedocs.io/en/latest/generics.html#type-variables-with-upper-bounds
Solved
by
using
.
ebextentions
to
run
pre-install
commands
in
the
container
during
deployment
","
which
required
setting
the
system
path
during
eb
deploy
Is
it
possible
to
deploy
multiple
platforms
to
AWS
?
I
have
a
PHP
application
that
I
would
also
like
to
run
a
small
python
script
.
I
see
the
PHP
platform
installs
Python
by
default
","
but
using
eb
deploy
AWS
does
not
pick
up
requirements.txt
and
install
the
dependencies
.
I
have
tried
installing
requirements.txt
manually
which
hangs
when
trying
to
install
lxml
.
I
also
tried
adding
a
config
file
:
But
now
eb
deploy
complains
about
:
Which
I
believe
is
because
eb
thinks
this
is
only
a
PHP
app
.
What
is
the
proper
way
to
run
multiple
platforms
side-by-side
?
So
i
am
trying
to
call
the
mysql
function
TIMEDATEDIFF
with
the
SECOND
constant
as
the
first
parameter
like
so
I
have
tried
it
as
a
string
and
I
get
a
mysql
/
mariadb
error
:
Gives
me
this
I
am
sure
it
is
something
simple
","
some
sort
of
escape
sequence
or
import that
I
am
missing
.
I
have
looked
through
the
sqlalchemy
documentation
to
no
avail
.
To
get
sqlalchemy
to
parse
the
string
exactly
into
the
query
I
used
the
_literal_as_text()
function
Working
solution
I
wrote
a
small
program
to
do
the
following
.
I'm
wondering
if
there
is
an
obviously
more
optimal
solution
:
1
)
Take
2
lists
of
strings
.
In
general
","
the
strings
in
the
second
list
will
be
longer
than
in
the
first
list
","
but
this
is
not
guaranteed
2
)
Return
a
list
of
strings
derived
from
the
second
list
that
has
removed
any
matching
strings
from
the
first
list
.
The
list
will
therefore
contain
strings
that
are
<
=
the
length
of
the
strings
in
the
second
list
.
Below
I've
displayed
a
picture
example
of
what
I'm
talking
about
:
so
far
I
this
is
what
I
have
.
It
seems
to
be
working
fine
","
but
I'm
just
curious
if
there
is
a
more
elegant
solution
that
I'm
missing
.
By
the
way
","
I'm
keeping
track
of
the
""""
positions
""""
of
each
start
and
end
of
the
string
","
which
is
important
for
a
later
part
of
this
program
.
A
Sample
output
:
Let's
define
this
string
","
s
","
and
this
list
list1
of
strings
to
remove
:
Now
","
let's
remove
those
strings
:
One
of
the
powerful
features
of
the
above
is
that
the
strings
in
list1
can
contain
regex-active
characters
.
That
may
also
be
undesirable
.
As
John
La
Rooy
points
out
in
the
comments
","
the
strings
in
list1
can
be
made
inactive
with
:
Using
regular
expressions
simplifies
the
code
","
but
it
may
or
may
not
be
more
efficient
.
get
the
positions
like
this
(
you'll
need
to
add
your
offsets
to
these
)
get
the
new
strings
like
this
or
the
flattened
list
You
have
a
contingency
table
.
To
perform
the
Ï‡2
test
on
this
data
","
you
can
use
scipy.stats.chi2_contingency
:
Your
contingency
table
is
2x2
","
so
you
can
use
Fisher's
exact
test
.
This
is
implemented
in
scipy
as
scipy.stats.fisher_exact
:
scipy
doesn't
have
much
more
for
contingency
tables
.
It
looks
like
the
next
release
of
statsmodels
will
have
more
tools
for
the
analysis
of
contingency
tables
","
but
that
doesn't
help
right
now
.
It
isn't
hard
to
write
some
code
to
compute
the
difference
in
proportion
and
its
95
%
confidence
interval
.
Here's
one
way
:
For
example
","
The
first
value
returned
is
the
difference
in
proportions
delta
.
The
next
two
pairs
are
the
Wald
95
%
confidence
interval
for
delta
","
and
the
Wald
95
%
confidence
interval
with
Yates
continuity
correction
.
If
you
don't
like
those
negative
values
","
you
can
reverse
the
rows
first
:
For
comparison
","
here
is
a
similar
calculation
in
R
:
I
am
new
to
chi
squared
testing
and
trying
to
figure
out
what
the
'
standard
'
way
of
running
a
chi-squared
test
and
also
getting
a
95
%
confidence
interval
on
the
difference
between
success
rates
in
two
experiments
.
My
data
looks
like
this
:
These
numbers
represent
the
counts
of
what
was
observed
during
an
experiment
.
As
you
can
see
","
the
number
of
samples
for
Condition
A
vs
.
Condition
B
were
not
the
same
.
What
I'd
like
to
get
is
:
A
test
statistic
indicating
whether
Condition
A's
success
rate
of
33
%
is
statistically
different
from
Condition
B's
success
rate
of
50
%
.
I'd
also
like
to
get
a
95
%
confidence
interval
on
the
difference
between
the
two
success
rates
.
It
seems
like
scipy.stats.chisquare
expects
the
user
to
adjust
the
'
expected
'
counts
so
that
they
appear
to
be
taken
out
of
the
same
sample
size
as
the
'
observed
'
counts
.
Is
that
the
only
transformation
I
need
to
do
?
If
not
","
what
else
do
I
need
to
do
?
Finally
","
how
would
I
go
about
calculating
the
95
%
confidence
interval
for
the
difference
in
proportions
?
You
do
need
to
import the
modules
somehow
but
the
“
recommended
”
way
is
a
bit
tricky
since
things
kept
changing
with
new
versions
of
Python
","
see
this
answer
.
Here's
an
example
based
on
the
test
runner
of
the
gorilla
library
that
is
compatible
with
Python
2
and
Python
3
:
Here
/
path
/
to
/
your
/
package
would
be
the
repository
to
the
package
","
that
is
the
directory
containing
the
readme
","
the
docs
","
and
so
on
like
it
is
commonly
the
case
with
Python's
packages
","
and
package_name
would
be
the
directory
name
inside
the
repository
that
contains
the
root
__init__.py
file
.
In
fact
the
function
module_iterator
also
accepts
a
module
for
its
parameter
package_dotted_path
","
such
as
'
package.subpackage.module
'
for
example
","
but
then
only
the
functions
from
this
specific
module
would
be
retrieved
.
If
a
package
is
passed
instead
","
then
all
the
recursively
nested
modules
will
be
inspected
.
I
am
trying
to
list
all
the
functions
in
every
encountered
module
to
enforce
the
writing
of
test
cases
.
However
","
I
am
having
issues
doing
this
with
just
the
fullpath
to
the
file
as
a
string
and
not
importing
it
.
When
I
use
"inspect.getmembers(fullpath, inspect.isfunction)"
it
returns
an
empty
array
.
Is
there
any
way
to
accomplish
this
?
Here
is
my
code
so
far
:
I
am
trying
to
format
the
results
of
a
query
such
that
results
are
printed
on
their
respective
lines
.
For
example
","
I
am
querying
stores
by
store
number
and
obtaining
the
location
from
a
JSON
file
","
but
when
printing
","
the
store
number
and
location
are
printing
on
separate
lines
:
Code
Snippet
:
(
Searching
for
stores
35
and
96
)
Current
Output
:
Store
:
35
{
'
location
'
:
Iowa
}
Store
:
96
{
'
location
'
:
Minnesota
}
Desired
output
(
or
something
similar
)
:
Store
:
35
","
'
location
'
:
Iowa
Store
:
96
","
'
location
'
:
Minnesota
I
would
write
all
the
output
in
a
variable
and
print
the
variable
only
once
at
the
end
.
This
also
allows
you
to
save
time
(
despite
using
more
memory
)
since
you
need
only
a
single
access
to
the
stdout
.
The
code
is
also
easier
to
follow
(
in
my
opinion
)
:
You
can
also
print
at
the
end
of
each
iteration
(
you
still
access
half
of
the
times
to
the
stdout
)
with
the
following
:
If
you
want
a
new
line
for
each
Store
but
not
for
each
""""
location
""""
:
I
think
this
approach
is
much
more
flexible
","
easier
to
read
in
the
code
and
is
also
faster
.
Hope
to
be
helpful
Adding
end
=
'
'
to
your
first
print
statement
should
fix
the
problem
.
By
specifying
that
the
end
character
is
an
empty
string
you
will
override
the
default
\
n
character
(
by
default
print
statements
end
with
a
new
line
character
)
.
We
will
only
add
end
=
'
'
to
the
first
print
statement
because
we
want
the
new
line
to
print
after
you
print
out
the
location
.
If
you
want
to
separate
your
prints
with
a
","
of
course
you
would
just
add
+
'
","
'
to
your
first
print
statement
.
This
will
work
right
off
the
bat
if
you're
using
Python
3
.
If
you're
using
Python
2.X
you
will
have
to
add
this
line
to
the
top
of
your
file
:
from
__future__
import print_function
Here's
a
simple
example
of
this
in
action
:
If
we
took
the
same
code
but
altered
it
slightly
and
just
removed
the
end
=
'
'
","
this
is
what
would
happen
:
As
you
can
see
each
line
would
end
with
a
new
line
character
","
this
printing
a
new
line
for
each
statement
.
Okay
so
two
ways
I
did
quickly
!
The
second
loop
is
definitely
the
way
to
go
.
It
uses
re.sub
(
as
someone
else
commented
too
)
.
It
replaces
with
the
lowercase
search
term
bear
in
mind
.
Maybe
I'm
misinterpretting
your
question
","
but
wouldn't
re.sub
be
the
best
option
?
Example
:
https://repl.it/DExs
I
wrote
a
script
in
Python
for
custom
HTML
page
that
finds
a
word
within
a
string
/
line
and
highlights
just
that
word
with
use
of
following
tags
where
instance
is
the
word
that
is
searched
for
.
With
the
following
result
:
I
need
to
find
a
word
(
case
insensitive
)
let's
say
""""
port
""""
within
a
string
that
can
be
port
","
Port
","
SUPPORT
","
Support
","
support
etc
","
which
is
easy
enough
.
However
my
strings
often
contain
2
or
more
instances
in
single
line
","
and
I
need
to
append
<
b><font
"color=\""red\"">""+instance"
+
""""
<
/
font
>
<
/
b
>
to
each
of
those
instances
","
without
changing
cases
.
Problem
with
my
approach
","
is
that
I
am
attempting
to
itterate
over
each
of
instances
found
with
findall
(
exact
match
)
","
while
multiple
same
matches
can
also
be
found
within
the
string
.
This
results
in
following
:
when
I
need
I
was
thinking
","
I
would
be
able
to
avoid
this
if
I
was
able
to
find
out
exact
part
of
the
string
that
the
pattern.sub
substitutes
at
the
moment
of
doing
it
","
however
I
was
not
able
to
find
any
examples
of
that
kind
of
usage
","
which
leads
me
to
believe
that
I
am
doing
something
very
wrong
.
If
anyone
have
a
way
I
could
use
to
insert
<
b><font
"color=""red"">instance"
<
/
font
>
<
/
b
>
without
replacing
instance
for
all
matches(case insensitive)
","
then
I
would
be
grateful
.
By
my
lights
the
easiest
thing
to
do
would
be
to
pass
the
spi
object
to
the
helper
function
as
a
parameter
:
Once
it's
passed
in
","
you
can
call
methods
on
it
in
the
body
of
the
function
.
I
removed
your
variable
assignment
because
it
seemed
redundant
.
I
have
two
.
py
script
files
.
The
""""
main
""""
script
will
import the
second
script
containing
misc
""""
helper
""""
functions
.
In
the
main
script
","
I
have
set
up
an
object
for
a
SPI
interface
.
I
would
like
to
write
functions
in
the
imported
file
that
use
the
SPI
interface
directly
.
I'm
a
noob
at
this
and
tried
writing
and
passing
in
various
ways
but
always
get
errors
.
mainscript.py
helperfunctions.py
I
have
the
same
general
question
often
regarding
global
variable
values
as
well
.
I'm
sure
there
is
a
""""
better
""""
way
to
do
it
","
but
out
of
convenience
for
now
","
I
often
wish
to
define
some
global
variables
in
mainscript.py
then
reference
those
globals
inside
functions
of
helperfunctions.py
.
I
can't
figure
a
way
to
do
this
.
Going
the
other
way
is
easy
-
declare
the
globals
inside
helperfunctions.py
then
reference
them
from
mainscript.py
as
helper.variableName
","
but
I
don't
know
how
to
go
the
other
direction
.
Any
direction
is
much
appreciated
.
Thank
you
.
You
can
do
something
like
np.empty(shape = [1] * (dimensions - 1)
+
[0]
)
.
Example
:
Iteratively
adding
rows
of
that
rank-1
using
"np.concatenate(a,b,axis=0)"
Don't
.
Creating
an
array
iteratively
is
slow
","
since
it
has
to
create
a
new
array
at
each
step
.
Plus
a
and
b
have
to
match
in
all
dimensions
except
the
concatenation
one
.
will
give
you
dimensions
error
.
The
only
thing
you
can
concatenate
to
such
an
array
is
an
array
with
size
0
dimenions
To
work
iteratively
","
start
with
a
empty
list
","
and
append
to
it
;
then
make
the
array
at
the
end
.
a
=
"np.zeros((1,1,1,1,1,0)"
)
could
be
concatenated
on
the
last
axis
with
another
"np.ones((1,1,1,1,1,n)"
)
array
.
You
could
directly
use
the
ndarray
constructor
:
Or
the
empty
variant
","
since
it
seems
to
be
more
popular
:
You
can
use
empty
or
zeros
.
For
example
","
to
create
a
new
array
of
2x3
","
filled
with
zeros
","
use
:
"numpy.zeros(shape=(2,3)"
)
I
basically
want
to
initialize
an
empty
6-tensor
","
like
this
:
Is
there
a
better
way
than
writing
the
brackets
explicitly
?
I'm
a
little
confused
regarding
a
Python
exercise
that
requires
a
case
insensitive
input
.
This
part
of
the
excercise
that
confuses
me
:
Make
sure
your
comparison
is
case
insensitive
.
If
'
John
'
has
been
used
","
'
JOHN
'
should
not
be
accepted
.
Here
is
my
code
:
My
problem
is
that
I'm
trying
to
get
my
code
to
reject
""""
Username_2
""""
because
of
the
capital
""""
U
""""
but
I
have
no
idea
how
to
do
this
.
I'm
reading
through
Python
Crash
Course
by
Eric
Matthes
and
am
currently
on
chapter
5
but
I
don't
recall
being
taught
how
to
reject
case
insensitive
inputs
yet
.
I
know
of
the
upper()
","
lower()
","
and
title()
string
methods
and
I
tried
using
:
before
my
for
loop
","
but
this
just
results
in
a
syntax
error
.
You
can
convert
each
new
username
to
lower
case
and
compare
it
to
a
lower
case
version
of
the
user
list
(
created
using
a
list
comprehension
)
.
You
may
want
to
store
your
usernames
in
lowercase
when
they
are
first
created
to
avoid
creating
a
lower
case
version
from
the
user
list
.
You
can
convert
Booleans
to
integers
as
follows
:
Depending
on
the
nature
of
your
text
in
Column
A
","
you
can
do
a
few
things
:
a
)
Split
on
the
space
and
take
the
second
part
(
either
string
or
int
depending
on
your
needs
)
.
b
)
Use
regex
to
extract
any
digits
(
\
d
*
)
from
the
end
of
the
string
.
I
have
a
set
of
data
like
How
can
I
keep
the
numerical
part
of
column
a
and
transfer
ture
to
1
","
false
to
zero
at
the
same
time
.
Below
is
what
I
want
.
This
works
","
but
the
actual
image
no
longer
shows
","
if
I
keep
the
PhotoImage
OUT
of
the
class
it
will
print
but
I
want
to
have
it
print
IF
they
click
the
specific
button
:
You
need
to
maintain
a
reference
to
your
PhotoImage
object
.
Unfortunately
there
is
an
inconsistency
in
tkinter
in
that
attaching
a
Button
to
a
parent
widget
increments
the
reference
count
","
but
adding
an
image
to
a
widget
does
not
increment
the
reference
count
.
As
a
consequence
at
the
moment
the
CharPhoto
variable
goes
out
of
scope
at
the
end
of
the
function
CharClick
","
the
number
of
reference
to
the
PhotoImage
falls
to
zero
and
the
object
is
made
available
for
garbage
collection
.
If
you
keep
a
reference
to
the
image
somewhere
","
it
will
appear
.
When
you
kept
it
globally
it
remained
in
scope
for
the
entire
script
and
hence
appeared
.
You
can
keep
a
reference
to
it
in
the
PokemonClass
object
or
in
the
Label
widget
.
Below
is
the
later
of
those
options
First
time
here
so
forgive
me
as
this
is
my
FIRST
attempt
at
making
a
silly
GUI
game
(
if
you
want
to
call
it
that
)
.
I'm
trying
to
get
the
user
to
click
a
button
and
the
image
of
their
selection
pops
up
.
I
can't
seem
to
figure
out
how
to
get
the
image
to
pop
up
though
.
Image
does
show
if
I
run
it
separately
.
My
code
:
If
I
have
a
df
similar
to
this
one
:
Thanks
to
a
lot
of
input
from
the
community
I
have
this
code
now
which
allows
me
to
upsample
my
df
to
second
intervals
","
applying
different
methods
to
different
dtypes
I
am
looking
for
a
way
now
","
to
only
interpolate
between
my
actual
measurements
.
The
interpolate
function
extends
my
last
measurement
until
the
end
of
the
df
:
But
I
would
like
to
stop
this
when
the
last
measurement
took
place
(
for
example
at
14:04:00
col
[
'
D
'
]
and
14:06:00
col
[
'
D
'
]
)
and
leave
the
NaNs
.
It
tried
adding
a
zero
value
for
'
limit
'
and
'
limit_direction
'
to
'
both
'
:
but
this
didn't
change
anything
to
the
output
.
I
than
tried
to
incorporate
the
solution
I
found
to
this
question
:
Pandas
:
interpolation
where
first
and
last
data
point
in
column
is
NaN
into
my
code
:
...
but
that
did
not
work
and
my
float64
columns
are
purely
NaNs
now...Also
","
the
way
I
tried
to
insert
the
code
","
I
know
it
would
only
have
affected
the
float
columns
.
In
an
ideal
solution
I
would
hope
to
do
the
set
this
first_valid_index()
:
.
last_valid_index()
selection
also
to
the
object
and
int64
columns
.
Can
somebody
help
me
?
.
.
thank
you
You
can
backfill
null
values
and
then
use
boolean
indexing
to
take
the
null
values
of
each
column
(
which
must
be
the
tail
nulls
)
.
You
were
very
close
!
Here's
an
example
to
get
the
point
across
that's
very
similar
to
the
code
you
posted
toward
the
end
of
your
post
:
result
:
The
two
problems
in
your
code
were
:
If
you
are
going
to
do
df
[
...
]
=
df
[
...
]
.
interpolate()
","
you
need
to
remove
inplace=True
since
that
will
make
it
return
None
.
That
is
your
main
problem
and
why
you
were
getting
all
NaNs
.
Though
it
seems
to
work
here
","
in
general
","
chained
indexing
is
bad
:
You
want
:
Not
:
See
here
for
more
detail
:
http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
Your
__init__
function
for
your
admin
class
needs
a
line
I'm
trying
to
load
my
csv
in
QTableView
and
when
I
click
on
the
button
it
gives
this
errorFile
Zap
Lunchies.py
","
line
61
","
in
on_show_clicked
self.loadCsv(self.fileName)
AttributeError
:
'
admin
'
object
has
no
attribute
'
fileName
'
This
is
my
code
currently
If
anyone
can
help
that
would
be
much
appreciated
.
And
the
csv
file
is
called
""""
data.csv
""""
Groupby
index
and
sum
the
columns
should
give
you
what
you
need
:
I
have
dataframe
below
;
I
would
like
to
conditional
summing
dataframe
shown
as
below
;
each
elements
shows
conditional
sum
of
df.for
instance
","
(
I
am
inconfident
about
expression
)
how
can
I
get
this
dataframe
?
You
have
misspelled
djangoproject01
as
djangoproyect01
in
the
INSTALLED_APPS
variable
in
settings.py
(
you
have
an
app
named
djangoproject01
but
reference
djangoproyect01
instead
)
","
this
is
what
is
causing
your
error
.
Update
the
last
entry
in
INSTALLED_APPS
to
'
djangoproject01
'
.
Make
sure
you
are
consistent
too
","
otherwise
you
will
make
things
more
difficult
for
yourself
.
It
looks
like
your
project
sometimes
uses
the
""""
word
""""
proyect
and
sometimes
uses
the
word
project
(
e.g
.
","
you
also
have
a
directory
DjangoProyects
.
Also
","
in
the
future
it
is
best
to
post
the
text
of
your
code
and
any
tracebacks
you
are
seeing
rather
than
screen
shots
.
I
have
a
problem
using
the
following
command
:
But
it
prints
me
an
error
:
ImportError
:
No
module
named
djangoproyect01
I
don't
have
any
idea
what
would
it
be
.
Looking
for
some
help
understanding
how
to
optimize
some
array
processing
","
targeting
some
postgis
compatible
data
types
.
The
input
data
looks
like
this
:
Here
is
what
I've
tried
:
This
is
the
output
:
Why
are
the
operations
using
the
numpy
arrays
so
much
slower
than
the
normal
python
array
?
As
general
rule
","
iterative
operations
on
numpy
arrays
is
slower
than
equivalent
ones
on
lists
.
In
part
that's
because
creating
an
array
from
a
list
takes
time
","
whether
it's
the
initial
creation
or
some
intermediate
step
.
Numpy
arrays
gain
their
speed
advantage
when
you
perform
compiled
operations
on
them
-
ones
where
the
iteration
takes
place
at
compiled
speeds
rather
than
interpreted
ones
.
In
your
example
","
the
dictionary
source
isn't
important
Take
the
simple
task
of
adding
1
to
all
values
:
Your
string
formatting
case
:
or
with
list
comprehensions
instead
of
map
:
Performing
this
same
action
on
narr.tolist()
is
nearly
as
good
","
and
better
than
iterating
on
narr
directly
.
[
c
for
c
on
narr
]
produces
a
list
of
4
arrays
;
[
str(i)
for
i
in
c
]
then
requires
iterating
on
each
of
those
subarrays
.
Making
a
list
of
Point
objects
from
a
numpy
array
is
a
poor
use
of
the
structure
.
You
could
make
a
structured
array
","
and
access
'
x
'
values
for
a
whole
set
of
points
.
That's
the
key
-
use
arrays
when
you
want
to
work
with
the
whole
structure
","
or
at
least
whole
rows
and
columns
.
I
have
a
SQLAlchemy
db.Model
like
this
:
Now
I
have
a
query
like
:
I
want
to
execute
the
update
method
on
all
elements
of
this
query
.
The
following
is
very
inefficient
because
every
call
to
update
takes
2
/
3
seconds
.
How
can
I
run
update
on
all
elements
in
my
query
in
parallel
?
I
ended
up
changing
the
way
I
did
things
","
and
put
a
REST
API
around
the
Action
model
.
Updates
are
done
by
sending
PUT
to
a
/
api
/
action
/
{
id
}
endpoint
.
My
API
client
can
be
asynchronous
so
that
the
requests
are
executed
in
parallel
.
Now
the
REST
API
itself
is
multiprocessed
and
spread
over
multiple
instances
and
SQLAlchemy
ensures
connections
to
the
database
are
concurrent
.
Client
usage
looks
like
this
:
I
have
been
trying
to
use
the
following
code
to
move
files
that
are
listed
in
a
csv
list
.
But
at
most
it
will
copy
the
last
file
in
the
list
but
not
the
rest
.
I
keep
hitting
this
wall
with
every
example
I
have
seen
listed
what
am
I
doing
wrong
?
My
CVS
list
will
have
a
list
like
:
Here's
my
code
The
reason
why
it's
only
the
last
file
that
could
be
copied
(
if
it
was
)
is
because
in
this
line
:
you
are
referencing
rowDict
outside
of
the
first
for-loop
.
So
at
that
execution
moment
","
it
contains
the
last
value
of
this
loop
.
If
I
understand
correctly
what
you
are
trying
to
do
you
could
try
something
like
this
(
untested
code
)
:
So
instead
of
:
Constructing
a
dictionary
with
all
the
values
in
your
CSV
file
Somehow
check
for
every
file
in
your
source
directory
that
it
is
included
in
your
dictionary
(
which
is
what
I
interpreted
you
were
trying
to
do
)
And
move
it
if
it
does
.
You
could
:
For
every
file
extracted
from
your
CSV
","
check
that
it
exists
in
your
source
directory
.
If
it
does
","
you
move
it
to
the
destination
directory
.
Is
that
what
you
were
trying
to
doÂ
?
[
And
if
the
filename
stays
the
same
","
you
only
need
to
specify
the
destination
directory
for
the
second
argument
of
shutil.move()
]
I
think
doing
something
like
this
will
work
(
untested
)
:
As
an
optimization
","
unless
you
need
the
keys
dictionary
for
other
processing
","
you
could
change
the
first
part
so
it
just
creates
the
valid_files
set
variable
used
in
the
second
for
loop
:
This
old
answer
tells
to
apply
mode=ctypes.RTLD_GLOBAL
","
i.e.
in
this
case
I
want
to
use
some
functions
from
a
shared
library
in
python
.
From
the
python
doc
","
I
know
ctypes
is
a
good
choice
.
However
such
library
have
some
undefined
symbols
and
I
should
link
it
to
another
shared
library
to
get
the
symbols
.
In
g
+
+
","
it
is
simple
:
g
+
+
main.cpp
-
la
-
lb
.
The
function
I
need
is
in
liba.so
","
and
liba.so
has
some
undefined
function
which
can
be
solved
in
libb.so
.
But
how
to
do
that
in
ctypes
?
ctypes.cdll.LoadLibrary('liba.so')
said
that
there
are
some
undefined
symbols
","
how
to
tell
ctypes
to
find
libb.so
?
Because
ldd
liba.so
not
show
a
link
to
libb.so
.
Example
:
I
want
to
use
some
functions
in
gsl
.
In
g
+
+
:
and
ldd
libgsl.so
does
not
show
a
link
to
libgslcblas
In
python
:
how
to
tell
ctypes
to
find
libgslcblas
?
The
same
problem
also
happen
if
I
use
scalapack
.
I
use
ubuntu
16.04
One
option
is
to
clear
the
axes
each
iteration
with
cla()
.
Alternately
you
could
store
each
text
object
and
remove
them
when
needed
.
I
have
some
standard
boiler
plate
code
to
draw
a
simple
bar
graph
with
a
""""
count
""""
label
on
top
of
each
bar
.
I
am
live
updating
this
bar
graph
as
the
data
changes
.
I
am
able
to
successfully
live
update
the
graph
.
Here
is
the
code
:
which
as
you
can
see
is
standard
boiler
plate
code
.
In
my
main
I
do
:
get_updated_data
will
just
pull
new
data
and
call
the
code
to
generate
the
graph
.
This
is
correctly
updating
the
bars
.
However
","
the
labels
added
above
each
bar
aren't
overwritten
.
They
get
""""
stacked
""""
above
each
other
each
time
the
graph
is
refreshed
.
Whats
the
appropriate
way
to
fix
this
?
I
have
tried
fig.canvas.draw()
but
it
is
not
working
.
Maybe
I
am
adding
it
in
the
wrong
place
.
Whats
the
right
approach
?
Thanks
What
you
can
do
is
add
a
third
parameter
that
will
keep
track
of
the
original
size
","
and
have
your
arguments
in
your
overlay_frac()
function
be
based
off
of
that
variable
","
so
that
the
values
don't
alter
in
recursion
.
Here's
how
I've
done
it
:
Implement
the
function
tree
that
takes
a
number
of
slices
n
and
a
rune
as
arguments
","
and
generates
a
stack
of
runes
(
shape
)
scaled
and
overlayed
on
top
of
each
other
.
the
shape
at
the
top
must
be
1
/
n
the
size
of
the
original
shape
.
the
shape
at
the
second
level
must
be
2
/
n
the
size
of
the
original
shape
and
so
on
.
i.e.
the
shape
at
the
bottom
is
the
original
shape
.
So
I
tried
to
do
this
in
a
recursive
manner
using
some
ideas
from
mathematical
induction
.
My
question
is
how
would
you
fix
the
initial
value
of
n
because
this
recursion
keeps
altering
the
value
?
I
can't
set
a
=
n
outside
of
the
recursion
as
n
is
defined
in
the
function
.
Is
there
a
way
for
me
to
set
the
initial
value
of
n
to
another
letter
so
it
wouldn't
change
?
Right
now
I'm
not
even
sure
if
this
code
works
.
But
without
being
able
to
fix
it
","
I
can't
test
it
.
FYI
:
I
am
aware
that
you
guys
might
not
know
what
overlay_frac
and
scale
does
but
the
codes
for
those
are
given
to
me
.
My
question
really
revolves
around
a
way
to
set
the
initial
input
of
a
recursion
so
that
it
will
not
be
affected
as
the
process
loops
.
Adding
another
parameter
is
certainly
a
feasible
option
.
In
the
event
that
you
under
certain
constraints
and
unable
to
change
the
formal
parameters
defined
for
the
outer
function
","
you
may
consider
using
a
helper
function
as
well
.
Both
of
them
are
calling
python.exe
.
As
for
the
python.exe
there
is
no
difference
to
run
.
During
python
installation
","
if
you
selected
'
add
python
to
PATH
'
to
add
path
of
python.exe
into
environment
variables
","
then
","
you
can
run
python.exe
directly
in
PS
.
If
you
didn't
check
it
","
you
may
need
to
locate
the
folder
contains
python.exe
to
run
.
py
file
.
In
addition
","
python.exe
can
be
run
under
other
user
privilege
using
'
start-process
'
with
parameter
'
-
credential
'
.
If
you
run
python.exe
in
PS
console
it
will
inherits
user
privilege
of
PS
console
.
I
am
learning
powershell
and
trying
to
start
a
python
service
using
the
powershell
IDE
.
I
came
to
know
I
can
start
the
python
services
either
by
one
of
these
commands
.
Though
both
of
these
gives
the
same
result
","
I
am
curious
to
know
what
is
the
difference
between
these
two
in
terms
of
internal
functions
and
which
is
better
to
use
?
You
can't
pass
CPython-specific
PyXxx
structures
via
CFFI
:
you
need
to
pass
standard
C
data
.
Normally
I'd
answer
that
you
need
to
design
your
cdef()
'
ed
function
with
a
standard
C
interface
","
for
example
something
like
:
Then
you
need
to
manually
convert
the
myimage
to
a
numpy
array
","
somewhere
in
the
""""
...
""""
code
above
.
One
better
alternative
is
to
use
a
Python
callback
:
a
callback
that
makes
the
numpy
array
according
to
spec
and
returns
a
C-standard
float
*
pointer
.
The
numpy
array
itself
is
saved
somewhere
in
the
callback
.
You
could
save
it
as
a
Python
global
","
or
more
cleanly
use
a
""""
handle
""""
you
pass
via
C
.
Requires
the
API
version
","
not
the
ABI
.
In
_example_build.py
:
In
file
example.py
:
Idea
is
to
be
able
to
modify
array
from
library
","
like
an
""""
output
""""
from
a
function
.
Example
:
The
problem
is
that
you're
expecting
input
to
return
you
a
int
which
it
doesn't
do
on
Python
3
.
Python
3
input
works
the
same
way
as
raw_input
on
Python
2
returning
a
string
that
you
need
to
convert
to
other
type
yourself
.
You
should
be
able
to
fix
your
code
by
doing
the
conversions
at
the
required
places
and
switching
all
raw_input
calls
to
input
since
there's
no
raw_input
on
Python
3
.
Example
:
I'm
trying
to
have
only
one
defined
function
for
this
WELCOME
TO
Your
Test
Word
1
/
5
:
Potato
How
many
consanants
does
the
word
contain
?
3
Correct
!
Word
2
/
5
:
Potato
How
many
vowels
does
the
word
contain
?
1
Correct
!
Word
3
/
5
:
Name
How
many
vowels
does
the
word
contain
5
Incorrect
!
Correct
answer
4
Word
4
/
5
:
YES
How
many
letters
does
the
word
contain
?
3
Correct
!
Word
5
/
5
:
Day
What
is
letter
3
of
the
word
?
Y
Correct
!
Game
Over
.
Your
Score
is
4
/
5
@Niemmi
Like
this
this
?
import random
import string
I
am
recently
into
session
and
cookies
.
I
comprehend
session
and
cookies
well
in
theory
but
i
have
one
issue
in
understanding
the
code
of
session
.
It
is
about
getting
the
last_visit
by
the
user
.
The
code
is
from
the
tangowithdjango.com
When
i
tried
to
understand
what
request.session.get('last_visit')
does
","
i
get
None
all
the
time
.
What
i
don't
understand
is
the
key
'
last_visit
'
.
Is
it
default
object
inside
session
?
If
it
is
default
in
the
session
object
then
why
it
shows
None
every
time
in
my
terminal
.
Please
someone
make
me
understand
the
object
passed
inside
get()
.
Move
your
RequestContext
stuff
to
bottom
Each
item
in
blender
must
have
a
unique
name
within
the
list
of
items
it
belongs
to
(
each
name
is
a
dictionary
key
)
and
will
make
a
name
unique
by
appending
a
numeric
suffix
based
on
the
other
items
within
the
file
","
note
that
it
is
based
on
the
file
-
not
the
scene
","
as
a
blend
file
can
contain
multiple
scenes
.
Objects
that
have
been
deleted
are
not
considered
in
this
process
","
while
other
items
like
materials
and
mesh
data
remain
in
the
lists
until
the
file
is
closed
.
The
obj
importer
first
creates
the
mesh
datablock
and
then
creates
an
object
using
the
same
name
as
the
mesh
data
-
this
leads
to
the
new
objects
always
having
a
numeric
suffix
larger
than
previous
objects
.
If
you
are
importing
multiple
objects
using
a
python
script
you
can
rename
the
object
after
you
import it
.
In
this
scenario
any
existing
object
with
the
name
""""
Object
""""
will
get
renamed
to
have
a
suffix
.
I'm
using
a
python
script
to
import and
export
wavefront
obj
files
in
Blender
.
The
problem
is
that
Blender
adds
an
index
to
an
object's
name
if
an
object
with
the
same
name
was
already
added
.
For
example
myObject
becomes
myObject.001
if
there
was
already
an
object
called
myObject
added
in
the
past
(
even
if
said
object
was
removed
)
.
When
I
export
the
object
as
.
obj
the
names
are
no
longer
the
same
as
before
.
How
do
I
reset
that
""""
name-counter
""""
?
A
cheap
trick
to
accomplish
this
would
be
to
filter
by
disjoint
testing
using
a
dynamically
updated
set
of
exclusion
values
","
but
it
wouldn't
actually
avoid
generating
the
combinations
you
wish
to
exclude
","
so
it's
not
a
major
performance
benefit
(
though
filtering
using
a
C
built-in
function
like
isdisjoint
will
be
faster
than
Python
level
if
checks
with
continue
statements
typically
","
by
pushing
the
filter
work
to
the
C
layer
)
:
If
you
want
to
remove
all
tuples
containing
the
number
x
from
the
list
of
combinations
"itertools.combinations(l, 2)"
","
consider
that
you
there
is
a
one-to-one
mapping
(
mathematically
speaking
)
from
the
set
"itertools.combinations([i for i in range(1,len(l)"
]
","
2
)
to
the
"itertools.combinations(l, 2)"
that
don't
contain
the
number
x
.
Example
:
The
set
of
all
of
combinations
from
"itertools.combinations([1,2,3,4], 2)"
that
don't
contain
the
number
1
is
given
by
[
(
2
","
3
)
","
(
2
","
4
)
","
(
3
","
4
)
]
.
Notice
that
the
number
of
elements
in
this
list
is
equal
to
the
number
of
elements
of
combinations
in
the
list
"itertools.combinations([1,2,3], 2)"
=
[
(
1
","
2
)
","
(
1
","
3
)
","
(
2
","
3
)
]
.
Since
order
doesn't
matter
in
combinations
","
you
can
map
1
to
4
in
[
(
1
","
2
)
","
(
1
","
3
)
","
(
2
","
3
)
]
to
get
[
(
1
","
2
)
","
(
1
","
3
)
","
(
2
","
3
)
]
=
[
(
4
","
2
)
","
(
4
","
3
)
","
(
2
","
3
)
]
=
[
(
2
","
4
)
","
(
3
","
4
)
","
(
2
","
3
)
]
=
[
(
2
","
3
)
","
(
2
","
4
)
","
(
3
","
4
)
]
.
Given
a
list
l
and
all
combinations
of
the
list
elements
is
it
possible
to
remove
any
combination
containing
x
while
iterating
over
all
combinations
","
so
that
you
never
consider
a
combination
containing
x
during
the
iteration
after
it
is
removed
?
My
list
l
is
pretty
big
so
I
don't
want
to
keep
the
combinations
in
memory
.
You
want
to
do
the
math
between
a
vector
and
its
tranposition
.
Transpose
with
.
T
and
apply
the
matrix
dot
function
between
the
two
dataframes
.
numpy
as
a
faster
alternative
Timing
Given
sample
30
","
000
rows
I
have
loaded
the
below
CSV
file
containing
code
and
coefficient
data
into
the
below
dataframe
df
:
giving
From
the
above
dataframe
","
I
need
to
create
a
final
dataframe
as
below
which
has
a
matrix
structure
with
the
product
of
the
coefficients
:
I
am
using
np.multiply
but
I
am
not
successful
in
producing
the
result
.
Interactive
mode
or
Calculator
mode
","
is
an
interactive
mode
that
comes
with
python
.
If
you
have
installed
python
","
then
you
have
also
","
installed
something
called
the
Python
Shell
.
There
are
two
ways
you
can
access
the
Python
Shell
:
Typing
python
or
python
[
version-number
]
in
your
command
prompt
/
terminal
window
:
Assuming
that
you
have
python
in
your
PATH
variable
","
you
can
access
the
Python
Shell
by
simply
typing
python
or
python
[
version-number
]
in
your
command
prompt
/
terminal
window
.
Running
the
Python
Shell
in
the
Python
IDLE(Integrated Development Environment)
GUI
:
To
run
the
Python
Shell
in
the
Python
IDLE
GUI
","
you
can
"type(again i'm assuming that the path to your python installation folder, is in your PATH variable)"
","
just
type
idle
into
your
command
prompt\terminal
window
and
this
should
start
the
Python
Shell
in
the
Python
IDLE
GUI
.
Of
course
the
exact
text
for
the
Python
Shell
heading
will
vary
between
OS's
","
but
all
of
them
look
very
similar
.
Here
is
an
example
of
what
the
heading
appears
like
on
my
mac
:
As
you
might
can
tell
from
the
text
above
","
a
newline
in
the
Python
Shell
is
denoted
by
three
caret
symbols
:
>
>
>
.
Each
newline
","
a
new
three
carets
are
printed
.
When
using
Python
Shell
","
there
are
a
few
differences
","
then
just
typing
a
script
up
","
but
the
main
one
is
that
it
execute
your
code
line
by
line
.
In
the
Python
Shell
you
can
only
type
one
line
of
code
at
a
time
.
Here
is
an
example
to
illustrate
my
point
further
:
As
you
can
tell
from
the
above
program
","
indention
is
noted
by
three
dots
:
...
","
and
the
only
time
the
Python
Shell
shows
more
then
one
line
at
a
time
","
is
when
it
is
'
echoing
'
back
what
you
typed
in
.
Why
is
it
called
interactive
?
One
of
the
main
reason
its
called
interactive
is
that
to
display
variable
values
or
run
the
module
in
general
","
you
don't
have
to
explicitly
invoke
the
python
interpreter
.
Take
the
example
below
:
As
displayed
above
","
I
was
able
to
print
the
value
of
name
and
run
the
function
func()
without
having
to
use
a
print
command
or
explicitly
calling
the
python
interpreter
.
The
Python
Shell
is
not
really
a
practical
way
to
write
long
/
complex
programs
","
as
writing
code
__init__(see what i did there)
would
prove
to
be
cumbersome
and
awkward
.
A
better
choice
would
be
to
use
the
Python
IDLE
.
It
is
a
script
editor
for
python
that
comes
builtin
into
the
python
installation
package
.
Disclaimer
:
i
could
be
completely
wrong
about
this
.
I
am
simply
going
off
of
the
python
docs
and
my
own
experience
;
)
.
I
don't
know
about
the
meaning
of
calculator
mode
in
Python
","
and
I've
put
the
portion
of
documentation
below
.
If
you
quit
from
the
Python
interpreter
and
enter
it
again
","
the
definitions
you
have
made
(
functions
and
variables
)
are
lost
.
Therefore
","
if
you
want
to
write
a
somewhat
longer
program
","
you
are
better
off
using
a
text
editor
to
prepare
the
input
for
the
interpreter
and
running
it
with
that
file
as
input
instead
.
This
is
known
as
creating
a
script
.
As
your
program
gets
longer
","
you
may
want
to
split
it
into
several
files
for
easier
maintenance
.
You
may
also
want
to
use
a
handy
function
that
youâ€™ve
written
in
several
programs
without
copying
its
definition
into
each
program
.
To
support
this
","
Python
has
a
way
to
put
definitions
in
a
file
and
use
them
in
a
script
or
in
an
interactive
instance
of
the
interpreter
.
Such
a
file
is
called
a
module
;
definitions
from
a
module
can
be
imported
into
other
modules
or
into
the
main
module
(
the
collection
of
variables
that
you
have
access
to
in
a
script
executed
at
the
top
level
and
in
calculator
mode
)
.
(
Emphasis
mine
)
Here's
the
original
document
.
I
am
new
to
Django
and
I
am
currently
having
problems
in
showing
uploaded
images
in
Django
Admin
.
I
have
followed
many
posted
Q
and
A
here
in
stackoverflow
but
of
those
worked
in
my
problem
.
I
hope
any
active
of
the
coding
ninja
here
could
help
me
with
this
problem
.
Here
is
the
detailed
view
of
the
problem
:
I
have
defined
the
MEDIA_ROOT
and
MEDIA_URL
in
settings.py
This
is
my
upload
model
in
models.py
:
In
my
Application
urls.py
:
In
my
admin.py
:
The
image
was
successfully
stored
at
ProjectDIR
/
media
.
The
HTML
returns
the
url
:
http://127.0.0.1:8000/media/imagename.jpg
at
img
tag
.
But
the
page
fails
to
load
the
image
(
I
will
be
redirected
to
index
page
whenever
when
using
the
url
http://127.0.0.1:8000/media/imagename.jpg
)
.
I
am
using
Django
version
1.10
As
suspected
","
this
problem
is
about
URLs
in
Django
.
The
problem
occurred
because
I
declared
the
following
urlpattern
in
an
app
urls.py
:
I
solved
the
problem
by
changing
the
code
to
:
And
adding
the
following
code
at
the
project's
main
urls.py
:
Here's
one
method
that
splits
the
data
into
nested
lists
and
then
constructs
a
dict
.
I'm
largely
guessing
at
the
desired
data
structure
.
Disclaimer
:
there
is
likely
a
more
efficient
way
of
doing
this
with
regex
","
and
this
method
isn't
ideal
if
reading
the
entire
file
into
memory
is
going
to
be
an
issue
.
Output
:
I
am
trying
to
parse
and
then
edit
the
text
that
lives
between
{
and
a
}
.
The
information
is
different(in between the curly braces)
so
I
would
need
to
be
able
to
have
these
results
separated
in
some
type
of
buffer
to
test
the
/
test
and
/
example
for
each
one
.
Information
in
file
Script
I
am
not
sure
where
to
start
calling
the
dictionary
as
I
want
to
somehow
call
everything
between
the
first
server
{
and
}
.
The
final
goal
is
to
be
able
to
run
a
curl
to
test
the
example.com
/
test
redirection
and
test.com
/
example
redirect
get
the
output
from
this
and
diff
that
to
what
is
currently
in
our
configuration
files
.
The
purpose
of
this
is
to
audit
these
prior
to
moving
them
to
avoid
keeping
configurations
that
are
no
longer
needed
.
I
had
trouble
finding
info
on
this
","
if
there
is
a
document
somewhere
I
need
to
RTFM
then
please
feel
free
to
drop
a
link
.
Edit
Things
looking
for
script
to
do
:
Take
nginx
server
block's
and
grab
value(s)
of
""""
server_name
""""
Grab
value
of
rewrite
/
test
/
example
<--
actual
string
is
random
Test
both
of
these
value's
with
curl
-
w
""""
%
{
url_effective}\n
""""
-
I
-
L
-
s
-
S
$m
-
o
/
dev
/
null
Then
diff
the
curl
output
to
whats
rewritten
.
Take
note
that
I
don't
need
to
parse
example.com
/
example
as
the
/
example
redirection
is
in
a
different
server
block
.
Hope
this
makes
more
since
.
There
are
thousands
of
server
blocks
hence
the
reason
for
this
.
My
solution
was
to
set
up
the
module
like
this
:
The
setup.py
would
contain
something
similar
to
this
:
and
the
file
a.py
would
contain
the
following
lines
in
the
header
:
The
way
it
works
is
very
simple
:
if
you
don't
do
anything
(
i.e.
if
you
don't
compile
the
cython
files
)
then
module
a.py
imports
module
b.py
;
however
","
if
you
run
python
setup.py
build_ext
-
-
inplace
then
the
compiled
cython
files
will
appear
inside
cython_my_module
and
the
next
time
you
run
a.py
it
will
automatically
import the
cython
module
b.pyx
(
actually
it
will
import the
compiled
library
b.so
)
.
So
far
it
seems
to
work
and
requires
almost
no
effort
.
Hope
it
helps
.
fish2000
solution
seems
more
generic
but
I
haven't
tried
it
yet
.
I
have
a
pure
Python
module
and
I
want
to
rewrite
some
of
submodules
using
Cython
.
Then
I
would
like
to
add
the
new
Cython
submodules
to
the
original
Python
module
and
make
them
available
only
as
an
option
","
meaning
that
cythoning
the
module
is
not
compulsory
(
in
which
case
the
'
old
'
pure
Python
module
should
be
used
)
.
Here
is
an
example
:
where
a.py
contains
import b
.
I
want
to
write
b.py
in
Cython
.
The
idea
would
be
to
add
a
folder
containing
the
.
pyx
file
","
for
example
:
setup.py
would
contain
the
direction
to
compile
b.pyx
and
to
install
the
module
.
However
","
I
would
like
that
if
someone
runs
python
setup.py
install
then
the
pure
Python
code
is
installed
","
whereas
if
an
option
is
added
then
the
Cython
code
is
compiled
and
installed
.
Any
idea
how
to
do
that
?
Also
","
how
should
the
file
a.py
be
modified
in
order
to
import the
correct
module
?
I
am
not
sure
about
your
setup.py
requirement
(
I
don’t
know
why
you
would
need
that
)
but
as
for
the
runtime
import issue
","
I
wrote
a
decorator
to
do
just
that
:
…
this
lets
you
decorate
functions
as
@external
–
and
they
are
replaced
at
runtime
automatically
with
the
Cython-optimized
versions
you’ve
provided
.
If
you
wanted
to
extend
this
idea
to
replacing
entire
Cythonized
classes
","
it’d
be
straightforward
to
use
the
same
logic
in
the
__new__
method
of
a
metaclass
(
e.g
.
opportunistic
find-and-replace
in
the
optimized
module
)
.
The
entire
block
function
definition
for
letterGradeConversion
needs
to
be
indented
.
Take
a
look
at
Python
docs
on
indentation
-
https://docs.python.org/2.3/ref/indentation.html
Just
a
beginner
at
Python
trying
to
make
a
GPA
calculator
as
a
basic
project
","
but
getting
an
error
in
syntax
:
""""
expected
an
indented
block
""""
Not
in
code
:
iuashd;alksdjas;kjdha;slkjaskljsakdljas;ljlkjf;lkjsFLKAJSD;KJASD;KLk;jaS'LKDJ;ldjk;'DH'AHKL'ASJKLASJKQIOWJWQOIH
Your
function
definition
(
everything
after
def
...
:
)
has
to
be
indented
.
So
instead
of
The
element
was
clicked
and
I
didn't
get
any
error
but
the
popup
(
""""
add
featured
photos
""""
popup
in
Facebook
)
is
still
there
.
It
is
not
closed
.
This
is
html
code
:
And
this
is
my
code
:
How
to
click
""""
save
""""
button
to
close
popup
?
Thank
you
very
much
:
)
Try
this
You
can
scroll
to
the
button
before
clicking
to
it
In
your
REST_FRAMEWORK
settings
you
haven't
mentioned
the
Authentication
scheme
so
DRF
uses
the
default
authentication
scheme
which
is
SessionAuthentication
.
This
scheme
makes
it
mandatory
for
you
to
put
csrf
token
with
your
requests
.
You
can
overcome
this
by
:
To
make
this
setting
for
the
whole
project
in
settings.py
add
To
make
this
setting
in
specific
view
do
the
following
in
your
view
source
:
http://www.django-rest-framework.org/api-guide/authentication/#sessionauthentication
BTW
","
csrf
token
is
saved
as
a
cookie
called
'
csrftoken
'
.
You
can
retrieve
it
from
HTTP
Response
and
attach
it
to
your
request
header
with
the
key
'
X-CSRFToken
'
.
You
can
see
some
details
of
this
on
:
https://docs.djangoproject.com/en/dev/ref/csrf/#ajax
I'm
using
using
django
rest
framework
browsable
api
with
ModelViewSet
to
do
CRUD
actions
and
want
to
use
permissions.IsAuthenticatedOrReadOnly
","
but
when
I'm
logged
and
try
to
DELETE
or
PUT
I
get
""""
detail
""""
:
""""
CSRF
Failed
:
CSRF
token
missing
or
incorrect
.
""""
My
view
looks
like
this
Settings.py
Serializer
is
just
Although
when
I
delete
permission_classes
(
so
the
default
allowAny
triggers
)
I
can
it
works
just
fine
.
What
I
want
To
be
able
to
PUT
/
DELETE
only
when
I'm
authenticated
.
I
don't
know
how
to
send
CSRF
token
","
when
all
happens
automatically
(
modalviewset
does
the
whole
work
)
You
might
have
used
SessionAuthentication
and
session
auth
checks
for
the
csrf_token
always
and
you
can
avoid
the
checks
by
exempting
but
Lets
not
do
that
.
I
think
you
can
keep
both
Authentication
classes
.
or
just
TokenAuthentication
.
ie
","
And
If
you
are
not
going
for
TokenAuth
and
just
the
session
Auth
.
you
can
always
pass
csrf_token
via
X-CSRFToken
header
.
Or
you
just
can
go
for
csrf_except
things
.
which
will
avoid
the
csrf
missing
issues
.
This
should
work
.
Also
refer
the
links
below
.
ref
:
https://stackoverflow.com/a/26639895/3520792
ref
:
https://stackoverflow.com/a/30875830/3520792
You
can
remove
CSRF
check
on
individual
urls
.
Try
this
on
your
urls.py
:
What
I
want
Inclusion
Templatetag
which
returns
number
of
uses
and
don't
break
when
using
template
inheritance
.
I
tried
to
storage
counter
in
context
","
but
it
does
not
work
as
I
intended
.
base.html
page.html
rendered
result
:
What
I
tried
And
result
:
And
what
worked
Added
middleware
which
added
counter
to
request
and
changed
template
tag
Thanks
to
@SardorbekImomaliev
for
his
suggestion
!
:
)
I
suggest
putting
your
counter
in
request
.
Something
like
this
.
having
a
dataframe
","
I
want
to
update
subset
of
columns
with
a
series
of
same
length
as
number
of
columns
being
updated
:
it
fails
","
however
","
I
am
able
to
do
the
same
thing
using
list
:
could
you
please
help
me
to
understand
","
why
updating
with
series
fails
?
do
I
have
to
perform
some
particular
reshaping
?
When
assigning
with
a
pandas
object
","
pandas
treats
the
assignment
more
""""
rigorously
""""
.
A
pandas
to
pandas
assignment
must
pass
stricter
protocols
.
Only
when
you
turn
it
to
a
list
(
or
equivalently
"pd.Series([0, 1])"
.
values
)
did
pandas
give
in
and
allow
you
to
assign
in
the
way
you'd
imagine
it
should
work
.
That
higher
standard
of
assignment
requires
that
the
indices
line
up
as
well
","
so
even
if
you
had
the
right
shape
","
it
still
wouldn't
have
worked
without
the
correct
indices
.
You
can
use
argpase
module
.
Refer
to
the
following
links
:
)
https://pymotw.com/2/argparse
Dead
simple
argparse
example
wanted
:
1
argument
","
3
results
For
me
I
think
python
getopt
is
the
way
to
go
.
Usage
:
It's
easier
to
run
when
you
have
a
name
for
the
argument
.
Ref
:
http://www.tutorialspoint.com/python/python_command_line_arguments.htm
My
command
line
script
takes
several
arguments
including
strings
","
ints
","
floats
and
lists
","
and
it
is
becoming
a
bit
tricky
to
do
the
call
with
all
arguments
:
To
avoid
having
to
write
all
arguments
in
the
command
line
I
have
created
a
text
file
with
all
arguments
which
I
simply
read
line
by
line
to
collect
arguments
.
This
is
working
fine
","
but
is
this
the
best
practice
for
doing
this
?
Are
there
a
more
effective
way
?
This
program
randomly
generates
weights
for
several
artificial
neural
networks
using
a
loop
","
calculates
the
output
of
the
network
and
appends
that
to
the
end
of
each
array
","
then
appends
the
cost
to
the
end
of
each
array
.
When
run
without
sorting
the
arrays
this
program
does
all
calculations
correctly
","
but
when
I
try
to
sort
the
array
by
the
last
item
in
each
array
it
seems
like
it
breaks
the
loop
even
though
the
sort
function
is
called
after
all
the
other
functions
.
Your
current
approach
sorts
the
list
but
does
not
return
the
result
nor
update
the
network
.
Note
that
sorted
returns
a
sorted
list
.
You
want
an
in-place
sort
:
Are
all
5
of
your
processes
applying
the
same
function
","
but
to
different
inputs
?
If
this
is
the
case
","
then
using
Pool.map()
from
multiprocessing
is
appropriate
:
So
the
function
f(x)
is
being
applied
to
different
inputs
on
each
process
","
and
then
the
output
is
collected
in
a
list
(
equivalent
to
your
'
my_list
'
that
was
empty
to
begin
with
)
.
The
output
array
is
ordered
in
the
same
way
as
the
input
array
i.e.
if
input
=
"[x1,x2,x3,x4,x5]"
then
output
=
[
f(x1)
","
f(x2)
","
f(x3)
","
f(x4)
","
f(x5)
]
.
Let's
say
that
I
have
an
empty
list
my_list
and
5
processes
pr1
","
pr2
","
pr3
","
pr4
","
pr5
","
where
each
of
them
appends
something
to
my
list
.
My
question
is
:
Is
something
like
this
possible
?
And
will
it
behave
normaly
or
an
error
will
occur
?
If
these
processes
are
independent
then
something
like
this
is
not
possible
.
At
least
not
without
some
additional
mechanisms
like
sockets
(
which
greatly
increases
complexity
)
.
If
these
are
created
via
multiprocessing
and
my_list
is
just
a
list
then
each
process
will
have
its
own
copy
of
the
list
.
Again
in
multiprocessing
if
you
define
my_list
as
multiprocessing.Queue
then
indeed
this
will
be
a
shared
structure
between
processes
.
I
am
looking
for
guidance
in
how
to
achieve
start
/
stop
functionality
of
a
function
","
which
makes
a
restAPI
to
retrieve
data
in
JSON
format
continuously
","
by
clicking
a
button
on
a
rendered
html
page
through
webapp2
and
hosted
on
GAE
?
The
current
behaviour
is
once
the
http
request
is
completed
the
function
that
was
called
of
course
stops
(
while
self._running
=
=
True)(normal
behaviour
according
to
the
GAE
documentation
)
.
main.py
:
page.html
:
The
start
/
stop
functionality
is
simple
-
just
make
the
button
control
something
like
an
operation_is_stopped
flag
persisted
across
requests
(
in
the
datastore
","
for
example
)
.
In
case
you
didn't
realize
it
yet
your
difficulty
really
comes
from
achieving
the
continuous
operation
that
you
want
to
control
with
that
button
.
That's
what's
not
really
compatible
with
GAE
-
everything
in
GAE
revolves
around
responding
to
requests
","
in
a
limited
amount
of
time
.
You
can
not
really
have
indefinitely-running
proceses
/
threads
in
GAE
.
But
in
many
cases
it's
possible
to
implement
a
long-running
","
iterative
continuous
operation
(
like
yours
)
as
a
flow
of
short-lived
operations
.
In
GAE
that
can
be
easily
achieved
using
the
task
queues
-
each
iteration
(
in
your
case
the
body
of
the
while
self._running
=
=
True
loop
)
is
implemented
as
a
response
to
a
task
queue
request
.
The
flow
is
started
by
enqueueing
a
respective
task
when
the
""""
start
""""
action
is
triggered
.
The
flow
is
maintained
by
enqueueing
a
respective
task
after
processing
of
a
previous
task
request
.
And
it's
stopped
by
not
enqueueing
a
new
task
:
)
Something
along
these
lines
:
You
will
need
to
ensure
that
Django
itself
","
not
just
CMS
","
knows
the
app's
namespace
","
tv_article_hook
.
I
suspect
that
is
registered
with
CMS
","
but
not
a
namespace
which
Django
knows
about
.
Now
depending
on
what
version
of
Django
you're
using
there's
a
couple
of
ways
to
define
the
namespace
for
Django
.
But
have
a
read
of
the
application
docs
","
essentially
you
can
define
your
own
app
config
like
so
;
With
that
setup
","
you
should
then
be
able
to
modify
your
code
like
so
(
removing
'
_hook
'
from
your
CMS
namespace
as
well
because
thats
a
CMS
term
","
but
we're
just
talking
about
a
Django
app
here
)
;
i
write
a
command
for
django
app
like
this
:
get_absolute_url
method
looks
like
this
:
I
am
using
Django
CMS
apphooks
.
Apphook
urls
are
:
When
i
am
using
get_absolute_url
in
template
or
in
REST
api
it
works
fine
.
But
when
i
run
command
via
manage.py
feed_index
code
fails
on
p.get_absolute_url
with
error
:
django.core.urlresolvers.NoReverseMatch
:
Reverse
for
'
article_tv_detail
'
with
arguments
'
(
)
'
and
keyword
arguments
'
{
'
slug
'
:
'
some
slug
'
}
'
not
found
.
0
pattern(s)
tried
:
[]
How
can
i
solve
this
problem
?
I
have
a
textCTRL
(
Wxpython
)
with
event
binding
to
it
:
I
want
to
manually
trigger
this
event
as
I
wish
.
I
read
this
topic
:
wxPython
:
Calling
an
event
manually
but
nothing
works
.
I
tried
:
But
it
gives
:
TypeError
:
in
method
'
PostEvent
'
","
expected
argument
2
of
type
'
wxEvent
&
'
I
also
tried
:
Which
doesn't
work
as
well
.
The
things
like
wx.EVT_KILL_FOCUS
are
not
the
event
object
needed
here
.
Those
are
instances
of
wx.PyEventBinder
which
","
as
the
name
suggests
","
are
used
to
bind
events
to
handlers
.
The
event
object
needed
for
the
PostEvent
or
ProcessEvent
functions
will
be
the
same
type
of
object
as
what
is
received
by
the
event
handler
functions
.
In
this
case
it
would
be
an
instance
of
wx.FocusEvent
.
When
creating
the
event
object
you
may
also
need
to
set
the
event
type
if
that
event
class
is
used
with
more
than
one
type
of
event
.
The
binder
object
has
that
value
to
help
you
know
what
to
use
.
You
usually
also
need
to
set
the
ID
to
the
ID
of
the
window
where
the
event
originated
.
So
for
your
example
","
it
would
be
done
something
like
this
:
...
or
...
Is
there
any
way
to
create
an
iterator
to
repeat
elements
in
a
list
certain
times
?
For
example
","
a
list
is
given
:
Is
there
a
way
to
create
a
iterator
in
form
of
"itertools.repeatlist(color, 7)"
that
can
produce
the
following
list
?
You
can
use
itertools.cycle()
together
with
itertools.islice()
to
build
your
repeatlist()
function
:
This
returns
a
new
iterator
;
call
list()
on
it
if
you
must
have
a
list
object
.
Demo
:
Try
:
The
following
code
should
solve
your
problem
:
I'm
trying
to
send
and
email
including
Arabic
and
Persian
characters
","
using
smtplib
.
The
Following
is
my
Function
:
and
","
I
get
the
following
error
:
Any
ideas
on
how
to
fix
it
?
try
.
encode('UTF-8')
hope
it'll
help
I
want
to
make
a
program
that
parses
a
.
map
file
and
I
can't
figure
out
what
regular
expression
should
I
use
to
identify
the
name
of
a
section
.
.
text
00040000
00000d7e
I
want
to
get
.
text
string
from
this
line
.
There
is
no
other(whitespace character)
before
it
.
What
regular
expression
should
I
use
?
Simply
use
split
:
It
will
print
:
I
have
some
code
for
a
plot
I
want
to
create
:
In
this
plot
the
x
and
y
axis
are
moved
.
However
","
the
origin
is
not
moved
with
then
","
as
one
can
see
from
the
ticks
and
ticklabels
.
How
can
I
always
move
the
origin
with
the
x
and
y
axis
?
I
guess
it
would
be
the
same
as
simply
looking
at
another
area
of
the
plot
","
so
that
the
x
and
y
axis
are
at
the
lower
left
but
not
in
the
corner
as
they
usually
are
.
To
visualize
this
:
What
I
want
:
Where
the
arrow
points
to
the
x
and
y
axis
intersection
","
I
want
to
have
the
origin
","
(
0|0
)
.
Where
the
dashed
arrow
points
upwards
I
want
the
line
to
move
upwards
","
so
that
it
is
still
mathematically
at
the
correct
position
","
when
the
origin
moves
.
(
the
final
result
of
the
efforts
can
be
found
here
)
You've
done
a
lot
of
manual
tweaking
of
where
each
thing
goes
","
so
the
solution
is
not
very
portable
.
But
here
it
is
:
remove
the
ax.spines
[
'
bottom
'
]
.
set_position
and
ax.xaxis.set_label_coords
calls
from
your
original
code
","
and
add
this
instead
:
The
""""
bring
origin
up
""""
was
really
accomplished
by
just
ax.set_ylim
","
the
rest
is
to
get
your
labels
where
you
want
them
.
The
parent
of
table
id
has
kind
of
unique
in
which
every
time
the
number
changes
but
the
ReportCell
is
appended
to
it
so
using
that
we
can
find
its
children
like
this
I
am
working
with
python
selenium
web
driver.I
want
the
loop
through
the
table
for
the
text
inside
the
td
inside
tr
tag
like
thisBut
problem
here
is
the
class
name
is
always
changing
and
xpath
and
css_selector
is
also
changing.example
xpath
will
be
like
this
.
/
/
*
[
@id='P825048fc6b084257a601fde4805c8c33_1_oReportCell
'
]
/
table
/
tbody
/
tr
[2]
/
td
/
table
/
tbody
/
tr
/
td
/
table
/
tbody
/
tr
[3]
/
td
[8]
/
div
But
the
id
is
changing
always.so
couldn't
apply
driver.find_element_by_id()
.
I
think
regular
expressions
or
BeautifulSoup
can
be
used
to
solve
this.I
am
beginner
to
regex.So
is
there
any
way
this
could
be
solved
?
You
might
be
able
to
use
this
xpath
","
not
sure
if
it's
100
%
correct
:
To
make
it
variable
:
I'm
not
experienced
with
python
but
I'm
sure
you
get
the
idea
and
can
transform
this
to
a
python
function
.
This
code
only
returns
true
when
the
substring
is
at
the
end
.
How
do
I
fix
it
to
consider
any
substring
?
Data
looks
like
this
:
prediction
:
dog
cat
actual
:
red
","
dog
cat
","
blue
returns
true
then
returns
false
Edit
:
.
rstrips
works
!
The
expected
output
is
true
","
true
'
cat
'
in
'
cat
","
blue
'
returns
True
Please
make
small
change
to
your
script
The
items
in
the
list
returned
by
readlines
will
include
the
\
n
at
the
end
of
each
line
","
so
you
are
actually
testing
'
dog\n
'
in
'
red
","
dog\n
'
","
which
is
fine
","
and
'
cat\n
'
in
'
cat
","
blue\n
'
","
which
is
not
.
To
fix
it
","
you
can
use
strip()
to
remove
the
line
terminators
(
and
any
other
enclosing
whitespace
)
.
readlines
gives
you
each
line
include
the
newline
at
the
end
of
the
line
.
So
you're
doing
comparisons
like
""""
cat\n
""""
in
""""
cat
","
blue\n
""""
.
Since
there
isn't
a
newline
in
the
right
place
","
the
check
fails
.
Try
this
to
strip
the
newlines
off
the
end
of
each
line
:
EDIT
Perhaps
a
better
rewrite
is
to
use
zip
and
just
iterate
through
the
files
:
You
just
need
to
rstrip
the
newline
from
each
line
of
the
first
file
so
you
don't
compare
dog\n
with
red
","
dog\n
","
the
newline
on
the
string
you
are
testing
is
irrelevant
.
You
can
also
use
zip
and
read
the
files
line
by
line
withput
the
need
for
readlines
and
indexing
:
If
you
want
exact
matches
then
you
will
need
to
rstrip
and
split
on
the
comma
:
The
difference
on
splitting
would
mean
dog
would
not
match
dogs
","
cats
.
I'm
an
ubuntu
user
.
As
you
know
","
python
promt
is
a
very
useful
tool
.
And
I
want
to
replace
my
calculator
app
with
it
.
What
I
want
is
a
new
terminal
with
python
shell
running
on
it
to
show
up
when
I
press
a
key
combination
.
I
can
assign
scripts
to
shortcut
keys
with
compiz-config
settings
manager
","
but
I
can't
get
the
result
I
want
.
'
gnome-terminal;python3
;
'
launches
a
new
terminal
without
python
running
on
it
.
Looks
like
it
starts
python
in
background
on
tty7
.
So
how
can
I
get
the
result
I
want
.
Make
it
:
As
you
have
certainly
guessed
","
-
-
command
is
the
parameter
to
use
to
execute
a
command
inside
gnome-terminal
.
I
haven't
found
anywhere
any
information
on
setting
Enchant
for
case-insensitive
matching
","
so
for
the
time
being
this
is
my
solution
","
though
it
obviously
decreases
performance
considerably
:
As
per
you
example
","
it
should
return
True
.
Output
:
You
can
try
following
link
","
you
may
get
some
other
alternatives
to
achieve
this
http://pythonhosted.org/pyenchant/tutorial.html
Is
there
a
way
to
do
a
case-insensitive
search
in
enchant
?
I
am
trying
to
achieve
the
following
:
Both
cases
should
return
True
You
have
to
assign
the
result
of
replace
to
item
again
:
or
in
short
:
I'm
trying
to
remove
several
strings
from
a
list
of
URLs
.
I
have
more
than
300k
URLs
","
and
I'm
trying
to
find
which
are
variations
of
the
original
.
Here's
a
toy
example
that
I've
been
working
with
.
What
I'd
like
to
end
up
with
is
a
list
of
the
pages
without
the
language
or
locations
:
I've
tried
list
comprehension
and
nested
for
loops
","
nothing
has
worked
yet
.
Can
anyone
help
?
The
biggest
problem
you
have
is
that
you
don't
save
the
return
value
.
After
this
","
stripped
is
equal
to
EDIT
Alternatively
","
without
creating
a
new
list
","
you
can
do
After
this
","
urls
is
equal
to
You
could
first
abstract
the
removing
part
into
a
function
and
then
use
a
list
comprehension
:
Used
like
:
output
:
If
you're
using
a
queue
when
loading
your
data
and
follow
it
up
with
a
batch
input
then
this
shouldn't
be
a
problem
as
you
can
specify
the
max
amount
to
have
loaded
or
stored
in
the
queue
.
See
here
for
more
details
:
https://www.tensorflow.org/versions/r0.10/api_docs/python/io_ops.html#batch
Also
there's
an
alternative
approach
that
skips
the
tf.cond
completely
.
Just
define
two
losses
one
that
follows
the
data
through
the
autoencoder
and
discrimator
and
the
other
that
follows
the
data
through
just
the
discriminator
.
Then
it
just
becomes
a
matter
of
calling
or
In
this
way
the
graph
will
only
run
through
which
ever
loss
was
called
upon
.
Let
me
know
if
this
needs
more
explanation
.
Lastly
I
think
to
make
variant
one
work
you
need
to
do
something
like
this
if
you're
using
preloaded
data
.
https://www.tensorflow.org/versions/r0.10/how_tos/reading_data/index.html#preloaded-data
Otherwise
I'm
not
sure
what
the
issue
is
to
be
honest
.
Situation
I
want
to
train
a
specific
network
architecture
(
a
GAN
)
that
needs
inputs
from
different
sources
during
training
.
One
input
source
is
examples
loaded
from
disk
.
The
other
source
is
a
generator
sub-network
creating
examples
.
To
choose
which
kind
of
input
to
feed
to
the
network
I
use
tf.cond
.
There
is
one
caveat
though
that
has
already
been
explained
:
tf.cond
evaluates
the
inputs
to
both
conditional
branches
even
though
only
one
of
those
will
ultimately
be
used
.
Enough
setup
","
here
is
a
minimal
working
example
:
Problem
Variant
1
does
not
work
at
all
since
the
queue
runner
threads
don't
seem
to
run
.
print(threads)
outputs
something
like
[
<
"Thread(Thread-1, stopped daemon 140165838264064)"
>
","
...
]
.
Variant
2
does
work
and
print(threads)
outputs
something
like
[
<
"Thread(Thread-1, started daemon 140361854863104)"
>
","
...
]
.
But
since
load_input_data()
has
been
called
outside
of
tf.cond
","
batches
of
data
will
be
loaded
from
disk
even
when
load_inputs_pred
is
False
.
Is
it
possible
to
make
Variant
1
work
","
so
that
input
data
is
only
loaded
when
load_inputs_pred
is
True
and
not
for
every
call
to
session.run()
?
I
have
25
data
frames
which
I
need
to
merge
and
find
recurrently
occurring
rows
from
all
25
data
frames
","
For
example
","
my
data
frame
looks
like
following
","
And
In
the
end
","
I
am
aiming
to
have
an
output
data
frame
like
following
","
I
can
get
there
with
the
following
solution
","
By
dictionary
which
adds
all
these
three
data
frames
into
one
bigger
data
frame
dfs
dfs
=
{
'
df1
'
:
df1
","
'
df2
'
:
df2
}
Then
further
","
This
gives
out
the
resulting
data
frame
with
matching
rows
from
all
three
data
frames
","
but
I
have
25
data
frames
which
I
am
calling
as
list
from
the
directory
as
following
","
And
so
how
can
I
show
the
list
'
results
'
in
the
dictionary
and
proceed
further
to
get
the
desired
output
.
Any
help
or
suggestions
are
greatly
appreciated
.
Thank
you
Using
the
glob
module
","
you
can
use
Then
","
your
dictionary
can
be
created
with
dictionary
comprehension
using
http://wiki.ros.org/video_stream_opencv
can
open
a
video
stream
from
a
HTTP
/
TCP
URL
:
Note
the
/
video?x.mjpeg
at
the
end
of
the
URL
.
I
am
having
an
issue
where
by
I
am
unable
to
connect
to
the
AR
Drone
2
camera
when
the
drone
is
connected
through
ROS
using
the
ardrone_autonomy
ardrone.launch
.
I
think
the
issue
is
due
to
the
fact
that
I
am
trying
to
access
the
drone
camera
through
the
IP
address
with
OpenCV
and
Python
while
connected
through
ardrone_autonomy
.
Below
is
a
code
snippet
of
how
I
am
accomplishing
this
.
As
you
can
see
that
I
am
using
the
IP
address
for
the
camera
.
This
works
perfectly
when
the
drone
is
not
connected
through
ROS
which
is
essentially
like
a
webcam
.
My
end
goal
is
for
tracking
and
navigation
through
the
use
of
images
received
from
the
camera
using
OpenCV
which
means
I
will
have
to
issue
movement
commands(cmd_vel)
which
requires
a
connection
through
ardrone_autonomy
based
on
the
images
received
and
processed
by
OpenCV
.
Is
there
anyway
I
can
accomplish
this
by
using
the
IP
camera
from
the
drone
while
connected
to
ROS
?
Thanks
for
any
help
!
