,0
0,From
1,what
2,I
3,understand
4,","
5,you
6,cannot
7,specify
8,five
9,nodes
10,serve
11,as
12,map
13,nodes
14,and
15,one
16,as
17,a
18,reduce
19,node
20,within
21,a
22,single
23,spark
24,cluster
25,.
26,You
27,could
28,have
29,two
30,clusters
31,running
32,","
33,one
34,with
35,five
36,nodes
37,for
38,running
39,the
40,map
41,tasks
42,and
43,one
44,for
45,the
46,reduce
47,tasks
48,.
49,Then
50,","
51,you
52,could
53,break
54,your
55,code
56,into
57,two
58,different
59,jobs
60,and
61,submit
62,them
63,to
64,the
65,two
66,clusters
67,sequentially
68,","
69,writing
70,the
71,results
72,to
73,disk
74,in
75,between
76,.
77,However
78,","
79,this
80,might
81,be
82,less
83,efficient
84,than
85,letting
86,Spark
87,handle
88,shuffle
89,communication
90,.
91,In
92,Spark
93,","
94,the
95,call
96,to
97,.map()
98,is
99,"""lazy"""
100,in
101,the
102,sense
103,that
104,it
105,does
106,not
107,execute
108,until
109,the
110,call
111,to
112,an
113,"""action."""
114,In
115,your
116,code
117,","
118,this
119,would
120,be
121,the
122,call
123,to
124,.collect().
125,See
126,"https://spark.apache.org/docs/latest/programming-guide.html
Out of curiosity, is there a reason you want one node to handle all reductions?
Also, based on the documentation the .sample()"
127,function
128,takes
129,three
130,parameters
131,.
132,Could
133,you
134,post
135,stderr
136,and
137,stdout
138,from
139,this
140,code
141,?
