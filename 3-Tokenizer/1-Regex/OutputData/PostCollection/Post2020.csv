From,what,I,understand,",",you,cannot,specify,five,nodes,serve,as,map,nodes,and,one,as,a,reduce,node,within,a,single,spark,cluster,.,You,could,have,two,clusters,running,",",one,with,five,nodes,for,running,the,map,tasks,and,one,for,the,reduce,tasks,.,Then,",",you,could,break,your,code,into,two,different,jobs,and,submit,them,to,the,two,clusters,sequentially,",",writing,the,results,to,disk,in,between,.,However,",",this,might,be,less,efficient,than,letting,Spark,handle,shuffle,communication,.,In,Spark,",",the,call,to,.,map(),is,"""",lazy,"""",in,the,sense,that,it,does,not,execute,until,the,call,to,an,"""",action,.,"""",In,your,code,",",this,would,be,the,call,to,.,collect(),.,See,https://spark.apache.org/docs/latest/programming-guide.html,Out,of,curiosity,",",is,there,a,reason,you,want,one,node,to,handle,all,reductions,?,Also,",",based,on,the,documentation,the,.,sample(),function,takes,three,parameters,.,Could,you,post,stderr,and,stdout,from,this,code,?
