,0
0,All
1,classifiers
2,in
3,scikit
4,-learn(*)
5,expect
6,a
7,flat
8,feature
9,representation
10,for
11,samples
12,","
13,so
14,you
15,'ll
16,probably
17,want
18,to
19,turn
20,your
21,string
22,feature
23,into
24,a
25,vector
26,.
27,First
28,","
29,let
30,get
31,some
32,incorrect
33,assumptions
34,out
35,of
36,the
37,"way:
DictVectorizer is not for handling ""lines of text"", but for arbitrary symbolic features.
CountVectorizer is also not for handling lines, but for entire text documents.
Whether features are ""equal in importance"" is mostly up to the learning algorithm, though with a kernelized SVM, you can assign artificially small weights to features to make its dot products come out differently. I'm not saying that's a good idea, though.
There are two ways of handling this kind of data:
Build a FeatureUnion consisting of a CountVectorizer (or TfidfVectorizer)"
38,for
39,your
40,textual
41,data
42,and
43,a
44,DictVectorizer
45,for
46,the
47,additional
48,features
49,.
50,Manually
51,split
52,the
53,textual
54,data
55,into
56,words
57,","
58,then
59,use
60,each
61,word
62,as
63,a
64,feature
65,in
66,a
67,DictVectorizer
68,","
69,e
70,.g.
71,Then
72,the
73,related
74,"questions:
might this data structure indicate which SVM kernel is best?
Since you're handling textual data, try a LinearSVC first and a polynomial kernel of degree 2 if it doesn't work. RBF kernels are a bad match for textual data, and cubic or higher-order poly kernels tend to overfit badly. As an alternative to kernels, you can manually construct products of individual features and train a LinearSVC on that; sometimes, that works better than a kernel. It also gets rid of the feature importances issue as a LinearSVC learns per-feature weights.
Or would a Random Forest/Decision Tree, DBN, or Bayes classifier possibly do better in this case?
That's impossible to tell without trying. scikit-learn's random forests and dtrees unfortunately don't handle sparse matrices, so they're rather hard to apply. DBNs are not implemented.
Should I be using feature selection?
Impossible to tell without seeing the data.
(*)"
75,Except
76,SVMs
77,if
78,you
79,implement
80,custom
81,kernels
82,","
83,which
84,is
85,such
86,an
87,advanced
88,topic
89,that
90,I
91,won
92,'t
93,discuss
94,it
95,now
96,.
