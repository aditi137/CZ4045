After
some
experimentation
","
my
current
best
solution
is
to
have
a
main
graph
featuring
training
inputs
and
a
separate
graph
with
just
evaluation
data
operations
.
I
open
a
separate
session
to
get
evaluation
data
and
feed
this
to
the
training
graph
when
I
want
to
evaluate
.
Highly
inelegant
(
and
evaluation
runs
take
longer
than
they
ideally
would
as
they
have
to
come
ot
of
one
session
only
to
be
fed
to
another
)
","
but
assuming
evaluation
runs
are
rare
compared
to
training
runs
","
this
seems
preferable
to
the
original
version
...
Results
:
Update
:
when
using
this
approach
in
training
problems
with
tf.contrib.layers
and
regularization
","
I
find
the
regularization
losses
go
to
infinity
if
the
DataSupplier
graph
is
on
the
same
device
as
the
training
graph
.
I
cannot
for
the
life
of
me
explain
why
this
is
the
case
","
but
explicitly
setting
the
device
of
the
DataSupplier
to
the
CPU
(
given
the
training
graph
is
on
my
GPU
)
seems
to
work
...
