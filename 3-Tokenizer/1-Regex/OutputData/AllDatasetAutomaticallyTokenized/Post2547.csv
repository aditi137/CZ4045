I'm
trying
to
run
a
tensorflow
graph
to
train
a
model
and
periodically
evaluate
using
a
separate
evaluation
dataset
.
Both
training
and
evaluation
data
is
implemented
using
queue
runners
.
My
current
solution
is
to
create
both
inputs
in
the
same
graph
and
use
a
tf.cond
dependent
on
an
is_training
placeholder
.
My
issue
is
highlighted
by
the
following
code
:
I
also
had
to
comment
out
the
image_summary
line
133
of
tensorflow
/
models
/
image
/
cifar10
/
cifar10_inputs.py
.
This
yielded
the
following
results
:
It
would
seem
in
the
mixed
case
both
inputs
are
being
read
/
parsed
","
even
though
only
1
is
used
.
Is
there
a
way
of
avoiding
this
redundant
computation
?
Or
is
there
a
nicer
way
of
switching
between
training
/
evaluation
data
that
still
leverages
the
queue-runner
setup
?
