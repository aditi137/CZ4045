In
order
for
encode('utf-8')
to
make
sense
","
the
string
must
be
a
unicode
string
(
or
contain
all-ASCII
characters
...
)
.
So
","
unless
it's
a
unicode
instance
already
","
you
have
to
decode
it
first
from
whatever
encoding
it's
in
to
a
unicode
string
","
after
which
you
can
pass
it
into
your
legacy
interface
.
At
no
point
does
it
make
sense
for
anything
to
be
double-encoded
-
-
encoding
takes
a
string
and
transforms
it
to
a
series
of
bytes
;
decoding
takes
a
series
of
bytes
and
transforms
them
back
into
a
string
.
The
confusion
only
arises
because
Python
2
uses
the
str
for
both
plain-ASCII
strings
and
byte
sequences
.
