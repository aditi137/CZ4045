I
just
tried
using
the
IncrementalPCA
from
sklearn.decomposition
","
but
it
threw
a
MemoryError
just
like
the
PCA
and
RandomizedPCA
before
.
My
problem
is
","
that
the
matrix
I
am
trying
to
load
is
too
big
to
fit
into
RAM
.
Right
now
it
is
stored
in
an
hdf5
database
as
dataset
of
shape
~
(
1000000
","
1000
)
","
so
I
have
1.000.000.000
float32
values
.
I
thought
IncrementalPCA
loads
the
data
in
batches
","
but
apparently
it
tries
to
load
the
entire
dataset
","
which
does
not
help
.
How
is
this
library
meant
to
be
used
?
Is
the
hdf5
format
the
problem
?
Thanks
for
help
