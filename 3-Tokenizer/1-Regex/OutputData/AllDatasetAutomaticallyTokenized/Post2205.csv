As
@Foon
pointed
out
","
the
canonical
way
to
do
this
is
to
subtract
a
column
.
However
","
on
a
side
note
","
as
your
problem
is
overdetermined
","
you
have
to
use
a
method
such
as
least
squares
.
By
definition
","
if
it's
an
overdetermined
problem
","
there
is
no
""""
unique
","
exact
solution
""""
.
(
Otherwise
it
would
be
even-determined
-
A
square
matrix
.
)
That
aside
","
here's
how
you'd
go
about
it
:
Let's
take
your
example
equation
:
As
you
noted
","
this
is
overdetermined
.
If
we
know
one
of
our
""""
model
""""
variables
(
let's
say
n1
in
this
case
)
","
it
will
be
even
more
overdetermined
.
It's
not
a
problem
","
but
it
means
we'll
need
to
use
least
squares
","
and
there
isn't
a
completely
unique
solution
.
So
","
let's
say
we
know
what
n1
should
be
.
In
that
case
","
we'd
re-state
the
problem
by
subtracting
n1
multiplied
by
the
first
column
in
the
solution
matrix
from
our
vector
of
observations
(
This
is
what
@Foon
suggested
)
:
Let's
use
a
more
concrete
example
in
numpy
terms
.
Let's
solve
the
equation
y
=
Ax^2
+
Bx
+
C
.
To
start
with
","
let's
generate
our
data
and
""""
true
""""
model
parameters
:
First
","
we'll
solve
it
_without
)
the
knowledge
that
B
=
1
.
We
could
use
np.polyfit
for
this
","
but
to
lead
into
the
next
bit
","
we'll
use
a
lower-level
approach
:
As
you
can
see
","
we'll
get
something
close
to
","
but
not
quite
1
.
In
this
case
(
I
didn't
set
the
seed
","
so
this
will
vary
)
","
the
model
parameters
returned
are
While
the
true
parameters
are
:
Now
let's
take
the
fact
that
we
know
b
exactly
into
account
.
We'll
make
a
new
G
with
one
less
column
and
subtract
that
column
times
b
from
our
observations
(
d
/
y
)
:
Now
m
is
"[a, c]"
and
we've
solved
for
those
two
variables
using
our
knowledge
of
b
.
