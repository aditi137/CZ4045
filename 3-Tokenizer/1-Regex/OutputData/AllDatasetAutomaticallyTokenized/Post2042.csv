pyspark
relies
on
the
spark
SDK
.
You
need
to
have
that
installed
before
using
pyspark
.
Once
that's
set
you
need
to
set
the
environment
variable
SPARK_HOME
to
tell
pyspark
where
to
look
for
your
spark
installation
.
If
you're
on
a
*
nix
system
you
can
do
so
by
adding
the
follow
to
your
.
bashrc
If
you're
using
Windows
there's
a
convoluted
way
of
setting
variables
via
GUI
here
.
Through
DOS
you
can
use
set
in
the
place
of
export
:
