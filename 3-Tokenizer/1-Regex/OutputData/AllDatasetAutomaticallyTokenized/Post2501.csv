I've
read
a
lot
about
different
techniques
for
iterating
over
numpy
arrays
recently
and
it
seems
that
consensus
is
not
to
iterate
at
all
(
for
instance
","
see
a
comment
here
)
.
There
are
several
similar
questions
on
SO
","
but
my
case
is
a
bit
different
as
I
have
to
combine
""""
iterating
""""
(
or
not
iterating
)
and
accessing
previous
values
.
Let's
say
there
are
N
(
N
is
small
","
usually
4
","
might
be
up
to
7
)
1-D
numpy
arrays
of
float128
in
a
list
X
","
all
arrays
are
of
the
same
size
.
To
give
you
a
little
insight
","
these
are
data
from
PDE
integration
","
each
array
stands
for
one
function
","
and
I
would
like
to
apply
a
Poincare
section
.
Unfortunately
","
the
algorithm
should
be
both
memory
-
and
time-efficient
since
these
arrays
are
sometimes
~
1Gb
each
","
and
there
are
only
4Gb
of
RAM
on
board
(
I've
just
learnt
about
memmap'ing
of
numpy
arrays
and
now
consider
using
them
instead
of
regular
ones
)
.
One
of
these
arrays
is
used
for
""""
filtering
""""
the
others
","
so
I
start
with
secaxis
=
X.pop(idx)
.
Now
I
have
to
locate
pairs
of
indices
where
(
secaxis
[
i-1
]
>
0
and
secaxis
[i]
<
0
)
or
(
secaxis
[
i-1
]
<
0
and
secaxis
[i]
>
0
)
and
then
apply
simple
algebraic
transformations
to
remaining
arrays
","
X
(
and
save
results
)
.
Worth
mentioning
","
data
shouldn't
be
wasted
during
this
operation
.
There
are
multiple
ways
for
doing
that
","
but
none
of
them
seem
efficient
(
and
elegant
enough
)
to
me
.
One
is
a
C-like
approach
","
where
you
just
iterate
in
a
for-loop
:
This
is
clearly
very
inefficient
and
besides
not
a
Pythonic
way
.
Another
way
is
to
use
numpy.nditer
","
but
I
haven't
figured
out
yet
how
one
accesses
the
previous
value
","
though
it
allows
iterating
over
several
arrays
at
once
:
Third
possibility
is
to
first
find
sought
indices
with
efficient
numpy
slices
","
and
then
use
them
for
bulk
multiplication
/
addition
.
I
prefer
this
one
for
now
:
But
this
is
seemingly
done
in
7
+
2*(N
-
1
)
passes
","
moreover
","
I'm
not
sure
about
secaxis
[inds]
type
of
addressing
(
it
is
not
slicing
and
generally
it
has
to
find
all
elements
by
indices
just
like
in
the
first
method
","
doesn't
it
?
)
.
Finally
","
I've
also
tried
using
itertools
and
it
resulted
in
monstrous
and
obscure
structures
","
which
might
stem
from
the
fact
that
I'm
not
very
familiar
with
functional
programming
:
Not
only
this
looks
very
ugly
","
it
also
takes
an
awful
lot
of
time
to
complete
.
So
","
I
have
following
questions
:
Of
all
these
methods
is
the
third
one
indeed
the
best
?
If
so
","
what
can
be
done
to
impove
the
last
one
?
Are
there
any
other
","
better
ones
yet
?
Out
of
sheer
curiosity
","
is
there
a
way
to
solve
the
problem
using
nditer
?
Finally
","
will
I
be
better
off
using
memmap
versions
of
numpy
arrays
","
or
will
it
probably
slow
things
down
a
lot
?
Maybe
I
should
only
load
secaxis
array
into
RAM
","
keep
others
on
disk
and
use
third
method
?
(
bonus
question
)
List
of
equal
in
length
1-D
numpy
arrays
comes
from
loading
N
.
npy
files
whose
sizes
aren't
known
beforehand
(
but
N
is
)
.
Would
it
be
more
efficient
to
read
one
array
","
then
allocate
memory
for
one
2-D
numpy
array
(
slight
memory
overhead
here
)
and
read
remaining
into
that
2-D
array
?
