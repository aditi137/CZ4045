Those
are
very
detailed
answers
","
but
they
mostly
beg
the
question
","
how
do
I
get
these
benefits
without
leaving
Postgres
given
that
the
data
easily
fits
into
memory
","
requires
concurrent
reads
but
no
writes
and
is
queried
with
the
same
query
over
and
over
again
.
Is
it
possible
to
precompile
the
query
and
optimization
plan
?
I
would
have
thought
the
stored
procedure
would
do
this
","
but
it
doesn't
really
help
.
To
avoid
disk
access
it's
necessary
to
cache
the
whole
table
in
memory
","
can
I
force
Postgres
to
do
that
?
I
think
it's
already
doing
this
though
","
since
the
query
executes
in
just
200
ms
after
repeated
runs
.
Can
I
tell
Postgres
that
the
table
is
read
only
","
so
it
can
optimize
any
locking
code
?
I
think
it's
possible
to
estimate
the
query
construction
costs
with
an
empty
table
(
timings
range
from
20-60
ms
)
I
still
can't
see
why
the
Java
/
Python
tests
are
invalid
.
Postgres
just
isn't
doing
that
much
more
work
(
though
I
still
haven't
addressed
the
concurrency
aspect
","
just
the
caching
and
query
construction
)
UPDATE
:
I
don't
think
it's
fair
to
compare
the
SELECTS
as
suggested
by
pulling
350
","
000
through
the
driver
and
serialization
steps
into
Python
to
run
the
aggregation
","
nor
even
to
omit
the
aggregation
as
the
overhead
in
formatting
and
displaying
is
hard
to
separate
from
the
timing
.
If
both
engines
are
operating
on
in
memory
data
","
it
should
be
an
apples
to
apples
comparison
","
I'm
not
sure
how
to
guarantee
that's
already
happening
though
.
I
can't
figure
out
how
to
add
comments
","
maybe
i
don't
have
enough
reputation
?
