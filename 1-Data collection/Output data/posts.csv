ParentId,Id,PostTypeId,Text,Post length
4942,4942,1,"When asked to create system XYZ and you ask to do it in Python over PHP or Ruby, what are the main features you can mention when they require you to explain it?
",35
4942,5014,2,"I would consider that using python on a new project is completely dependent on what problem you are trying to solve with python.  If you want someone to agree with you that you should use python, then show them how python's features apply specifically to that problem.
In the case of web development with python, talk about WSGI and other web libraries and frameworks you could use that would make your life easier.  One note for python is that most of the frameworks for python web development can be plugged right into any current project. With ruby on rails, you're practically working in a DSL that anyone who uses your project will have to learn.  If they know python, then they can figure out what you are doing with django, etc in a day.
I'm only talking about web development because it appears that's what you are going to be working on seeing ruby, python and PHP in the same list.  The real message that's important is applying to whatever it is you like about python directly to some problem you are trying to solve.
",204
4942,15291,2,"I agree with mreggen. Tell them by working in Python you can get things done faster. Getting things done faster possibly means money saved by the client. In the least it means that you are working with a language you a more comfortable in, meaning faster development, debugging, and refactoring time. There will be less time spent looking up documentation on what function to use to find the length of a string, etc. 
",82
4942,15296,2,"It's one of the preferred languages over at Google - It's several years ahead of Ruby in terms of ""maturity"" (what ever that really means - but managers like that). Since it's prefered by Google you can also run it on the Google App Engine.
Mircosoft is also embracing Python, and will have a v2.0 of IronPython coming out shortly. They are working on a Ruby implementation as well, but the Python version is way ahead, and is actually ""ready for primetime"". That give you the possibility for easy integration with .NET code, as well as being able to write client side RIAs in Python when Silverlight 2 ships.
",127
4942,21221,2,"The best sell of Python I've ever seen was by a manager in our group who had a young daughter.  He used a quote attributed to Einstein:

If you can't explain something to a six-year-old, you really don't understand it yourself.

The next few slides of his presentation demonstrated how he was able to teach his young daughter some basic Python in less than 30 minutes, with examples of the code she wrote and an explanation of what it did.
He ended the presentation with a picture of his daughter and her quote ""Programming is fun!""
I would focus on Python's user friendliness and wealth of libraries and frameworks.  There are also a lot of little libraries that you might not get in other languages, and would have to write yourself (i.e. How a C++ developer writes Python).
Good luck!
",160
4942,9420311,2,"Give them a snippet of code in each (no more than a page) that performs some cool function that they will like. (e.g show outliers in a data set).
Show them each page. One in PHP, Ruby and Python.
Ask them which they find easiest to understand/read.
Tell them thats why you want to use Python. It's easier to read if you've not written it, more manageable, less buggy and quicker to build features because it is the most elegant (pythonic)
",98
4942,32847801,2,"Though All 3 languages are versatile and used worldwide by programmers, Python still have some advantages over the other two. Like From my personal experience :-


Non-programmers love it (most of 'em choose Python as their first computer language,check this infographic php vs python vs ruby here)
Multiple frameworks (You can automate your system tasks, can develop apps for web and windows/mac/android OSes)
Making OpenCV apps easily than MATLAB 
Testing done easy (you can work on Selenium for all kind of web testing)


OOPS concepts are followed by most languages now , so how come Python can stay behind! Inheritance, Abstraction and Encapsulation are followed by Python as well.
Python as of now is divided into two versions popularly that are not much different in terms of performance but features. Python2.x and Python 3.x both have same syntax ,except for some statements like :-

print ""..."" in Python2.x and print() in Python3.x
raw_input() in Python2.x and input() in Python3.x (for getting user input)

In the end, client only cares about money and Python helps you save a lot as compared to PHP and Ruby , because instead of hiring experienced programmers , you can make a newbie learn and use Python expertly.
",232
4942,4978,2,"This is one of those cases that really boil down to personal preference or situational details. If you're more comfortable and experienced with Python, then say so. Are they asking you to justify it because they're more comfortable with one of the other environments? After you're done, will the system be passed off to someone else for long-term maintenance?
If they ask you to use a technology or language that you're not as familiar with, then make sure they know up-front that it's going to take you longer.
",101
4942,4958,2,"Focus on the shorter time needed for development/prototype and possibly easier maintenance (none of this may apply against Ruby).
",22
5102,14791003,2,"The problem for me wasn't in Apache set up, but in understanding how mod_apache actually uses the .py files. Module-level statements (including those in a if __name__=='__main__' section) are not executed--I assumed that the stdout from running the script at the commandline would be what the server would output, but that's not how it works.
Instead, I wrote a module-level function called index(), and had it return as a string the HTML of the page. It's also possible to have other module-level functions (e.g., otherFunction()) that can be accessed as further segments in the URI (e.g., testScript/otherFunction for the file testScript.py.)
Obviously, this makes more sense than my original stdout conception. Better capability of actually using Python as a scripting language and not a humongous markup language.
",156
5102,5168,2,"Are you running Python on UNIX or Windows?
An alternative to mod_python and FastCGI is mod_wsgi. You can find out more at modwsgi
I have built and installed this on Solaris without problems. I had previously tried mod_python but ran into problems with shared libraries as part of the build. There are good install docs available.
",61
5102,5102,1,"I tried to follow a couple of googled up tutorials on setting up mod_python, but failed every time. Do you have a good, step-by step, rock-solid howto?
My dev box is OS X, production - Centos.
",43
5102,5165,2,"Yes, mod_python is pretty confusing to set up.  Here's how I did it.
In httpd.conf:
LoadModule python_module modules/mod_python.so

<Directory ""/serverbase/htdocs/myapp"">
  AddHandler mod_python .py
  PythonHandler myapp
  PythonDebug On

and in your application directory:
$ /serverbase/htdocs/myapp$ ls -l
total 16
-r-xr-xr-x 1 root sys        6484 May 21 15:54 myapp.py

Repeat the configuration for each python program you wish to have running under mod_python.
",73
5102,5129,2,"There are two main ways of running Python on Apache. The simplest would be to use CGI and write normal Python scripts while the second is using a web framework like Django or Pylons.
Using CGI is straightforward. Make sure your Apache config file has a cgi-bin set up. If not, follow their documentation (http://httpd.apache.org/docs/2.0/howto/cgi.html). At that point all you need to do is place your Python scripts in the cgi-bin directory and the standard output will become the HTTP response. Refer to Python's documentation for further info (https://docs.python.org/library/cgi.html).
If you want to use a web framework you'll need to setup mod_python or FastCGI. These steps are dependent on which framework you want to use. Django provides clear instructions on how to setup mod_python and Django with Apache (http://www.djangoproject.com/documentation/modpython/)
",153
5909,5909,1,"I'm downloading an entire directory from a web server. It works OK, but I can't figure how to get the file size before download to compare if it was updated on the server or not. Can this be done as if I was downloading the file from a FTP server?
import urllib
import re

url = ""http://www.someurl.com""

# Download the page locally
f = urllib.urlopen(url)
html = f.read()
f.close()

f = open (""temp.htm"", ""w"")
f.write (html)
f.close()

# List only the .TXT / .ZIP files
fnames = re.findall('^.*<a href=""(\w+(?:\.txt|.zip)?)"".*$', html, re.MULTILINE)

for fname in fnames:
    print fname, ""...""

    f = urllib.urlopen(url + ""/"" + fname)

    #### Here I want to check the filesize to download or not #### 
    file = f.read()
    f.close()

    f = open (fname, ""w"")
    f.write (file)
    f.close()


@Jon: thank for your quick answer. It works, but the filesize on the web server is slightly less than the filesize of the downloaded file. 
Examples:
Local Size  Server Size
 2.223.533  2.115.516
   664.603    662.121

It has anything to do with the CR/LF conversion?
",260
5909,5927,2,"The size of the file is sent as the Content-Length header. Here is how to get it with urllib:
>>> site = urllib.urlopen(""http://python.org"")
>>> meta = site.info()
>>> print meta.getheaders(""Content-Length"")
['16535']
>>>

",59
5909,5935,2,"Using the returned-urllib-object method info(), you can get various information on the retrived document. Example of grabbing the current Google logo:
>>> import urllib
>>> d = urllib.urlopen(""http://www.google.co.uk/logos/olympics08_opening.gif"")
>>> print d.info()

Content-Type: image/gif
Last-Modified: Thu, 07 Aug 2008 16:20:19 GMT  
Expires: Sun, 17 Jan 2038 19:14:07 GMT 
Cache-Control: public 
Date: Fri, 08 Aug 2008 13:40:41 GMT 
Server: gws 
Content-Length: 20172 
Connection: Close

It's a dict, so to get the size of the file, you do urllibobject.info()['Content-Length']
print f.info()['Content-Length']

And to get the size of the local file (for comparison), you can use the os.stat() command:
os.stat(""/the/local/file.zip"").st_size

",154
5909,5938,2,"Also if the server you are connecting to supports it, look at Etags and the If-Modified-Since and If-None-Match headers.
Using these will take advantage of the webserver's caching rules and will return a 304 Not Modified status code if the content hasn't changed.
",48
5909,5985,2,"I have reproduced what you are seeing:
import urllib, os
link = ""http://python.org""
print ""opening url:"", link
site = urllib.urlopen(link)
meta = site.info()
print ""Content-Length:"", meta.getheaders(""Content-Length"")[0]

f = open(""out.txt"", ""r"")
print ""File on disk:"",len(f.read())
f.close()


f = open(""out.txt"", ""w"")
f.write(site.read())
site.close()
f.close()

f = open(""out.txt"", ""r"")
print ""File on disk after download:"",len(f.read())
f.close()

print ""os.stat().st_size returns:"", os.stat(""out.txt"").st_size

Outputs this:
opening url: http://python.org
Content-Length: 16535
File on disk: 16535
File on disk after download: 16535
os.stat().st_size returns: 16861

What am I doing wrong here? Is os.stat().st_size not returning the correct size?

Edit:
OK, I figured out what the problem was:
import urllib, os
link = ""http://python.org""
print ""opening url:"", link
site = urllib.urlopen(link)
meta = site.info()
print ""Content-Length:"", meta.getheaders(""Content-Length"")[0]

f = open(""out.txt"", ""rb"")
print ""File on disk:"",len(f.read())
f.close()


f = open(""out.txt"", ""wb"")
f.write(site.read())
site.close()
f.close()

f = open(""out.txt"", ""rb"")
print ""File on disk after download:"",len(f.read())
f.close()

print ""os.stat().st_size returns:"", os.stat(""out.txt"").st_size

this outputs:
$ python test.py
opening url: http://python.org
Content-Length: 16535
File on disk: 16535
File on disk after download: 16535
os.stat().st_size returns: 16535

Make sure you are opening both files for binary read/write.
// open for binary write
open(filename, ""wb"")
// open for binary read
open(filename, ""rb"")

",432
5909,25502448,2,"In Python3:
>>> import urllib.request
>>> site = urllib.request.urlopen(""http://python.org"")
>>> print(""FileSize: "", site.length)

",33
5909,40957594,2,"A requests-based solution using HEAD instead of GET (also prints HTTP headers):
#!/usr/bin/python
# display size of a remote file without downloading

from __future__ import print_function
import sys
import requests

# number of bytes in a megabyte
MBFACTOR = float(1 << 20)

response = requests.head(sys.argv[1], allow_redirects=True)

print(""\n"".join([('{:<40}: {}'.format(k, v)) for k, v in response.headers.items()]))
size = response.headers.get('content-length', 0)
print('{:<40}: {:.2f} MB'.format('FILE SIZE', int(size) / MBFACTOR))

Usage

$ python filesize-remote-url.py https://httpbin.org/image/jpeg
...
Content-Length                          : 35588
FILE SIZE (MB)                          : 0.03 MB


",153
5909,46440161,2,"For a python3 (tested on 3.5) approach I'd recommend:
    with urlopen(file_url) as in_file, open(local_file_address, 'wb') as out_file:
        print(in_file.getheader('Content-Length'))
        out_file.write(response.read())

",45
5966,5966,1,"Basically, I've written an API to www.thetvdb.com in Python. The current code can be found here.
It grabs data from the API as requested, and has to store the data somehow, and make it available by doing:
print tvdbinstance[1][23]['episodename'] # get the name of episode 23 of season 1

What is the ""best"" way to abstract this data within the Tvdb() class?
I originally used a extended Dict() that automatically created sub-dicts (so you could do x[1][2][3][4] = ""something"" without having to do if x[1].has_key(2): x[1][2] = [] and so on)
Then I just stored the data by doing self.data[show_id][season_number][episode_number][attribute_name] = ""something""
This worked okay, but there was no easy way of checking if x[3][24] was supposed to exist or not (so I couldn't raise the season_not_found exception).
Currently it's using four classes: ShowContainer, Show, Season and Episode. Each one is a very basic dict, which I can easily add extra functionality in (the search() function on Show() for example). Each has a __setitem__, __getitem_ and has_key.
This works mostly fine, I can check in Shows if it has that season in it's self.data dict, if not, raise season_not_found. I can also check in Season() if it has that episode and so on.
The problem now is it's presenting itself as a dict, but doesn't have all the functionality, and because I'm overriding the __getitem__ and __setitem__ functions, it's easy to accidentally recursively call __getitem__ (so I'm not sure if extending the Dict class will cause problems).
The other slight problem is adding data into the dict is a lot more work than the old Dict method (which was self.data[seas_no][ep_no]['attribute'] = 'something'). See _setItem and _setData. It's not too bad, since it's currently only a read-only API interface (so the users of the API should only ever retrieve data, not add more), but it's hardly... Elegant.
I think the series-of-classes system is probably the best way, but does anyone have a better idea for storing the data? And would extending the ShowContainer/etc classes with Dict cause problems?
",479
5966,9080,2,"Bartosz/To clarify ""This worked okay, but there was no easy way of checking if x[3][24] was supposed to exist or not""
x['some show'][3][24] would return season 3, episode 24 of ""some show"". If there was no season 3, I want the pseudo-dict to raise tvdb_seasonnotfound, if ""some show"" doesn't exist, then raise tvdb_shownotfound
The current system of a series of classes, each with a __getitem__ - Show checks if self.seasons.has_key(requested_season_number), the Season class checks if self.episodes.has_key(requested_episode_number) and so on.
It works, but it there seems to be a lot of repeated code (each class is basically the same, but raises a different error)
",145
5966,8165,2,"I don't get this part here:

This worked okay, but there was no easy way of checking if x[3][24] was supposed to exist or not (so I couldn't raise the season_not_found exception)

There is a way to do it - called in:
>>>x={}
>>>x[1]={}
>>>x[1][2]={}
>>>x
{1: {2: {}}}
>>> 2 in x[1]
True
>>> 3 in x[1]
False

what seems to be the problem with that?
",127
5966,6805,2,"I have done something similar in the past and used an in-memory XML document as a quick and dirty hierachical database for storage. You can store each show/season/episode as an element (nested appropriately) and attributes of these things as xml attributes on the elements. Then you can use XQuery to get info back out.
NOTE: I'm not a Python guy so I don't know what your xml support is like.
NOTE 2: You'll want to profile this because it'll be bigger and slower than the solution you've already got. Likely enough if you are doing some high-volume processing then XML is probably not going to be your friend.
",123
5966,6125,2,"Why not use SQLite? There is good support in Python and you can write SQL queries to get the data out. Here is the Python docs for sqlite3

If you don't want to use SQLite you could do an array of dicts.
episodes = []
episodes.append({'season':1, 'episode': 2, 'name':'Something'})
episodes.append({'season':1, 'episode': 2, 'name':'Something', 'actors':['Billy Bob', 'Sean Penn']})

That way you add metadata to any record and search it very easily
season_1 = [e for e in episodes if e['season'] == 1]
billy_bob = [e for e in episodes if 'actors' in e and 'Billy Bob' in e['actors']]

for episode in billy_bob:
    print ""Billy bob was in Season %s Episode %s"" % (episode['season'], episode['episode'])

",183
5966,10778,2,"OK, what you need is classobj from new module. That would allow you to construct exception classes dynamically (classobj takes a string as an argument for the class name). 
import new
myexc=new.classobj(""ExcName"",(Exception,),{})
i=myexc(""This is the exc msg!"")
raise i

this gives you:
Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
__main__.ExcName: This is the exc msg!

remember that you can always get the class name through:
self.__class__.__name__

So, after some string mangling and concatenation, you should be able to obtain appropriate exception class name and construct a class object using that name and then raise that exception.
P.S. - you can also raise strings, but this is deprecated.
raise(self.__class__.__name__+""Exception"")

",163
12591,12591,1,"Is there a way, when I parse an XML document using lxml, to validate that document against its DTD using an external catalog file?   I need to be able to work the fixed attributes defined in a document’s DTD.
",45
12591,13040,2,"Can you give an example? According to the lxml validation docs, lxml can handle DTD validation (specified in the XML doc or externally in code) and system catalogs, which covers most cases I can think of.
f = StringIO(""<!ELEMENT b EMPTY>"")
dtd = etree.DTD(f)
dtd = etree.DTD(external_id = ""-//OASIS//DTD DocBook XML V4.2//EN"")

",74
12591,36219,2,"It seems that lxml does not expose this libxml2 feature, grepping the source only turns up some #defines for the error handling:
C:\Dev>grep -ir --include=*.px[id] catalog lxml-2.1.1/src | sed -r ""s/\s+/ /g""
lxml-2.1.1/src/lxml/dtd.pxi: catalog.
lxml-2.1.1/src/lxml/xmlerror.pxd: XML_FROM_CATALOG = 20 # The Catalog module
lxml-2.1.1/src/lxml/xmlerror.pxd: XML_WAR_CATALOG_PI = 93 # 93
lxml-2.1.1/src/lxml/xmlerror.pxd: XML_CATALOG_MISSING_ATTR = 1650
lxml-2.1.1/src/lxml/xmlerror.pxd: XML_CATALOG_ENTRY_BROKEN = 1651 # 1651
lxml-2.1.1/src/lxml/xmlerror.pxd: XML_CATALOG_PREFER_VALUE = 1652 # 1652
lxml-2.1.1/src/lxml/xmlerror.pxd: XML_CATALOG_NOT_CATALOG = 1653 # 1653
lxml-2.1.1/src/lxml/xmlerror.pxd: XML_CATALOG_RECURSION = 1654 # 1654
lxml-2.1.1/src/lxml/xmlerror.pxi:CATALOG=20
lxml-2.1.1/src/lxml/xmlerror.pxi:WAR_CATALOG_PI=93
lxml-2.1.1/src/lxml/xmlerror.pxi:CATALOG_MISSING_ATTR=1650
lxml-2.1.1/src/lxml/xmlerror.pxi:CATALOG_ENTRY_BROKEN=1651
lxml-2.1.1/src/lxml/xmlerror.pxi:CATALOG_PREFER_VALUE=1652
lxml-2.1.1/src/lxml/xmlerror.pxi:CATALOG_NOT_CATALOG=1653
lxml-2.1.1/src/lxml/xmlerror.pxi:CATALOG_RECURSION=1654

From the catalog implementation in libxml2 page it seems possible that the 'transparent' handling through installation in /etc/xml/catalog may still work in lxml, but if you need more than that you can always abandon lxml and use the default python bindings, which do expose the catalog functions.
",170
12591,8391738,2,"You can add the catalog to the XML_CATALOG_FILES environment variable:
os.environ['XML_CATALOG_FILES'] = 'file:///to/my/catalog.xml'

See this thread. Note that entries in XML_CATALOG_FILES are space-separated URLs. You can use Python's pathname2url and urljoin (with file:) to generate the URL from a pathname.
",54
12592,12592,1,"Is it possible to write a doctest unit test that will check that an exception is raised?
For example, if I have a function foo(x) that is supposed to raise an exception if x<0, how would I write the doctest for that? 
",51
12592,12609,2,"Yes. You can do it. The doctest module documentation and Wikipedia has an example of it.
   >>> x
   Traceback (most recent call last):
     ...
   NameError: name 'x' is not defined

",40
12592,33787471,2,">>> import math
>>> math.log(-2)
Traceback (most recent call last):
 ...
ValueError: math domain error

ellipsis flag # doctest: +ELLIPSIS is not required to use ... in Traceback doctest 
",41
12692,21149,2,"Keep a look out for ASP.NET MVC
The IronRuby guys have got some internal builds of MVC to work with IronRuby, and IronPython 2 and IronRuby have a lot of code in common with the DLR.
I'm not sure if they'll support IronPython/IronRuby when MVC is released, but it's definitely worth keeping your eye on anyway - The old ASP.NET forms-based development model is old, busted, and the sooner it goes away the better.
",83
12692,239346,2,"Check out the Dynamic Languages in ASP.NET page on Codeplex. This has the newest IronPython bits. It doesn't give you any Visual Studio integration, other than the sample website project, but that's coming.
",40
12692,12713,2,"The current version of ASP.NET integration for IronPython is not very up-to-date and is more of a ""proof-of-concept."" I don't think I'd build a production website based on it.
Edit:: I have a very high level of expectation for how things like this should work, and might setting the bar a little high.  Maybe you should take what's in ""ASP.NET Futures"", write a test application for it and see how it works for you.  If you're successful, I'd like to hear about it.  Otherwise, I think there should be a newer CTP of this in the next six months.
(I'm a developer on IronPython and IronRuby.)
Edit 2: Since I originally posted this, a newer version has been released.
",147
12692,12692,1,"Has anyone built a website with IronPython and ASP.NET.  What were your experiences and is the combination ready for prime-time?
",22
13941,13941,1,"I'd like to have a python program alert me when it has completed its task by making a beep noise.  Currently,  I use import os and then use a command line speech program to say ""Process complete.""  I much rather it be a simple ""bell.""
I know that there's a function that can be used in Cocoa apps, NSBeep, but I don't think that has much anything to do with this.
I've also tried print(\a) but that didn't work.
I'm using a Mac, if you couldn't tell by my Cocoa comment, so that may help.
Thanks!
",122
13941,13949,2,"Have you tried :
import sys
sys.stdout.write('\a')
sys.stdout.flush()

That works for me here on Mac OS 10.5
Actually, I think your original attempt works also with a little modification:
print('\a')

(You just need the single quotes around the character sequence).
",55
13941,13959,2,"I had to turn off the ""Silence terminal bell"" option in my active Terminal Profile in iTerm for print('\a') to work. It seemed to work fine by default in Terminal.
You can also use the Mac module Carbon.Snd to play the system beep:
>>> import Carbon.Snd
>>> Carbon.Snd.SysBeep(1)
>>> 

The Carbon modules don't have any documentation, so I had to use help(Carbon.Snd) to see what functions were available. It seems to be a direct interface onto Carbon, so the docs on Apple Developer Connection probably help.
",112
13941,34482,2,"If you have PyObjC (the Python - Objective-C bridge) installed or are running on OS X 10.5's system python (which ships with PyObjC), you can do
from AppKit import NSBeep
NSBeep()

to play the system alert.
",45
13941,6110229,2,"I tried the mixer from the pygame module, and it works fine. First install the module:
$ sudo apt-get install python-pygame

Then in the program, write this:
from pygame import mixer
mixer.init() #you must initialize the mixer
alert=mixer.Sound('bell.wav')
alert.play()

With pygame you have a lot of customization options, which you may additionally experiment with.
",70
13941,46743047,2,"Building on Barry Wark's answer...
NSBeep() from AppKit works fine, but also makes the terminal/app icon in the taskbar jump.
A few extra lines with NSSound() avoids that and gives the opportunity to use another sound:
from AppKit import NSSound
#prepare sound:
sound = NSSound.alloc()
sound.initWithContentsOfFile_byReference_('/System/Library/Sounds/Ping.aiff', True)
#rewind and play whenever you need it:
sound.stop() #rewind
sound.play()

Standard sound files can be found via commandline locate /System/Library/Sounds/*.aiff
The file used by NSBeep() seems to be '/System/Library/Sounds/Funk.aiff'
",104
20794,20794,1,"If I call os.stat() on a broken symlink, python throws an OSError exception. This makes it useful for finding them. However, there are a few other reasons that os.stat() might throw a similar exception. Is there a more precise way of detecting broken symlinks with Python under Linux?
",58
20794,20843,2,"Can I mention testing for hardlinks without python? /bin/test has the FILE1 -ef FILE2 condition that is true when files share an inode.
Therefore, something like find . -type f -exec test \{} -ef /path/to/file \; -print works for hard link testing to a specific file.
Which brings me to reading man test and the mentions of -L and -h which both work on one file and return true if that file is a symbolic link, however that doesn't tell you if the target is missing.
I did find that head -0 FILE1 would return an exit code of 0 if the file can be opened and a 1 if it cannot, which in the case of a symbolic link to a regular file works as a test for whether it's target can be read.
",149
20794,20845,2,"I'm not a python guy but it looks like os.readlink()?  The logic I would use in perl is to use readlink() to find the target and the use stat() to test to see if the target exists.
Edit: I banged out some perl that demos readlink.  I believe perl's stat and readlink and python's os.stat() and os.readlink()are both wrappers for the system calls, so this should translate reasonable well as proof of concept code:
wembley 0 /home/jj33/swap > cat p
my $f = shift;

while (my $l = readlink($f)) {
  print ""$f -> $l\n"";
  $f = $l;
}

if (!-e $f) {
  print ""$f doesn't exist\n"";
}
wembley 0 /home/jj33/swap > ls -l | grep ^l
lrwxrwxrwx    1 jj33  users          17 Aug 21 14:30 link -> non-existant-file
lrwxrwxrwx    1 root     users          31 Oct 10  2007 mm -> ../systems/mm/20071009-rewrite//
lrwxrwxrwx    1 jj33  users           2 Aug 21 14:34 mmm -> mm/
wembley 0 /home/jj33/swap > perl p mm
mm -> ../systems/mm/20071009-rewrite/
wembley 0 /home/jj33/swap > perl p mmm
mmm -> mm
mm -> ../systems/mm/20071009-rewrite/
wembley 0 /home/jj33/swap > perl p link
link -> non-existant-file
non-existant-file doesn't exist
wembley 0 /home/jj33/swap >

",244
20794,20848,2,"os.lstat() may be helpful. If lstat() succeeds and stat() fails, then it's probably a broken link.
",26
20794,20859,2,"os.path
You may try using realpath() to get what the symlink points to, then trying to determine if it's a valid file using is file.
(I'm not able to try that out at the moment, so you'll have to play around with it and see what you get)
",58
20794,26957,2,"A common Python saying is that it's easier to ask forgiveness than permission.  While I'm not a fan of this statement in real life, it does apply in a lot of cases.  Usually you want to avoid code that chains two system calls on the same file, because you never know what will happen to the file in between your two calls in your code.
A typical mistake is to write something like:
if os.path.exists(path):
    os.unlink(path)

The second call (os.unlink) may fail if something else deleted it after your if test, raise an Exception, and stop the rest of your function from executing.  (You might think this doesn't happen in real life, but we just fished another bug like that out of our codebase last week - and it was the kind of bug that left a few programmers scratching their head and claiming 'Heisenbug' for the last few months)
So, in your particular case, I would probably do:
try:
    os.stat(path)
except OSError, e:
    if e.errno == errno.ENOENT:
        print 'path %s does not exist or is a broken symlink' % path
    else:
        raise e

The annoyance here is that stat returns the same error code for a symlink that just isn't there and a broken symlink.
So, I guess you have no choice than to break the atomicity, and do something like
if not os.path.exists(os.readlink(path)):
    print 'path %s is a broken symlink' % path

",285
20794,31102280,2,"This is not atomic but it works.
os.path.islink(filename) and not os.path.exists(filename)
Indeed by RTFM
 (reading the fantastic manual) we see

os.path.exists(path)
Return True if path refers to an existing path. Returns False for  broken symbolic links.

It also says:

On some platforms, this function may return False if permission is not granted to execute os.stat() on the requested file, even if the path physically exists.

So if you are worried about permissions, you should add other clauses.
",99
20794,40274852,2,"I had a similar problem: how to catch broken symlinks, even when they occur in some parent dir?  I also wanted to log all of them (in an application dealing with a fairly large number of files), but without too many repeats.
Here is what I came up with, including unit tests.
fileutil.py:
import os
from functools import lru_cache
import logging

logger = logging.getLogger(__name__)

@lru_cache(maxsize=2000)
def check_broken_link(filename):
    """"""
    Check for broken symlinks, either at the file level, or in the
    hierarchy of parent dirs.
    If it finds a broken link, an ERROR message is logged.
    The function is cached, so that the same error messages are not repeated.

    Args:
        filename: file to check

    Returns:
        True if the file (or one of its parents) is a broken symlink.
        False otherwise (i.e. either it exists or not, but no element
        on its path is a broken link).

    """"""
    if os.path.isfile(filename) or os.path.isdir(filename):
        return False
    if os.path.islink(filename):
        # there is a symlink, but it is dead (pointing nowhere)
        link = os.readlink(filename)
        logger.error('broken symlink: {} -> {}'.format(filename, link))
        return True
    # ok, we have either:
    #   1. a filename that simply doesn't exist (but the containing dir
           does exist), or
    #   2. a broken link in some parent dir
    parent = os.path.dirname(filename)
    if parent == filename:
        # reached root
        return False
    return check_broken_link(parent)

Unit tests:
import logging
import shutil
import tempfile
import os

import unittest
from ..util import fileutil


class TestFile(unittest.TestCase):

    def _mkdir(self, path, create=True):
        d = os.path.join(self.test_dir, path)
        if create:
            os.makedirs(d, exist_ok=True)
        return d

    def _mkfile(self, path, create=True):
        f = os.path.join(self.test_dir, path)
        if create:
            d = os.path.dirname(f)
            os.makedirs(d, exist_ok=True)
            with open(f, mode='w') as fp:
                fp.write('hello')
        return f

    def _mklink(self, target, path):
        f = os.path.join(self.test_dir, path)
        d = os.path.dirname(f)
        os.makedirs(d, exist_ok=True)
        os.symlink(target, f)
        return f

    def setUp(self):
        # reset the lru_cache of check_broken_link
        fileutil.check_broken_link.cache_clear()

        # create a temporary directory for our tests
        self.test_dir = tempfile.mkdtemp()

        # create a small tree of dirs, files, and symlinks
        self._mkfile('a/b/c/foo.txt')
        self._mklink('b', 'a/x')
        self._mklink('b/c/foo.txt', 'a/f')
        self._mklink('../..', 'a/b/c/y')
        self._mklink('not_exist.txt', 'a/b/c/bad_link.txt')
        bad_path = self._mkfile('a/XXX/c/foo.txt', create=False)
        self._mklink(bad_path, 'a/b/c/bad_path.txt')
        self._mklink('not_a_dir', 'a/bad_dir')

    def tearDown(self):
        # Remove the directory after the test
        shutil.rmtree(self.test_dir)

    def catch_check_broken_link(self, expected_errors, expected_result, path):
        filename = self._mkfile(path, create=False)
        with self.assertLogs(level='ERROR') as cm:
            result = fileutil.check_broken_link(filename)
            logging.critical('nothing')  # trick: emit one extra message, so the with assertLogs block doesn't fail
        error_logs = [r for r in cm.records if r.levelname is 'ERROR']
        actual_errors = len(error_logs)
        self.assertEqual(expected_result, result, msg=path)
        self.assertEqual(expected_errors, actual_errors, msg=path)

    def test_check_broken_link_exists(self):
        self.catch_check_broken_link(0, False, 'a/b/c/foo.txt')
        self.catch_check_broken_link(0, False, 'a/x/c/foo.txt')
        self.catch_check_broken_link(0, False, 'a/f')
        self.catch_check_broken_link(0, False, 'a/b/c/y/b/c/y/b/c/foo.txt')

    def test_check_broken_link_notfound(self):
        self.catch_check_broken_link(0, False, 'a/b/c/not_found.txt')

    def test_check_broken_link_badlink(self):
        self.catch_check_broken_link(1, True, 'a/b/c/bad_link.txt')
        self.catch_check_broken_link(0, True, 'a/b/c/bad_link.txt')

    def test_check_broken_link_badpath(self):
        self.catch_check_broken_link(1, True, 'a/b/c/bad_path.txt')
        self.catch_check_broken_link(0, True, 'a/b/c/bad_path.txt')

    def test_check_broken_link_badparent(self):
        self.catch_check_broken_link(1, True, 'a/bad_dir/c/foo.txt')
        self.catch_check_broken_link(0, True, 'a/bad_dir/c/foo.txt')
        # bad link, but shouldn't log a new error:
        self.catch_check_broken_link(0, True, 'a/bad_dir/c')
        # bad link, but shouldn't log a new error:
        self.catch_check_broken_link(0, True, 'a/bad_dir')

if __name__ == '__main__':
    unittest.main()

",835
20927,20927,1,"I've got two models: Message and Attachment. Each attachment is attached to a specific message, using a ForeignKey on the Attachment model. Both models have an auto_now DateTimeField called updated. I'm trying to make it so that when any attachment is saved, it also sets the updated field on the associated message to now. Here's my code:
def save(self):
    super(Attachment, self).save()
    self.message.updated = self.updated

Will this work, and if you can explain it to me, why? If not, how would I accomplish this?
",110
20927,33449486,2,"Proper version to work is: (attention to last line self.message.save())
class Message(models.Model):
    updated = models.DateTimeField(auto_now = True)
    ...

class Attachment(models.Model):
    updated = models.DateTimeField(auto_now = True)
    message = models.ForeignKey(Message)

    def save(self):
        super(Attachment, self).save()
        self.message.save()

",68
20927,20983,2,"You would also need to then save the message.  Then it that should work.
",16
20927,72359,2,"DateTime fields with auto_now are automatically updated upon calling save(), so you do not need to update them manually. Django will do this work for you.
",31
21454,21454,1,"How do I go about specifying and using an ENUM in a Django model?
",15
21454,28408589,2,"There're currently two github projects based on adding these, though I've not looked into exactly how they're implemented:

Django-EnumField:
Provides an enumeration Django model field (using IntegerField) with reusable enums and transition validation. 
Django-EnumFields:
This package lets you use real Python (PEP435-style) enums with Django.

I don't think either use DB enum types, but they are in the works for first one.
",78
21454,22155357,2,"A the top of your models.py file, add this line after you do your imports:
    enum = lambda *l: [(s,_(s)) for s in l]

",36
21454,21468,2,"From the Django documentation:
MAYBECHOICE = (
    ('y', 'Yes'),
    ('n', 'No'),
    ('u', 'Unknown'),
)

And you define a charfield in your model :
married = models.CharField(max_length=1, choices=MAYBECHOICE)

You can do the same with integer fields if you don't like to have letters
in your db.
In that case, rewrite your choices:
MAYBECHOICE = (
    (0, 'Yes'),
    (1, 'No'),
    (2, 'Unknown'),
)

",103
21454,13089465,2,"http://www.b-list.org/weblog/2007/nov/02/handle-choices-right-way/

class Entry(models.Model):
    LIVE_STATUS = 1
    DRAFT_STATUS = 2
    HIDDEN_STATUS = 3
    STATUS_CHOICES = (
        (LIVE_STATUS, 'Live'),
        (DRAFT_STATUS, 'Draft'),
        (HIDDEN_STATUS, 'Hidden'),
    )
    # ...some other fields here...
    status = models.IntegerField(choices=STATUS_CHOICES, default=LIVE_STATUS)

live_entries = Entry.objects.filter(status=Entry.LIVE_STATUS)
draft_entries = Entry.objects.filter(status=Entry.DRAFT_STATUS)

if entry_object.status == Entry.LIVE_STATUS:


This is another nice and easy way of implementing enums although it doesn't really save enums in the database.
However it does allow you to reference the 'label' whenever querying or specifying defaults as opposed to the top-rated answer where you have to use the 'value' (which may be a number).
",133
21454,19040441,2,"Setting choices on the field will allow some validation on the Django end, but it won't define any form of an enumerated type on the database end.
As others have mentioned, the solution is to specify db_type on a custom field.
If you're using a SQL backend (e.g. MySQL), you can do this like so:
from django.db import models


class EnumField(models.Field):
    def __init__(self, *args, **kwargs):
        super(EnumField, self).__init__(*args, **kwargs)
        assert self.choices, ""Need choices for enumeration""

    def db_type(self, connection):
        if not all(isinstance(col, basestring) for col, _ in self.choices):
            raise ValueError(""MySQL ENUM values should be strings"")
        return ""ENUM({})"".format(','.join(""'{}'"".format(col) 
                                          for col, _ in self.choices))


class IceCreamFlavor(EnumField, models.CharField):
    def __init__(self, *args, **kwargs):
        flavors = [('chocolate', 'Chocolate'),
                   ('vanilla', 'Vanilla'),
                  ]
        super(IceCreamFlavor, self).__init__(*args, choices=flavors, **kwargs)


class IceCream(models.Model):
    price = models.DecimalField(max_digits=4, decimal_places=2)
    flavor = IceCreamFlavor(max_length=20)

Run syncdb, and inspect your table to see that the ENUM was created properly.
mysql> SHOW COLUMNS IN icecream;
+--------+-----------------------------+------+-----+---------+----------------+
| Field  | Type                        | Null | Key | Default | Extra          |
+--------+-----------------------------+------+-----+---------+----------------+
| id     | int(11)                     | NO   | PRI | NULL    | auto_increment |
| price  | decimal(4,2)                | NO   |     | NULL    |                |
| flavor | enum('chocolate','vanilla') | NO   |     | NULL    |                |
+--------+-----------------------------+------+-----+---------+----------------+

",459
21454,33932,2,"Using the choices parameter won't use the ENUM db type; it will just create a VARCHAR or INTEGER, depending on whether you use choices with a CharField or IntegerField.  Generally, this is just fine.  If it's important to you that the ENUM type is used at the database level, you have three options:

Use ""./manage.py sql appname"" to see the SQL Django generates, manually modify it to use the ENUM type, and run it yourself.  If you create the table manually first, ""./manage.py syncdb"" won't mess with it.
If you don't want to do this manually every time you generate your DB, put some custom SQL in appname/sql/modelname.sql to perform the appropriate ALTER TABLE command.
Create a custom field type and define the db_type method appropriately.

With any of these options, it would be your responsibility to deal with the implications for cross-database portability.  In option 2, you could use database-backend-specific custom SQL to ensure your ALTER TABLE is only run on MySQL.  In option 3, your db_type method would need to check the database engine and set the db column type to a type that actually exists in that database.
UPDATE: Since the migrations framework was added in Django 1.7, options 1 and 2 above are entirely obsolete. Option 3 was always the best option anyway. The new version of options 1/2 would involve a complex custom migration using SeparateDatabaseAndState -- but really you want option 3.
",272
21454,334932,2,"If you really want to use your databases ENUM type:

Use Django 1.x
Recognize your application will only work on some databases.
Puzzle through this documentation page:http://docs.djangoproject.com/en/dev/howto/custom-model-fields/#howto-custom-model-fields

Good luck!
",38
21454,1530858,2,"from django.db import models

class EnumField(models.Field):
    """"""
    A field class that maps to MySQL's ENUM type.

    Usage:

    class Card(models.Model):
        suit = EnumField(values=('Clubs', 'Diamonds', 'Spades', 'Hearts'))

    c = Card()
    c.suit = 'Clubs'
    c.save()
    """"""
    def __init__(self, *args, **kwargs):
        self.values = kwargs.pop('values')
        kwargs['choices'] = [(v, v) for v in self.values]
        kwargs['default'] = self.values[0]
        super(EnumField, self).__init__(*args, **kwargs)

    def db_type(self):
        return ""enum({0})"".format( ','.join(""'%s'"" % v for v in self.values) )

",156
31340,21608282,2,"One easy solution to the GIL is the multiprocessing module. It can be used as a drop in replacement to the threading module but uses multiple Interpreter processes instead of threads. Because of this there is a little more overhead than plain threading for simple things but it gives you the advantage of real parallelization if you need it.
It also easily scales to multiple physical machines.
If you need truly large scale parallelization than I would look further but if you just want to scale to all the cores of one computer or a few different ones without all the work that would go into implementing a more comprehensive framework, than this is for you.
",122
31340,31340,1,"I've been trying to wrap my head around how threads work in Python, and it's hard to find good information on how they operate. I may just be missing a link or something, but it seems like the official documentation isn't very thorough on the subject, and I haven't been able to find a good write-up.
From what I can tell, only one thread can be running at once, and the active thread switches every 10 instructions or so?
Where is there a good explanation, or can you provide one? It would also be very nice to be aware of common problems that you run into while using threads with Python.
",126
31340,31552,2,"Use threads in python if the individual workers are doing I/O bound operations. If you are trying to scale across multiple cores on a machine either find a good IPC framework for python or pick a different language.
",40
31340,31372,2,"Python's a fairly easy language to thread in, but there are caveats.  The biggest thing you need to know about is the Global Interpreter Lock.  This allows only one thread to access the interpreter.  This means two things:  1)  you rarely ever find yourself using a lock statement in python and 2) if you want to take advantage of multi-processor systems, you have to use separate processes.  EDIT:  I should also point out that you can put some of the code in C/C++ if you want to get around the GIL as well.
Thus, you need to re-consider why you want to use threads.  If you want to parallelize your app to take advantage of dual-core architecture, you need to consider breaking your app up into multiple processes.
If you want to improve responsiveness, you should CONSIDER using threads.  There are other alternatives though, namely microthreading.  There are also some frameworks that you should look into:

stackless python
greenlets
gevent
monocle

",182
31340,31358,2,"Below is a basic threading sample. It will spawn 20 threads; each thread will output its thread number. Run it and observe the order in which they print.
import threading
class Foo (threading.Thread):
    def __init__(self,x):
        self.__x = x
        threading.Thread.__init__(self)
    def run (self):
          print str(self.__x)

for x in xrange(20):
    Foo(x).start()

As you have hinted at Python threads are implemented through time-slicing. This is how they get the ""parallel"" effect. 
In my example my Foo class extends thread, I then implement the run method, which is where the code that you would like to run in a thread goes. To start the thread you call start() on the thread object, which will automatically invoke the run method...
Of course, this is just the very basics. You will eventually want to learn about semaphores, mutexes, and locks for thread synchronization and message passing.
",188
31340,1197151,2,"Try to remember that the GIL is set to poll around every so often in order to do show the appearance of multiple tasks. This setting can be fine tuned, but I offer the suggestion that there should be work that the threads are doing or lots of context switches are going to cause problems.
I would go so far as to suggest multiple parents on processors and try to keep like jobs on the same core(s).
",84
31340,31398,2,"Yes, because of the Global Interpreter Lock (GIL) there can only run one thread at a time. Here are some links with some insights about this:

http://www.artima.com/weblogs/viewpost.jsp?thread=214235
http://smoothspan.wordpress.com/2007/09/14/guido-is-right-to-leave-the-gil-in-python-not-for-multicore-but-for-utility-computing/

From the last link an interesting quote:

Let me explain what all that means. 
  Threads run inside the same virtual
  machine, and hence run on the same
  physical machine.  Processes can run
  on the same physical machine or in
  another physical machine.  If you
  architect your application around
  threads, you’ve done nothing to access
  multiple machines.  So, you can scale
  to as many cores are on the single
  machine (which will be quite a few
  over time), but to really reach web
  scales, you’ll need to solve the
  multiple machine problem anyway.

If you want to use multi core, pyprocessing defines an process based API to do real parallelization. The PEP also includes some interesting benchmarks.
",175
31412,31412,1,"I am developing a GPL-licensed application in Python and need to know if the GPL allows my program to use proprietary plug-ins. This is what the FSF has to say on the issue:

If a program released under the GPL uses plug-ins, what are the requirements for the licenses of a plug-in?
It depends on how the program invokes its plug-ins. If the program uses fork and exec to invoke plug-ins, then the plug-ins are separate programs, so the license for the main program makes no requirements for them.
If the program dynamically links plug-ins, and they make function calls to each other and share data structures, we believe they form a single program, which must be treated as an extension of both the main program and the plug-ins. This means the plug-ins must be released under the GPL or a GPL-compatible free software license, and that the terms of the GPL must be followed when those plug-ins are distributed.
If the program dynamically links plug-ins, but the communication between them is limited to invoking the ‘main’ function of the plug-in with some options and waiting for it to return, that is a borderline case. 

The distinction between fork/exec and dynamic linking, besides being kind of artificial, doesn't carry over to interpreted languages: what about a Python/Perl/Ruby plugin, which gets loaded via import or execfile?
(edit: I understand why the distinction between fork/exec and dynamic linking, but it seems like someone who wanted to comply with the GPL but go against the ""spirit"" --I don't-- could just use fork/exec and interprocess communication to do pretty much anything).
The best solution would be to add an exception to my license to explicitly allow the use of proprietary plugins, but I am unable to do so since I'm using Qt/PyQt which is GPL.
",339
31412,31420,2,"@Daniel The distinction between fork/exec and dynamic linking, besides being kind of artificial, doesn't carry over to interpreted languages: what about a Python/Perl/Ruby plugin, which gets loaded via import or execfile?
I'm not sure that the distinction is artificial. After a dynamic load the plugin code shares an execution context with the GPLed code. After a fork/exec it does not.
In anycase I would guess that importing causes the new code to run in the same execution context as the GPLed bit, and you should treat it like the dynamic link case. No?
",107
31412,31423,2,"How much info are you sharing between the Plugins and the main program? If you are doing anything more than just executing them and waiting for the results (sharing no data between the program and the plugin in the process) then you could most likely get away with them being proprietary, otherwise they would probably need to be GPL'd.
",65
31412,31421,2,"
he distinction between fork/exec and dynamic linking, besides being kind of artificial,

I don't think its artificial at all.  Basically they are just making the division based upon the level of integration.  If the program has ""plugins"" which are essentially fire and forget with no API level integration, then the resulting work is unlikely to be considered a derived work.  Generally speaking a plugin which is merely forked/exec'ed would fit this criteria, though there may be cases where it does not.  This case especially applies if the ""plugin"" code would work independently of your code as well.
If, on the other hand, the code is deeply dependent upon the GPL'ed work, such as extensively calling APIs, or tight data structure integration, then things are more likely to be considered a derived work.  Ie, the ""plugin"" cannot exist on its own without the GPL product, and a product with this plugin installed is essentially a derived work of the GPLed product.
So to make it a little more clear, the same principles could apply to your interpreted code.  If the interpreted code relies heavily upon your APIs (or vice-versa) then it would be considered a derived work.  If it is just a script that executes on its own with extremely little integration, then it may not.
Does that make more sense?
",254
38435,38718,2,"I believe this is a bug in the Oracle ODBC driver. Basically, the Oracle ODBC driver does not support the TIMESTAMP WITH (LOCAL) TIME ZONE data types, only the TIMESTAMP data type. As you have discovered, one workaround is in fact to use the TO_CHAR method.
In your example you are not actually reading the time zone information. If you have control of the table you could convert it to a straight TIMESTAMP column. If you don't have control over the table, another solution may be to create a view that converts from TIMESTAMP WITH TIME ZONE to TIMESTAMP via a string - sorry, I don't know if there is a way to convert directly from TIMESTAMP WITH TIME ZONE to TIMESTAMP.
",137
38435,38435,1,"Given an Oracle table created using the following:
CREATE TABLE Log(WhenAdded TIMESTAMP(6) WITH TIME ZONE);

Using the Python ODBC module from its Win32 extensions (from the win32all package), I tried the following:
import dbi, odbc

connection = odbc.odbc(""Driver=Oracle in OraHome92;Dbq=SERVER;Uid=USER;Pwd=PASSWD"")

cursor = connection.cursor()
cursor.execute(""SELECT WhenAdded FROM Log"")

results = cursor.fetchall()

When I run this, I get the following:
Traceback (most recent call last):
...
    results = cursor.fetchall()
dbi.operation-error: [Oracle][ODBC][Ora]ORA-00932: inconsistent datatypes: expected %s got %s 
in FETCH

The other data types I've tried (VARCHAR2, BLOB) do not cause this problem. Is there a way of retrieving timestamps?
",157
38435,38442,2,"My solution to this, that I hope can be bettered, is to use Oracle to explicitly convert the TIMESTAMP into a string:
cursor.execute(""SELECT TO_CHAR(WhenAdded, 'YYYY-MM-DD HH:MI:SSAM') FROM Log"")

This works, but isn't portable. I'd like to use the same Python script against a SQL Server database, so an Oracle-specific solution (such as TO_CHAR) won't work.
",81
38508,38516,2,"def f(in_str):
    out_str = in_str.upper()
    return True, out_str # Creates tuple automatically

succeeded, b = f(""a"") # Automatic tuple unpacking

",33
38508,38524,2,"Why not throw an exception if the operation wasn't successful?  Personally, I tend to be of the opinion that if you need to return more than one value from a function, you should reconsider if you're doing things the right way or use an object.
But more directly to the point, if you throw an exception, you're forcing them to deal with the problem.  If you try to return a value that indicates failure, it's very well possible somebody could not check the value and end up with some potentially hard to debug errors.
",107
38508,38760,2,"Throwing an exception for failure is one good way to proceed, and if you're returning a lot of different values, you can return a tuple.  For the specific case you're citing, I often take an intermediate approach: return the modified string on success, and return None on failure.  I'm enough of an unreconstructed C programmer to want to return a NULL pointer to char on failure.
If I were writing a routine to be used as part of a larger library and consumed by other developers, I'd throw an exception on failure.  When I'm eating my own dogfood, I'll probably return different types and test on return.
",125
38508,38513,2,"Returning a tuple is the usual way to do this in Python.
",13
38508,38508,1,"I have a function where I need to do something to a string.  I need the function to return a boolean indicating whether or not the operation succeeded, and I also need to return the modified string.  
In C#, I would use an out parameter for the string, but there is no equivalent in Python.  I'm still very new to Python and the only thing I can think of is to return a tuple with the boolean and modified string.
Related question: Is it pythonic for a function to return multiple values?
",103
38508,38514,2,"Return a tuple.
def f(x):
    # do stuff
    return (True, modified_string)

success, modified_string = f(something)

",27
38601,11446609,2,"My head code for 1.4 version(some new and some removed)
{% block extrahead %}

<link rel=""stylesheet"" type=""text/css"" href=""{{ STATIC_URL }}admin/css/forms.css""/>
<link rel=""stylesheet"" type=""text/css"" href=""{{ STATIC_URL }}admin/css/base.css""/>
<link rel=""stylesheet"" type=""text/css"" href=""{{ STATIC_URL }}admin/css/global.css""/>
<link rel=""stylesheet"" type=""text/css"" href=""{{ STATIC_URL }}admin/css/widgets.css""/>

<script type=""text/javascript"" src=""/admin/jsi18n/""></script>
<script type=""text/javascript"" src=""{{ STATIC_URL }}admin/js/core.js""></script>
<script type=""text/javascript"" src=""{{ STATIC_URL }}admin/js/admin/RelatedObjectLookups.js""></script>
<script type=""text/javascript"" src=""{{ STATIC_URL }}admin/js/jquery.js""></script>
<script type=""text/javascript"" src=""{{ STATIC_URL }}admin/js/jquery.init.js""></script>
<script type=""text/javascript"" src=""{{ STATIC_URL }}admin/js/actions.js""></script>
<script type=""text/javascript"" src=""{{ STATIC_URL }}admin/js/calendar.js""></script>
<script type=""text/javascript"" src=""{{ STATIC_URL }}admin/js/admin/DateTimeShortcuts.js""></script>

{% endblock %}

",255
38601,38601,1,"How can I use the nifty JavaScript date and time widgets that the default admin uses with my custom view?
I have looked through the Django forms documentation, and it briefly mentions django.contrib.admin.widgets, but I don't know how to use it?
Here is my template that I want it applied on.
<form action=""."" method=""POST"">
    <table>
        {% for f in form %}
           <tr> <td> {{ f.name }}</td> <td>{{ f }}</td> </tr>
        {% endfor %}
    </table>
    <input type=""submit"" name=""submit"" value=""Add Product"">
</form>

Also, I think it should be noted that I haven't really written a view up myself for this form, I am using a generic view. Here is the entry from the url.py:
(r'^admin/products/add/$', create_object, {'model': Product, 'post_save_redirect': ''}),

And I am relevantly new to the whole Django/MVC/MTV thing, so please go easy...
",207
38601,39946546,2,"In Django 10.
myproject/urls.py:
at the beginning of urlpatterns
  from django.views.i18n import JavaScriptCatalog

urlpatterns = [
    url(r'^jsi18n/$', JavaScriptCatalog.as_view(), name='javascript-catalog'),
.
.
.]

In my template.html:
{% load staticfiles %}

    <script src=""{% static ""js/jquery-2.2.3.min.js"" %}""></script>
    <script src=""{% static ""js/bootstrap.min.js"" %}""></script>
    {# Loading internazionalization for js #}
    {% load i18n admin_modify %}
    <script type=""text/javascript"" src=""{% url 'javascript-catalog' %}""></script>
    <script type=""text/javascript"" src=""{% static ""/admin/js/jquery.init.js"" %}""></script>

    <link rel=""stylesheet"" type=""text/css"" href=""{% static ""/admin/css/base.css"" %}"">
    <link rel=""stylesheet"" type=""text/css"" href=""{% static ""/admin/css/forms.css"" %}"">
    <link rel=""stylesheet"" type=""text/css"" href=""{% static ""/admin/css/login.css"" %}"">
    <link rel=""stylesheet"" type=""text/css"" href=""{% static ""/admin/css/widgets.css"" %}"">



    <script type=""text/javascript"" src=""{% static ""/admin/js/core.js"" %}""></script>
    <script type=""text/javascript"" src=""{% static ""/admin/js/SelectFilter2.js"" %}""></script>
    <script type=""text/javascript"" src=""{% static ""/admin/js/admin/RelatedObjectLookups.js"" %}""></script>
    <script type=""text/javascript"" src=""{% static ""/admin/js/actions.js"" %}""></script>
    <script type=""text/javascript"" src=""{% static ""/admin/js/calendar.js"" %}""></script>
    <script type=""text/javascript"" src=""{% static ""/admin/js/admin/DateTimeShortcuts.js"" %}""></script>

",349
38601,9139017,2,"What about just assigning a class to your widget and then binding that class to the JQuery datepicker?
Django forms.py:
class MyForm(forms.ModelForm):

  class Meta:
    model = MyModel

  def __init__(self, *args, **kwargs):
    super(MyForm, self).__init__(*args, **kwargs)
    self.fields['my_date_field'].widget.attrs['class'] = 'datepicker'

And some JavaScript for the template:
  $("".datepicker"").datepicker();

",85
38601,3284874,2,"(I'm trying to comment on people suggesting to roll their own Calendar widget, but either I don't see the comment button, or I don't have enough rep.)
What happened to DRY? I think it would be best to re-use the admin widget, but perhaps it should be separated from admin, and easier to use. Thanks for this information anyways.
",71
38601,72284,2,"As the solution is hackish, I think using your own date/time widget with some JavaScript is more feasible.
",20
38601,408230,2,"Yep, I ended up overriding the /admin/jsi18n/ url.
Here's what I added in my urls.py.  Make sure it's above the /admin/ url
    (r'^admin/jsi18n', i18n_javascript),

And here is the i18n_javascript function I created.
from django.contrib import admin
def i18n_javascript(request):
  return admin.site.i18n_javascript(request)

",58
38601,719583,2,"Complementing the answer by Carl Meyer, I would like to comment that you need to put that header in some valid block (inside the header) within your template.
{% block extra_head %}

<link rel=""stylesheet"" type=""text/css"" href=""/media/admin/css/forms.css""/>
<link rel=""stylesheet"" type=""text/css"" href=""/media/admin/css/base.css""/>
<link rel=""stylesheet"" type=""text/css"" href=""/media/admin/css/global.css""/>
<link rel=""stylesheet"" type=""text/css"" href=""/media/admin/css/widgets.css""/>

<script type=""text/javascript"" src=""/admin/jsi18n/""></script>
<script type=""text/javascript"" src=""/media/admin/js/core.js""></script>
<script type=""text/javascript"" src=""/media/admin/js/admin/RelatedObjectLookups.js""></script>

{{ form.media }}

{% endblock %}

",154
38601,1392329,2,"I find myself referencing this post a lot, and found that the documentation defines a slightly less hacky way to override default widgets. 
(No need to override the ModelForm's __init__ method)
However, you still need to wire your JS and CSS appropriately as Carl mentions.
forms.py
from django import forms
from my_app.models import Product
from django.contrib.admin import widgets                                       


class ProductForm(forms.ModelForm):
    mydate = forms.DateField(widget=widgets.AdminDateWidget)
    mytime = forms.TimeField(widget=widgets.AdminTimeWidget)
    mydatetime = forms.SplitDateTimeField(widget=widgets.AdminSplitDateTime)

    class Meta:
        model = Product

Reference Field Types to find the default form fields.
",105
38601,1833247,2,"Updated solution and workaround for SplitDateTime with required=False:
forms.py
from django import forms

class SplitDateTimeJSField(forms.SplitDateTimeField):
    def __init__(self, *args, **kwargs):
        super(SplitDateTimeJSField, self).__init__(*args, **kwargs)
        self.widget.widgets[0].attrs = {'class': 'vDateField'}
        self.widget.widgets[1].attrs = {'class': 'vTimeField'}  


class AnyFormOrModelForm(forms.Form):
    date = forms.DateField(widget=forms.TextInput(attrs={'class':'vDateField'}))
    time = forms.TimeField(widget=forms.TextInput(attrs={'class':'vTimeField'}))
    timestamp = SplitDateTimeJSField(required=False,)

form.html
<script type=""text/javascript"" src=""/admin/jsi18n/""></script>
<script type=""text/javascript"" src=""/admin_media/js/core.js""></script>
<script type=""text/javascript"" src=""/admin_media/js/calendar.js""></script>
<script type=""text/javascript"" src=""/admin_media/js/admin/DateTimeShortcuts.js""></script>

urls.py
(r'^admin/jsi18n/', 'django.views.i18n.javascript_catalog'),

",179
38601,38916,2,"The growing complexity of this answer over time, and the many hacks required, probably ought to caution you against doing this at all. It's relying on undocumented internal implementation details of the admin, is likely to break again in future versions of Django, and is no easier to implement than just finding another JS calendar widget and using that.
That said, here's what you have to do if you're determined to make this work:

Define your own ModelForm subclass for your model (best to put it in forms.py in your app), and tell it to use the AdminDateWidget / AdminTimeWidget / AdminSplitDateTime (replace 'mydate' etc with the proper field names from your model):
from django import forms
from my_app.models import Product
from django.contrib.admin import widgets                                       

class ProductForm(forms.ModelForm):
    class Meta:
        model = Product
    def __init__(self, *args, **kwargs):
        super(ProductForm, self).__init__(*args, **kwargs)
        self.fields['mydate'].widget = widgets.AdminDateWidget()
        self.fields['mytime'].widget = widgets.AdminTimeWidget()
        self.fields['mydatetime'].widget = widgets.AdminSplitDateTime()

Change your URLconf to pass 'form_class': ProductForm instead of 'model': Product to the generic create_object view (that'll mean ""from my_app.forms import ProductForm"" instead of ""from my_app.models import Product"", of course).
In the head of your template, include {{ form.media }} to output the links to the Javascript files.
And the hacky part: the admin date/time widgets presume that the i18n JS stuff has been loaded, and also require core.js, but don't provide either one automatically.  So in your template above {{ form.media }} you'll need:
<script type=""text/javascript"" src=""/my_admin/jsi18n/""></script>
<script type=""text/javascript"" src=""/media/admin/js/core.js""></script>

You may also wish to use the following admin CSS (thanks Alex for mentioning this):
<link rel=""stylesheet"" type=""text/css"" href=""/media/admin/css/forms.css""/>
<link rel=""stylesheet"" type=""text/css"" href=""/media/admin/css/base.css""/>
<link rel=""stylesheet"" type=""text/css"" href=""/media/admin/css/global.css""/>
<link rel=""stylesheet"" type=""text/css"" href=""/media/admin/css/widgets.css""/>


This implies that Django's admin media (ADMIN_MEDIA_PREFIX) is at /media/admin/ - you can change that for your setup.  Ideally you'd use a context processor to pass this values to your template instead of hardcoding it, but that's beyond the scope of this question.
This also requires that the URL /my_admin/jsi18n/ be manually wired up to the django.views.i18n.javascript_catalog view (or null_javascript_catalog if you aren't using I18N).  You have to do this yourself instead of going through the admin application so it's accessible regardless of whether you're logged into the admin (thanks Jeremy for pointing this out).  Sample code for your URLconf:
(r'^my_admin/jsi18n', 'django.views.i18n.javascript_catalog'),

Lastly, if you are using Django 1.2 or later, you need some additional code in your template to help the widgets find their media:
{% load adminmedia %} /* At the top of the template. */

/* In the head section of the template. */
<script type=""text/javascript"">
window.__admin_media_prefix__ = ""{% filter escapejs %}{% admin_media_prefix %}{% endfilter %}"";
</script>

Thanks lupefiasco for this addition.
",643
38601,2396907,2,"The below will also work as a last resort if the above failed
class PaymentsForm(forms.ModelForm):
    class Meta:
        model = Payments

    def __init__(self, *args, **kwargs):
        super(PaymentsForm, self).__init__(*args, **kwargs)
        self.fields['date'].widget = SelectDateWidget()

Same as 
class PaymentsForm(forms.ModelForm):
    date = forms.DateField(widget=SelectDateWidget())

    class Meta:
        model = Payments

put this in your forms.py from django.forms.extras.widgets import SelectDateWidget
",88
38601,2716963,2,"I finally managed to get this widget working on the dev server, only to have it break on deployment.  I finally decided it wasn't worth shoehorning into my site, and wrote my own widget.  It's not as flexible, but it will probably work well for many: http://www.copiesofcopies.org/webl/?p=81
",59
38601,2818128,2,"Starting in Django 1.2 RC1, if you're using the Django admin date picker widge trick, the following has to be added to your template, or you'll see the calendar icon url being referenced through ""/missing-admin-media-prefix/"".
{% load adminmedia %} /* At the top of the template. */

/* In the head section of the template. */
<script type=""text/javascript"">
window.__admin_media_prefix__ = ""{% filter escapejs %}{% admin_media_prefix %}{% endfilter %}"";
</script>

",99
38987,40677646,2,"You can use toolz.merge([x, y]) for this.
",14
38987,18665968,2,"** creates an intermediary dict, which means that the total number of copies
is actually higher doing the dict(one, **two) form, but all that happens in C
so it's still generally faster than going to itertools, unless there are a huge number of copies (or, probably, if the copies are very expensive). As always if you actually care about speed you should time your use case.
Timing on Python 2.7.3 with an empty dict:
$ python -m timeit ""dict({}, **{})""
1000000 loops, best of 3: 0.405 usec per loop

$ python -m timeit -s ""from itertools import chain"" \
    ""dict(chain({}.iteritems(), {}.iteritems()))""
1000000 loops, best of 3: 1.18 usec per loop

With 10,000 (tiny) items:
$ python -m timeit -s 'd = {i: str(i) for i in xrange(10000)}' \
    ""dict(d, **d)""
1000 loops, best of 3: 550 usec per loop

$ python -m timeit -s ""from itertools import chain"" -s 'd = {i: str(i) for i in xrange(10000)}' \
    ""dict(chain(d.iteritems(), d.iteritems()))""
1000 loops, best of 3: 1.11 msec per loop

With 100,000 items:
$ python -m timeit -s 'd = {i: str(i) for i in xrange(100000)}' \
    ""dict(d, **d)""
10 loops, best of 3: 19.6 msec per loop

$ python -m timeit -s ""from itertools import chain"" -s 'd = {i: str(i) for i in xrange(100000)}' \
    ""dict(chain(d.iteritems(), d.iteritems()))""
10 loops, best of 3: 20.1 msec per loop

With 1,000,000 items:
$ python -m timeit -s 'd = {i: str(i) for i in xrange(1000000)}' \
    ""dict(d, **d)""
10 loops, best of 3: 273 msec per loop

$ python -m timeit -s ""from itertools import chain"" -s 'd = {i: str(i) for i in xrange(1000000)}' \
    ""dict(chain(d.iteritems(), d.iteritems()))""
10 loops, best of 3: 233 msec per loop

",472
38987,19279501,2,"In python3, the items method no longer returns a list, but rather a view, which acts like a set. In this case you'll need to take the set union since concatenating with + won't work:
dict(x.items() | y.items())

For python3-like behavior in version 2.7, the viewitems method should work in place of items:
dict(x.viewitems() | y.viewitems())

I prefer this notation anyways since it seems more natural to think of it as a set union operation rather than concatenation (as the title shows).
Edit:
A couple more points for python 3. First, note that the dict(x, **y) trick won't work in python 3 unless the keys in y are strings.
Also, Raymond Hettinger's Chainmap answer is pretty elegant, since it can take an arbitrary number of dicts as arguments, but from the docs it looks like it sequentially looks through a list of all the dicts for each lookup:

Lookups search the underlying mappings successively until a key is found.

This can slow you down if you have a lot of lookups in your application:
In [1]: from collections import ChainMap
In [2]: from string import ascii_uppercase as up, ascii_lowercase as lo; x = dict(zip(lo, up)); y = dict(zip(up, lo))
In [3]: chainmap_dict = ChainMap(y, x)
In [4]: union_dict = dict(x.items() | y.items())
In [5]: timeit for k in union_dict: union_dict[k]
100000 loops, best of 3: 2.15 µs per loop
In [6]: timeit for k in chainmap_dict: chainmap_dict[k]
10000 loops, best of 3: 27.1 µs per loop

So about an order of magnitude slower for lookups. I'm a fan of Chainmap, but looks less practical where there may be many lookups.
",374
38987,19950727,2,">>> x = {'a':1, 'b': 2}
>>> y = {'b':10, 'c': 11}
>>> x, z = dict(x), x.update(y) or x
>>> x
{'a': 1, 'b': 2}
>>> y
{'c': 11, 'b': 10}
>>> z
{'a': 1, 'c': 11, 'b': 10}

",94
38987,20358548,2,"The problem I have with solutions listed to date is that, in the merged dictionary, the value for key ""b"" is 10 but, to my way of thinking, it should be 12.
In that light, I present the following:
import timeit

n=100000
su = """"""
x = {'a':1, 'b': 2}
y = {'b':10, 'c': 11}
""""""

def timeMerge(f,su,niter):
    print ""{:4f} sec for: {:30s}"".format(timeit.Timer(f,setup=su).timeit(n),f)

timeMerge(""dict(x, **y)"",su,n)
timeMerge(""x.update(y)"",su,n)
timeMerge(""dict(x.items() + y.items())"",su,n)
timeMerge(""for k in y.keys(): x[k] = k in x and x[k]+y[k] or y[k] "",su,n)

#confirm for loop adds b entries together
x = {'a':1, 'b': 2}
y = {'b':10, 'c': 11}
for k in y.keys(): x[k] = k in x and x[k]+y[k] or y[k]
print ""confirm b elements are added:"",x

Results:
0.049465 sec for: dict(x, **y)
0.033729 sec for: x.update(y)                   
0.150380 sec for: dict(x.items() + y.items())   
0.083120 sec for: for k in y.keys(): x[k] = k in x and x[k]+y[k] or y[k]

confirm b elements are added: {'a': 1, 'c': 11, 'b': 12}

",358
38987,20394520,2,"I have a solution which is not specified here(Man I LOVE python) :-)
z = {}
z.update(x) or z.update(y)

This will not update x as well as y. Performance? I don't think it will be terribly slow :-)
NOTE: It is supposed to be 'or' operation and not 'and' operation. Edited to correct the code.
",77
38987,22122836,2,"It's so silly that .update returns nothing.
I just use a simple helper function to solve the problem:
def merge(dict1,*dicts):
    for dict2 in dicts:
        dict1.update(dict2)
    return dict1

Examples:
merge(dict1,dict2)
merge(dict1,dict2,dict3)
merge(dict1,dict2,dict3,dict4)
merge({},dict1,dict2)  # this one returns a new copy

",82
38987,26111877,2,"A union of the OP's two dictionaries would be something like:
{'a': 1, 'b': 2, 10, 'c': 11}

Specifically, the union of two entities(x and y) contains all the elements of x and/or y.
Unfortunately, what the OP asks for is not a union, despite the title of the post.
My code below is neither elegant nor a one-liner, but I believe it is consistent with the meaning of union.
From the OP's example:
x = {'a':1, 'b': 2}
y = {'b':10, 'c': 11}

z = {}
for k, v in x.items():
    if not k in z:
        z[k] = [(v)]
    else:
        z[k].append((v))
for k, v in y.items():
    if not k in z:
        z[k] = [(v)]
    else:
        z[k].append((v))

{'a': [1], 'b': [2, 10], 'c': [11]}

Whether one wants lists could be changed, but the above will work if a dictionary contains lists (and nested lists) as values in either dictionary.
",250
38987,26853961,2,"
How can I merge two Python dictionaries in a single expression?

Say you have two dicts and you want to merge them into a new dict without altering the original dicts:
x = {'a': 1, 'b': 2}
y = {'b': 3, 'c': 4}

The desired result is to get a new dictionary (z) with the values merged, and the second dict's values overwriting those from the first.
>>> z
{'a': 1, 'b': 3, 'c': 4}

A new syntax for this, proposed in PEP 448 and available as of Python 3.5, is 
z = {**x, **y}

And it is indeed a single expression. It is now showing as implemented in the release schedule for 3.5, PEP 478, and it has now made its way into What's New in Python 3.5 document.
However, since many organizations are still on Python 2, you may wish to do this in a backwards compatible way. The classically Pythonic way, available in Python 2 and Python 3.0-3.4, is to do this as a two-step process:
z = x.copy()
z.update(y) # which returns None since it mutates z

In both approaches, y will come second and its values will replace x's values, thus 'b' will point to 3 in our final result.
Not yet on Python 3.5, but want a single expression
If you are not yet on Python 3.5, or need to write backward-compatible code, and you want this in a single expression, the most performant while correct approach is to put it in a function:
def merge_two_dicts(x, y):
    """"""Given two dicts, merge them into a new dict as a shallow copy.""""""
    z = x.copy()
    z.update(y)
    return z

and then you have a single expression:
z = merge_two_dicts(x, y)

You can also make a function to merge an undefined number of dicts, from zero to a very large number:
def merge_dicts(*dict_args):
    """"""
    Given any number of dicts, shallow copy and merge into a new dict,
    precedence goes to key value pairs in latter dicts.
    """"""
    result = {}
    for dictionary in dict_args:
        result.update(dictionary)
    return result

This function will work in Python 2 and 3 for all dicts. e.g. given dicts a to g:
z = merge_dicts(a, b, c, d, e, f, g) 

and key value pairs in g will take precedence over dicts a to f, and so on.
Critiques of Other Answers
Don't use what you see in the formerly accepted answer:
z = dict(x.items() + y.items())

In Python 2, you create two lists in memory for each dict, create a third list in memory with length equal to the length of the first two put together, and then discard all three lists to create the dict. In Python 3, this will fail because you're adding two dict_items objects together, not two lists - 
>>> c = dict(a.items() + b.items())
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: unsupported operand type(s) for +: 'dict_items' and 'dict_items'

and you would have to explicitly create them as lists, e.g. z = dict(list(x.items()) + list(y.items())). This is a waste of resources and computation power. 
Similarly, taking the union of items() in Python 3 (viewitems() in Python 2.7) will also fail when values are unhashable objects (like lists, for example). Even if your values are hashable, since sets are semantically unordered, the behavior is undefined in regards to precedence. So don't do this:
>>> c = dict(a.items() | b.items())

This example demonstrates what happens when values are unhashable:
>>> x = {'a': []}
>>> y = {'b': []}
>>> dict(x.items() | y.items())
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: unhashable type: 'list'

Here's an example where y should have precedence, but instead the value from x is retained due to the arbitrary order of sets:
>>> x = {'a': 2}
>>> y = {'a': 1}
>>> dict(x.items() | y.items())
{'a': 2}

Another hack you should not use:
z = dict(x, **y)

This uses the dict constructor, and is very fast and memory efficient (even slightly more-so than our two-step process) but unless you know precisely what is happening here (that is, the second dict is being passed as keyword arguments to the dict constructor), it's difficult to read, it's not the intended usage, and so it is not Pythonic. 
Here's an example of the usage being remediated in django.
Dicts are intended to take hashable keys (e.g. frozensets or tuples), but this method fails in Python 3 when keys are not strings.
>>> c = dict(a, **b)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: keyword arguments must be strings

From the mailing list, Guido van Rossum, the creator of the language, wrote:

I am fine with
  declaring dict({}, **{1:3}) illegal, since after all it is abuse of
  the ** mechanism.

and 

Apparently dict(x, **y) is going around as ""cool hack"" for ""call
  x.update(y) and return x"". Personally I find it more despicable than
  cool.

It is my understanding (as well as the understanding of the creator of the language) that the intended usage for dict(**y) is for creating dicts for readability purposes, e.g.:
dict(a=1, b=10, c=11)

instead of 
{'a': 1, 'b': 10, 'c': 11}

Response to comments

Despite what Guido says, dict(x, **y) is in line with the dict specification, which btw. works for both Python 2 and 3. The fact that this only works for string keys is a direct consequence of how keyword parameters work and not a short-comming of dict. Nor is using the ** operator in this place an abuse of the mechanism, in fact ** was designed precisely to pass dicts as keywords. 

Again, it doesn't work for 3 when keys are non-strings. The implicit calling contract is that namespaces take ordinary dicts, while users must only pass keyword arguments that are strings. All other callables enforced it. dict broke this consistency in Python 2:
>>> foo(**{('a', 'b'): None})
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: foo() keywords must be strings
>>> dict(**{('a', 'b'): None})
{('a', 'b'): None}

This inconsistency was bad given other implementations of Python (Pypy, Jython, IronPython). Thus it was fixed in Python 3, as this usage could be a breaking change.
I submit to you that it is malicious incompetence to intentionally write code that only works in one version of a language or that only works given certain arbitrary constraints.
Another comment:

dict(x.items() + y.items()) is still the most readable solution for Python 2. Readability counts. 

My response: merge_two_dicts(x, y) actually seems much clearer to me, if we're actually concerned about readability. And it is not forward compatible, as Python 2 is increasingly deprecated.
Less Performant But Correct Ad-hocs
These approaches are less performant, but they will provide correct behavior.
They will be much less performant than copy and update or the new unpacking because they iterate through each key-value pair at a higher level of abstraction, but they do respect the order of precedence (latter dicts have precedence)
You can also chain the dicts manually inside a dict comprehension:
{k: v for d in dicts for k, v in d.items()} # iteritems in Python 2.7

or in python 2.6 (and perhaps as early as 2.4 when generator expressions were introduced):
dict((k, v) for d in dicts for k, v in d.items())

itertools.chain will chain the iterators over the key-value pairs in the correct order:
import itertools
z = dict(itertools.chain(x.iteritems(), y.iteritems()))

Performance Analysis
I'm only going to do the performance analysis of the usages known to behave correctly. 
import timeit

The following is done on Ubuntu 14.04
In Python 2.7 (system Python):
>>> min(timeit.repeat(lambda: merge_two_dicts(x, y)))
0.5726828575134277
>>> min(timeit.repeat(lambda: {k: v for d in (x, y) for k, v in d.items()} ))
1.163769006729126
>>> min(timeit.repeat(lambda: dict(itertools.chain(x.iteritems(), y.iteritems()))))
1.1614501476287842
>>> min(timeit.repeat(lambda: dict((k, v) for d in (x, y) for k, v in d.items())))
2.2345519065856934

In Python 3.5 (deadsnakes PPA):
>>> min(timeit.repeat(lambda: {**x, **y}))
0.4094954460160807
>>> min(timeit.repeat(lambda: merge_two_dicts(x, y)))
0.7881555100320838
>>> min(timeit.repeat(lambda: {k: v for d in (x, y) for k, v in d.items()} ))
1.4525277839857154
>>> min(timeit.repeat(lambda: dict(itertools.chain(x.items(), y.items()))))
2.3143140770262107
>>> min(timeit.repeat(lambda: dict((k, v) for d in (x, y) for k, v in d.items())))
3.2069112799945287

Resources on Dictionaries

My explanation of Python's dictionary implementation, updated for 3.6.
Answer on how to add new keys to a dictionary
Mapping two lists into a dictionary
The official Python docs on dictionaries 
The Dictionary Even Mightier - talk by Brandon Rhodes at Pycon 2017
Modern Python Dictionaries, A Confluence of Great Ideas - talk by Raymond Hettinger at Pycon 2017

",2073
38987,28753078,2,"Python 3.5 (PEP 448) allows a nicer syntax option:
x = {'a': 1, 'b': 1}
y = {'a': 2, 'c': 2}
final = {**x, **y} 
final
# {'a': 2, 'b': 1, 'c': 2}

Or even 
final = {'a': 1, 'b': 1, **x, **y}

",82
38987,29177685,2,"a = {1: 2, 3: 4, 5: 6}
b = {7:8, 1:2}
combined = dict(a.items() + b.items())
print combined

",36
38987,31812635,2,"Simple solution using itertools that preserves order (latter dicts have precedence)
import itertools as it
merge = lambda *args: dict(it.chain.from_iterable(it.imap(dict.iteritems, args)))

And it's usage:
>>> x = {'a':1, 'b': 2}
>>> y = {'b':10, 'c': 11}
>>> merge(x, y)
{'a': 1, 'b': 10, 'c': 11}

>>> z = {'c': 3, 'd': 4}
>>> merge(x, y, z)
{'a': 1, 'b': 10, 'c': 3, 'd': 4}

",138
38987,44151666,2,"In python 3:
import collections
a = {1: 1, 2: 2}
b = {2: 3, 3: 4}
c = {3: 5}

r = dict(collections.ChainMap(a, b, c))
print(r)

Out:
{1: 1, 2: 2, 3: 4}

Docs: https://docs.python.org/3/library/collections.html#collections.ChainMap:
",75
38987,33999337,2,"from collections import Counter
dict1 = {'a':1, 'b': 2}
dict2 = {'b':10, 'c': 11}
result = dict(Counter(dict1) + Counter(dict2))

This should solve your problem.
",44
38987,34899183,2,"Be pythonic. Use a comprehension:
z={i:d[i] for d in [x,y] for i in d}

>>> print z
{'a': 1, 'c': 11, 'b': 10}

",49
38987,36263150,2,"(For Python2.7* only; there are simpler solutions for Python3*.)
If you're not averse to importing a standard library module, you can do
from functools import reduce

def merge_dicts(*dicts):
    return reduce(lambda a, d: a.update(d) or a, dicts, {})

(The or a bit in the lambda is necessary because dict.update always returns None on success.)
",77
38987,18114065,2,"Abuse leading to a one-expression solution for Matthew's answer:
>>> x = {'a':1, 'b': 2}
>>> y = {'b':10, 'c': 11}
>>> z = (lambda f=x.copy(): (f.update(y), f)[1])()
>>> z
{'a': 1, 'c': 11, 'b': 10}

You said you wanted one expression, so I abused lambda to bind a name, and tuples to override lambda's one-expression limit. Feel free to cringe.
You could also do this of course if you don't care about copying it:
>>> x = {'a':1, 'b': 2}
>>> y = {'b':10, 'c': 11}
>>> z = (x.update(y), x)[1]
>>> z
{'a': 1, 'b': 10, 'c': 11}

",190
38987,46439094,2,"Pythonic way (Python 2)
dict1 = {
    'a': 'Apple',
    'b': 'Boy'
}

dict2 = {
    'c': 'Cow',
    'd': 'Dog'
}

merged_dict = dict1.copy()
merged_dict.update(dict2)

",43
38987,46356150,2,"If you don't mind mutating x,
x.update(y) or x

Simple, readable, performant. You know update() always returns None, which is a false value. So it will always evaluate to x.
Mutating methods in the standard library, like update, return None by convention, so this trick will work on those too.
If you're using a library that doesn't follow this convention, you can use a tuple display and index to make it a single expression, instead of or, but it's not as readable.
(x.update(y), x)[-1]

If you don't have x in a variable yet, you can use lambda to make a local without using an assignment statement. This amounts to using lambda as a let expression, which is common technique in functional languages, but rather unpythonic.
(lambda x: x.update(y) or x)({'a':1, 'b': 2})

If you do want a copy, PEP 448 is best {**x, **y}. But if that's not available, let works here too.
(lambda z: z.update(y) or z)(x.copy())

",229
38987,37304637,2,"I know this does not really fit the specifics of the questions (""one liner""), but since none of the answers above went into this direction while lots and lots of answers addressed the performance issue, I felt I should contribute my thoughts.
Depending on the use case it might not be necessary to create a ""real"" merged dictionary of the given input dictionaries.  A view which does this might be sufficient in many cases, i. e. an object which acts like the merged dictionary would without computing it completely.  A lazy version of the merged dictionary, so to speak.
In Python, this is rather simple and can be done with the code shown at the end of my post.  This given, the answer to the original question would be:
z = MergeDict(x, y)

When using this new object, it will behave like a merged dictionary but it will have constant creation time and constant memory footprint while leaving the original dictionaries untouched.  Creating it is way cheaper than in the other solutions proposed.
Of course, if you use the result a lot, then you will at some point reach the limit where creating a real merged dictionary would have been the faster solution.  As I said, it depends on your use case.
If you ever felt you would prefer to have a real merged dict, then calling dict(z) would produce it (but way more costly than the other solutions of course, so this is just worth mentioning).
You can also use this class to make a kind of copy-on-write dictionary:
a = { 'x': 3, 'y': 4 }
b = MergeDict(a)  # we merge just one dict
b['x'] = 5
print b  # will print {'x': 5, 'y': 4}
print a  # will print {'y': 4, 'x': 3}

Here's the straight-forward code of MergeDict:
class MergeDict(object):
  def __init__(self, *originals):
    self.originals = ({},) + originals[::-1]  # reversed

  def __getitem__(self, key):
    for original in self.originals:
      try:
        return original[key]
      except KeyError:
        pass
    raise KeyError(key)

  def __setitem__(self, key, value):
    self.originals[0][key] = value

  def __iter__(self):
    return iter(self.keys())

  def __repr__(self):
    return '%s(%s)' % (
      self.__class__.__name__,
      ', '.join(repr(original)
          for original in reversed(self.originals)))

  def __str__(self):
    return '{%s}' % ', '.join(
        '%r: %r' % i for i in self.iteritems())

  def iteritems(self):
    found = set()
    for original in self.originals:
      for k, v in original.iteritems():
        if k not in found:
          yield k, v
          found.add(k)

  def items(self):
    return list(self.iteritems())

  def keys(self):
    return list(k for k, _ in self.iteritems())

  def values(self):
    return list(v for _, v in self.iteritems())

",621
38987,39251599,2,"For Python 2 :
x = {'a':1, 'b': 2}
y = {'b':10, 'c': 11}
z = dict(x.items()+y.items())
print(z)

For Python 3:
x = {'a':1, 'b': 2}
y = {'b':10, 'c': 11}
z = dict(x.items()|y.items())
print(z)

It gives output:{'a': 1, 'c': 11, 'b': 10}
",98
38987,39736284,2,"In Python 3.5 you can use unpack ** in order to create new dictionary. 
This method has no been showed in past answers. Also, it's better to use {} instead of dict(). Because {} is a python literal and dict() involves a function call.
dict1 = {'a':1}
dict2 = {'b':2}
new_dict = {**dict1, **dict2}
>>>new_dict
{'a':1, 'a':2}

",82
38987,31478567,2,"This can be done with a single dict comprehension:
>>> x = {'a':1, 'b': 2}
>>> y = {'b':10, 'c': 11}
>>> { key: y[key] if key in y else x[key]
      for key in set(x) + set(y)
    }

In my view the best answer for the 'single expression' part as no extra functions are needed, and it is short.
",92
38987,44262317,2,"The question is tagged python-3x but, taking into account that it's a relatively recent addition and that the most voted, accepted answer deals extensively with a Python 2.x solution, I dare add a one liner that draws on an irritating feature of Python 2.x list comprehension, that is name leaking...
$ python2
Python 2.7.13 (default, Jan 19 2017, 14:48:08) 
[GCC 6.3.0 20170118] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> x = {'a':1, 'b': 2}
>>> y = {'b':10, 'c': 11}
>>> [z.update(d) for z in [{}] for d in (x, y)]
[None, None]
>>> z
{'a': 1, 'c': 11, 'b': 10}
>>> ...

I'm happy to say that the above doesn't work any more on any version of Python 3.
",195
38987,17587183,2,"Here is some code, it seems to work ok:
def merge(d1, d2, mode=0):
    if not type(d2) is dict:
        raise Exception(""d2 is not a dict"")

    if not type(d1) is dict:
        if mode == 0:
            raise Exception(""d1 is not a dict"")
        return d2

    result = dict(d1)

    for k, v in d2.iteritems():
        if k in result and type(v) is dict:
            result[k] = merge(result[k], v, 1)
        else:
            if mode == 1:
                result.update(d2)
            else:
                result[k] = v
    return result

",132
38987,16769722,2,"Using  a dict comprehension, you may
x = {'a':1, 'b': 2}
y = {'b':10, 'c': 11}

dc = {xi:(x[xi] if xi not in list(y.keys()) 
           else y[xi]) for xi in list(x.keys())+(list(y.keys()))}

gives
>>> dc
{'a': 1, 'c': 11, 'b': 10}

Note the syntax for if else in comprehension 
{ (some_key if condition else default_key):(something_if_true if condition 
          else something_if_false) for key, value in dict_.items() }

",126
38987,17738920,2,"Drawing on ideas here and elsewhere I've comprehended a function:
def merge(*dicts, **kv): 
      return { k:v for d in list(dicts) + [kv] for k,v in d.items() }

Usage (tested in python 3):
assert (merge({1:11,'a':'aaa'},{1:99, 'b':'bbb'},foo='bar')==\
    {1: 99, 'foo': 'bar', 'b': 'bbb', 'a': 'aaa'})

assert (merge(foo='bar')=={'foo': 'bar'})

assert (merge({1:11},{1:99},foo='bar',baz='quux')==\
    {1: 99, 'foo': 'bar', 'baz':'quux'})

assert (merge({1:11},{1:99})=={1: 99})

You could use a lambda instead.
",184
38987,38987,1,"I have two Python dictionaries, and I want to write a single expression that returns these two dictionaries, merged.  The update() method would be what I need, if it returned its result instead of modifying a dict in-place.
>>> x = {'a':1, 'b': 2}
>>> y = {'b':10, 'c': 11}
>>> z = x.update(y)
>>> print(z)
None
>>> x
{'a': 1, 'b': 10, 'c': 11}

How can I get that final merged dict in z, not x?
(To be extra-clear, the last-one-wins conflict-handling of dict.update() is what I'm looking for as well.)
",144
38987,38989,2,"x = {'a':1, 'b': 2}
y = {'b':10, 'c': 11}
z = dict(x.items() + y.items())
print z

For items with keys in both dictionaries ('b'), you can control which one ends up in the output by putting that one last.
",62
38987,38990,2,"In your case, what you can do is:
z = dict(x.items() + y.items())

This will, as you want it, put the final dict in z, and make the value for key b be properly overridden by the second (y) dict's value:
>>> x = {'a':1, 'b': 2}
>>> y = {'b':10, 'c': 11}
>>> z = dict(x.items() + y.items())
>>> z
{'a': 1, 'c': 11, 'b': 10}

If you use Python 3, it is only a little more complicated.  To create z:
>>> z = dict(list(x.items()) + list(y.items()))
>>> z
{'a': 1, 'c': 11, 'b': 10}

",177
38987,39437,2,"An alternative:
z = x.copy()
z.update(y)

",12
38987,44512,2,"I wanted something similar, but with the ability to specify how the values on duplicate keys were merged, so I hacked this out (but did not heavily test it).  Obviously this is not a single expression, but it is a single function call.
def merge(d1, d2, merge_fn=lambda x,y:y):
    """"""
    Merges two dictionaries, non-destructively, combining 
    values on duplicate keys as defined by the optional merge
    function.  The default behavior replaces the values in d1
    with corresponding values in d2.  (There is no other generally
    applicable merge strategy, but often you'll have homogeneous 
    types in your dicts, so specifying a merge technique can be 
    valuable.)

    Examples:

    >>> d1
    {'a': 1, 'c': 3, 'b': 2}
    >>> merge(d1, d1)
    {'a': 1, 'c': 3, 'b': 2}
    >>> merge(d1, d1, lambda x,y: x+y)
    {'a': 2, 'c': 6, 'b': 4}

    """"""
    result = dict(d1)
    for k,v in d2.iteritems():
        if k in result:
            result[k] = merge_fn(result[k], v)
        else:
            result[k] = v
    return result

",258
38987,49492,2,"This probably won't be a popular answer, but you almost certainly do not want to do this.  If you want a copy that's a merge, then use copy (or deepcopy, depending on what you want) and then update.  The two lines of code are much more readable - more Pythonic - than the single line creation with .items() + .items().  Explicit is better than implicit.
In addition, when you use .items() (pre Python 3.0), you're creating a new list that contains the items from the dict.  If your dictionaries are large, then that is quite a lot of overhead (two large lists that will be thrown away as soon as the merged dict is created).  update() can work more efficiently, because it can run through the second dict item-by-item.
In terms of time:
>>> timeit.Timer(""dict(x, **y)"", ""x = dict(zip(range(1000), range(1000)))\ny=dict(zip(range(1000,2000), range(1000,2000)))"").timeit(100000)
15.52571702003479
>>> timeit.Timer(""temp = x.copy()\ntemp.update(y)"", ""x = dict(zip(range(1000), range(1000)))\ny=dict(zip(range(1000,2000), range(1000,2000)))"").timeit(100000)
15.694622993469238
>>> timeit.Timer(""dict(x.items() + y.items())"", ""x = dict(zip(range(1000), range(1000)))\ny=dict(zip(range(1000,2000), range(1000,2000)))"").timeit(100000)
41.484580039978027

IMO the tiny slowdown between the first two is worth it for the readability.  In addition, keyword arguments for dictionary creation was only added in Python 2.3, whereas copy() and update() will work in older versions.
",378
38987,228366,2,"In a follow-up answer, you asked about the relative performance of these two alternatives:
z1 = dict(x.items() + y.items())
z2 = dict(x, **y)

On my machine, at least (a fairly ordinary x86_64 running Python 2.5.2), alternative z2 is not only shorter and simpler but also significantly faster.  You can verify this for yourself using the timeit module that comes with Python.
Example 1: identical dictionaries mapping 20 consecutive integers to themselves:
% python -m timeit -s 'x=y=dict((i,i) for i in range(20))' 'z1=dict(x.items() + y.items())'
100000 loops, best of 3: 5.67 usec per loop
% python -m timeit -s 'x=y=dict((i,i) for i in range(20))' 'z2=dict(x, **y)' 
100000 loops, best of 3: 1.53 usec per loop

z2 wins by a factor of 3.5 or so.  Different dictionaries seem to yield quite different results, but z2 always seems to come out ahead.  (If you get inconsistent results for the same test, try passing in -r with a number larger than the default 3.)
Example 2: non-overlapping dictionaries mapping 252 short strings to integers and vice versa:
% python -m timeit -s 'from htmlentitydefs import codepoint2name as x, name2codepoint as y' 'z1=dict(x.items() + y.items())'
1000 loops, best of 3: 260 usec per loop
% python -m timeit -s 'from htmlentitydefs import codepoint2name as x, name2codepoint as y' 'z2=dict(x, **y)'               
10000 loops, best of 3: 26.9 usec per loop

z2 wins by about a factor of 10.  That's a pretty big win in my book!
After comparing those two, I wondered if z1's poor performance could be attributed to the overhead of constructing the two item lists, which in turn led me to wonder if this variation might work better:
from itertools import chain
z3 = dict(chain(x.iteritems(), y.iteritems()))

A few quick tests, e.g.
% python -m timeit -s 'from itertools import chain; from htmlentitydefs import codepoint2name as x, name2codepoint as y' 'z3=dict(chain(x.iteritems(), y.iteritems()))'
10000 loops, best of 3: 66 usec per loop

lead me to conclude that z3 is somewhat faster than z1, but not nearly as fast as z2.  Definitely not worth all the extra typing.
This discussion is still missing something important, which is a performance comparison of these alternatives with the ""obvious"" way of merging two lists: using the update method.  To try to keep things on an equal footing with the expressions, none of which modify x or y, I'm going to make a copy of x instead of modifying it in-place, as follows:
z0 = dict(x)
z0.update(y)

A typical result:
% python -m timeit -s 'from htmlentitydefs import codepoint2name as x, name2codepoint as y' 'z0=dict(x); z0.update(y)'
10000 loops, best of 3: 26.9 usec per loop

In other words, z0 and z2 seem to have essentially identical performance.  Do you think this might be a coincidence?  I don't....
In fact, I'd go so far as to claim that it's impossible for pure Python code to do any better than this.  And if you can do significantly better in a C extension module, I imagine the Python folks might well be interested in incorporating your code (or a variation on your approach) into the Python core.  Python uses dict in lots of places; optimizing its operations is a big deal.
You could also write this as
z0 = x.copy()
z0.update(y)

as Tony does, but (not surprisingly) the difference in notation turns out not to have any measurable effect on performance.  Use whichever looks right to you.  Of course, he's absolutely correct to point out that the two-statement version is much easier to understand.
",769
38987,39858,2,"Another, more concise, option:
z = dict(x, **y)

Note: this has become a popular answer, but it is important to point out that if y has any non-string keys, the fact that this works at all is an abuse of a CPython implementation detail, and it does not work in Python 3, or in PyPy, IronPython, or Jython. Also, Guido is not a fan. So I can't recommend this technique for forward-compatible or cross-implementation portable code, which really means it should be avoided entirely.
",104
38987,7770473,2,"While the question has already been answered several times,
this simple solution to the problem has not been listed yet.
x = {'a':1, 'b': 2}
y = {'b':10, 'c': 11}
z4 = {}
z4.update(x)
z4.update(y)

It is as fast as z0 and the evil z2 mentioned above, but easy to understand and change.
",74
38987,8247023,2,"If you think lambdas are evil then read no further.
As requested, you can write the fast and memory-efficient solution with one expression:
x = {'a':1, 'b':2}
y = {'b':10, 'c':11}
z = (lambda a, b: (lambda a_copy: a_copy.update(b) or a_copy)(a.copy()))(x, y)
print z
{'a': 1, 'c': 11, 'b': 10}
print x
{'a': 1, 'b': 2}

As suggested above, using two lines or writing a function is probably a better way to go.
",120
38987,8310229,2,"Recursively/deep update a dict
def deepupdate(original, update):
    """"""
    Recursively update a dict.
    Subdict's won't be overwritten but also updated.
    """"""
    for key, value in original.iteritems(): 
        if key not in update:
            update[key] = value
        elif isinstance(value, dict):
            deepupdate(value, update[key]) 
    return update
Demonstration:
pluto_original = {
    'name': 'Pluto',
    'details': {
        'tail': True,
        'color': 'orange'
    }
}

pluto_update = {
    'name': 'Pluutoo',
    'details': {
        'color': 'blue'
    }
}

print deepupdate(pluto_original, pluto_update)
Outputs:
{
    'name': 'Pluutoo',
    'details': {
        'color': 'blue',
        'tail': True
    }
}
Thanks rednaw for edits.
",154
38987,11804613,2,"Even though the answers were good for this shallow dictionary, none of the methods defined here actually do a deep dictionary merge.
Examples follow:
a = { 'one': { 'depth_2': True }, 'two': True }
b = { 'one': { 'extra': False } }
print dict(a.items() + b.items())

One would expect a result of something like this:
{ 'one': { 'extra': False', 'depth_2': True }, 'two': True }

Instead, we get this:
{'two': True, 'one': {'extra': False}}

The 'one' entry should have had 'depth_2' and 'extra' as items inside its dictionary if it truly was a merge.
Using chain also, does not work:
from itertools import chain
print dict(chain(a.iteritems(), b.iteritems()))

Results in:
{'two': True, 'one': {'extra': False}}

The deep merge that rcwesick gave also creates the same result.
Yes, it will work to merge the sample dictionaries, but none of them are a generic mechanism to merge.  I'll update this later once I write a method that does a true merge.
",242
38987,11825563,2,"def dict_merge(a, b):
  c = a.copy()
  c.update(b)
  return c

new = dict_merge(old, extras)

Among such shady and dubious answers, this shining example is the one and only good way to merge dicts in Python, endorsed by dictator for life Guido van Rossum himself!  Someone else suggested half of this, but did not put it in a function.
print dict_merge(
      {'color':'red', 'model':'Mini'},
      {'model':'Ferrari', 'owner':'Carl'})

gives:
{'color': 'red', 'owner': 'Carl', 'model': 'Ferrari'}

",128
38987,12926103,2,"Two dictionaries
def union2(dict1, dict2):
    return dict(list(dict1.items()) + list(dict2.items()))

n dictionaries
def union(*dicts):
    return dict(itertools.chain.from_iterable(dct.items() for dct in dicts))

sum has bad performance. See https://mathieularose.com/how-not-to-flatten-a-list-of-lists-in-python/
",58
38987,16259217,2,"In Python 3, you can use collections.ChainMap which groups multiple dicts or other mappings together to create a single, updateable view:
>>> from collections import ChainMap
>>> x = {'a':1, 'b': 2}
>>> y = {'b':10, 'c': 11}
>>> z = ChainMap({}, y, x)
>>> for k, v in z.items():
        print(k, '-->', v)

a --> 1
b --> 10
c --> 11

",106
38987,3936548,2,"The best version I could think while not using copy would be:
from itertools import chain
x = {'a':1, 'b': 2}
y = {'b':10, 'c': 11}
dict(chain(x.iteritems(), y.iteritems()))

It's faster than dict(x.items() + y.items()) but not as fast as n = copy(a); n.update(b), at least on CPython. This version also works in Python 3 if you change iteritems() to items(), which is automatically done by the 2to3 tool.
Personally I like this version best because it describes fairly good what I want in a single  functional syntax. The only minor problem is that it doesn't make completely obvious that values from y takes precedence over values from x, but I don't believe it's difficult to figure that out.
",169
39086,315088,2,"Here's another example that was tested, and will match search & replace patterns:
import fileinput
import sys

def replaceAll(file,searchExp,replaceExp):
    for line in fileinput.input(file, inplace=1):
        if searchExp in line:
            line = line.replace(searchExp,replaceExp)
        sys.stdout.write(line)

Example use:
replaceAll(""/fooBar.txt"",""Hello\sWorld!$"",""Goodbye\sWorld."")

",77
39086,23426834,2,"Expanding on @Kiran's answer, which I agree is more succinct and Pythonic, this adds codecs to support the reading and writing of UTF-8:
import codecs 

from tempfile import mkstemp
from shutil import move
from os import remove


def replace(source_file_path, pattern, substring):
    fh, target_file_path = mkstemp()

    with codecs.open(target_file_path, 'w', 'utf-8') as target_file:
        with codecs.open(source_file_path, 'r', 'utf-8') as source_file:
            for line in source_file:
                target_file.write(line.replace(pattern, substring))
    remove(source_file_path)
    move(target_file_path, source_file_path)

",111
39086,21857132,2,"If you're wanting a generic function that replaces any text with some other text, this is likely the best way to go, particularly if you're a fan of regex's:
import re
def replace( filePath, text, subs, flags=0 ):
    with open( filePath, ""r+"" ) as file:
        fileContents = file.read()
        textPattern = re.compile( re.escape( text ), flags )
        fileContents = textPattern.sub( subs, fileContents )
        file.seek( 0 )
        file.truncate()
        file.write( fileContents )

",96
39086,18676598,2,"A more pythonic way would be to use context managers like the code below:
from tempfile import mkstemp
from shutil import move
from os import remove

def replace(source_file_path, pattern, substring):
    fh, target_file_path = mkstemp()
    with open(target_file_path, 'w') as target_file:
        with open(source_file_path, 'r') as source_file:
            for line in source_file:
                target_file.write(line.replace(pattern, substring))
    remove(source_file_path)
    move(target_file_path, source_file_path)

You can find the full snippet here.
",98
39086,13641746,2,"Based on the answer by Thomas Watnedal. 
However, this does not answer the line-to-line part of the original question exactly. The function can still replace on a line-to-line basis 
This implementation replaces the file contents without using temporary files, as a consequence file permissions remain unchanged.
Also re.sub instead of replace, allows regex replacement instead of plain text replacement only.
Reading the file as a single string instead of line by line allows for multiline match and replacement.
import re

def replace(file, pattern, subst):
    # Read contents from file as a single string
    file_handle = open(file, 'r')
    file_string = file_handle.read()
    file_handle.close()

    # Use RE package to allow for replacement (also allowing for (multiline) REGEX)
    file_string = (re.sub(pattern, subst, file_string))

    # Write contents to file.
    # Using mode 'w' truncates the file.
    file_handle = open(file, 'w')
    file_handle.write(file_string)
    file_handle.close()

",184
39086,11784227,2,"if you remove the indent at the like below, it will search and replace in multiple line.
See below for example.
def replace(file, pattern, subst):
    #Create temp file
    fh, abs_path = mkstemp()
    print fh, abs_path
    new_file = open(abs_path,'w')
    old_file = open(file)
    for line in old_file:
        new_file.write(line.replace(pattern, subst))
    #close temp file
    new_file.close()
    close(fh)
    old_file.close()
    #Remove original file
    remove(file)
    #Move new file
    move(abs_path, file)

",110
39086,1388570,2,"This should work: (inplace editing)
import fileinput

# Does a list of files, and
# redirects STDOUT to the file in question
for line in fileinput.input(files, inplace = 1): 
      print line.replace(""foo"", ""bar""),

",50
39086,290494,2,"The shortest way would probably be to use the fileinput module. For example, the following adds line numbers to a file, in-place:
import fileinput

for line in fileinput.input(""test.txt"", inplace=True):
    print ""%d: %s"" % (fileinput.filelineno(), line),

What happens here is:

The original file is moved to a backup file
The standard output is redirected to the original file within the loop
Thus any print statements write back into the original file

fileinput has more bells and whistles. For example, it can be used to automatically operate on all files in sys.args[1:], without your having to iterate over them explicitly. Starting with Python 3.2 it also provides a convenient context manager for use in a with statement.

While fileinput is great for throwaway scripts, I would be wary of using it in real code because admittedly it's not very readable or familiar. In real (production) code it's worthwhile to spend just a few more lines of code to make the process explicit and thus make the code readable.
There are two options:

The file is not overly large, and you can just read it wholly to memory. Then close the file, reopen it in writing mode and write the modified contents back.
The file is too large to be stored in memory; you can move it over to a temporary file and open that, reading it line by line, writing back into the original file. Note that this requires twice the storage.

",289
39086,23123426,2,"Using hamishmcn's answer as a template I was able to search for a line in a file that match my regex and replacing it with empty string.
import re 

fin = open(""in.txt"", 'r') # in file
fout = open(""out.txt"", 'w') # out file
for line in fin:
    p = re.compile('[-][0-9]*[.][0-9]*[,]|[-][0-9]*[,]') # pattern
    newline = p.sub('',line) # replace matching strings with empty string
    print newline
    fout.write(newline)
fin.close()
fout.close()

",128
39086,39110,2,"I guess something like this should do it. It basically writes the content to a new file and replaces the old file with the new file:
from tempfile import mkstemp
from shutil import move
from os import fdopen, remove

def replace(file_path, pattern, subst):
    #Create temp file
    fh, abs_path = mkstemp()
    with fdopen(fh,'w') as new_file:
        with open(file_path) as old_file:
            for line in old_file:
                new_file.write(line.replace(pattern, subst))
    #Remove original file
    remove(file_path)
    #Move new file
    move(abs_path, file_path)

",114
39086,39089,2,"Create a new file, copy lines from the old to the new, and do the replacing before you write the lines to the new file.
",28
39086,39113,2,"As lassevk suggests, write out the new file as you go, here is some example code:
fin = open(""a.txt"")
fout = open(""b.txt"", ""wt"")
for line in fin:
    fout.write( line.replace('foo', 'bar') )
fin.close()
fout.close()

",61
39086,39086,1,"I want to loop over the contents of a text file and do a search and replace on some lines and write the result back to the file. I could first load the whole file in memory and then write it back, but that probably is not the best way to do it.
What is the best way to do this, within the following code?
f = open(file)
for line in f:
    if line.contains('foo'):
        newline = line.replace('foo', 'bar')
        # how to write this newline back to the file

",108
39104,39104,1,"I've written a Python package that includes a bsddb database of pre-computed values for one of the more time-consuming computations.  For simplicity, my setup script installs the database file in the same directory as the code which accesses the database (on Unix, something like /usr/lib/python2.5/site-packages/mypackage/).
How do I store the final location of the database file so my code can access it?  Right now, I'm using a hack based on the __file__ variable in the module which accesses the database:

dbname = os.path.join(os.path.dirname(__file__), ""database.dat"")

It works, but it seems... hackish.  Is there a better way to do this?  I'd like to have the setup script just grab the final installation location from the distutils module and stuff it into a ""dbconfig.py"" file that gets installed alongside the code that accesses the database.
",160
39104,39295,2,"That's probably the way to do it, without resorting to something more advanced like using setuptools to install the files where they belong.
Notice there's a problem with that approach, because on OSes with real a security framework (UNIXes, etc.) the user running your script might not have the rights to access the DB in the system directory where it gets installed.
",72
39104,39659,2,"Try using pkg_resources, which is part of setuptools (and available on all of the pythons I have access to right now):
>>> import pkg_resources
>>> pkg_resources.resource_ filename(__name__, ""foo.config"")
'foo.config'
>>> pkg_resources.resource_filename('tempfile', ""foo.config"")
'/usr/lib/python2.4/foo.config'

There's more discussion about using pkg_resources to get resources on the eggs page and the pkg_resources page.
Also note, where possible it's probably advisable to use pkg_resources.resource_stream or pkg_resources.resource_string because if the package is part of an egg, resource_filename will copy the file to a temporary directory.
",109
39104,9918496,2,"Use pkgutil.get_data. It’s the cousin of pkg_resources.resource_stream, but in the standard library, and should work with flat filesystem installs as well as zipped packages and other importers.
",33
41547,4815619,2,"its possible by default, by doing the following steps, ensure you have added the context 'django.contrib.auth.context_processors.auth' in your settings. By default its added in settings.py, so its looks like this 
TEMPLATE_CONTEXT_PROCESSORS = (
'django.core.context_processors.request',
'django.contrib.auth.context_processors.auth',
'django.core.context_processors.auth',)

And you can access user object like this,
{% if user.is_authenticated %}
<p>Welcome, {{ user.username }}. Thanks for logging in.</p>
{% else %}
    <p>Welcome, new user. Please log in.</p>
{% endif %}

For more information, refer here http://docs.djangoproject.com/en/1.2/topics/auth/#authentication-data-in-templates 
",116
41547,1064621,2,"There is no need to write a context processor for the user object if you already have the ""django.core.context_processors.auth"" in TEMPLATE_CONTEXT_PROCESSORS and if you're using RequestContext in your views. 
if you are using django 1.4 or latest the module has been moved to django.contrib.auth.context_processors.auth
",48
41547,269249,2,"@Ryan: Documentation about preprocessors is a bit small
@Staale: Adding user to the Context every time one is calling the template in view, DRY
Solution is to use a preprocessor
A: In your settings add
TEMPLATE_CONTEXT_PROCESSORS = (
    'myapp.processor_file_name.user',
)

B: In myapp/processor_file_name.py insert
def user(request):
    if hasattr(request, 'user'):
        return {'user':request.user }
    return {}

From now on you're able to use user object functionalities in your templates.
{{ user.get_full_name }}

",98
41547,41560,2,"@Dave
To use {{user.username}} in my templates, I will then have to use 
requestcontext rather than just a normal map/hash: http://www.djangoproject.com/documentation/templates_python/#subclassing-context-requestcontext
So I guess there are no globals that the template engine checks.
But the RequestContext has some prepopulate classes that I can look into to solve my problems. Thanks.
",64
41547,41558,2,"In a more general sense of not having to explicitly set variables in each view, it sounds like you want to look at writing your own context processor.
From the docs:

A context processor has a very simple interface: It's just a Python function that takes one argument, an HttpRequest object, and returns a dictionary that gets added to the template context. Each context processor must return a dictionary.

",78
41547,11878636,2,"The hints are in every answer, but once again, from ""scratch"", for newbies:
authentication data is in templates (almost) by default -- with a small trick:
in views.py:
from django.template import RequestContext
...
def index(request):
    return render_to_response('index.html', 
                              {'var': 'value'},
                              context_instance=RequestContext(request))

in index.html:
...
Hi, {{ user.username }}
var: {{ value }}
... 

From here: https://docs.djangoproject.com/en/1.4/topics/auth/#authentication-data-in-templates

This template context variable is not available if a RequestContext is
  not being used.

",110
41547,41555,2,"If you can hook your authentication into the Django authentication scheme you'll be able to use request.user.
I think this should just be a case of calling authenticate() and login() based on the contents of your Cookie.
Edit: @Staale - I always use the locals() trick for my context so all my templates can see request and so request.user.  If you're not then I guess it wouldn't be so straightforward.
",85
41547,41547,1,"I am working on a small intranet site for a small company, where user should be able to post. I have imagined a very simple authentication mechanism where people just enter their email address, and gets sent a unique login url, that sets a cookie that will always identify them for future requests.
In my template setup, I have base.html, and the other pages extend this. I want to show logged in or register button in the base.html, but how can I ensure that the necessary variables are always a part of the context? It seems that each view just sets up the context as they like, and there is no global context population. Is there a way of doing this without including the user in each context creation?
Or will I have to make my own custom shortcuts to setup the context properly?
",158
48458,67533,2,"I think the first option is considered the best practice. And make the code folder your first package. The Rietveld project developed by Guido van Rossum is a very good model to learn from. Have a look at it: http://code.google.com/p/rietveld
With regard to Django 1.0, I suggest you start using the Django trunk code instead of the GAE built in django port. Again, have a look at how it's done in Rietveld.
",83
48458,12535000,2,"I implemented a google app engine boilerplate today and checked it on github. This is along the lines described by Nick Johnson above (who used to work for Google).
Follow this link gae-boilerplate
",37
48458,48467,2,"I am not entirely up to date on the latest best practices, et cetera when it comes to code layout, but when I did my first GAE application, I used something along your second option, where the code and templates are next to eachother.
There was two reasons for this - one, it kept the code and template nearby, and secondly, I had the directory structure layout mimic that of the website - making it (for me) a bit easier too remember where everything was.
",96
48458,3105295,2,"I like webpy so I've adopted it as templating framework on Google App Engine.
My package folders are typically organized like this:
app.yaml
application.py
index.yaml
/app
   /config
   /controllers
   /db
   /lib
   /models
   /static
        /docs
        /images
        /javascripts
        /stylesheets
   test/
   utility/
   views/

Here is an example.
",47
48458,48458,1,"I started an application in Google App Engine right when it came out, to play with the technology and work on a pet project that I had been thinking about for a long time but never gotten around to starting.  The result is BowlSK.  However, as it has grown, and features have been added, it has gotten really difficult to keep things organized - mainly due to the fact that this is my first python project, and I didn't know anything about it until I started working.
What I have:

Main Level contains:


all .py files (didn't know how to make packages work)
all .html templates for main level pages

Subdirectories:


separate folders for css, images, js, etc.
folders that hold .html templates for subdirecty-type urls


Example:
http://www.bowlsk.com/ maps to HomePage (default package), template at ""index.html""
http://www.bowlsk.com/games/view-series.html?series=7130 maps to ViewSeriesPage (again, default package), template at ""games/view-series.html""
It's nasty.  How do I restructure?  I had 2 ideas:

Main Folder containing: appdef, indexes, main.py?

Subfolder for code.  Does this have to be my first package?
Subfolder for templates.  Folder heirarchy would match package heirarchy
Individual subfolders for css, images, js, etc.

Main Folder containing appdef, indexes, main.py?

Subfolder for code + templates.  This way I have the handler class right next to the template, because in this stage, I'm adding lots of features, so modifications to one mean modifications to the other.  Again, do I have to have this folder name be the first package name for my classes?  I'd like the folder to be ""src"", but I don't want my classes to be ""src.WhateverPage""


Is there a best practice?  With Django 1.0 on the horizon, is there something I can do now to improve my ability to integrate with it when it becomes the official GAE templating engine?  I would simply start trying these things, and seeing which seems better, but pyDev's refactoring support doesn't seem to handle package moves very well, so it will likely be a non-trivial task to get all of this working again.
",415
48458,70271,2,"First, I would suggest you have a look at ""Rapid Development with Python, Django, and Google App Engine""
GvR describes a general/standard project layout on page 10 of his slide presentation.  
Here I'll post a slightly modified version of the layout/structure from that page. I pretty much follow this pattern myself. You also mentioned you had trouble with packages. Just make sure each of your sub folders has an __init__.py file. It's ok if its empty.
Boilerplate files

These hardly vary between projects
app.yaml: direct all non-static requests to main.py 
main.py: initialize app and send it all requests 

Project lay-out

static/*: static files; served directly by App Engine
myapp/*.py: app-specific python code


views.py, models.py, tests.py, __init__.py, and more

templates/*.html: templates (or myapp/templates/*.html)

Here are some code examples that may help as well:
main.py
import wsgiref.handlers

from google.appengine.ext import webapp
from myapp.views import *

application = webapp.WSGIApplication([
  ('/', IndexHandler),
  ('/foo', FooHandler)
], debug=True)

def main():
  wsgiref.handlers.CGIHandler().run(application)

myapp/views.py
import os
import datetime
import logging
import time

from google.appengine.api import urlfetch
from google.appengine.ext.webapp import template
from google.appengine.api import users
from google.appengine.ext import webapp
from models import *

class IndexHandler(webapp.RequestHandler):
  def get(self):
    date = ""foo""
    # Do some processing        
    template_values = {'data': data }
    path = os.path.join(os.path.dirname(__file__) + '/../templates/', 'main.html')
    self.response.out.write(template.render(path, template_values))

class FooHandler(webapp.RequestHandler):
  def get(self):
    #logging.debug(""start of handler"")

myapp/models.py
from google.appengine.ext import db

class SampleModel(db.Model):

I think this layout works great for new and relatively small to medium projects. For larger projects I would suggest breaking up the views and models to have their own sub-folders with something like:
Project lay-out

static/: static files; served directly by App Engine


js/*.js
images/*.gif|png|jpg
css/*.css

myapp/: app structure


models/*.py
views/*.py
tests/*.py
templates/*.html: templates


",378
48458,153862,2,"My usual layout looks something like this:

app.yaml
index.yaml
request.py - contains the basic WSGI app
lib


__init__.py - common functionality, including a request handler base class

controllers - contains all the handlers. request.yaml imports these.
templates


all the django templates, used by the controllers

model


all the datastore model classes

static


static files (css, images, etc). Mapped to /static by app.yaml


I can provide examples of what my app.yaml, request.py, lib/init.py, and sample controllers look like, if this isn't clear.
",97
48562,48575,2,"What exactly are you trying to do?
Of course, there are numerous places to learn about svn pre-commit hooks (e.g.  here ,  here, and in the Red Book) but it depends what you're trying to do and what is available on your system.  
Can you be more specific? 
",57
48562,50507,2,"I think you can avoid a commit hook script in this case by using the svn:eol-style property as described in the SVNBook:

End-of-Line Character Sequences
Subversion Properties

This way SVN can worry about your line endings for you.
Good luck!
",45
48562,48562,1,"I was wondering if anyone here had some experience writing this type of script and if they could give me some pointers.
I would like to modify this script to validate that the check-in file does not have a Carriage Return in the EOL formatting. The EOL format is CR LF in Windows and LF in Unix. When a User checks-in code with the Windows format. It does not compile in Unix anymore. I know this can be done on the client side but I need to have this validation done on the server side. To achieve this, I need to do the following:
1) Make sure the file I check is not a binary, I dont know how to do this with svnlook, should I check the mime:type of the file? The Red Book does not indicate this clearly or I must have not seen it.
2) I would like to run the dos2unix command to validate that the file has the correct EOL format. I would compare the output of the dos2unix command against the original file. If there is a diff between both, I give an error message to the client and cancel the check-in.
I would like your comments/feedback on this approach.
",227
48777,48777,1,"I'm trying to get Google AppEngine to work on my Debian box and am getting the following error when I try to access my page:
<type 'exceptions.ImportError'>: No module named core.exceptions 

The same app works fine for me when I run it on my other Ubuntu box, so I know it's not a problem with the app itself. However, I need to get it working on this Debian box. It originally had python 2.4 but after AppEngine complained about it I installed the python2.5 and python2.5-dev packages (to no avail). 
I saw on this Google Group post that it may be due to the version of AppEngine and just to reinstall it, but that didn't work. Any ideas?
Edit 1: Also tried uninstalling python2.4 and 2.5 then reinstalling 2.5, which also didn't work.
Edit 2: Turns out when I made AppEngine into a CVS project it didn't add the core directory into my project, so when I checked it out there literally was no module named core.exceptions. Re-downloading that folder resolved the problem.
",200
48777,48806,2,"core.exceptions is part of django; what version of django do you have installed? The AppEngine comes with the appropriate version for whatever release you've downloaded (in the lib/django directory). It can be installed by going to that directory and running python setup.py install
",49
48884,74335,2,"pyaws seems to be the best one out there.  I used it here (my source code)  It worked fine for me.
",25
48884,49222,2,"The only other library I'm aware of is pyAmazon, which is the predecessor of pyaws.  If you're familiar with the Amazon API (or are willing to become so), you could probably put together something yourself with ZSI.
",45
48884,1084257,2,"pyaws is the best in my opinion.  The most available version is 0.2.0, but there is also a version 0.3.0 that is somewhat harder to find.  The best maintained version of it that I have found though, which is based on 0.3.0, is on bitbucket.
",51
48884,48884,1,"What Python libraries do folks use for querying Amazon product data? (Amazon Associates Web Service - used to be called E-Commerce API, or something along those lines).
Based on my research, PyAWS seems okay, but still pretty raw (and hasn't been updated in a while).  Wondering if there's an obvious canonical library that I'm just missing.
",70
48884,1533430,2,"PyAWS is no longer hosted on SourceForge. The latest version (0.3.0) is available via the authors website.
Make sure you also grab the patch for Amazons latest API changes, mentioned in the comments.
",39
48884,2448578,2,"There is now another alternative: python-amazon-product-api. It supports API version 2009-11-01 2010-12-01.
",15
48884,10076757,2,"If what you are looking for is a simple, object oriented access to Amazon products (lookup and search), try python-amazon-simple-product-api. Its a new project i've just released:
http://github.com/yoavaviram/python-amazon-simple-product-api
Its the new kid on the block!
",45
48884,1087797,2,"How about boto? Anyone have any experience with it? I just started looking for a Python package for Amazon and boto looks up to date (v1.8c release 28-Jun-2009), active and complete (has a long list of supported interfaces).
",46
48884,4251893,2,"I'm using Bottlenose, Dan Loewenherz's ""super awesome Python wrapper for the Amazon Product Advertising API"". It doesn't parse the XML, so I'm using lxml.objectify:
ACCESS_KEY_ID = ""...""
SECRET_KEY = ""...""
ASSOC_TAG = ""...""

import bottlenose
amazon = bottlenose.Amazon(ACCESS_KEY_ID, SECRET_KEY, ASSOC_TAG)
response=amazon.ItemLookup(ItemId=""B0018AFK38"", ResponseGroup=""OfferSummary"")

from lxml import objectify
root = objectify.fromstring(response)
root.Items.Item.OfferSummary.LowestNewPrice.FormattedPrice

",84
49137,49148,2,"Boost has a python interface library which could help you.
Boost.Python
",12
49137,49319,2,"Embeding the Python interpreter inside your C++ app will let you run Python programs using your application run Python scripts. It will also make it easier possible for those scripts to call C++ functions in your application. If this is what you want then the Boost library mentioned previously may be what you want to make it easier to create the link. In the past I have used SWIG to generate Python interfaces to C++ code. It was not clear from your question whether you wanted the Python scripts to call your C++ program or whether you just wanted the C++ to call Python.
Many of the Python functions use modules which are not built into the Python interpreter. If your Python scripts call these functions then you will either need to have your users install Python or include the python runtime files with your application. It will depend on what modules you import in you Python scripts.
",166
49137,49439,2,"Boost is probably the best choice, however if you're wanting something that's more standalone, and if this is for use with Windows (which seems feasible given that they are the people least likely to have Python installed), then you can use py2exe to create a DLL with entry points suitable for COM objects.  You can then interface with the library via COM.  (Obviously this is not at all useful as a cross-platform solution).
",85
49137,328451,2,"
I would like to call python script files from my c++ program.

This means that you want to embed Python in your C++ application. As mentioned in Embedding Python in Another Application:

Embedding Python is similar to
  extending it, but not quite. The
  difference is that when you extend
  Python, the main program of the
  application is still the Python
  interpreter, while if you embed
  Python, the main program may have
  nothing to do with Python — instead,
  some parts of the application
  occasionally call the Python
  interpreter to run some Python code.

I suggest that you first go through Embedding Python in Another Application. Then refer the following examples

Embedding Python in C/C++: Part I
Embedding Python in C/C++: Part II
Embedding Python in Multi-Threaded C/C++ Applications

If you like Boost.Python, you may visit the following links:

Embedding Python with Boost.Python Part 1

",159
49137,49137,1,"I would like to call python script files from my c++ program. 
I am not sure that the people I will distribute to will have python installed.
Basically I'm looking for a .lib file that I can use that has an Apache like distribution license.
",49
49146,15667090,2,"Use cx_Freeze to make exe your python program
",8
49146,49155,2,"py2exe is probably what you want, but it only works on Windows.
PyInstaller works on Windows and Linux.
Py2app works on the Mac.
",27
49146,49179,2,"See a short list of python packaging tools on FreeHackers.org.
",11
49146,2245736,2,"I found this presentation to be very helpfull.
How I Distribute Python applications on Windows - py2exe & InnoSetup
From the site:

There are many deployment options for
  Python code. I'll share what has
  worked well for me on Windows,
  packaging command line tools and
  services using py2exe and InnoSetup.
  I'll demonstrate a simple build script
  which creates windows binaries and an
  InnoSetup installer in one step. In
  addition, I'll go over common errors
  which come up when using py2exe and
  hints on troubleshooting them. This is
  a short talk, so there will be a
  follow-up Open Space session to share
  experience and help each other solve
  distribution problems.

",122
49146,3384483,2,"Not on the freehackers list is gui2exe which can be used to build standalone Windows executables, Linux applications and Mac OS application bundles and plugins starting from Python scripts.
",31
49146,10241354,2,"Also known as Frozen Binaries but not the same as as the output of a true compiler- they run byte code through a virtual machine (PVM). Run the same as a compiled program just larger because the program is being compiled along with the PVM. Py2exe can freeze standalone programs that use the tkinter, PMW, wxPython, and PyGTK GUI libraties; programs that use the pygame game programming toolkit; win32com client programs; and more. 
The Stackless Python system is a standard CPython implementation variant that does not save state on the C language call stack. This makes Python more easy to port to small stack architectures, provides efficient multiprocessing options, and fosters novel programming structures such as coroutines. Other systems of study that are working on future development: Pyrex is working on the Cython system, the Parrot project, the PyPy is working on replacing the PVM altogether, and of course the founder of Python is working with Google to get Python to run 5 times faster than C with the Unladen Swallow project. In short, py2exe is the easiest and Cython is more efficient for now until these projects improve the Python Virtual Machine (PVM) for standalone files.
",221
49146,49146,1,"I've used several modules to make EXEs for Python, but I'm not sure if I'm doing it right.
How should I go about this, and why?  Please base your answers on personal experience, and provide references where necessary.
",47
49146,49150,2,"py2exe:

py2exe is a Python Distutils extension which converts Python scripts into executable Windows programs, able to run without requiring a Python installation.

",26
49164,49169,2,"Setuptools is the software that creates .egg files. It's an extension of the distutils package in the standard library.
The process involves creating a setup.py file, then  python setup.py bdist_egg creates an .egg package.
",39
49164,49175,2,"Also, if you need to get an .egg package off a single .py file app, check this link: EasyInstall - Packaging others projects as eggs.
",29
49164,49164,1,"How do I turn a python program into an .egg file?
",12
49195,49195,1,"The first language I learnt  was PHP, but I have more recently picked up Python. As these are all 'high-level' languages, I have found them a bit difficult to pick up. I also tried to learn Objective-C but I gave up.
So, what language should I learn to bridge between Python to C
",60
49195,50673,2,"Pascal! Close enough syntax, still requires you to do some memory management, but not as rough for beginners.
",22
49195,49502,2,"Languages are easy to learn (especially one like C)... the hard part is learning the libraries and/or coding style of the language.  For instance, I know C++ fairly well, but most C/C++ code I see confuses me because the naming conventions are so different from what I work with on a daily basis.
Anyway, I guess what I'm trying to say is don't worry too much about the syntax, focus on said language's library.  This isn't specific to C, you can say the same about c#, vb.net, java and just about every other language out there.
",115
49195,49295,2,"try to learn a language which you are comfortable with, try different approach and the basics.
",18
49195,49285,2,"I generally agree with most of the others - There's not really a good stepping stone language.
It is, however, useful to understand what is difficult about learning C, which might help you understand what's making it difficult for you.
I'd say the things that would prove difficult in C for someone coming from PHP would be :

Pointers and memory management This is pretty much the reason you're learning C I imagine, so there's not really any getting around it. Learning lower level assembly type languages might make this easier, but C is probably a bridge to do that, not the other way around.
Lack of built in data structures PHP and co all have native String types, and useful things like hash tables built in, which is not the case in C. In C, a String is just an array of characters, which means you'll need to do a lot more work, or look seriously at libraries which add the features you're used to.
Lack of built in libraries Languages like PHP nowadays almost always come with stacks of libraries for things like database connections, image manipulation and stacks of other things. In C, this is not the case other than a very thin standard library which revolves mostly around file reading, writing and basic string manipulation. There are almost always good choices available to fill these needs, but you need to include them yourself.
Suitability for high level tasks If you try to implement the same type of application in C as you might in PHP, you'll find it very slow going. Generating a web page, for example, isn't really something plain C is suited for, so if you're trying to do that, you'll find it very slow going.
Preprocessor and compilation Most languages these days don't have a preprocessor, and if you're coming from PHP, the compilation cycle will seem painful. Both of these are performance trade offs in a way - Scripting languages make the trade off in terms of developer efficiency, where as C prefers performance.

I'm sure there are more that aren't springing to mind for me right now. The moral of the story is that trying to understand what you're finding difficult in C may help you proceed. If you're trying to generate web pages with it, try doing something lower level. If you're missing hash tables, try writing your own, or find a library. If you're struggling with pointers, stick with it :)
",475
49195,49217,2,"I'm feeling your pain, I also learned PHP first and I'm trying to learn C++, it's not easy, and I am really struggling, It's been 2 years since I started on c++ and Still the extent of what I can do is cout, cin, and math.
If anyone reads this and wonders where to start, START LOWER.
",70
49195,49247,2,"My suggestion is to get a good C-book that is relevant to what you want to do. I agree that K & R is considered to be ""The book"" on C, but I found ""UNIX Systems Programming"" by Kay A. Robbins and Steven Robbins to be more practical and hands on. The book is full of clean and short code snippets you can type in, compile and try in just a few minutes each.
There is a preview at http://books.google.com/books?id=tdsZHyH9bQEC&printsec=frontcover (Hyperlinking it didn't work.)
",103
49195,49237,2,"Java might actually be a good option here, believe it or not.  It is strongly based on C/C++, so if you can get the syntax and the strong typing, picking up C might be easier.  The benefit is you can learn the lower level syntax without having to learn pointers (since memory is managed for you just like in Python and PHP).  You will, however, learn a similar concept... references (or objects in general).
Also, it is strongly Object Oriented, so it may be difficult to pick up on that if you haven't dealt with OOP yet....  you might be better off just digging in with C like others suggested, but it is an option.
",136
49195,49234,2,"Learning any language takes time, I always ensure I have a measurable goal; I set myself an objective, then start learning the language to achieve this objective, as opposed to trying to learn every nook and cranny of the language and syntax. 
C is not easy, pointers can be hard to comprehend if you’re not coming assembler roots. I first learned C++, then retro fit C to my repertoire but I started with x86 and 68000 assembler.
",88
49195,49227,2,"The best place to start learning C is the book ""The C Programming Language"" by Kernighan and Ritchie.
You will recognise a lot of things from PHP, and you will be surprised how much PHP (and Perl, Python etc) do for you.
Oh and you also will need a C compiler, but i guess you knew that.
",67
49195,49202,2,"It's not clear why you need a bridge language. Why don't you start working with C directly? C is a very simple language itself. I think that hardest part for C learner is pointers and everything else related to memory management. Also C lang is oriented on structured programming, so you will need to learn how to implement data structures and algorithms without OOP goodness. Actually, your question is pretty hard, usually people go from low level langs to high level and I can understand frustration of those who goes in other direction.
",104
49195,49248,2,"I think C++ is a good ""bridge"" to C.  I learned C++ first at University, and since it's based on C you'll learn a lot of the same concepts - perhaps most notably pointers - but also Object Oriented Design.  OO can be applied to all kinds of modern languages, so it's worth learning.  
After learning C++, I found it wasn't too hard to pick up the differences between C++ and C as required (for example, when working on devices that didn't support C++).
",100
49195,49241,2,"Python is about as close to C as you're going to get.  It is in fact a very thin wrapper around C in a lot of places.  However, C does require that you know a little more about how the computer works on a low level.  Thus, you may benefit from trying an assembly language.
LC-3 is a simple assembly language with a simulated machine.
Alternatively, you could try playing with an interactive C interpreter like CINT.
Finally, toughing it out and reading K&R's book is usually the best approach.
",105
49195,49245,2,"Forget Java - it is not going to bring you anywhere closer to C (you have allready proved that you don't have a problem learning new syntax).
Either read K&R or go one lower: Learn about the machine itself. The only tricky part in C is pointers and memory management (which is closely related to pointers, but also has a bit to do with how functions are called). Learning a (simple, maybe even ""fake"" assembly) language should help you out here.
Then, start reading up on the standard library provided by C. It will be your daily bread and butter.
Oh: another tip! If you really do want to bridge, try FORTH. It helped me get into pointers. Also, using the win32 api from Visual Basic 6.0 can teach you some stuff about pointers ;)
",162
49195,49246,2,"C is a bridge onto itself.
K&R is the only programming language book you can read in one sitting and almost never pick it up again ... 
",30
49307,65903,2,"If x and y are column vectors, you can do:
for i=[x';y']
# do stuff with i(1) and i(2)
end

(with row vectors, just use x and y).
Here is an example run:
>> x=[1 ; 2; 3;]

x =

     1
     2
     3

>> y=[10 ; 20; 30;]

y =

    10
    20
    30

>> for i=[x';y']
disp(['size of i = ' num2str(size(i)) ', i(1) = ' num2str(i(1)) ', i(2) = ' num2str(i(2))])
end
size of i = 2  1, i(1) = 1, i(2) = 10
size of i = 2  1, i(1) = 2, i(2) = 20
size of i = 2  1, i(1) = 3, i(2) = 30
>> 

",206
49307,49307,1,"Using the zip function, Python allows for loops to traverse multiple sequences in parallel.  
for (x,y) in zip(List1, List2):
Does MATLAB have an equivalent syntax? If not, what is the best way to iterate over two parallel arrays at the same time using MATLAB?
",58
49307,49514,2,"If I'm not mistaken the zip function you use in python creates a pair of the items found in list1 and list2. Basically it still is a for loop with the addition that it will retrieve the data from the two seperate lists for you, instead that you have to do it yourself.
So maybe your best option is to use a standard for loop like this:
for i=1:length(a)
  c(i) = a(i) + b(i);
end

or whatever you have to do with the data.
If you really are talking about parallel computing then you should take a look at the Parallel Computing Toolbox for matlab, and more specifically at parfor
",132
49307,51137,2,"Tested only in octave... (no matlab license). Variations of arrayfun() exist, check the documentation.
#!/usr/bin/octave -qf

function result = dostuff(my_ten, my_one)
  result = my_ten + my_one
endfunction

tens = [ 10 20 30 ];
ones = [ 1 2 3];

x = arrayfun(@dostuff, tens, ones);

x

Yields...
x =

   11   22   33

",77
49307,138886,2,"for loops in MATLAB used to be slow, but this is not true anymore.
So vectorizing is not always the miracle solution. Just use the profiler, and tic and toc functions to help you identify possible bottlenecks.
",42
49307,218618,2,"I would recommend to join the two arrays for the computation:
% assuming you have column vectors a and b
x = [a b];

for i = 1:length(a)
    % do stuff with one row...
    x(i,:);
end

This will work great if your functions can work with vectors. Then again, many functions can even work with matrices, so you wouldn't even need the loop.
",84
49455,49485,2,"A plugin for GSview for viewing encrypted PDFs is here.
If this works for you, you may be able to look at the source.
",27
49455,110110,2,"xpdf is probably a good reference implementation for this sort of problem.  I have successfully used them to open encrypted pdfs before.
",24
49455,107838,2,"If I remember correctly, there is a fixed padding string of 32 (?) bytes to apply to any password. All passwords need to be 32 bytes at the start of computing the encryption key, either by truncating or adding some of those padding bytes.
If no user password was set you simply have to pad with all 32 bytes of the string, i.e. use the 32 padding bytes as the starting point for computing the encryption key.
I have to admit it's been a while since I've done this, I do remember that the encryption part of the PDF is an absolute mess as it got changed significantly in nearly every revision, requiring you to cope with a lot of cases to handle all PDF's.
Good luck.
",143
49455,49455,1,"Although the PDF specification is available from Adobe, it's not exactly the simplest document to read through.  PDF allows documents to be encrypted so that either a user password and/or an owner password is required to do various things with the document (display, print, etc).  A common use is to lock a PDF so that end users can read it without entering any password, but a password is required to do anything else.
I'm trying to parse PDFs that are locked in this way (to get the same privileges as you would get opening them in any reader).  Using an empty string as the user password doesn't work, but it seems (section 3.5.2 of the spec) that there has to be a user password to create the hash for the admin password.
What I would like is either an explanation of how to do this, or any code that I can read (ideally Python, C, or C++, but anything readable will do) that does this so that I can understand what I'm meant to be doing.  Standalone code, rather than reading through (e.g.) the gsview source, would be best.
",222
51412,51415,2,"Put the asterisks before the kwargs variable. This makes Python pass the variable (which is assumed to be a dictionary) as keyword arguments.
methodB(""argvalue"", **kwargs)

",35
51412,63943,2,"As an aside: When using functions instead of methods, you could also use functools.partial:
import functools

def foo(arg, **kwargs):
    ...

bar = functools.partial(foo, ""argvalue"")

The last line will define a function ""bar"" that, when called, will call foo with the first argument set to ""argvalue"" and all other functions just passed on:
bar(5, myarg=""value"")

will call
foo(""argvalue"", 5, myarg=""value"")

Unfortunately that will not work with methods.
",105
51412,51412,1,"Say I have the following methods:
def methodA(arg, **kwargs):
    pass

def methodB(arg, *args, **kwargs):
    pass

In methodA I wish to call methodB, passing on the kwargs. However, it seems that if I define methodA as follows, the second argument will be passed on as positional rather than named variable arguments.
def methodA(arg, **kwargs):
    methodB(""argvalue"", kwargs)

How do I make sure that the **kwargs in methodA gets passed as **kwargs to methodB?
",100
51412,51414,2,"Some experimentation and I figured this one out:
def methodA(arg, **kwargs):
    methodB(""argvalue"", **kwargs)
Seems obvious now...
",29
51502,51570,2,"Rather than focusing on making your existing structures more readable, you should focus on making more logical structures.  Make smaller blocks, try not to nest blocks excessively, make smaller functions, and try to think through your code flow more.
If you come to a point where you can't quickly determine the structure of your code, you should probably consider refactoring and adding some comments.  Code flow should always be immediately apparent -- the more you have to think about it, the less maintainable your code becomes.
",97
51502,3054853,2,"from __future__ import braces

Need I say more? :)
Seriously, PEP 8, 'Blank lines', §4 is the official way to do it.
",29
51502,52111,2,"I would look in to understanding more details about Python syntax. Often times if a piece of code looks odd, there usually is a better way to write it. For example, in the above example:
bar = foo if baz else None
while bar not biz:
    bar = i_am_going_to_find_you_biz_i_swear_on_my_life()

did_i_not_warn_you_biz()
my_father_is_avenged()

While it is a small change, it might help the readability. Also, in all honesty, I've never used a while loop, so there is a good change you would end up with a nice concise list comprehension or for loop instead. ;)
",113
51502,52090,2,"Part of learning a new programming language is learning to read code in that language.  A crutch like this may make it easier to read your own code, but it's going to impede the process of learning how to read anyone else's Python code.  I really think you'd be better off getting rid of the end of block comments and getting used to normal Python.
",72
51502,51551,2,"I like to put blank lines around blocks to make control flow more obvious. For example:
if foo:
   bar = baz

   while bar not biz:
      bar = i_am_going_to_find_you_biz_i_swear_on_my_life()

did_i_not_warn_you_biz()
my_father_is_avenged()

",40
51502,51531,2,"Perhaps the best thing would be to turn on ""show whitespace"" in your editor.  Then you would have a visual indication of how far in each line is tabbed (usually a bunch of dots), and it will be more apparent when that changes.
",50
51502,51505,2,"You could try increasing the indent size, but in general I would just say, relax, it will come with time. I don't think trying to make Python look like C is a very good idea.
",41
51502,51502,1,"I've been really enjoying Python programming lately. I come from a background of a strong love for C-based coding, where everything is perhaps more complicated than it should be (but puts hair on your chest, at least). So switching from C to Python for more complex things that don't require tons of speed has been more of a boon than a bane in writing projects.
However, coming from this land of brackets and parentheses and structs as far as the naked eye can see, I come across a small problem: I find Python difficult to read.
For example, the following block of text is hard for me to decipher unless I stare at it (which I dislike doing):
if foo:
   bar = baz
   while bar not biz:
      bar = i_am_going_to_find_you_biz_i_swear_on_my_life()

did_i_not_warn_you_biz()
my_father_is_avenged()

The problem occurs at the end of that if block: all the tabbing and then suddenly returning to a jarring block feels almost disturbing. As a solution, I've started coding my Python like this:
if foo:
   bar = baz
   while bar not biz:
      bar = i_am_going_to_find_you_biz_i_swear_on_my_life()
   #-- while --
#-- if --

did_i_not_warn_you_biz()
my_father_is_avenged()

And this, for some odd reason, makes me more able to read my own code. But I'm curious: has anyone else with my strange problem found easier ways to make their tabbed-out code more readable? I'd love to find out if there's a better way to do this before this becomes a huge habit for me.
",291
51520,51520,1,"Given a path such as ""mydir/myfile.txt"", how do I find the absolute filepath relative to the current working directory in Python? E.g. on Windows, I might end up with:
""C:/example/cwd/mydir/myfile.txt""

",41
51520,43691204,2,"filePath = os.path.abspath(directoryName)
filePathWithSlash = filePath + ""\\""
filenameWithPath = os.path.join(filePathWithSlash, filename)

",21
51520,51523,2,">>> import os
>>> os.path.abspath(""mydir/myfile.txt"")
'C:/example/cwd/mydir/myfile.txt'

Also works if it is already an absolute path:
>>> import os
>>> os.path.abspath(""C:/example/cwd/mydir/myfile.txt"")
'C:/example/cwd/mydir/myfile.txt'

",47
51520,51539,2,">>> import os
>>> os.path.abspath('mydir/myfile.txt')
'C:\\example\\cwd\\mydir\\myfile.txt'
>>> 

",19
51520,58417,2,"Better still, install the path.py module, it wraps all the os.path functions and other related functions into methods on an object that can be used wherever strings are used:
>>> from path import path
>>> path('mydir/myfile.txt').abspath()
'C:\\example\\cwd\\mydir\\myfile.txt'
>>>

",56
51520,15325066,2,"Today you can also use the unipath package which was based on path.py: http://sluggo.scrapping.cc/python/unipath/
>>> from unipath import Path
>>> absolute_path = Path('mydir/myfile.txt').absolute()
Path('C:\\example\\cwd\\mydir\\myfile.txt')
>>> str(absolute_path)
C:\\example\\cwd\\mydir\\myfile.txt
>>>

I would recommend using this package as it offers a clean interface to common os.path utilities.
",74
51520,26539947,2,"You could use the new Python 3.4 library pathlib. (You can also get it for Python 2.6 or 2.7 using pip install pathlib.) The authors wrote: ""The aim of this library is to provide a simple hierarchy of classes to handle filesystem paths and the common operations users do over them.""
To get an absolute path in Windows:
>>> from pathlib import Path
>>> p = Path(""pythonw.exe"").resolve()
>>> p
WindowsPath('C:/Python27/pythonw.exe')
>>> str(p)
'C:\\Python27\\pythonw.exe'

Or on UNIX:
>>> from pathlib import Path
>>> p = Path(""python3.4"").resolve()
>>> p
PosixPath('/opt/python3/bin/python3.4')
>>> str(p)
'/opt/python3/bin/python3.4'

Docs are here: https://docs.python.org/3/library/pathlib.html
",158
51520,38813098,2,"I prefer to use glob
here is how to list all file types in your current folder:
import glob
for x in glob.glob():
    print(x)

here is how to list all (for example) .txt files in your current folder:
import glob
for x in glob.glob('*.txt'):
    print(x)

here is how to list all file types in a chose directory:
import glob
for x in glob.glob('C:/example/hi/hello/'):
    print(x)

hope this helped you
",97
51553,51976,2,"You need to increase postgres' caches to the point where the whole working set fits into memory before you can expect to see perfomance comparable to doing it in-memory with a program.
",34
51553,51553,1,"I need a real DBA's opinion. Postgres 8.3 takes 200 ms to execute this query on my Macbook Pro while Java and Python perform the same calculation in under 20 ms (350,000 rows):
SELECT count(id), avg(a), avg(b), avg(c), avg(d) FROM tuples;

Is this normal behaviour when using a SQL database?
The schema (the table holds responses to a survey):
CREATE TABLE tuples (id integer primary key, a integer, b integer, c integer, d integer);

\copy tuples from '350,000 responses.csv' delimiter as ','

I wrote some tests in Java and Python for context and they crush SQL (except for pure python):
java   1.5 threads ~ 7 ms    
java   1.5         ~ 10 ms    
python 2.5 numpy   ~ 18 ms  
python 2.5         ~ 370 ms

Even sqlite3 is competitive with Postgres despite it assumping all columns are strings (for contrast: even using just switching to numeric columns instead of integers in Postgres results in 10x slowdown)
Tunings i've tried without success include (blindly following some web advice):
increased the shared memory available to Postgres to 256MB    
increased the working memory to 2MB
disabled connection and statement logging
used a stored procedure via CREATE FUNCTION ... LANGUAGE SQL

So my question is, is my experience here normal, and this is what I can expect when using a SQL database?  I can understand that ACID must come with costs, but this is kind of crazy in my opinion.  I'm not asking for realtime game speed, but since Java can process millions of doubles in under 20 ms, I feel a bit jealous. 
Is there a better way to do simple OLAP on the cheap (both in terms of money and server complexity)?  I've looked into Mondrian and Pig + Hadoop but not super excited about maintaining yet another server application and not sure if they would even help.

No the Python code and Java code do all the work in house so to speak.  I just generate 4 arrays with 350,000 random values each, then take the average.  I don't include the generation in the timings, only the averaging step.  The java threads timing uses 4 threads (one per array average), overkill but it's definitely the fastest.
The sqlite3 timing is driven by the Python program and is running from disk (not :memory:)
I realize Postgres is doing much more behind the scenes, but most of that work doesn't matter to me since this is read only data.
The Postgres query doesn't change timing on subsequent runs.
I've rerun the Python tests to include spooling it off the disk.  The timing slows down considerably to nearly 4 secs.  But I'm guessing that Python's file handling code is pretty much in C (though maybe not the csv lib?) so this indicates to me that Postgres isn't streaming from the disk either (or that you are correct and I should bow down before whoever wrote their storage layer!)
",576
51553,51668,2,"I don't think that your results are all that surprising -- if anything it is that Postgres is so fast.
Does the Postgres query run faster a second time once it has had a chance to cache the data?  To be a little fairer your test for Java and Python should cover the cost of acquiring the data in the first place (ideally loading it off disk).
If this performance level is a problem for your application in practice but you need a RDBMS for other reasons then you could look at memcached.  You would then have faster cached access to raw data and could do the calculations in code.
",118
51553,51933,2,"Those are very detailed answers, but they mostly beg the question, how do I get these benefits without leaving Postgres given that the data easily fits into memory, requires concurrent reads but no writes and is queried with the same query over and over again.
Is it possible to precompile the query and optimization plan? I would have thought the stored procedure would do this, but it doesn't really help.
To avoid disk access it's necessary to cache the whole table in memory, can I force Postgres to do that?  I think it's already doing this though, since the query executes in just 200 ms after repeated runs.
Can I tell Postgres that the table is read only, so it can optimize any locking code?
I think it's possible to estimate the query construction costs with an empty table (timings range from 20-60 ms) 
I still can't see why the Java/Python tests are invalid.  Postgres just isn't doing that much more work (though I still haven't addressed the concurrency aspect, just the caching and query construction)
UPDATE: 
I don't think it's fair to compare the SELECTS as suggested by pulling 350,000 through the driver and serialization steps into Python to run the aggregation, nor even to omit the aggregation as the overhead in formatting and displaying is hard to separate from the timing.  If both engines are operating on in memory data, it should be an apples to apples comparison, I'm not sure how to guarantee that's already happening though.
I can't figure out how to add comments, maybe i don't have enough reputation?
",304
51553,51745,2,"I would say your test scheme is not really useful. To fulfill the db query, the db server goes through several steps:

parse the SQL
work up a query plan, i. e. decide on which indices to use (if any), optimize etc.
if an index is used, search it for the pointers to the actual data, then go to the appropriate location in the data or
if no index is used, scan the whole table to determine which rows are needed
load the data from disk into a temporary location (hopefully, but not necessarily, memory)
perform the count() and avg() calculations

So, creating an array in Python and getting the average basically skips all these steps save the last one. As disk I/O is among the most expensive operations a program has to perform, this is a major flaw in the test (see also the answers to this question I asked here before). Even if you read the data from disk in your other test, the process is completely different and it's hard to tell how relevant the results are.
To obtain more information about where Postgres spends its time, I would suggest the following tests:

Compare the execution time of your query to a SELECT without the aggregating functions (i. e. cut step 5)
If you find that the aggregation leads to a significant slowdown, try if Python does it faster, obtaining the raw data through the plain SELECT from the comparison.

To speed up your query, reduce disk access first. I doubt very much that it's the aggregation that takes the time.
There's several ways to do that:

Cache data (in memory!) for subsequent access, either via the db engine's own capabilities or with tools like memcached
Reduce the size of your stored data
Optimize the use of indices. Sometimes this can mean to skip index use altogether (after all, it's disk access, too). For MySQL, I seem to remember that it's recommended to skip indices if you assume that the query fetches more than 10% of all the data in the table.
If your query makes good use of indices, I know that for MySQL databases it helps to put indices and data on separate physical disks. However, I don't know whether that's applicable for Postgres.
There also might be more sophisticated problems such as swapping rows to disk if for some reason the result set can't be completely processed in memory. But I would leave that kind of research until I run into serious performance problems that I can't find another way to fix, as it requires knowledge about a lot of little under-the-hood details in your process.

Update:
I just realized that you seem to have no use for indices for the above query and most likely aren't using any, too, so my advice on indices probably wasn't helpful. Sorry. Still, I'd say that the aggregation is not the problem but disk access is. I'll leave the index stuff in, anyway, it might still have some use.
",584
51553,53713,2,"Are you using TCP to access the Postgres? In that case Nagle is messing with your timing.
",19
51553,53333,2,"I retested with MySQL specifying ENGINE = MEMORY and it doesn't change a thing (still 200 ms).  Sqlite3 using an in-memory db gives similar timings as well (250 ms).
The math here looks correct (at least the size, as that's how big the sqlite db is :-)
I'm just not buying the disk-causes-slowness argument as there is every indication the tables are in memory (the postgres guys all warn against trying too hard to pin tables to memory as they swear the OS will do it better than the programmer)
To clarify the timings, the Java code is not reading from disk, making it a totally unfair comparison if Postgres is reading from the disk and calculating a complicated query, but that's really besides the point, the DB should be smart enough to bring a small table into memory and precompile a stored procedure IMHO.
UPDATE (in response to the first comment below):
I'm not sure how I'd test the query without using an aggregation function in a way that would be fair, since if i select all of the rows it'll spend tons of time serializing and formatting everything.  I'm not saying that the slowness is due to the aggregation function, it could still be just overhead from concurrency, integrity, and friends.  I just don't know how to isolate the aggregation as the sole independent variable.
",263
51553,53303,2,"I'm a MS-SQL guy myself, and we'd use DBCC PINTABLE to keep a table cached, and SET STATISTICS IO to see that it's reading from cache, and not disk. 
I can't find anything on Postgres to mimic PINTABLE, but pg_buffercache seems to give details on what is in the cache - you may want to check that, and see if your table is actually being cached.
A quick back of the envelope calculation makes me suspect that you're paging from disk. Assuming Postgres uses 4-byte integers, you have (6 * 4) bytes per row, so your table is a minimum of (24 * 350,000) bytes ~ 8.4MB. Assuming 40 MB/s sustained throughput on your HDD, you're looking at right around 200ms to read the data (which, as pointed out, should be where almost all of the time is being spent). 
Unless I screwed up my math somewhere, I don't see how it's possible that you are able to read 8MB into your Java app and process it in the times you're showing - unless that file is already cached by either the drive or your OS.
",218
51553,52179,2,"Thanks for the Oracle timings, that's the kind of stuff I'm looking for (disappointing though :-)
Materialized views are probably worth considering as I think I can precompute the most interesting forms of this query for most users.
I don't think query round trip time should be very high as i'm running the the queries on the same machine that runs Postgres, so it can't add much latency?
I've also done some checking into the cache sizes, and it seems Postgres relies on the OS to handle caching, they specifically mention BSD as the ideal OS for this, so I thinking Mac OS ought to be pretty smart about bringing the table into memory.  Unless someone has more specific params in mind I think more specific caching is out of my control.
In the end I can probably put up with 200 ms response times, but knowing that 7 ms is a possible target makes me feel unsatisfied, as even 20-50 ms times would enable more users to have more up to date queries and get rid of a lots of caching and precomputed hacks.
I just checked the timings using MySQL 5 and they are slightly worse than Postgres.  So barring some major caching breakthroughs, I guess this is what I can expect going the relational db route.
I wish I could up vote some of your answers, but I don't have enough points yet.
",263
51553,52006,2,"Postgres is doing a lot more than it looks like (maintaining data consistency for a start!)
If the values don't have to be 100% spot on, or if the table is updated rarely, but you are running this calculation often, you might want to look into Materialized Views to speed it up.
(Note, I have not used materialized views in Postgres, they look at little hacky, but might suite your situation).
Materialized Views
Also consider the overhead of actually connecting to the server and the round trip required to send the request to the server and back.
I'd consider 200ms for something like this to be pretty good, A quick test on my oracle server, the same table structure with about 500k rows and no indexes, takes about 1 - 1.5 seconds, which is almost all just oracle sucking the data off disk.
The real question is, is 200ms fast enough?
-------------- More --------------------
I was interested in solving this using materialized views, since I've never really played with them. This is in oracle.
First I created a MV which refreshes every minute.
create materialized view mv_so_x 
build immediate 
refresh complete 
START WITH SYSDATE NEXT SYSDATE + 1/24/60
 as select count(*),avg(a),avg(b),avg(c),avg(d) from so_x;

While its refreshing, there is no rows returned
SQL> select * from mv_so_x;

no rows selected

Elapsed: 00:00:00.00

Once it refreshes, its MUCH faster than doing the raw query
SQL> select count(*),avg(a),avg(b),avg(c),avg(d) from so_x;

  COUNT(*)     AVG(A)     AVG(B)     AVG(C)     AVG(D)
---------- ---------- ---------- ---------- ----------
   1899459 7495.38839 22.2905454 5.00276131 2.13432836

Elapsed: 00:00:05.74
SQL> select * from mv_so_x;

  COUNT(*)     AVG(A)     AVG(B)     AVG(C)     AVG(D)
---------- ---------- ---------- ---------- ----------
   1899459 7495.38839 22.2905454 5.00276131 2.13432836

Elapsed: 00:00:00.00
SQL> 

If we insert into the base table, the result is not immediately viewable view the MV.
SQL> insert into so_x values (1,2,3,4,5);

1 row created.

Elapsed: 00:00:00.00
SQL> commit;

Commit complete.

Elapsed: 00:00:00.00
SQL> select * from mv_so_x;

  COUNT(*)     AVG(A)     AVG(B)     AVG(C)     AVG(D)
---------- ---------- ---------- ---------- ----------
   1899459 7495.38839 22.2905454 5.00276131 2.13432836

Elapsed: 00:00:00.00
SQL> 

But wait a minute or so, and the MV will update behind the scenes, and the result is returned fast as you could want.
SQL> /

  COUNT(*)     AVG(A)     AVG(B)     AVG(C)     AVG(D)
---------- ---------- ---------- ---------- ----------
   1899460 7495.35823 22.2905352 5.00276078 2.17647059

Elapsed: 00:00:00.00
SQL> 

This isn't ideal. for a start, its not realtime, inserts/updates will not be immediately visible. Also, you've got a query running to update the MV whether you need it or not (this can be tune to whatever time frame, or on demand). But, this does show how much faster an MV can make it seem to the end user, if you can live with values which aren't quite upto the second accurate.
",729
51553,51817,2,"One other thing that an RDBMS generally does for you is to provide concurrency by protecting you from simultaneous access by another process.  This is done by placing locks, and there's some overhead from that.
If you're dealing with entirely static data that never changes, and especially if you're in a basically ""single user"" scenario, then using a relational database doesn't necessarily gain you much benefit.
",78
58294,8848856,2,"print (urllib.urlopen('http://automation.whatismyip.com/n09230945.asp').read())

",13
58294,2646313,2,"Using the address suggested in the source of http://whatismyip.com
import urllib
def get_my_ip_address():
    whatismyip = 'http://www.whatismyip.com/automation/n09230945.asp'
    return urllib.urlopen(whatismyip).readlines()[0]

",34
58294,256358,2,"import socket
s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
s.connect((""msn.com"",80))
s.getsockname()
",22
58294,58299,2,"The only way I can think of that's guaranteed to give it to you is to hit a service like http://whatismyip.com/ to get it.
",28
58294,58296,2,"This isn't possible without cooperation from an external server, because there could be any number of NATs between you and the other computer. If it's a custom protocol, you could ask the other system to report what address it's connected to.
",48
58294,58294,1,"When I call socket.getsockname() on a socket object, it returns a tuple of my machine's internal IP and the port. However, I would like to retrieve my external IP. What's the cheapest, most efficient manner of doing this?
",48
58294,8305740,2,"https://github.com/bobeirasa/mini-scripts/blob/master/externalip.py
'''
Finds your external IP address
'''

import urllib
import re

def get_ip():
    group = re.compile(u'(?P<ip>\d+\.\d+\.\d+\.\d+)').search(urllib.URLopener().open('http://jsonip.com/').read()).groupdict()
    return group['ip']

if __name__ == '__main__':
    print get_ip()

",72
58622,58622,1,"I like doxygen to create documentation of C or PHP code. I have an upcoming Python project and I think I remember that Python doesn't have /* .. */ comments and also has its own self-documentation facility which seems to be the pythonic way to document.
Can I just use doxygen? Anything particular to be aware of?
I have done some coding in Python but so far only on small projects where I was to lazy to document at all (yeah, I know ... but let's just pretend that's OK for now).
",103
58622,35377654,2,"In the end, you only have two options:
You generate your content using Doxygen, or you generate your content using Sphinx*.

Doxygen: It is not the tool of choice for most Python projects. But if you have to deal with other related projects written in C or C++ it could make sense. For this you can improve the integration between Doxygen and Python using doxypypy.
Sphinx: The defacto tool for documenting a Python project. You have three options here: manual, semi-automatic (stub generation) and fully automatic (Doxygen like). 

For manual API documentation you have Sphinx autodoc. This is great to write a user guide with embedded API generated elements.
For semi-automatic you have Sphinx autosummary. You can either setup your build system to call sphinx-autogen or setup your Sphinx with the autosummary_generate config. You will require to setup a page with the autosummaries, and then manually edit the pages. You have options, but my experience with this approach is that it requires way too much configuration, and at the end even after creating new templates, I found bugs and the impossibility to determine exactly what was exposed as public API and what not. My opinion is this tool is good for stub generation that will require manual editing, and nothing more. Is like a shortcut to end up in manual.
Fully automatic. This have been criticized many times and for long we didn't have a good fully automatic Python API generator integrated with Sphinx until AutoAPI came, which is a new kid in the block. This is by far the best for automatic API generation in Python (note: shameless self-promotion).


There are other options to note:

Breathe: this started as a very good idea, and makes sense when you work with several related project in other languages that use Doxygen. The idea is to use Doxygen XML output and feed it to Sphinx to generate your API. So, you can keep all the goodness of Doxygen and unify the documentation system in Sphinx. Awesome in theory. Now, in practice, the last time I checked the project wasn't ready for production.
pydoctor*: Very particular. Generates its own output. It has some basic integration with Sphinx, and some nice features.

",423
58622,58701,2,"This is documented on the doxygen website, but to summarize here:
You can use doxygen to document your Python code. You can either use the Python documentation string syntax:
""""""@package docstring
Documentation for this module.

More details.
""""""

def func():
    """"""Documentation for a function.

    More details.
    """"""
    pass

In which case the comments will be extracted by doxygen, but you won't be able to use any of the special doxygen commands.
Or you can (similar to C-style languages under doxygen) double up the comment marker (#) on the first line before the member:
## @package pyexample
#  Documentation for this module.
#
#  More details.

## Documentation for a function.
#
#  More details.
def func():
    pass

In that case, you can use the special doxygen commands. There's no particular Python output mode, but you can apparently improve the results by setting OPTMIZE_OUTPUT_JAVA to YES.
Honestly, I'm a little surprised at the difference - it seems like once doxygen can detect the comments in ## blocks or """""" blocks, most of the work would be done and you'd be able to use the special commands in either case. Maybe they expect people using """""" to adhere to more Pythonic documentation practices and that would interfere with the special doxygen commands?
",264
58622,59018,2,"An other very good documentation tool is sphinx. It will be used for the upcoming python 2.6 documentation and is used by django and a lot of other python projects.
From the sphinx website:

Output formats: HTML (including Windows HTML Help) and LaTeX, for printable PDF versions
Extensive cross-references: semantic markup and automatic links for functions, classes, glossary terms and similar pieces of information
Hierarchical structure: easy definition of a document tree, with automatic links to siblings, parents and children
Automatic indices: general index as well as a module index
Code handling: automatic highlighting using the Pygments highlighter
Extensions: automatic testing of code snippets, inclusion of docstrings from Python modules, and more

",130
58622,59955,2,"Sphinx is mainly a tool for formatting docs written independently from the source code, as I understand it.
For generating API docs from Python docstrings, the leading tools are pdoc and pydoctor. Here's pydoctor's generated API docs for Twisted and Bazaar.
Of course, if you just want to have a look at the docstrings while you're working on stuff, there's the ""pydoc"" command line tool and as well as the help() function available in the interactive interpreter.
",93
58622,497322,2,"The doxypy input filter allows you to use pretty much all of Doxygen's formatting tags in a standard Python docstring format.  I use it to document a large mixed C++ and Python game application framework, and it's working well.
",44
58711,335400,2,"Declarative is not necessarily more (or less) pythonic than functional IMHO. I think a layered approach would be the best (from buttom up):

A native layer that accepts and returns python data types.
A functional dynamic layer.
One or more declarative/object-oriented layers.

Similar to Elixir + SQLAlchemy.
",57
58711,58917,2,"You could actually pull this off, but it would require using metaclasses, which are deep magic (there be dragons). If you want an intro to metaclasses, there's a series of articles from IBM which manage to introduce the ideas without melting your brain.
The source code from an ORM like SQLObject might help, too, since it uses this same kind of declarative syntax.
",74
58711,58990,2,"Maybe not as slick as the Ruby version, but how about something like this:
from Boots import App, Para, Button, alert

def Shoeless(App):
    t = Para(text = 'Not Clicked')
    b = Button(label = 'The label')

    def on_b_clicked(self):
        alert('You clicked the button!')
        self.t.text = 'Clicked!'

Like Justin said, to implement this you would need to use a custom metaclass on class App, and a bunch of properties on Para and Button. This actually wouldn't be too hard.
The problem you run into next is: how do you keep track of the order that things appear in the class definition? In Python 2.x, there is no way to know if t should be above b or the other way around, since you receive the contents of the class definition as a python dict.
However, in Python 3.0 metaclasses are being changed in a couple of (minor) ways. One of them is the __prepare__ method, which allows you to supply your own custom dictionary-like object to be used instead -- this means you'll be able to track the order in which items are defined, and position them accordingly in the window.
",234
58711,60563,2,"This could be an oversimplification, i don't think it would be a good idea to try to make a general purpose ui library this way. On the other hand you could use this approach (metaclasses and friends) to simplify the definition of certain classes of user interfaces for an existing ui library and depending of the application that could actually save you a significant amount of time and code lines.
",76
58711,62780,2,"With some Metaclass magic to keep the ordering I have the following working. I'm not sure how pythonic it is but it is good fun for creating simple things. 
class w(Wndw):
  title='Hello World'
  class txt(Txt):  # either a new class
    text='Insert name here'
  lbl=Lbl(text='Hello') # or an instance
  class greet(Bbt):
    text='Greet'
    def click(self): #on_click method
      self.frame.lbl.text='Hello %s.'%self.frame.txt.text

app=w()

",89
58711,334828,2,"I have this same problem. I wan to to create a wrapper around any GUI toolkit for Python that is easy to use, and inspired by Shoes, but needs to be a OOP approach (against ruby blocks).
More information in: http://wiki.alcidesfonseca.com/blog/python-universal-gui-revisited
Anyone's welcome to join the project.
",58
58711,334938,2,"The only attempt to do this that I know of is Hans Nowak's Wax (which is unfortunately dead).
",22
58711,335077,2,"The closest you can get to rubyish blocks is the with statement from pep343: 
http://www.python.org/dev/peps/pep-0343/
",18
58711,58711,1,"I have been playing with the Ruby library ""shoes"". Basically you can write a GUI application in the following way:
Shoes.app do
  t = para ""Not clicked!""
  button ""The Label"" do
    alert ""You clicked the button!"" # when clicked, make an alert
    t.replace ""Clicked!"" # ..and replace the label's text
  end
end

This made me think - how would I design a similarly nice-to-use GUI framework in Python? One that doesn't have the usual tyings of basically being wrappers to a C* library (In the case of GTK, Tk, wx, QT etc etc)
Shoes takes things from web devlopment (like #f0c2f0 style colour notation, CSS layout techniques, like :margin => 10), and from ruby (extensively using blocks in sensible ways)
Python's lack of ""rubyish blocks"" makes a (metaphorically)-direct port impossible:
def Shoeless(Shoes.app):
    self.t = para(""Not clicked!"")

    def on_click_func(self):
        alert(""You clicked the button!"")
        self.t.replace(""clicked!"")

    b = button(""The label"", click=self.on_click_func)

No where near as clean, and wouldn't be nearly as flexible, and I'm not even sure if it would be implementable.
Using decorators seems like an interesting way to map blocks of code to a specific action:
class BaseControl:
    def __init__(self):
        self.func = None

    def clicked(self, func):
        self.func = func

    def __call__(self):
        if self.func is not None:
            self.func()

class Button(BaseControl):
    pass

class Label(BaseControl):
    pass

# The actual applications code (that the end-user would write)
class MyApp:
    ok = Button()
    la = Label()

    @ok.clicked
    def clickeryHappened():
        print ""OK Clicked!""

if __name__ == '__main__':
    a = MyApp()
    a.ok() # trigger the clicked action

Basically the decorator function stores the function, then when the action occurred (say, a click) the appropriate function would be executed.
The scope of various stuff (say, the la label in the above example) could be rather complicated, but it seems doable in a fairly neat manner..
",429
58711,335358,2,"If you really want to code UI, you could try to get something similar to django's ORM; sth like this to get a simple help browser:
class MyWindow(Window):
    class VBox:
        entry = Entry()
        bigtext = TextView()

        def on_entry_accepted(text):
            bigtext.value = eval(text).__doc__

The idea would be to interpret some containers (like windows) as simple classes, some containers (like tables, v/hboxes) recognized by object names, and simple widgets as objects.
I dont think one would have to name all containers inside a window, so some shortcuts (like old-style classes being recognized as widgets by names) would be desirable.
About the order of elements: in MyWindow above you don't have to track this (window is conceptually a one-slot container). In other containers you can try to keep track of the order assuming that each widget constructor have access to some global widget list. This is how it is done in django (AFAIK).
Few hacks here, few tweaks there... There are still few things to think of, but I believe it is possible... and usable, as long as you don't build complicated UIs.
However I am pretty happy with PyGTK+Glade. UI is just kind of data for me and it should be treated as data. There's just too much parameters to tweak (like spacing in different places) and it is better to manage that using a GUI tool. Therefore I build my UI in glade, save as xml and parse using gtk.glade.XML().
",297
58711,335132,2,"I was never satisfied with David Mertz's articles at IBM on metaclsses so I recently wrote my own metaclass article.  Enjoy.
",24
58711,336583,2,"If you use PyGTK with glade and this glade wrapper, then PyGTK actually becomes somewhat pythonic. A little at least.
Basically, you create the GUI layout in Glade. You also specify event callbacks in glade. Then you write a class for your window like this:
class MyWindow(GladeWrapper):
    GladeWrapper.__init__(self, ""my_glade_file.xml"", ""mainWindow"")
    self.GtkWindow.show()

    def button_click_event (self, *args):
        self.button1.set_label(""CLICKED"")

Here, I'm assuming that I have a GTK Button somewhere called button1 and that I specified button_click_event as the clicked callback. The glade wrapper takes a lot of effort out of event mapping.
If I were to design a Pythonic GUI library, I would support something similar, to aid rapid development. The only difference is that I would ensure that the widgets have a more pythonic interface too. The current PyGTK classes seem very C to me, except that I use foo.bar(...) instead of bar(foo, ...) though I'm not sure exactly what I'd do differently. Probably allow for a Django models style declarative means of specifying widgets and events in code and allowing you to access data though iterators (where it makes sense, eg widget lists perhaps), though I haven't really thought about it.
",246
58711,335443,2,"Personally, I would try to implement JQuery like API in a GUI framework.
class MyWindow(Window):
    contents = (
        para('Hello World!'),
        button('Click Me', id='ok'),
        para('Epilog'),
    )

    def __init__(self):
        self['#ok'].click(self.message)
        self['para'].hover(self.blend_in, self.blend_out)

    def message(self):
        print 'You clicked!'

    def blend_in(self, object):
        object.background = '#333333'

    def blend_out(self, object):
        object.background = 'WindowBackground'

",113
58711,335887,2,"This is extremely contrived and not pythonic at all, but here's my attempt at a semi-literal translation using the new ""with"" statement.
with Shoes():
  t = Para(""Not clicked!"")
  with Button(""The Label""):
    Alert(""You clicked the button!"")
    t.replace(""Clicked!"")

The hardest part is dealing with the fact that python will not give us anonymous functions with more than one statement in them.  To get around that, we could create a list of commands and run through those...
Anyway, here's the backend code I ran this with:
context = None

class Nestable(object):
  def __init__(self,caption=None):
    self.caption = caption
    self.things = []

    global context
    if context:
      context.add(self)

  def __enter__(self):
    global context
    self.parent = context
    context = self

  def __exit__(self, type, value, traceback):
    global context
    context = self.parent

  def add(self,thing):
    self.things.append(thing)
    print ""Adding a %s to %s"" % (thing,self)

  def __str__(self):
    return ""%s(%s)"" % (self.__class__.__name__, self.caption)


class Shoes(Nestable):
  pass

class Button(Nestable):
  pass

class Alert(Nestable):
  pass

class Para(Nestable):
  def replace(self,caption):
    Command(self,""replace"",caption)

class Command(Nestable):
  def __init__(self, target, command, caption):
    self.command = command
    self.target  = target
    Nestable.__init__(self,caption)

  def __str__(self):
    return ""Command(%s text of %s with \""%s\"")"" % (self.command, self.target, self.caption)

  def execute(self):
    self.target.caption = self.caption

",350
58711,336089,2,"Here's an approach that goes about GUI definitions a bit differently using class-based meta-programming rather than inheritance.
This is largley Django/SQLAlchemy inspired in that it is heavily based on meta-programming and separates your GUI code from your ""code code"".  I also think it should make heavy use of layout managers like Java does because when you're dropping code, no one wants to constantly tweak pixel alignment.  I also think it would be cool if we could have CSS-like properties.
Here is a rough brainstormed example that will show a column with a label on top, then a text box, then a button to click on the bottom which shows a message.

from happygui.controls import *

MAIN_WINDOW = Window(width=""500px"", height=""350px"",
    my_layout=ColumnLayout(padding=""10px"",
        my_label=Label(text=""What's your name kiddo?"", bold=True, align=""center""),
        my_edit=EditBox(placeholder=""""),
        my_btn=Button(text=""CLICK ME!"", on_click=Handler('module.file.btn_clicked')),
    ),
)
MAIN_WINDOW.show()

def btn_clicked(sender): # could easily be in a handlers.py file
    name = MAIN_WINDOW.my_layout.my_edit.text
    # same thing: name = sender.parent.my_edit.text
    # best practice, immune to structure change: MAIN_WINDOW.find('my_edit').text
    MessageBox(""Your name is '%s'"" % ()).show(modal=True)

One cool thing to notice is the way you can reference the input of my_edit by saying MAIN_WINDOW.my_layout.my_edit.text.  In the declaration for the window, I think it's important to be able to arbitrarily name controls in the function kwargs.
Here is the same app only using absolute positioning (the controls will appear in different places because we're not using a fancy layout manager):

from happygui.controls import *

MAIN_WINDOW = Window(width=""500px"", height=""350px"",
    my_label=Label(text=""What's your name kiddo?"", bold=True, align=""center"", x=""10px"", y=""10px"", width=""300px"", height=""100px""),
    my_edit=EditBox(placeholder="""", x=""10px"", y=""110px"", width=""300px"", height=""100px""),
    my_btn=Button(text=""CLICK ME!"", on_click=Handler('module.file.btn_clicked'), x=""10px"", y=""210px"", width=""300px"", height=""100px""),
)
MAIN_WINDOW.show()

def btn_clicked(sender): # could easily be in a handlers.py file
    name = MAIN_WINDOW.my_edit.text
    # same thing: name = sender.parent.my_edit.text
    # best practice, immune to structure change: MAIN_WINDOW.find('my_edit').text
    MessageBox(""Your name is '%s'"" % ()).show(modal=True)

I'm not entirely sure yet if this is a super great approach, but I definitely think it's on the right path.  I don't have time to explore this idea more, but if someone took this up as a project, I would love them.
",560
58711,336525,2,"## All you need is this class:

class MainWindow(Window):
    my_button = Button('Click Me')
    my_paragraph = Text('This is the text you wish to place')
    my_alert = AlertBox('What what what!!!')

    @my_button.clicked
    def my_button_clicked(self, button, event):
        self.my_paragraph.text.append('And now you clicked on it, the button that is.')

    @my_paragraph.text.changed
    def my_paragraph_text_changed(self, text, event):
        self.button.text = 'No more clicks!'

    @my_button.text.changed
    def my_button_text_changed(self, text, event):
        self.my_alert.show()


## The Style class is automatically gnerated by the framework
## but you can override it by defining it in the class:
##
##      class MainWindow(Window):
##          class Style:
##              my_blah = {'style-info': 'value'}
##
## or like you see below:

class Style:
    my_button = {
        'background-color': '#ccc',
        'font-size': '14px'}
    my_paragraph = {
        'background-color': '#fff',
        'color': '#000',
        'font-size': '14px',
        'border': '1px solid black',
        'border-radius': '3px'}

MainWindow.Style = Style

## The layout class is automatically generated
## by the framework but you can override it by defining it
## in the class, same as the Style class above, or by
## defining it like this:

class MainLayout(Layout):
    def __init__(self, style):
        # It takes the custom or automatically generated style class upon instantiation
        style.window.pack(HBox().pack(style.my_paragraph, style.my_button))

MainWindow.Layout = MainLayout

if __name__ == '__main__':
    run(App(main=MainWindow))

It would be relatively easy to do in python with a bit of that metaclass python magic know how. Which I have. And a knowledge of PyGTK. Which I also have. Gets ideas?
",369
61151,61151,1,"If you're writing a library, or an app, where do the unit test files go?  
It's nice to separate the test files from the main app code, but it's awkward to put them into a ""tests"" subdirectory inside of the app root directory, because it makes it harder to import the modules that you'll be testing.  
Is there a best practice here?
",75
61151,63645,2,"When writing a package called ""foo"", I will put unit tests into a separate package ""foo_test"". Modules and subpackages will then have the same name as the SUT package module. E.g. tests for a module foo.x.y are found in foo_test.x.y. The __init__.py files of each testing package then contain an AllTests suite that includes all test suites of the package. setuptools provides a convenient way to specify the main testing package, so that after ""python setup.py develop"" you can just use ""python setup.py test"" or ""python setup.py test -s foo_test.x.SomeTestSuite"" to the just a specific suite.
",114
61151,103610,2,"I also tend to put my unit tests in the file itself, as Jeremy Cantrell above notes, although I tend to not put the test function in the main body, but rather put everything in an
if __name__ == '__main__':
   do tests...

block.  This ends up adding documentation to the file as 'example code' for how to use the python file you are testing.
I should add, I tend to write very tight modules/classes.  If your modules require very large numbers of tests, you can put them in another, but even then, I'd still add:
if __name__ == '__main__':
   import tests.thisModule
   tests.thisModule.runtests

This lets anybody reading your source code know where to look for the test code.
",136
61151,62527,2,"For a file module.py, the unit test should normally be called test_module.py, following Pythonic naming conventions.
There are several commonly accepted places to put test_module.py:

In the same directory as module.py.
In ../tests/test_module.py (at the same level as the code directory).
In tests/test_module.py (one level under the code directory).

I prefer #1 for its simplicity of finding the tests and importing them. Whatever build system you're using can easily be configured to run files starting with test_. Actually, the default unittest pattern used for test discovery is test*.py.
",105
61151,61820,2,"In C#, I've generally separated the tests into a separate assembly.
In Python -- so far -- I've tended to either write doctests, where the test is in the docstring of a function, or put them in the if __name__ == ""__main__"" block at the bottom of the module.
",59
61151,61531,2,"I use a tests/ directory, and then import the main application modules using relative imports. So in MyApp/tests/foo.py, there might be:
from .. import foo

to import the MyApp.foo module.
",35
61151,61518,2,"I've recently started to program in Python, so I've not really had chance to find out best practice yet.
But, I've written a module that goes and finds all the tests and runs them.
So, I have:

app/
 appfile.py
test/
 appfileTest.py

I'll have to see how it goes as I progress to larger projects.
",65
61151,61169,2,"A common practice is to put the tests directory in the same parent directory as your module/package. So if your module was called foo.py your directory layout would look like:
parent_dir/
  foo.py
  tests/

Of course there is no one way of doing it. You could also make a tests subdirectory and import the module using absolute import.
Wherever you put your tests, I would recommend you use nose to run them. Nose searches through your directories for tests. This way, you can put tests wherever they make the most sense organizationally.
",100
61151,61168,2,"I don't believe there is an established ""best practice"".
I put my tests in another directory outside of the app code. I then add the main app directory to sys.path (allowing you to import the modules from anywhere) in my test runner script (which does some other stuff as well) before running all the tests. This way I never have to remove the tests directory from the main code when I release it, saving me time and effort, if an ever so tiny amount.
",97
61151,128616,2,"We use 
app/src/code.py
app/testing/code_test.py 
app/docs/..
In each test file we insert ""../src/"" in sys.path. It's not the nicest solution but works. I think it would be great if someone came up w/ something like maven in java that gives you standard conventions that just work, no matter what project you work on.
",59
61151,39740835,2,"Every once in a while I find myself checking out the topic of test placement, and every time the majority recommends a separate folder structure beside the library code, but I find that every time the arguments are the same and are not that convincing. I end up putting my test modules somewhere beside the core modules. 
The main reason for doing this is: refactoring.
When I move things around I do want test modules to move with the code; it's easy to lose tests if they are in a separate tree. Let's be honest, sooner or later you end up with a totally different folder structure, like django, flask and many others. Which is fine if you don't care.
The main question you should ask yourself is this:  
Am I writing:  

a) reusable library or  
b) building a project than bundles together some semi-separated modules?

If a: 
A separate folder and the extra effort to maintain its structure may be better suited. No one will complain about your tests getting deployed to production. 
But it's also just as easy to exclude tests from being distributed when they are mixed with the core folders; put this in the setup.py:
find_packages(""src"", exclude=[""*.tests"", ""*.tests.*"", ""tests.*"", ""tests""]) 

If b: 
You may wish — as every one of us do — that you are writing reusable libraries, but most of the time their life is tied to the life of the project. Ability to easily maintain your project should be a priority. 
Then if you did a good job and your module is a good fit for another project, it will probably get copied — not forked or made into a separate library — into this new project, and moving tests that lay beside it in the same folder structure is easy in comparison to fishing up tests in a mess that a separate test folder had become. (You may argue that it shouldn't be a mess in the first place but let's be realistic here).
So the choice is still yours, but I would argue that with mixed up tests you achieve all the same things as with a separate folder, but with less effort on keeping things tidy.    
",432
61151,37122327,2,"I recommend you check some main Python projects on GitHub and get some ideas.
When your code gets larger and you add more libraries it's better to create a test folder in the same directory you have setup.py and mirror your project directory structure for each test type (unittest, integration, ...)
For example if you have a directory structure like:
myPackage/
    myapp/
       moduleA/
          __init__.py
          module_A.py
       moduleB/
          __init__.py
          module_B.py
setup.py

After adding test folder you will have a directory structure like:
myPackage/
    myapp/
       moduleA/
          __init__.py
          module_A.py
       moduleB/
          __init__.py
          module_B.py
test/
   unit/
      myapp/
         moduleA/
            module_A_test.py
         moduleB/
            module_B_test.py
   integration/
          myapp/
             moduleA/
                module_A_test.py
             moduleB/
                module_B_test.py
setup.py

Many properly written Python packages uses the same structure. A very good example is the Boto package.
Check https://github.com/boto/boto
",133
61151,23386287,2,"Only 1 test file
If doesn't have many test files, putting it in a top-level directory is nice (I think this is a pythonic (recommended) way):
module/
  lib/
    __init__.py
    module.py
  test.py

Many test files
If has many test files, put it in a tests folder:
module/
  lib/
    __init__.py
    module.py
  tests/
    test_module.py
    test_module2.py

but if you put the tests in tests folder, test can't import ..lib in CLI because __main__  can't import relative modules, so instead we can use nose, or we can add a parent directory to the python import path, and for that I will create a
env.py
import sys
import os

# append module root directory to sys.path
sys.path.append(
    os.path.dirname(
        os.path.dirname(
            os.path.abspath(__file__)
        )
    )
)

in
module/
  tests/
    test_module.py
    env.py

and import env before test import module
test_module.py
import unittest
# append parent directory to import path
import env
# now we can import the lib module
from lib import module

if __name__ == '__main__':
    unittest.main()

",183
61151,22704148,2,"From my experience in developing Testing frameworks in Python, I would suggest to put python unit tests in a separate directory. Maintain a symmetric directory structure. This would be helpful in packaging just the core libraries and not package the unit tests. Below is implemented through a schematic diagram. 
                              <Main Package>
                               /          \
                              /            \
                            lib           tests
                            /                \
             [module1.py, module2.py,  [ut_module1.py, ut_module2.py,
              module3.py  module4.py,   ut_module3.py, ut_module.py]
              __init__.py]

In this way when you package these libraries using an rpm, you can just package the main library modules (only). This helps maintainability particularly in agile environment. 
",117
61151,2363162,2,"How I do it...
Folder structure:
project/
    src/
        code.py
    tests/
    setup.py

Setup.py points to src/ as the location containing my projects modules, then i run:
setup.py develop

Which adds my project into site-packages, pointing to my working copy. To run my tests i use:
setup.py tests

Using whichever test runner I've configured.
",61
61151,815212,2,"We had the very same question when writing Pythoscope (http://pythoscope.org), which generates unit tests for Python programs.  We polled people on the testing in python list before we chose a directory, there were many different opinions.  In the end we chose to put a ""tests"" directory in the same directory as the source code. In that directory we generate a test file for each module in the parent directory.  
",81
61151,382596,2,"I prefer toplevel tests directory. This does mean imports become a little more difficult. For that I have two solutions:

Use setuptools. Then you can pass test_suite='tests.runalltests.suite' into setup(), and can run the tests simply: python setup.py test
Set PYTHONPATH when running the tests: PYTHONPATH=. python tests/runalltests.py

Here's how that stuff is supported by code in M2Crypto:

http://svn.osafoundation.org/m2crypto/trunk/setup.py
http://svn.osafoundation.org/m2crypto/trunk/tests/alltests.py

If you prefer to run tests with nosetests you might need do something a little different.
",93
61151,77145,2,"If the tests are simple, simply put them in the docstring -- most of the test frameworks for Python will be able to use that:
>>> import module
>>> module.method('test')
'testresult'

For other more involved tests, I'd put them either in ../tests/test_module.py or in tests/test_module.py.
",58
67631,43602557,2,"I have come up with a slightly modified version of @SebastianRittau's wonderful answer (for Python > 3.4 I think), which will allow you to load a file with any extension as a module using spec_from_loader instead of spec_from_file_location:
from importlib.util import spec_from_loader
from importlib.machinery import SourceFileLoader 

spec = spec_from_loader(""module.name"", SourceFileLoader(""module.name"", ""/path/to/file.py""))
mod = module_from_spec(spec)
spec.loader.exec_module(mod)

The advantage of encoding the path in an explicit SourceFileLoader is that the machinery will not try to figure out the type of the file from the extension. This means that you can load something like a .txt file using this method, but you could not do it with spec_from_file_location without specifying the loader because .txt is not in importlib.machinery.SOURCE_SUFFIXES.
",144
67631,37611448,2,"Here is some code that works in all Python versions, from 2.7-3.5 and probably even others.
config_file = ""/tmp/config.py""
with open(config_file) as f:
    code = compile(f.read(), config_file, 'exec')
    exec(code, globals(), locals())

I tested it. It may be ugly but so far is the only one that works in all versions.
",77
67631,27127448,2,"The best way, I think, is from the official documentation (29.1. imp — Access the import internals):
import imp
import sys

def __import__(name, globals=None, locals=None, fromlist=None):
    # Fast path: see if the module has already been imported.
    try:
        return sys.modules[name]
    except KeyError:
        pass

    # If any of the following calls raises an exception,
    # there's a problem we can't handle -- let the caller handle it.

    fp, pathname, description = imp.find_module(name)

    try:
        return imp.load_module(name, fp, pathname, description)
    finally:
        # Since we may exit via an exception, close fp explicitly.
        if fp:
            fp.close()

",133
67631,34570493,2,"It may be obvious but in interactive shell:
cd path
import module

",13
67631,32905959,2,"To import a module from a given filename, you can temporarily extend the path, and restore the system path in the finally block reference:
filename = ""directory/module.py""

directory, module_name = os.path.split(filename)
module_name = os.path.splitext(module_name)[0]

path = list(sys.path)
sys.path.insert(0, directory)
try:
    module = __import__(module_name)
finally:
    sys.path[:] = path # restore

",79
67631,29589414,2,"This area of Python 3.4 seems to be extremely tortuous to understand! However with a bit of hacking using the code from Chris Calloway as a start I managed to get something working. Here's the basic function.
def import_module_from_file(full_path_to_module):
    """"""
    Import a module given the full path/filename of the .py file

    Python 3.4

    """"""

    module = None

    try:

        # Get module name and path from full path
        module_dir, module_file = os.path.split(full_path_to_module)
        module_name, module_ext = os.path.splitext(module_file)

        # Get module ""spec"" from filename
        spec = importlib.util.spec_from_file_location(module_name,full_path_to_module)

        module = spec.loader.load_module()

    except Exception as ec:
        # Simple error printing
        # Insert ""sophisticated"" stuff here
        print(ec)

    finally:
        return module

This appears to use non-deprecated modules from Python 3.4. I don't pretend to understand why, but it seems to work from within a program. I found Chris' solution worked on the command line but not from inside a program.
",186
67631,26995106,2,"In Linux, adding a symbolic link in the directory your python script is located works.
ie: 
ln -s /absolute/path/to/module/module.py /absolute/path/to/script/module.py
python will create /absolute/path/to/script/module.pyc and will update it if you change the contents of /absolute/path/to/module/module.py
then include the following in mypythonscript.py
from module import *
",48
67631,25827116,2,"You can use the pkgutil module (specifically the walk_packages method) to get a list of the packages in the current directory. From there it's trivial to use the importlib machinery to import the modules you want:
import pkgutil
import importlib

packages = pkgutil.walk_packages(path='.')
for importer, name, is_package in packages:
    mod = importlib.import_module(name)
    # do whatever you want with module now, it's been imported!

",83
67631,8721254,2,"This should work
path = os.path.join('./path/to/folder/with/py/files', '*.py')
for infile in glob.glob(path):
    basename = os.path.basename(infile)
    basename_without_extension = basename[:-3]

    # http://docs.python.org/library/imp.html?highlight=imp#module-imp
    imp.load_source(basename_without_extension, infile)

",48
67631,6284270,2,"I made a package that uses imp for you. I call it import_file and this is how it's used:
>>>from import_file import import_file
>>>mylib = import_file('c:\\mylib.py')
>>>another = import_file('relative_subdir/another.py')

You can get it at:
http://pypi.python.org/pypi/import_file
or at
http://code.google.com/p/import-file/
",65
67631,129374,2,"The advantage of adding a path to sys.path (over using imp) is that it simplifies things when importing more than one module from a single package.  For example:
import sys
# the mock-0.3.1 dir contains testcase.py, testutils.py & mock.py
sys.path.append('/foo/bar/mock-0.3.1')

from testcase import TestCase
from testutils import RunTests
from mock import Mock, sentinel, patch

",65
67631,68628,2,"def import_file(full_path_to_module):
    try:
        import os
        module_dir, module_file = os.path.split(full_path_to_module)
        module_name, module_ext = os.path.splitext(module_file)
        save_cwd = os.getcwd()
        os.chdir(module_dir)
        module_obj = __import__(module_name)
        module_obj.__file__ = full_path_to_module
        globals()[module_name] = module_obj
        os.chdir(save_cwd)
    except:
        raise ImportError

import_file('/home/somebody/somemodule.py')

",65
67631,67708,2,"You can also do something like this and add the directory that the configuration file is sitting in to the Python load path, and then just do a normal import, assuming you know the name of the file in advance, in this case ""config"".
Messy, but it works.
configfile = '~/config.py'

import os
import sys

sys.path.append(os.path.dirname(os.path.expanduser(configfile)))

import config

",75
67631,67705,2,"Import package modules at runtime (Python recipe) 
http://code.activestate.com/recipes/223972/
###################
##                #
## classloader.py #
##                #
###################

import sys, types

def _get_mod(modulePath):
    try:
        aMod = sys.modules[modulePath]
        if not isinstance(aMod, types.ModuleType):
            raise KeyError
    except KeyError:
        # The last [''] is very important!
        aMod = __import__(modulePath, globals(), locals(), [''])
        sys.modules[modulePath] = aMod
    return aMod

def _get_func(fullFuncName):
    """"""Retrieve a function object from a full dotted-package name.""""""

    # Parse out the path, module, and function
    lastDot = fullFuncName.rfind(u""."")
    funcName = fullFuncName[lastDot + 1:]
    modPath = fullFuncName[:lastDot]

    aMod = _get_mod(modPath)
    aFunc = getattr(aMod, funcName)

    # Assert that the function is a *callable* attribute.
    assert callable(aFunc), u""%s is not callable."" % fullFuncName

    # Return a reference to the function itself,
    # not the results of the function.
    return aFunc

def _get_class(fullClassName, parentClass=None):
    """"""Load a module and retrieve a class (NOT an instance).

    If the parentClass is supplied, className must be of parentClass
    or a subclass of parentClass (or None is returned).
    """"""
    aClass = _get_func(fullClassName)

    # Assert that the class is a subclass of parentClass.
    if parentClass is not None:
        if not issubclass(aClass, parentClass):
            raise TypeError(u""%s is not a subclass of %s"" %
                            (fullClassName, parentClass))

    # Return a reference to the class itself, not an instantiated object.
    return aClass


######################
##       Usage      ##
######################

class StorageManager: pass
class StorageManagerMySQL(StorageManager): pass

def storage_object(aFullClassName, allOptions={}):
    aStoreClass = _get_class(aFullClassName, StorageManager)
    return aStoreClass(allOptions)

",448
67631,67693,2,"You can use the 
load_source(module_name, path_to_file) 

method from imp module.
",15
67631,67692,2,"For Python 3.5+ use:
import importlib.util
spec = importlib.util.spec_from_file_location(""module.name"", ""/path/to/file.py"")
foo = importlib.util.module_from_spec(spec)
spec.loader.exec_module(foo)
foo.MyClass()

For Python 3.3 and 3.4 use:
from importlib.machinery import SourceFileLoader

foo = SourceFileLoader(""module.name"", ""/path/to/file.py"").load_module()
foo.MyClass()

(Although this has been deprecated in Python 3.4.)
Python 2 use:
import imp

foo = imp.load_source('module.name', '/path/to/file.py')
foo.MyClass()

There are equivalent convenience functions for compiled Python files and DLLs.
See also. http://bugs.python.org/issue21436.
",110
67631,67672,2,"I believe you can use imp.find_module() and imp.load_module() to load the specified module.  You'll need to split the module name off of the path, i.e. if you wanted to load /home/mypath/mymodule.py you'd need to do:
imp.find_module('mymodule', '/home/mypath/')

...but that should get the job done.
",62
67631,37339817,2,"It sounds like you don't want to specifically import the configuration file (which has a whole lot of side effects and additional complications involved), you just want to run it, and be able to access the resulting namespace. The standard library provides an API specifically for that in the form of runpy.run_path:
from runpy import run_path
settings = run_path(""/path/to/file.py"")

That interface is available in Python 2.7 and Python 3.2+
",81
67631,67715,2,"Do you mean load or import?
You can manipulate the sys.path list specify the path to your module, then import your module. For example, given a module at:
/foo/bar.py

You could do:
import sys
sys.path[0:0] = '/foo' # puts the /foo directory at the start of your path
import bar

",60
67631,30605451,2,"I'm not saying that it is better, but for the sake of completeness, I wanted to suggest the exec function, available in both python 2 and 3.
exec allows you to execute arbitrary code in either the global scope, or in an internal scope, provided as a dictionary.
For example, if you have a module stored in ""/path/to/module"" with the function foo(), you could run it by doing the following:
module = dict()
with open(""/path/to/module"") as f:
    exec(f.read(), module)
module['foo']()

This makes it a bit more explicit that you're loading code dynamically, and grants you some additional power, such as the ability to provide custom builtins. 
And if having access through attributes, instead of keys is important to you, you can design a custom dict class for the globals, that provides such access, e.g.:
class MyModuleClass(dict):
    def __getattr__(self, name):
        return self.__getitem__(name)

",197
67631,67631,1,"How can I load a Python module given its full path? Note that the file can be anywhere in the filesystem, as it is a configuration option.
",30
68477,37142773,2,"I am trying to test django rest api and its working for me:
def test_upload_file(self):
        filename = ""/Users/Ranvijay/tests/test_price_matrix.csv""
        data = {'file': open(filename, 'rb')}
        client = APIClient()
        # client.credentials(HTTP_AUTHORIZATION='Token ' + token.key)
        response = client.post(reverse('price-matrix-csv'), data, format='multipart')

        print response
        self.assertEqual(response.status_code, status.HTTP_200_OK)

",75
68477,68477,1,"Is there a way to send a file using POST from a Python script?
",15
68477,75158,2,"You may also want to have a look at httplib2, with examples. I find using httplib2 is more concise than using the built-in HTTP modules.
",28
68477,525193,2,"Chris Atlee's poster library works really well for this (particularly the convenience function poster.encode.multipart_encode()). As a bonus, it supports streaming of large files without loading an entire file into memory. See also Python issue 3244.
",44
68477,7969778,2,"The only thing that stops you from using urlopen directly on a file object is the fact that the builtin file object lacks a len definition. A simple way is to create a subclass, which provides urlopen with the correct file. 
I have also modified the Content-Type header in the file below.
import os
import urllib2
class EnhancedFile(file):
    def __init__(self, *args, **keyws):
        file.__init__(self, *args, **keyws)

    def __len__(self):
        return int(os.fstat(self.fileno())[6])

theFile = EnhancedFile('a.xml', 'r')
theUrl = ""http://example.com/abcde""
theHeaders= {'Content-Type': 'text/xml'}

theRequest = urllib2.Request(theUrl, theFile, theHeaders)

response = urllib2.urlopen(theRequest)

theFile.close()


for line in response:
    print line

",154
68477,10234640,2,"From http://docs.python-requests.org/en/latest/user/quickstart/#post-a-multipart-encoded-file
Requests makes it very simple to upload Multipart-encoded files:
>>> with open('report.xls', 'rb') as f: r = requests.post('http://httpbin.org/post', files={'report.xls': f})

That's it. I'm not joking - this is one line of code. File was sent. Let's check:
>>> r.text
{
  ""origin"": ""179.13.100.4"",
  ""files"": {
    ""report.xls"": ""<censored...binary...data>""
  },
  ""form"": {},
  ""url"": ""http://httpbin.org/post"",
  ""args"": {},
  ""headers"": {
    ""Content-Length"": ""3196"",
    ""Accept-Encoding"": ""identity, deflate, compress, gzip"",
    ""Accept"": ""*/*"",
    ""User-Agent"": ""python-requests/0.8.0"",
    ""Host"": ""httpbin.org:80"",
    ""Content-Type"": ""multipart/form-data; boundary=127.0.0.1.502.21746.1321131593.786.1""
  },
  ""data"": """"
}

",198
68477,31305207,2,"Looks like python requests does not handle extremely large multi-part files.
The documentation recommends you look into requests-toolbelt.
Here's the pertinent page from their documentation.
",29
68477,36078069,2,"def visit_v2(device_code, camera_code):
    image1 = MultipartParam.from_file(""files"", ""/home/yuzx/1.txt"")
    image2 = MultipartParam.from_file(""files"", ""/home/yuzx/2.txt"")
    datagen, headers = multipart_encode([('device_code', device_code), ('position', 3), ('person_data', person_data), image1, image2])
    print """".join(datagen)
    if server_port == 80:
        port_str = """"
    else:
        port_str = "":%s"" % (server_port,)
    url_str = ""http://"" + server_ip + port_str + ""/adopen/device/visit_v2""
    headers['nothing'] = 'nothing'
    request = urllib2.Request(url_str, datagen, headers)
    try:
        response = urllib2.urlopen(request)
        resp = response.read()
        print ""http_status ="", response.code
        result = json.loads(resp)
        print resp
        return result
    except urllib2.HTTPError, e:
        print ""http_status ="", e.code
        print e.read()

",173
68477,68502,2,"Yes. You'd use the urllib2 module, and encode using the multipart/form-data content type. Here is some sample code to get you started -- it's a bit more than just file uploading, but you should be able to read through it and see how it works:
user_agent = ""image uploader""
default_message = ""Image $current of $total""

import logging
import os
from os.path import abspath, isabs, isdir, isfile, join
import random
import string
import sys
import mimetypes
import urllib2
import httplib
import time
import re

def random_string (length):
    return ''.join (random.choice (string.letters) for ii in range (length + 1))

def encode_multipart_data (data, files):
    boundary = random_string (30)

    def get_content_type (filename):
        return mimetypes.guess_type (filename)[0] or 'application/octet-stream'

    def encode_field (field_name):
        return ('--' + boundary,
                'Content-Disposition: form-data; name=""%s""' % field_name,
                '', str (data [field_name]))

    def encode_file (field_name):
        filename = files [field_name]
        return ('--' + boundary,
                'Content-Disposition: form-data; name=""%s""; filename=""%s""' % (field_name, filename),
                'Content-Type: %s' % get_content_type(filename),
                '', open (filename, 'rb').read ())

    lines = []
    for name in data:
        lines.extend (encode_field (name))
    for name in files:
        lines.extend (encode_file (name))
    lines.extend (('--%s--' % boundary, ''))
    body = '\r\n'.join (lines)

    headers = {'content-type': 'multipart/form-data; boundary=' + boundary,
               'content-length': str (len (body))}

    return body, headers

def send_post (url, data, files):
    req = urllib2.Request (url)
    connection = httplib.HTTPConnection (req.get_host ())
    connection.request ('POST', req.get_selector (),
                        *encode_multipart_data (data, files))
    response = connection.getresponse ()
    logging.debug ('response = %s', response.read ())
    logging.debug ('Code: %s %s', response.status, response.reason)

def make_upload_file (server, thread, delay = 15, message = None,
                      username = None, email = None, password = None):

    delay = max (int (delay or '0'), 15)

    def upload_file (path, current, total):
        assert isabs (path)
        assert isfile (path)

        logging.debug ('Uploading %r to %r', path, server)
        message_template = string.Template (message or default_message)

        data = {'MAX_FILE_SIZE': '3145728',
                'sub': '',
                'mode': 'regist',
                'com': message_template.safe_substitute (current = current, total = total),
                'resto': thread,
                'name': username or '',
                'email': email or '',
                'pwd': password or random_string (20),}
        files = {'upfile': path}

        send_post (server, data, files)

        logging.info ('Uploaded %r', path)
        rand_delay = random.randint (delay, delay + 5)
        logging.debug ('Sleeping for %.2f seconds------------------------------\n\n', rand_delay)
        time.sleep (rand_delay)

    return upload_file

def upload_directory (path, upload_file):
    assert isabs (path)
    assert isdir (path)

    matching_filenames = []
    file_matcher = re.compile (r'\.(?:jpe?g|gif|png)$', re.IGNORECASE)

    for dirpath, dirnames, filenames in os.walk (path):
        for name in filenames:
            file_path = join (dirpath, name)
            logging.debug ('Testing file_path %r', file_path)
            if file_matcher.search (file_path):
                matching_filenames.append (file_path)
            else:
                logging.info ('Ignoring non-image file %r', path)

    total_count = len (matching_filenames)
    for index, file_path in enumerate (matching_filenames):
        upload_file (file_path, index + 1, total_count)

def run_upload (options, paths):
    upload_file = make_upload_file (**options)

    for arg in paths:
        path = abspath (arg)
        if isdir (path):
            upload_directory (path, upload_file)
        elif isfile (path):
            upload_file (path)
        else:
            logging.error ('No such path: %r' % path)

    logging.info ('Done!')

",811
942148,945160,2,"Just wanted to post the solution I came up with.  The problem was in this line:
{% url django.contrib.auth.views.password_reset_confirm uidb36=uid, token=token %}

I'm not really a 100% why either, so I just hard coded the url like this:
http://mysite.com/accounts/reset/{{uid}}-{{token}}/

",62
942148,944440,2,"Edit: I used your example, and had to change to not use keyword parameters.
{% url django.contrib.auth.views.password_reset_confirm uid, token %}

Named parameters do work, as long as both uid and token are defined.  If either are not defined or blank I get the same error you do:
{% url django.contrib.auth.views.password_reset_confirm uidb36=uid, token=token %}

",65
942148,942169,2,"This is a problem I figured out myself not 10 minutes ago. The solution is to add the post_change_redirect value to the dictionary of arguments you are passing to the password_reset view.
So this is what mine now look like:
(r'^/password/$', password_change, {'template_name': 'testing/password.html', 'post_change_redirect': '/account/'})

I hope that does it for you! I agree that the documentation for this particular feature is lacking somewhat, but this solved the exact same issue for my project.
Edit: I really should have scrolled across - you've included that already. Apologies for that, but I hope you get it sorted :)
",124
942148,942148,1,"I'm trying to use the password reset setup that comes with Django, but the documentation is not very good for it.  I'm using Django 1.0 and I keep getting this error:
Caught an exception while rendering: Reverse for 'mysite.django.contrib.auth.views.password_reset_confirm' with arguments '()' and keyword arguments ...

in my urlconf I have something like this:
#django.contrib.auth.views
urlpatterns = patterns('django.contrib.auth.views',    
    (r'^password_reset/$', 'password_reset', {'template_name': 'accounts/registration/password_reset_form.html', 'email_template_name':'accounts/registration/password_reset_email.html', 'post_reset_redirect':'accounts/login/'}),
    (r'^password_reset/done/$', 'password_reset_done', {'template_name': 'accounts/registration/password_reset_done.html'}),
    (r'^reset/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$', 'password_reset_confirm', {'template_name': 'accounts/registration/password_reset_confirm.html', 'post_reset_redirect':'accounts/login/', 'post_reset_redirect':'accounts/reset/done/'}),
    (r'^reset/done/$', 'password_reset_complete', {'template_name': 'accounts/registration/password_reset_complete.html'}),
)

The problem seems to be in this file: 
password_reset_email.html 

on line 7
{% url django.contrib.auth.views.password_reset_confirm uidb36=uid, token=token %}

I'm at loss as to what's going on, so any help would be appreciated.
Thanks
",229
942148,6640223,2,"I've struggled with this for over an hour trying everything on this page and every other page on the internet. Finally to solve the problem in my case i had to delete 
{% load url from future %}

from the top of my password_reset_email.html template.
Also note, ""uidb36=uid"" in the url script. Here's my full password_reset_email.html template, I hope it saves someone some time:
{% autoescape off %}
    You're receiving this e-mail because you requested a password reset for your user account at {{ site_name }}.


Please go to the following page and choose a new password:
{% block reset_link %}
{{ protocol }}://{{ domain }}{% url django.contrib.auth.views.password_reset_confirm uidb36=uid token=token %}
{% endblock %}

Your username, in case you've forgotten:"" %} {{ user.username }}

Thanks for using our site!

The {{ site_name }} team

{% endautoescape %}

",182
7825361,7825523,2,"for el in tree.getiterator(tag='page'):
    page_id = el.get('id', None) # returns second arg if id not exists
    if page_id:
        print page_id, el.find('title').text
    else:
        pprint(el.attrib)

Edit:  Updated for commment: ""Thanks can i print page_id and title at same time? Means 31239628 - Title""
",67
7825361,7825769,2,"The element.get() method is used to retrieve option attribute values in a tag:
>>> page_id = tree.find('page').get('id')
>>> if page_id:
        print page_id

31239628

",40
7825361,7825361,1,"I'm reading XML file using Etree module. Im using following code to print the value of <page> and <title> tags. My code working fine. But I want little change. If the <page id='...'>  attribute id is exists then print the value of tag. Is it possible? thanks 
import xml.etree.cElementTree as etree
from pprint import pprint
tree = etree.parse('find_title.xml')
for value in tree.getiterator(tag='title'):
    print value.text
for value in tree.getiterator(tag='page'):
    pprint(value.attrib)

Here is my xml File.
 <mediawiki>
      <siteinfo>
        <sitename>Wiki</sitename>
        <namespaces>
          <namespace key=""-2"" case=""first-letter"">Media</namespace>
        </namespaces>
      </siteinfo>
    <page id=""31239628"" orglength=""6822"" newlength=""4524"" stub=""0"" categories=""0"" outlinks=""1"" urls=""10"">
    <title>Title</title>
    <categories></categories>
    <links>15099779</links>
    <urls>
    </urls>
    <text>

    Books

    </text>
    </page>

    </mediawiki>

",215
7825395,7825517,2,"Yes, it's possible. The ""how"" part depends on the GUI library you choose for which there are many options, but most people will recommend the following two: wxPython or PySide which is Qt for Python.
PySide has good documentation and tutorials.
What you will want to do is create a QMainWindow instance and set the WindowFlags to your requirements. You probably want the following combination Qt::Window | Qt::CustomizeWindowHint | Qt::WindowStaysOnTopHint.
Something like this: 
import sys
from PySide.QtCore import *
from PySide.QtGui import *

class Form(QMainWindow):

    def __init__(self, parent=None):
        super(Form, self).__init__(parent)
        self.setWindowFlags(Qt::Window | Qt::CustomizeWindowHint | Qt::WindowStaysOnTopHint)


if __name__ == '__main__':
    # Create the Qt Application
    app = QApplication(sys.argv)
    # Create and show the form
    form = Form()
    form.show()
    # Run the main Qt loop
    sys.exit(app.exec_())

Note, that there is a limit to the ""staying on top"" nature of such windows. There are Win32-specific ways to fight it and get even higher, but such requirement would be a design error. 
",222
7825395,7825395,1,"Sorry for the vague title, couldn't come up with anything more informative %)
What I want is a 5px horizontal panel on the top of the screen that I can draw on (and, possible, handle clicks on too).
One of the following features would be awesome (although I understand it's probably not really possible to combine both of them):

the panel should be just like the Windows's own taskbar, i.e., maximized windows should not overlap it, but start below it instead
the panel should show in fullscreen apps too

Is it possible to do this in Python?
Thanks.
",116
7825991,7828399,2,"Sikuli might be a perfect for this. It is based on Jython, thus can be extended with Python or Java tools.

Sikuli is a visual technology to automate and test graphical user
  interfaces (GUI) using images (screenshots). Sikuli includes Sikuli
  Script, a visual scripting API for Jython, and Sikuli IDE, an
  integrated development environment for writing visual scripts with
  screenshots easily.

Edit
If the image is that well defined as in your example, that can be easily matched without much image processing. Here I used matplotlib (small image must be exact cropped version of large image).
Target image (65x173):

Image to be matched (29x29):

In [48]: import matplotlib.pyplot as pl

In [49]: target = pl.imread(""r.png"")

In [50]: match = pl.imread(""xx.png"")

In [51]: target.shape
Out[51]: (65, 173, 4)

In [52]: match.shape
Out[52]: (29, 29, 4)

In [53]: for y in range(2):
   ....:     for x in range(5):
   ....:         sub_x = 36*x
   ....:         sub_y = 36*y
   ....:         sub_target = target[sub_y:sub_y+29,sub_x:sub_x+29,:]
   ....:         if (match == sub_target).all():
   ....:             print ""Match found between (%d,%d) and (%d,%d)"" % (sub_x,sub_y,sub_x+29,sub_y+29)
   ....:
Match found between (72,0) and (101,29)

",311
7825991,7995971,2,"I take it the images are always the same so the 6 is at the same offset and covered by the same  tag. Clicking the image at this point will follow the URL defined in the  href attribute. So follow that link.
",45
7825991,7825991,1,"In a html5 page using image map, I would like to use python (or Perl, Ruby, C) to crawl it to find a particular image (those are separated by a transparency area) in another one and click it when this image is found. The image are the same all the times. What is the best way to achieve this ? 
Edit: for french readers, the trick is implemented, see http://www.sputnick-area.net/?p=572
",86
7828717,7828784,2,"one is defined inside the if __name__=='__main__' block.
Consequently, one will get defined only if test.py is run as a script (rather than imported). 
For the module new to access one from the test module, you'll need to pull one out of the if __name__ block:
test.py:
class Test:
    def __init__(self):
        return
    def run(self):
        return 1

one=Test()

if __name__ == ""__main__"":
    one.run()

Then access one by the qualified name test.one:
new.py:
import test

class New:
    def __init__(self):
        test.one.run()

New()

",116
7828717,7828789,2,"No, you can't. The closest you can get is to pass the thing you need in to the constructor:
class New(object):
    def __init__(self, one):
        one.run()

",40
7828717,7828775,2,"If you want your New class to use the instance of Test you created, you have to pass it in as part of the constructor.
new.py
class New:
    def __init__(self, one):
        one.run()

test.py
import new

class Test:
    def __init__(self):
        return
    def run(self):
        return 1


if __name__ == ""__main__"":
    one=Test()
    two = new.New(one);

Playing around with globals is a great way to break your code without realizing how you did it. It is better to explicitly pass in the reference you want to use.
",113
7828717,7828717,1,"I have two files, one of the test.py is
import new.py

class Test:

    def __init__(self):
        return
    def run(self):
        return 1

if __name__ == ""__main__"":
    one=Test()
    one.run()

and new.py
class New:
    def __init__(self):
        one.run()

New()

Now when i run python test.py I get this error,
Traceback (most recent call last):
  File ""test.py"", line 1, in <module>
    import new.py
  File ""/home/phanindra/Desktop/new.py"", line 5, in <module>
    New()
  File ""/home/phanindra/Desktop/new.py"", line 3, in __init__
    one.run()
NameError: global name 'one' is not defined

But I want to use this instance of one in my New!!
Can I do this??
edit:
I want to access the variable in test.py in new.py to do some process and give them back to test.py. Isn't this possible?
",178
7828867,7828867,1,"a = [1, 2, 3, 1, 2, 3]
b = [3, 2, 1, 3, 2, 1]

a & b should be considered equal, because they have exactly the same elements, only in different order.
The thing is, my actual lists will consist of objects (my class instances), not integers.
",71
7828867,41433671,2,"If the list contains items that are not hashable (such as a list of objects) you might be able to use the Counter Class and the id() function such as:
from collections import Counter
...
if Counter(map(id,a)) == Counter(map(id,b)):
    print(""Lists a and b contain the same objects"")

",74
7828867,40016570,2,"https://docs.python.org/3.5/library/unittest.html#unittest.TestCase.assertCountEqual
assertCountEqual(first, second, msg=None)
Test that sequence first contains the same elements as second, regardless of their order. When they don’t, an error message listing the differences between the sequences will be generated.
Duplicate elements are not ignored when comparing first and second. It verifies whether each element has the same count in both sequences. Equivalent to: assertEqual(Counter(list(first)), Counter(list(second))) but works with sequences of unhashable objects as well.
New in version 3.2. 
or in 2.7:
https://docs.python.org/2.7/library/unittest.html#unittest.TestCase.assertItemsEqual
",116
7828867,34694504,2,"If the comparison is to be performed in a testing context, use assertCountEqual(a, b) (py>=3.2) and assertItemsEqual(a, b) (2.7<=py<3.2).
Works on sequences of unhashable objects too.
",47
7828867,28089750,2,"I hope the below piece of code might work in your case :- 
if ((len(a) == len(b)) and
   (all(i in a for i in b))):
    print 'True'
else:
    print 'False'

This will ensure that all the elements in both the lists a & b are same, regardless of whether they are in same order or not.
For better understanding, refer to my answer in this question
",87
7828867,7829554,2,"Let a,b lists
def ass_equal(a,b):
try:
    map(lambda x: a.pop(a.index(x)), b) # try to remove all the elements of b from a, on fail, throw exception
    if len(a) == 0: # if a is empty, means that b has removed them all
        return True 
except:
    return False # b failed to remove some items from a

No need to make them hashable or sort them.
",93
7828867,7829388,2,"O(n):  The Counter() method is best (if your objects are hashable):
def compare(s, t):
    return Counter(s) == Counter(t)

O(n log n):  The sorted() method is next best (if your objects are orderable):
def compare(s, t):
    return sorted(s) == sorted(t)

O(n * n): If the objects are neither hashable, nor orderable, you can use equality:
def compare(s, t):
    t = list(t)   # make a mutable copy
    try:
        for elem in s:
            t.remove(elem)
    except ValueError:
        return False
    return not t

",139
7828867,7829249,2,"If you know the items are always hashable, you can use a Counter() which is O(n)
If you know the items are always sortable, you can use sorted() which is O(n log n)
In the general case you can't rely on being able to sort, or has the elements, so you need a fallback like this, which is unfortunately O(n^2)
len(a)==len(b) and all(a.count(i)==b.count(i) for i in a)

",103
7828867,7828964,2,"The best way to do this is by sorting the lists and comparing them. (Using Counter won't work with objects that aren't hashable.) This is straightforward for integers:
sorted(a) == sorted(b)

It gets a little trickier with arbitrary objects. If you care about object identity, i.e., whether the same objects are in both lists, you can use the id() function as the sort key.
sorted(a, key=id) == sorted(b, key==id)

(In Python 2.x you don't actually need the key= parameter, because you can compare any object to any object. The ordering is arbitrary but stable, so it works fine for this purpose; it doesn't matter what order the objects are in, only that the ordering is the same for both lists. In Python 3, though, comparing objects of different types is disallowed in many circumstances -- for example, you can't compare strings to integers -- so if you will have objects of various types, best to explicitly use the object's ID.)
If you want to compare the objects in the list by value, on the other hand, first you need to define what ""value"" means for the objects. Then you will need some way to provide that as a key (and for Python 3, as a consistent type). One potential way that would work for a lot of arbitrary objects is to sort by their repr(). Of course, this could waste a lot of extra time and memory building repr() strings for large lists and so on.
sorted(a, key=repr) == sorted(b, key==repr)

If the objects are all your own types, you can define __lt__() on them so that the object knows how to compare itself to others. Then you can just sort them and not worry about the key= parameter. Of course you could also define __hash__() and use Counter, which will be faster.
",381
7828867,7828896,2,"You can sort both:
sorted(a) == sorted(b)

A counting sort could also be more efficient (but it requires the object to be hashable).
>>> from collections import Counter
>>> a = [1, 2, 3, 1, 2, 3]
>>> b = [3, 2, 1, 3, 2, 1]
>>> print (Counter(a) == Counter(b))
True

",92
7828891,7833202,2,"2.1 GB of data should take between 21 (@ 100 MB/s) to 70 (@ 30 MB/s) seconds just to read. You're then formatting that into and writing data which is perhaps five times as large. This means a total of 13 GB to read and write requiring 130-420 seconds.
Your sampling shows that reading takes 24 seconds. Writing should therefore require about two minutes. The reading and writing times can be improved using an SSD for example.
When I convert files (using programs I write in C) I assume that a conversion should take no more time than it takes to read the data itself, a lot less is usually possible. Overlapped reads and writes can also reduce the I/O time. I write my own custom formatting routines since printf is usually far too slow.
How much is 24 seconds? On a modern CPU at least 40 billion instructions. That means that in that time you can process every single byte of data with at least 19 instructions. Easily doable for a C program but not for an interpreted language (Python, Java, C#, VB).
Your 525 second processing (549-24) remainder indicates that Python is spending at least 875 billion instructions processing or 415 instructions per byte of data read. That comes out to 22 to 1: a not uncommon ratio between interpreted and compiled languages. A well-constructed C program should be down around ten instructions per byte or less.
",272
7828891,7831219,2,"Given that formatString is the slowest operation, try this:
def formatString(data_list):
    return "" "".join((str(data_list[1]), str(data_list[2]), str(data_list[3])))

",48
7828891,7829202,2,"To more precisely attack the problem, I suggest measuring the file read operation by making 'convertXYZ' a no-op function and timing the result.  And measuring the convert function, by changing the 'read' to always return a simple point, but calling the conversion and output the same number of times as if you were really reading the file.  (And probably another run where the final post-conversion output is a no-op.)  Depending on where the time is going, it may make a lot more sense to attack one or the other.
You might be able to get the local OS to do some interleaving for you by writing the output to the Python's stdout, and having the shell do the actual file IO.  And similarly by streaming the file into stdin (e.g., cat oldformat | python conversion.py > outputfile)
What sort of storage are the input and output files on?  The storage characteristics may have a lot more to do with the performance than the Python code.
Update: Given the output is the slowest, and your storage is pretty slow and shared between both reads and writes, try adding some buffering.  From the python doc you should be able to add some buffering by adding a third argument to the os.open call.  Try something pretty large like 128*1024?
",242
7828891,7828891,1,"Right now, I'm trying to convert a large quantity of binary files of points in latitude longitude altitude format to text based ECEF cartesian format (x, y, z). The problem right now is that the process is very very very slow. 
I have over 100 gigabytes of this stuff to run through, and more data could be coming in. I would like to make this bit of code as fast as possible.
Right now my code looks something like this: 
import mmap
import sys
import struct
import time

pointSize = 41

def getArguments():
    if len(sys.argv) != 2:
        print """"""Not enough arguments.
        example:
            python tllargbin_reader.py input_filename.tllargbin output_filename
        """"""
        return None
    else:
        return sys.argv

print getArguments()

def read_tllargbin(filename, outputCallback):
    f = open(filename, ""r+"")
    map = mmap.mmap(f.fileno(),0)
    t = time.clock()
    if (map.size() % pointSize) != 0:
        print ""File size not aligned.""
        #return
    for i in xrange(0,map.size(),pointSize):
        data_list = struct.unpack('=4d9B',map[i:i+pointSize])
        writeStr = formatString(data_list)
        if i % (41*1000) == 0:
            print ""%d/%d points processed"" % (i,map.size())
    print ""Time elapsed: %f"" % (time.clock() - t)
    map.close()


def generate_write_xyz(filename):
    f = open(filename, 'w', 128*1024)
    def write_xyz(writeStr):
        f.write(writeStr)
    return write_xyz

def formatString(data_list):
    return ""%f %f %f"" % (data_list[1], data_list[2],data_list[3])
args = getArguments()
if args != None:
    read_tllargbin(args[1],generate_write_xyz(""out.xyz""))

convertXYZ() is basically the conversion formula here:
http://en.wikipedia.org/wiki/Geodetic_system
I was wondering if it would be faster to read things in chunks of ~4MB with one thread, put them in a bounded buffer, have a different thread for conversion to string format, and have a final thread write the string back into a file on a different harddisk. I might be jumping the gun though...
I'm using python right now for testing, but I wouldn't be opposed to switching if I can work through these files faster.
Any suggestions would be great. Thanks
EDIT:
I have profiled the code with cProfile again and this time split the string format and the io. It seems I'm actually being killed by the string format... Here's the profiler report
         20010155 function calls in 548.993 CPU seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000  548.993  548.993 <string>:1(<module>)
        1    0.016    0.016  548.991  548.991 tllargbin_reader.py:1(<module>)
        1   24.018   24.018  548.955  548.955 tllargbin_reader.py:20(read_tllargbin)
        1    0.000    0.000    0.020    0.020 tllargbin_reader.py:36(generate_write_xyz)
 10000068  517.233    0.000  517.233    0.000 tllargbin_reader.py:42(formatString)
        2    0.000    0.000    0.000    0.000 tllargbin_reader.py:8(getArguments)
 10000068    6.684    0.000    6.684    0.000 {_struct.unpack}
        1    0.002    0.002  548.993  548.993 {execfile}
        2    0.000    0.000    0.000    0.000 {len}
        1    0.065    0.065    0.065    0.065 {method 'close' of 'mmap.mmap' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        1    0.000    0.000    0.000    0.000 {method 'fileno' of 'file' objects}
    10003    0.955    0.000    0.955    0.000 {method 'size' of 'mmap.mmap' objects}
        2    0.020    0.010    0.020    0.010 {open}
        2    0.000    0.000    0.000    0.000 {time.clock}            

Is there a faster way to format strings?
",697
7829007,7829376,2,"One way to fix it is to modify the helper function to:
def ConfigSectionMap(section):
    dict1 = {}
    options = Config.options(section)
    for option in options:
        try:
            dict1[option] = Config.get(section, option).replace('\\n', '')
            if dict1[option] == -1:
                DebugPrint(""skip: %s"" % option)
        except:
            print(""exception on %s!"" % option)
            dict1[option] = None
    return dict1

",95
7829007,7829185,2,"Update: The link I gave you below was to Python 3.0, my apologies I forgot your tag.
The 2.7 docs do not mention blank lines in values, so I suspect they are not supported at all.
See also this SO question (which looks like Python 3): How to read multiline .properties file in python

From the documentation:

Values can also span multiple lines, as long as they are indented
  deeper than the first line of the value. Depending on the parser’s
  mode, blank lines may be treated as parts of multiline values or
  ignored.

I don't know what 'parser mode' this is referring to, but not sure if what you want is doable.
On the other hand, the docs also mention the empty_lines_in_values option, which seems to indicate that blank lines are supported.
Seems somewhat contradictory to me.
",161
7829007,7829007,1,"The following is the file parsed by ConfigParser:
[Ticket]
description = This is a multiline string.
 1
 2

 4
 5

 7 

As described by the official Python wiki for ConfigParser examples, here is the helper function:
def ConfigSectionMap(section):
    dict1 = {}
    options = Config.options(section)
    for option in options:
        try:
            dict1[option] = Config.get(section, option)
            if dict1[option] == -1:
                DebugPrint(""skip: %s"" % option)
        except:
            print(""exception on %s!"" % option)
            dict1[option] = None
    return dict1

The resulting value is:
>>> print ConfigSectionMap('Ticket')['description']
This is a multiline string.
1
2
4
5
7

The expected value was:
>>> print ConfigSectionMap('Ticket')['description']
This is a multiline string.
1
2

4
5

7 

How do I fix this?
",181
7829188,7829294,2,"The if thing in somelist is the preferred and fastest way.
Under-the-hood that use of the in-operator translates to somelist.__contains__(thing) whose implementation is equivalent to:  any((x is thing or x == thing) for x in somelist).
Note the condition tests identity and then equality.
",56
7829188,7829222,2,"the ""if x in thing:"" format is strongly preferred, not just because it takes less code, but it also works on other data types and is (to me) easier to read.
I'm not sure how it's implemented, but I'd expect it to be quite a lot more efficient on datatypes that are stored in a more searchable form. eg. sets or dictionary keys. 
",78
7829188,7829220,2,"In your simple example it is of course better to use in.
However... in the question you link to, in doesn't work (at least not directly) because the OP does not want to find an object that is equal to something, but an object whose attribute n is equal to something.
One answer does mention using in on a list comprehension, though I'm not sure why a generator expression wasn't used instead:
if 5 in (data.n for data in myList):
    print ""Found it""

But this is hardly much of an improvement over the other approaches, such as this one using any:
if any(data.n == 5 for data in myList):
    print ""Found it""

",137
7829188,7829336,2,"for i in list
    if i == thingIAmLookingFor
        return True

The above is a terrible way to test whether an item exists in a collection. It returns True from the function, so if you need the test as part of some code you'd need to move this into a separate utility function, or add thingWasFound = False before the loop and set it to True in the if statement (and then break), either of which is several lines of boilerplate for what could be a simple expression.
Plus, if you just use thingIAmLookingFor in list, this might execute more efficiently by doing fewer Python level operations (it'll need to do the same operations, but maybe in C, as list is a builtin type). But even more importantly, if list is actually bound to some other collection like a set or a dictionary thingIAmLookingFor in list will use the hash lookup mechanism such types support and be much more efficient, while using a for loop will force Python to go through every item in turn.
Obligatory post-script: list is a terrible name for a variable that contains a list as it shadows the list builtin, which can confuse you or anyone who reads your code. You're much better off naming it something that tells you something about what it means.
",243
7829188,7829188,1,"I'm fairly new to python and have found that I need to query a list about whether it contains a certain item.
The majority of the postings I have seen on various websites (including this similar stackoverflow question) have all suggested something along the lines of
for i in list
    if i == thingIAmLookingFor
        return True

However, I have also found from one lone forum that 
if thingIAmLookingFor in list
    # do work

works.  
I am wondering if the if thing in list method is shorthand for the for i in list method, or if it is implemented differently.
I would also like to which, if either, is more preferred.
",121
7829200,7829200,1,"I am just installing TurboGears2 on Windows 7 running a virtual environment (python 2.7). I am not seeing the paster quickstart option when I run 
paster --help

Instead, here's that output:
(VirtualEnv_1) C:\VirtualEnv_1\Scripts>paster --help
Usage: paster-script.py [paster_options] COMMAND [command_options]

Options:
  --version         show program's version number and exit
  --plugin=PLUGINS  Add a plugin to the list of commands (plugins are Egg
                    specs; will also require() the Egg)
-h, --help        Show this help message

Commands:
 create       Create the file layout for a Python distribution
 help         Display help
 make-config  Install a package and create a fresh config file/directory
 points       Show information about entry points
 post         Run a request for the described application
 request      Run a request for the described application
 serve        Serve the described application
 setup-app    Setup an application, given a config file

TurboGears2:
 tginfo       Show TurboGears 2 related projects and their versions

So, it's no surprise that when I run :
(VirtualEnv_1) C:\VirtualEnv_1\Scripts>paster quickstart

I get: 
Command 'quickstart' not known (you may need to run setup.py egg_info)
Known commands:
  create       Create the file layout for a Python distribution
  exe          Run #! executable files
  help         Display help
  make-config  Install a package and create a fresh config file/directory
  points       Show information about entry points
  post         Run a request for the described application
  request      Run a request for the described application
  serve        Serve the described application
  setup-app    Setup an application, given a config file
  tginfo       Show TurboGears 2 related projects and their versions

My questions: Why is it missing, and how do I get it?
",300
7829200,7829286,2,"You get into this situation when you've installed the package required to run a TurboGears app (TurboGears2), but not the one required to develop a TurboGears app (tg.devtools).
Running the following command while in your virtualenv should install the correct package:
easy_install -i http://www.turbogears.org/2.1/downloads/current/index tg.devtools

",55
7829212,7829212,1,"The example in Simple wrapping of C code with cython describes nicely how to evaluate a function written in C on an array passed from numpy and return the result in a numpy array.
How would one go about doing the same thing but returning a 2D array? I.e. I'd like to evaluate a C function on a grid defined by two numpy arrays, and return the result as a numpy 2D array.
It would be something like this (using same functions as in the link above). Obviously one can't use double z[] now, but I'm not sure how to pass a 2D numpy array to C.
/*  fc.cpp    */
int fc( int N, const double a[], const double b[], double z[] )
    {
    for( int i = 0;  i < N;  i ++ ){
        for( int j = 0;  j < N;  j ++ ){
            z[i][j] = somefunction(a[i],b[j]);
    }
    return N;
}

This is the original .pyx file (see below).
import numpy as np
cimport numpy as np
cdef extern from ""fc.h"": 
    int fc( int N, double* a, double* b, double* z )  # z = a + b

def fpy( N,
    np.ndarray[np.double_t,ndim=1] A,
    np.ndarray[np.double_t,ndim=1] B,
    np.ndarray[np.double_t,ndim=1] Z ):
    """""" wrap np arrays to fc( a.data ... ) """"""
       assert N <= len(A) == len(B) == len(Z)
       fcret = fc( N, <double*> A.data, <double*> B.data, <double*> Z.data )

    return fcret

Many thanks.
",340
7829212,7829463,2,"You can use a normal array for a 2D Matrix. You need only give the length of the dimension to the function.
In the C file do something as that: 
(z is now an array of length N*N)
int fc( int N, const double a[], const double b[], double z[] )
{
    for( int i = 0;  i < N;  i++ ){
        for( int j = 0;  j < N;  j ++ ){
            z[(i*N)+j] = somefunction(a[i],b[j]);
    }
    return N;
}

In Python you need to do the same, so you can use a 1D Array with N*N elements instead of an 2D Matrix.
Update 3D case
(z is now an array of length N*N*N)
int fc( int N, const double a[], const double b[],const double c[], double z[] )
{
    for( int i = 0;  i < N;  i++ ){
        for( int j = 0;  j < N;  j ++ ){
           for( int k = 0;  k < N;  k ++ ){
            z[((i*N)+j)*N+k] = somefunction(a[i],b[j],c[k]);
    }
    return N;
}

",268
7829279,7829279,1,"I have a bit of a problem with using the Python YouTube API to get the title of the videos in the playlist. I have the enviroment configured correctly, also when I copied the example from the API documentation it works for the added playlist id, but when I try to use one from an other playlist I get an error.
Here is some code I wrote: (In this example I try to get the titles of the videos from here)
import gdata.youtube
import gdata.youtube.service

yt_service = gdata.youtube.service.YouTubeService()
yt_service.ssl = True

# a typical playlist URI
playlist_uri = ""http://gdata.youtube.com/feeds/api/playlists/PLCD939C4D974A5815""

playlist_video_feed = yt_service.GetYouTubePlaylistVideoFeed(playlist_uri)

# iterate through the feed as you would with any other
for playlist_video_entry in playlist_video_feed.entry:
    print playlist_video_entry.title.text

Here is the error I get: 
RequestError: {'status': 400, 'body': 'Invalid playlist id', 'reason': 'Bad Request'}

I'm quite frustrated with this and would appreciate some help. Thank you!
",181
7829279,7829295,2,"Remove PL in your request URI:
playlist_uri = ""http://gdata.youtube.com/feeds/api/playlists/CD939C4D974A5815""

I'm not sure why YouTube needs it to be in that format, but it needs to be.
You can also just do a .replace('playlists/PL', 'playlists/') on your string.
",52
7829311,7829311,1,"The goal is just to retrieve a specific file without downloading the entire contents, using the HTTP range method as described:
http://www.codeproject.com/KB/cs/remotezip.aspx
",26
7829311,7852229,2,"You can solve this a bit more generally with less code.  Essentially, create enough of a file-like object for ZipFile to use.  So you wind up with z = ZipFile(HttpFile(url)) and it dynamically downloads just the portion needed.  The advantage with this is you write less code, and it applies to more than just zip files.  (In fact, I wonder if there is something like this already... I'm not finding it though.)
Using the same idea, you could also create a caching wrapper for HttpFile to avoid repeated downloads.
And here's the code: (note the lack of error-handling)
#!/usr/bin/python
import urllib2

class HttpFile(object):
    def __init__(self, url):
        self.url = url
        self.offset = 0
        self._size = -1

    def size(self):
        if self._size < 0:
            f = urllib2.urlopen(self.url)
            self._size = int(f.headers[""Content-length""])
        return self._size

    def read(self, count=-1):
        req = urllib2.Request(self.url)
        if count < 0:
            end = self.size() - 1
        else:
            end = self.offset + count - 1
        req.headers['Range'] = ""bytes=%s-%s"" % (self.offset, end)
        f = urllib2.urlopen(req)
        data = f.read()
        # FIXME: should check that we got the range expected, etc.
        chunk = len(data)
        if count >= 0:
            assert chunk == count
        self.offset += chunk
        return data

    def seek(self, offset, whence=0):
        if whence == 0:
            self.offset = offset
        elif whence == 1:
            self.offset += offset
        elif whence == 2:
            self.offset = self.size() + offset
        else:
            raise Exception(""Invalid whence"")

    def tell(self):
        return self.offset

",336
7829311,7843535,2,"Since there was no such library I have written a small module myself, most code and logic is is from zipfile with the seek/reads translated to HTTP range requests.
Feel free to review and suggest improvements:
The code:
""""""
Read remote ZIP files using HTTP range requests
""""""
import struct
import urllib2
import zlib
import cStringIO
from zipfile import ZipInfo, ZipExtFile, ZipInfo
from os.path import join, basename

# The code is mostly adatpted from the zipfile module
# NOTE: ZIP64 is not supported

# The ""end of central directory"" structure, magic number, size, and indices
# (section V.I in the format document)
structEndArchive = ""<4s4H2LH""
stringEndArchive = ""PK\005\006""
sizeEndCentDir = struct.calcsize(structEndArchive)

_ECD_SIGNATURE = 0
_ECD_DISK_NUMBER = 1
_ECD_DISK_START = 2
_ECD_ENTRIES_THIS_DISK = 3
_ECD_ENTRIES_TOTAL = 4
_ECD_SIZE = 5
_ECD_OFFSET = 6
_ECD_COMMENT_SIZE = 7
# These last two indices are not part of the structure as defined in the
# spec, but they are used internally by this module as a convenience
_ECD_COMMENT = 8
_ECD_LOCATION = 9

# The ""central directory"" structure, magic number, size, and indices
# of entries in the structure (section V.F in the format document)
structCentralDir = ""<4s4B4HL2L5H2L""
stringCentralDir = ""PK\001\002""
sizeCentralDir = struct.calcsize(structCentralDir)

# indexes of entries in the central directory structure
_CD_SIGNATURE = 0
_CD_CREATE_VERSION = 1
_CD_CREATE_SYSTEM = 2
_CD_EXTRACT_VERSION = 3
_CD_EXTRACT_SYSTEM = 4
_CD_FLAG_BITS = 5
_CD_COMPRESS_TYPE = 6
_CD_TIME = 7
_CD_DATE = 8
_CD_CRC = 9
_CD_COMPRESSED_SIZE = 10
_CD_UNCOMPRESSED_SIZE = 11
_CD_FILENAME_LENGTH = 12
_CD_EXTRA_FIELD_LENGTH = 13
_CD_COMMENT_LENGTH = 14
_CD_DISK_NUMBER_START = 15
_CD_INTERNAL_FILE_ATTRIBUTES = 16
_CD_EXTERNAL_FILE_ATTRIBUTES = 17
_CD_LOCAL_HEADER_OFFSET = 18

# The ""local file header"" structure, magic number, size, and indices
# (section V.A in the format document)
structFileHeader = ""<4s2B4HL2L2H""
stringFileHeader = ""PK\003\004""
sizeFileHeader = struct.calcsize(structFileHeader)

_FH_SIGNATURE = 0
_FH_EXTRACT_VERSION = 1
_FH_EXTRACT_SYSTEM = 2
_FH_GENERAL_PURPOSE_FLAG_BITS = 3
_FH_COMPRESSION_METHOD = 4
_FH_LAST_MOD_TIME = 5
_FH_LAST_MOD_DATE = 6
_FH_CRC = 7
_FH_COMPRESSED_SIZE = 8
_FH_UNCOMPRESSED_SIZE = 9
_FH_FILENAME_LENGTH = 10
_FH_EXTRA_FIELD_LENGTH = 11


def _http_get_partial_data(url, start_range, end_range=None):
    req = urllib2.Request(url)
    range_header = ""bytes=%s"" % start_range
    if end_range is not None:
        range_header += ""-%s"" % end_range
    req.headers['Range'] = range_header
    f = urllib2.urlopen(req)    
    return f


def _EndRecData(url):
    """"""Return data from the ""End of Central Directory"" record, or None.

    The data is a list of the nine items in the ZIP ""End of central dir""
    record followed by a tenth item, the file seek offset of this record.""""""
    ECD = _http_get_partial_data(url, -sizeEndCentDir)
    content_range =  ECD.headers.get('Content-Range')
    filesize = int(content_range.split('/')[1]) if content_range and '/' in content_range else 0
    data = ECD.read()
    ECD.close() 
    if data[0:4] == stringEndArchive and data[-2:] == ""\000\000"":
        # the signature is correct and there's no comment, unpack structure
        endrec = struct.unpack(structEndArchive, data)
        endrec = list(endrec)

        # Append a blank comment and record start offset
        endrec.append("""")
        endrec.append(filesize - sizeEndCentDir)
        return endrec
    # Either this is not a ZIP file, or it is a ZIP file with an archive
    # comment.  Search the end of the file for the ""end of central directory""
    # record signature. The comment is the last item in the ZIP file and may be
    # up to 64K long.  It is assumed that the ""end of central directory"" magic
    # number does not appear in the comment.

    # Search by retrieving chunks of 256, 1k and 64k
    try_ranges = (1 << 8, 1 << 10, 1 << 16)
    for check_range in try_ranges:
        ECD = _http_get_partial_data(url, -(check_range + sizeEndCentDir))      
        data = ECD.read()       
        content_range =  ECD.headers.get('Content-Range')       
        ECD.close()
        download_start = content_range.split('-')[0]
        start = data.rfind(stringEndArchive)        
        if start >= 0:          
            # found the magic number; attempt to unpack and interpret
            recData = data[start:start+sizeEndCentDir]
            endrec = list(struct.unpack(structEndArchive, recData))
            commentSize = endrec[_ECD_COMMENT_SIZE] #as claimed by the zip file
            comment = data[start+sizeEndCentDir:start+sizeEndCentDir+commentSize]
            endrec.append(comment)
            endrec.append(download_start + start)           
            return endrec

    raise IOError


class HTTPZipFile:
    def __init__(self, url):
        self.url = url
        self.NameToInfo = {}    # Find file info given name
        self.filelist = []      # List of ZipInfo instances for archive
        self.pwd = None
        self.comment = ''
        self.debug = 0
        self._RealGetContents()     

    def _RealGetContents(self):
        """"""Read in the table of contents for the ZIP file.""""""
        try:
            endrec = _EndRecData(self.url)
        except IOError:
            raise BadZipfile(""File is not a zip file"")
        if not endrec:
            raise BadZipfile, ""File is not a zip file""
        if self.debug > 1:
            print endrec
        size_cd = endrec[_ECD_SIZE]             # bytes in central directory
        offset_cd = endrec[_ECD_OFFSET]         # offset of central directory
        self.comment = endrec[_ECD_COMMENT]     # archive comment

        # ""concat"" is zero, unless zip was concatenated to another file
        concat = endrec[_ECD_LOCATION] - size_cd - offset_cd
        #if endrec[_ECD_SIGNATURE] == stringEndArchive64:
        #   # If Zip64 extension structures are present, account for them
        #   concat -= (sizeEndCentDir64 + sizeEndCentDir64Locator)

        if self.debug > 2:
            inferred = concat + offset_cd
            print ""given, inferred, offset"", offset_cd, inferred, concat
        # self.start_dir:  Position of start of central directory
        self.start_dir = offset_cd + concat
        ECD = _http_get_partial_data(self.url, self.start_dir, self.start_dir+size_cd-1)
        data = ECD.read()
        ECD.close()
        fp = cStringIO.StringIO(data)               
        total = 0
        while total < size_cd:
            centdir = fp.read(sizeCentralDir)
            if centdir[0:4] != stringCentralDir:
                raise BadZipfile, ""Bad magic number for central directory""
            centdir = struct.unpack(structCentralDir, centdir)
            if self.debug > 2:
                print centdir
            filename = fp.read(centdir[_CD_FILENAME_LENGTH])
            # Create ZipInfo instance to store file information
            x = ZipInfo(filename)
            x.extra = fp.read(centdir[_CD_EXTRA_FIELD_LENGTH])
            x.comment = fp.read(centdir[_CD_COMMENT_LENGTH])
            x.header_offset = centdir[_CD_LOCAL_HEADER_OFFSET]
            (x.create_version, x.create_system, x.extract_version, x.reserved,
                x.flag_bits, x.compress_type, t, d,
                x.CRC, x.compress_size, x.file_size) = centdir[1:12]
            x.volume, x.internal_attr, x.external_attr = centdir[15:18]
            # Convert date/time code to (year, month, day, hour, min, sec)
            x._raw_time = t
            x.date_time = ( (d>>9)+1980, (d>>5)&0xF, d&0x1F,
                                     t>>11, (t>>5)&0x3F, (t&0x1F) * 2 )

            x._decodeExtra()
            x.header_offset = x.header_offset + concat
            x.filename = x._decodeFilename()
            self.filelist.append(x)
            self.NameToInfo[x.filename] = x

            # update total bytes read from central directory
            total = (total + sizeCentralDir + centdir[_CD_FILENAME_LENGTH]
                     + centdir[_CD_EXTRA_FIELD_LENGTH]
                     + centdir[_CD_COMMENT_LENGTH])

        if self.debug > 2:
            print ""total"", total

    def namelist(self):
        """"""Return a list of file names in the archive.""""""
        l = []
        for data in self.filelist:
            l.append(data.filename)
        return l

    def infolist(self):
        """"""Return a list of class ZipInfo instances for files in the
        archive.""""""
        return self.filelist

    def printdir(self):
        """"""Print a table of contents for the zip file.""""""
        print ""%-46s %19s %12s"" % (""File Name"", ""Modified    "", ""Size"")
        for zinfo in self.filelist:
            date = ""%d-%02d-%02d %02d:%02d:%02d"" % zinfo.date_time[:6]
            print ""%-46s %s %12d"" % (zinfo.filename, date, zinfo.file_size)

    def getinfo(self, name):
        """"""Return the instance of ZipInfo given 'name'.""""""
        info = self.NameToInfo.get(name)
        if info is None:
            raise KeyError(
                'There is no item named %r in the archive' % name)

        return info         

    def open(self, name, pwd=None):
        """"""Return file-like object for 'name'.""""""
        if not self.url:
            raise RuntimeError, \
                  ""Attempt to read ZIP archive that was already closed""
        zinfo = self.getinfo(name)
        offset = zinfo.header_offset
        f = _http_get_partial_data(self.url, offset, offset+sizeFileHeader-1)
        fheader = f.read()
        f.close()

        fheader = struct.unpack(structFileHeader, fheader)
        offset += sizeFileHeader
        f = _http_get_partial_data(self.url, offset, offset+fheader[_FH_FILENAME_LENGTH]-1)
        fname = f.read()
        f.close()

        if fname != zinfo.orig_filename:
            raise BadZipfile, \
                      'File name in directory ""%s"" and header ""%s"" differ.' % (
                          zinfo.orig_filename, fname)

        is_encrypted = zinfo.flag_bits & 0x1
        if is_encrypted:
            raise RuntimeError, ""File %s is encrypted, "" \
                  ""not supported."" % name

        offset += fheader[_FH_FILENAME_LENGTH]+fheader[_FH_EXTRA_FIELD_LENGTH]
        f = _http_get_partial_data(self.url, offset, offset+fheader[_FH_COMPRESSED_SIZE]-1)
        data = f.read()
        return ZipExtFile(cStringIO.StringIO(data), 'r', zinfo)


if __name__ == ""__main__"":
    # Some tests
    link=""http://dfn.dl.sourceforge.net/project/filezilla/FileZilla_Client/3.5.1/FileZilla_3.5.1_win32.zip""
    hzfile = HTTPZipFile(link)
    hzfile.printdir()
    for fname in ('GPL.html', 'resources/blukis/48x48/filter.png', 'resources/finished.wav'):
        source_name = join('FileZilla-3.5.1', fname)
        dest_fname = join('/tmp', basename(fname))
        print ""Extracing %s to %s"" % (source_name, dest_fname)
        with hzfile.open(source_name) as f:
            data = f.read()
            new_file = open(dest_fname, 'w')
            new_file.write(data)
            new_file.close()

",1884
7833533,7833533,1,"Example table:
  A  |  B  |  C  | ...
-----+-----+-----+----
  3  |  2  |  2  |    
  5  |  3  |  4  |    
  7  |  4  |  6  |    
  9  |  5  |  8  |    

I would like somehow to temper it with Gnumeric and produce matching cells across columns:
  A  |  B  |  C  | ...
-----+-----+-----+----
  -  |  2  |  2  |    
  3  |  3  |  -  |    
  -  |  4  |  4  |    
  5  |  5  |  -  |    
  -  |  -  |  6  |    
  7  |  -  |  -  |    
  -  |  -  |  8  |    
  9  |  -  |  -  |    

Real example if with string values instead numbers but it is easier to explain with numbers I think
If this is not trivial and someone has idea how this can be done with Python lists instead table columns in Gnumeric please post a Python solution.
",173
7833533,7833642,2,"It's quite easy to do in Python:
a = [3, 5, 7, 9]
b = [2, 3, 4, 5]
c = [2, 4, 6, 8]

a_ex, b_ex, c_ex = zip(*(
                        [elem if elem in col else None
                            for col in a, b, c] 
                                for elem in set(a).union(b, c)
                      ))

Seems the most direct if you're not worried about the speed.
I also just noticed my answer to Joining multiple iteratorars by a key sort of applies:
def paditers(*args):
    iters = [iter(x) for x in args]

    this = [next(i) for i in iters]

    while True:
        try:
            key = min(i for i in this if i != None)
        except ValueError:
            break
        for i, val in enumerate(this):
            if val == key:
                yield val
                this[i] = next(iters[i], None)
            else:
                yield None

padded = list(paditers(a, b, c))
next_item = iter(padded).next
print zip(*((next_item(), next_item(), next_item()) 
         for _ in range(len(padded) // 3)))

You can adapt that if you need performance to scale linearly.
",267
7833668,7835936,2,"If you do not want to change Primay key values or do not to add new objects to the table, which could be duplicate of the old info - then you need some kind of data which you can use as lookup parameters in your database. 
If you have model which represents this data, then its really easy using 
m = Model.objects.get(somecolumn = somedata)
m.someothervalue = someotherdata
m.save()

But why include django in this anyway? If you have CSV table, then updating this info is really a case of writing queries. and programs like Excel and openoffice make this very easy. 
If you already have data in CSV format, then just open the data as spreadsheet and use excels/openoffice's Concactenate function to create update queries
Update mytable set value1 = data1, value2 = data2 where somevalue = somedata;

If you used openoffice for this, then openoffice has this nifty Text to columns function (under data in program menu), which turns concactenated values into string. Then you can copypaste those strings into command prompt or phppgadmin and run.. and voila, you get updated data in your database.
Edit (In response to you comment.):
Look into this: https://docs.djangoproject.com/en/dev/ref/models/querysets/#get-or-create
If you want to use django for this, then use get_or_create. But you need to remember here, that if any of the parameters you use in get_or_create method have changed, then new object will be created. Thats why i said in the beginning of the post, that you need some kind of data, which will not change.
for example (taken from the link above)
obj, created = Person.objects.get_or_create(first_name='John', last_name='Lennon',
                  defaults={'birthday': date(1940, 10, 9)})

will create new obj(Person) when used first time. But if used 2nd time and the date has changed, then new Person with same name and last name but new date will be created.
So to avoid this, you'll still need to do something like
obj.someothervalue = someotherdata
obj.save()

if you want to have more control over the data, that could have been changed.
Alan.
",406
7833668,7833668,1,"I have a table which already contains some data in it. Now i want to upload new data from a csv file and want to update some of the previous values of that table. I am using django 1.3 and sqlite3 as database. But i am not able to update the table.
",56
7833715,7833827,2,"If the program works and the speed is acceptable, I wouldn't change it.
Otherwise, you could try unutbu's answer.
Generally, I would leave away the 
png = ""png""
jpg = ""jpg""

stuff as I don't see any purpose in not using the strings directly.
And better test for "".png"" instead of ""png"".
An even better solution would be to define 
extensions = ('.png', '.jpg')

somewhere centally and use that in
if any(currentFile.endswith(ext) for ext in extensions):
    os.remove(currentFile)

.
",111
7833715,7833781,2,"Since you are recursing through subdirectories, use os.walk:
import os

def scandirs(path):
    for root, dirs, files in os.walk(path):
        for currentFile in files:
            print ""processing file: "" + currentFile
            exts = ('.png', '.jpg')
            if any(currentFile.lower().endswith(ext) for ext in exts):
                os.remove(os.path.join(root, currentFile))

",77
7833715,7833715,1,"I'm fairly new to Python, but I have gotten this code to work, and in fact, do what it's intended to do.
However, I'm wondering if there is a more efficient way to code this, perhaps to enhance the processing speed.
 import os, glob


def scandirs(path):
    for currentFile in glob.glob( os.path.join(path, '*') ):
        if os.path.isdir(currentFile):
            print 'got a directory: ' + currentFile
            scandirs(currentFile)
        print ""processing file: "" + currentFile
        png = ""png"";
        jpg = ""jpg"";
        if currentFile.endswith(png) or currentFile.endswith(jpg):
            os.remove(currentFile)

scandirs('C:\Program Files (x86)\music\Songs')

Right now, there are about 8000 files, and it takes quite some time to process every file and check if it indeed ends in png or jpg.  
",170
7833807,35569519,2,"we can use ffmpeg to get the duration of any video or audio files.
To install ffmpeg follow this link
import subprocess
import re

process = subprocess.Popen(['ffmpeg',  '-i', path_of_wav_file], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
stdout, stderr = process.communicate()
matches = re.search(r""Duration:\s{1}(?P<hours>\d+?):(?P<minutes>\d+?):(?P<seconds>\d+\.\d+?),"", stdout, re.DOTALL).groupdict()

print matches['hours']
print matches['minutes']
print matches['seconds']

",119
7833807,7842081,2,"import os
path=""c:\\windows\\system32\\loopymusic.wav""
f=open(path,""r"")

#read the ByteRate field from file (see the Microsoft RIFF WAVE file format)
#https://ccrma.stanford.edu/courses/422/projects/WaveFormat/
#ByteRate is located at the first 28th byte
f.seek(28)
a=f.read(4)

#convert string a into integer/longint value
#a is little endian, so proper conversion is required
byteRate=0
for i in range(4):
    byteRate=byteRate + ord(a[i])*pow(256,i)

#get the file size in bytes
fileSize=os.path.getsize(path)  

#the duration of the data, in milliseconds, is given by
ms=((fileSize-44)*1000)/byteRate

print ""File duration in miliseconds : "" % ms
print ""File duration in H,M,S,mS : "" % ms/(3600*1000) % "","" % ms/(60*1000) % "","" % ms/1000 % "","" ms%1000
print ""Actual sound data (in bytes) : "" % fileSize-44  
f.close()

",194
7833807,7833963,2,"The duration is equal to the number of frames divided by the framerate (frames per second):
import wave
import contextlib
fname = '/tmp/test.wav'
with contextlib.closing(wave.open(fname,'r')) as f:
    frames = f.getnframes()
    rate = f.getframerate()
    duration = frames / float(rate)
    print(duration)


Regarding @edwards' comment, here is some code to produce a 2-channel wave file:
import math
import wave
import struct
FILENAME = ""/tmp/test.wav""
freq = 440.0
data_size = 40000
frate = 1000.0
amp = 64000.0
nchannels = 2
sampwidth = 2
framerate = int(frate)
nframes = data_size
comptype = ""NONE""
compname = ""not compressed""
data = [(math.sin(2 * math.pi * freq * (x / frate)),
        math.cos(2 * math.pi * freq * (x / frate))) for x in range(data_size)]
try:
    wav_file = wave.open(FILENAME, 'w')
    wav_file.setparams(
        (nchannels, sampwidth, framerate, nframes, comptype, compname))
    for values in data:
        for v in values:
            wav_file.writeframes(struct.pack('h', int(v * amp / 2)))
finally:
    wav_file.close()

If you play the resultant file in an audio player, you'll find that is 40 seconds in duration. If you run the code above it also computes the duration to be 40 seconds. So I believe the number of frames is not influenced by the number of channels and the formula above is correct. 
",288
7833807,41617943,2,"A very simple method is to use pysoundfile, https://github.com/bastibe/PySoundFile
Here's some example code of how to do this:
import soundfile as sf
f = sf.SoundFile('447c040d.wav')
print('samples = {}'.format(len(f)))
print('sample rate = {}'.format(f.samplerate))
print('seconds = {}'.format(len(f) / f.samplerate))

The output for that particular file is:
samples = 232569
sample rate = 16000
seconds = 14.5355625

This aligns with soxi:
Input File     : '447c040d.wav'
Channels       : 1
Sample Rate    : 16000
Precision      : 16-bit
Duration       : 00:00:14.54 = 232569 samples ~ 1090.17 CDDA sectors
File Size      : 465k
Bit Rate       : 256k
Sample Encoding: 16-bit Signed Integer PCM

",138
7833807,7833807,1,"I'm looking for a way to find out the duration of a audio file (.wav) in python. So far i had a look at python wave library, mutagen, pymedia, pymad i was not able to get the duration of the wav file. Pymad gave me the duration but its not consistent. 
Thanks in advance. 
",64
7833831,7833831,1,"I have a VB6 application running on a number of old 486 Windows 95 machines and sometimes the application is upgraded. The only way to accomplish this today is to use Hyperterminal and send the file over a null modem cable. Replacing the machines is not an option at this point.
I want to write an application that can take care of transferring the updating app over null modem without rewriting the VB6 app. This means I'm free to use anything I see fit. What alternatives are there?
These are the ones I can think of but I'd like to know if I'm wrong and any pros/cons. Also, I'd prefer to avoid C/C++ if at all possible.

Python with py2exe
Another VB6 app
C/C++

Edit: Some clarifications after reading the comments:
I want to make the process as easy as possible, today we have to remove and dismantle the computer, connect a keyboard and then fire up Hyperterminal to get going. That's why I want something more automatic. I'm open to suggestion of existing solutions but given the specific needs I didn't think there were any.
There is no ethernet on some of the computers either so the solution needs to be able to run RS232.
And again: Replacing the machines is not an option at this point. Just trust me on this.
",250
7833831,7835341,2,"If you must use a null modem, how about the built in serial line networking support?
Then you can just use normal network methods (psexec, file share, etc) methods to do the update.
",40
7833831,7835687,2,"We've got a few testing laboratories in a similar situation-the labs make money for the company so no touching the ancient computers that run the tests under pain of death.  :-)
Anyway, pySerial seems like it'd work for this application, maybe take a look at their wxPython examples for some ideas on a GUI.
",62
7833831,7835783,2,"I guess the answer is pretty simple if you are happy using VB6 and the other app is already VB6, then use it. 
That will do whatever serial comms you require quite adequately. Remember though you may want to update the application you write to do the updating in which case you are back to using hyperterminal!
",61
7833831,7849945,2,"Network Method
I would first get network cards installed in everything.  If you want something that you can just plugin and go, look for any card compatible with NE2000.  That card will work out of the box on Windows 3.11 and 95, no problem.  In particular, you can find the 3Com EtherLink II or the 3C509B for very cheap online.  It's an excellent card.  (The Google Shopping results list several for under $20.)
From there, just enable the Windows File/Print Sharing service over TCP/IP, and you're good to go!  After you've done this, you can remotely manage and upgrade these machines, saving you a lot of headache later on.
Serial-Port Method
Deanna's suggestion of using the serial port as a network device and transferring files normally will work as well.  However, there is a bit of setup involved, and it can be a hassle if you've never done it.  There are several other software options.  I recommend LapLink.  It's fairly painless.
You could even go all-out and pickup a multi-port serial interface for fairly cheap these days, and manage these computers centrally.  RS232 is very robust and can go a long distance over the proper cabling.
Networking over Ethernet is the way to go though.  If at all possible, choose that option.
",249
7833919,7833919,1,"I am looking for a regexp that returns only three matched groups for the string ""A   :B C:D""
where A,B,C,D are words examples (\w+)
The following Python code prints unwanted (None,None).
I just want ('A',None) (None,'B') and ('C','D') using one regexp (No added python code for filtering).
for m in re.compile(r'(?:(\w+)|)(?:(?::)(\w+)|)').finditer('A :B C:D'):
    print m.groups()

",129
7833919,7834115,2,"This might do the trick:
(?=[\w:])(\w*)(?::(\w*))?

(\w*)(?::(\w*))? describes the structure you want, but it has a problem that it also matches empty string; thus we have to assure that there is at least one non-space character at the start (which will get matched by the greedy operators), and the lookahead at the start does it.
Edit: wrong paste :)
",98
7833919,7834326,2,"import re

print([m.groups() for m in re.finditer(
    r'''(?x)               # verbose mode
        (\w+)?             # match zero-or-more \w's
        (?: :|\s)          # match (non-groupingly) a colon or a space 
        (\w+ (?:\s|\Z))?   # match zero-or-more \w's followed by a space or EOL
        ''',
    'A :B C:D')])

yields
[('A', None), (None, 'B '), ('C', 'D')]

",104
7834907,7835084,2,"The following snippet should do what you want:
import re
from xml.etree import ElementTree                                               

with open('films.xml') as f:                                                    
    xml = ElementTree.parse(f)                                                  

for t in xml.findall('.//{http://www.mediawiki.org/xml/export-0.5/}text'):
    print '===================='
    m = re.search(r'(?s).*?{{(Infobox film.*?)}}', t.text)
    if m:
        print m.group(1)

The regular expression there begins with (?s), which turns on the DOTALL option, meaning that . matches newlines as well as any other character.  The two instances of .*? are non-greedy matches of any charcter - i.e. they will find the shortest stretch of zero or more characters until the rest of the expression can be matched.
",146
7834907,7834907,1,"I'm using etree module. I'm trying to extract the information around <text ...> tag. Here is my XML file. I want if <text ..."">{{Infobox film start with Infobox film then copy all the text between {{ }}. Is it possible? thanks
Update: XML file updated
",63
7834990,7834990,1,"Currently i filter by some option in django's admin interface. For instance lets say i filter by 'By status'. Is it possible to select multiple statuses to filter results from? Here is the screenshot of the filter:

Can i select multiple items from this list? 
",52
7834990,7835497,2,"Not in the admin UI, but if you modify the URL, you can make the filtering criterion more complex.
For instance, now the URL (after you click on a filter) probably ends with something like
?status__exact=a

You can change this to
status__in=a%2Cm

in order to see both statuses a and m. The %2C encodes a comma.
",67
7834990,34410509,2,"You can also add the following query to the URL of your list page.
in my case if i have multiple option.
?bookingstatus__in=booked,refunded.

",29
7834990,29198828,2,"You can also add the following query to the URL of your list display page
?my_field__gte=1&myfield__lte=10

for numeric fields. That way you can have a ranged selection. 
Basically you can use any query you would also use in your code. 
",46
7835030,7835030,1,"I'm currently writing a basic dispatch model server based on the Python Eventlet library (http://eventlet.net/doc/). Having looked at the WSGI docs on Eventlet (http://eventlet.net/doc/modules/wsgi.html), I can see that the eventlet.wsgi.server function logs the x-forwarded-for header in addition to the client IP address. 
However, the way to obtain this is to attach a file-like object (the default which is sys.stderr) and then have the server pipe that to that object.
I would like to be able to obtain the client IP from within the application itself (i.e. the function that has start_response and environ as parameters). Indeed, an environ key would be perfect for this. Is there a way to obtain the IP address simply (i.e. through the environ dictionary or similar), without having to resort to redirecting the log object somehow?
",158
7835030,7839576,2,"What you want is in the wsgi environ, specifically environ['REMOTE_ADDR']. 
However, if there is a proxy involved, then REMOTE_ADDR will be the address of the proxy, and the client address will be included (most likely) in HTTP_X_FORWARDED_FOR. 
Here's a function that should do what you want, for most cases (all credit to Sævar):
def get_client_address(environ):
    try:
        return environ['HTTP_X_FORWARDED_FOR'].split(',')[-1].strip()
    except KeyError:
        return environ['REMOTE_ADDR']


You can easily see what is included in the wsgi environ by writing a simple wsgi app and pointing a browser at it, for example:
from eventlet import wsgi
import eventlet

from pprint import pformat

def show_env(env, start_response):
    start_response('200 OK', [('Content-Type', 'text/plain')])
    return ['%s\r\n' % pformat(env)]

wsgi.server(eventlet.listen(('', 8090)), show_env)


And combining the two ...
from eventlet import wsgi
import eventlet

from pprint import pformat

def get_client_address(environ):
    try:
        return environ['HTTP_X_FORWARDED_FOR'].split(',')[-1].strip()
    except KeyError:
        return environ['REMOTE_ADDR']

def show_env(env, start_response):
    start_response('200 OK', [('Content-Type', 'text/plain')])
    return ['%s\r\n\r\nClient Address: %s\r\n' % (pformat(env), get_client_address(env))]

wsgi.server(eventlet.listen(('', 8090)), show_env)

",301
7838241,7838241,1,"I'm building a PyGTK application with several widgets that when changed, need to notify other widgets about the change. I would like to avoid code like this:
def on_entry_color_updated(self, widget):
    self.paint_tools_panel.current_color_pane.update_color()
    self.main_window.status_bar.update_color()
    self.current_tool.get_brush().update_color()

And do something like this instead:
def on_entry_color_updated(self, widget):
    self.update_notify('color-changed')

The status bar, current color pane and current tool would subscribe to that notification event and act accordingly. From what I can tell, the GObject signaling mechanism only allows me to register a callback on a particular widget, so each object that wants to receive a notification has to be aware of that widget.
Does GTK provide such a system or should I build it myself? Developers of Shotwell, a photo organization application for GNOME, had to build their own signaling mechanism, if I understand their design doc correctly. Searching here on SO didn't turn out any definitive answers.
Edit:
Clarification why I think GObject signaling is not what I need (or just a part of what I need). With GObject, I need to explicitly connect an object to another object, like so:
emitter.connect('custom-event', receiver.event_handler)

So in my application, I would have to do this:
class ColorPane(gtk.Something):
    def __init__(self, application):
        # init stuff goes here...

        application.color_pallette.connect('color-changed', self.update_color)

    def update_color(self, widget):
        """"""Show the new color.""""""
        pass

class StatusBar(gtk.Something):
    def __init__(self, application):
        # init stuff goes here...

        application.color_pallette.connect('color-changed', self.update_color)

    def update_color(self, widget):
        """"""Show the new color name.""""""
        pass

class Brush(gtk.Something):
    def __init__(self, application):
        # init stuff goes here...

        application.color_pallette.connect('color-changed', self.update_color)

    def update_color(self, widget):
        """"""Draw with new color.""""""
        pass

In other words, I have to pass the application object or some other object that knows about the color_pallete to other objects in my application so that they connect to color_pallette signals. This is the kind of coupling that I want to avoid.
",430
7838241,7840285,2,"For one, you could create a custom subclass of GObject, which provides some custom signals. The following example is a slightly adapted version of the one given in the linked article:
import pygtk
pygtk.require('2.0')
import gobject

class Car(gobject.GObject):

    __gsignals__ = {
        'engine-started': (gobject.SIGNAL_RUN_LAST, gobject.TYPE_NONE, ()),
        'engine-stopped': (gobject.SIGNAL_RUN_LAST, gobject.TYPE_NONE, ()),
    }

    def __init__(self):
        gobject.GObject.__init__(self)
        self._state = 0

    def start(self):
        if not self._state:
            self._state = 1
            self.emit('engine-started')

    def stop(self):
        if self._state:
            self._state = 0
            self.emit('engine-stopped')


gobject.type_register(Car)

def kill_switch(c):
    def callback(*unused, **ignored):
        c.stop()
    return callback

def on_start(*unused, **ignored):
    print ""Started...""

def on_stop(*unused, **ignored):
    print ""Stopped...""

some_car = Car()
some_car.connect('engine-started', on_start)
some_car.connect('engine-started', kill_switch(some_car))
some_car.connect('engine-stopped', on_stop)
some_car.start()

Another approach would be to take advantage of one of the many event/signalling packages already on PyPI, for example:

Zope Event
Louie
PyDispatcher
Darts Events
Trellis

",235
7838241,7840578,2,"GObjects don't have to be widgets. For example, your application class can be a GObject which defines signals that other widgets connect to.
Also, I don't think you understood the Shotwell design document correctly. It looks to me like their signalling system is 100% GObject signalling system, just with particular guarantees about the order in which signals are handled. As they say in their design document, such things are possible in plain GObject, but Vala makes it easier to code it their way.
",96
7838290,7838290,1,"I have data in XML format. Example is shown as follow. I want to extract data from <text> tag. 
Here is my XML data.
    <text>
    The 40-Year-Old Virgin is a 2005 American buddy comedy
    film about a middle-aged man's journey to finally have sex.

    <h1>The plot</h1>
    Andy Stitzer (Steve Carell) is the eponymous 40-year-old virgin.
    <h1>Cast</h1>

    <h1>Soundtrack</h1>

    <h1>External Links</h1>
</text>

I need only The 40-Year-Old Virgin is a 2005 American buddy comedy film about a middle-aged man's journey to finally have sex. Is it possible? thanks
",128
7838290,7838352,2,"Whenever you find yourself looking at XML data and thinking about regular expressions, you should stop and ask yourself why you aren't considering a real XML parser. The structure of XML makes it perfectly suited for a proper parser, and maddeningly frustrating for regular expressions.
If you must use regular expressions, the following should do. Until your document changes!
import re
p = re.compile(""<text>(.*)<h1>"")
p.search(xml_text).group(1)

Spoiler: regular expressions might be appropriate if this is just a one-off problem that needs a quick and dirty solution. Or they  might be appropriate if you know the input data will be fairly static and can't tolerate the overhead of a parser.
",141
7838290,7838395,2,"Use an XML parser to parse XML. Using lxml:
import lxml.etree as ET

content='''\
<text>
    The 40-Year-Old Virgin is a 2005 American buddy comedy
    film about a middle-aged man's journey to finally have sex.

    <h1>The plot</h1>
    Andy Stitzer (Steve Carell) is the eponymous 40-year-old virgin.
    <h1>Cast</h1>

    <h1>Soundtrack</h1>

    <h1>External Links</h1>
</text>
'''

text=ET.fromstring(content)
print(text.text)

yields
    The 40-Year-Old Virgin is a 2005 American buddy comedy
    film about a middle-aged man's journey to finally have sex.

",119
7838290,7838328,2,"Don't use regular expression to parse XML/HTML. Use a proper parser like BeautifulSoup or lxml in python.
",20
7838290,7838436,2,"Here is an example using xml.etree.ElementTree:
>>> import xml.etree.ElementTree as ET
>>> data = """"""<text>
...     The 40-Year-Old Virgin is a 2005 American buddy comedy
...     film about a middle-aged man's journey to finally have sex.
... 
...     <h1>The plot</h1>
...     Andy Stitzer (Steve Carell) is the eponymous 40-year-old virgin.
...     <h1>Cast</h1>
... 
...     <h1>Soundtrack</h1>
... 
...     <h1>External Links</h1>
... </text>""""""
>>> xml = ET.XML(data)
>>> xml.text
""\n    The 40-Year-Old Virgin is a 2005 American buddy comedy\n    film about a middle-aged man's journey to finally have sex.\n\n    ""
>>> xml.text.strip().replace('\n   ', '')
""The 40-Year-Old Virgin is a 2005 American buddy comedy film about a middle-aged man's journey to finally have sex.""

And there you go!
",182
7838290,7838410,2,"Here is how you could do this using ElementTree:
In [18]: import xml.etree.ElementTree as et

In [19]: t = et.parse('f.xml')

In [20]: print t.getroot().text.strip()
The 40-Year-Old Virgin is a 2005 American buddy comedy
    film about a middle-aged man's journey to finally have sex.

",64
7838356,7841601,2,"Read this document:
http://docs.celeryproject.org/en/latest/internals/app-overview.html
The App is an instance of the Celery library, you can subclass app to override almost any part and corner of how Celery behaves.
",33
7838356,7838356,1,"Can somebody please explain me what is celery Application object, noted here and what purposes it should be used for?
",22
7838401,7841692,2,"Thanks Prusse,It works now - I have increased time limit from 1 to 200 on function queue(s) 
",22
7838401,7838401,1,"I'm using a python tool called ""Ajaxterm""(http://wiki.kartbuilding.net/index.php/Ajaxterm). After setting it up. The problem  , I face is,when type few chars - they appear as jumbled character.For example,When i type ""abcde"" it may come as ""cdbea"" - but no character missing.   From my understanding - I tried to debug the python code,it seems like ajaxterm internal server (qweb - QWebRequest Request Handler) processes the request it.There seems to be nothing wrong with it.
The problem is (I assume) - From .js file (javascript) to ajaxterm.py , for every character i type a connect() is initiated and thus  characters typed later may be arriving sooner than the previously typed character. 
How should I make sure,character appear in the order it was typed. 
(The problem is does (mostly) happen only when network is slow)
Thanks for any help.
",174
7838564,7838636,2,"You can use the getpass module. This doesn't exactly answer the question because the getpass function doesn't output anything to the console except for the prompt. The reason for this is that it's an extra layer of security. If someone is watching over your shoulder, they won't be able to figure out how long your password is.
Here's an example of how to use it:
from getpass import getpass
getpass('Enter your password: ')

This example will display ""Enter your password: "" and then you can type in your password.
",107
7838564,7838660,2,"The getpass module is written in Python. You could easily modify it to do this. In fact, here is a modified version of getpass.win_getpass() that you could just paste into your code:
import sys

def win_getpass(prompt='Password: ', stream=None):
    """"""Prompt for password with echo off, using Windows getch().""""""
    import msvcrt
    for c in prompt:
        msvcrt.putch(c)
    pw = """"
    while 1:
        c = msvcrt.getch()
        if c == '\r' or c == '\n':
            break
        if c == '\003':
            raise KeyboardInterrupt
        if c == '\b':
            pw = pw[:-1]
            msvcrt.putch('\b')
        else:
            pw = pw + c
            msvcrt.putch(""*"")
    msvcrt.putch('\r')
    msvcrt.putch('\n')
    return pw

You might want to reconsider this, however. The Linux way is better; even just knowing the number of characters in a password is a significant hint to someone who wants to crack it.
",192
7838564,16670956,2,"kindall's answer is close, but it has issues with backspace not erasing the asterisks, as well as backspace being able to go back beyond the input prompt.
Try:
def win_getpass(prompt='Password: ', stream=None):
    """"""Prompt for password with echo off, using Windows getch().""""""
    if sys.stdin is not sys.__stdin__:
        return fallback_getpass(prompt, stream)
    import msvcrt
    for c in prompt:
        msvcrt.putwch(c)
    pw = """"
    while 1:
        c = msvcrt.getwch()
        if c == '\r' or c == '\n':
            break
        if c == '\003':
            raise KeyboardInterrupt
        if c == '\b':
            if pw == '':
                pass
            else:
                pw = pw[:-1]
                msvcrt.putwch('\b')
                msvcrt.putwch("" "")
                msvcrt.putwch('\b')
        else:
            pw = pw + c
            msvcrt.putwch(""*"")
    msvcrt.putwch('\r')
    msvcrt.putwch('\n')
    return pw

Note mscvrt.putwch does not work with python 2.x, you need to use mscvrt.putch instead.
",195
7838564,7838564,1,"I'm writing a console program with Python under Windows.
The user need to login to use the program, when he input his password, I'd like they to be echoed as ""*"", while I can get what the user input.
I found in the standard library a module called getpass, but it will not echo anything when you input(linux like).
Thanks.
",75
7838606,7838606,1,"Edit 3: I replaced __file__ with sys.argv[0], when I need to know the location of my script/executable. This is not exactly the same, but in my case it seems to run fine (at least on executable version...). Now everything is working fine, in one-file mode, with use of accepted answer's function to access resource files!

Edit 2: as shown in accepted answer's comments, problem is coming from path resolution in my script; I try to use __file__ to get the location of the script, so that I can access to its resource files. This does not work once packaged, as __file__ will return filename from Python.dll to the script, so quite always no path and just a file name. So I have to find another trick to make access to resource files; a work-around for the moment is to move current directory to the executable path.
By the way, this means that the ConfigParser should report problem when accessing the file, and not that a section is missing.
I'll update this question with the way I resolved this path resolution question.

I've got problems with pyinstaller, and as it's the first time I'm using it, it's sure that I've done something wrong.
So, here's the problem: pyisntaller runs smoothly on a script I wrote, and generates some stuff in dist folder. Ok, so now I want to execute it to see if all went well, and here's what I get:
C:\Program Files\PyInstaller\pyinstaller-1.5.1>p_tool\dist\p_tool\p_tool.exe -?
Traceback (most recent call last):
  File ""<string>"", line 104, in <module>
  File ""p_tool\build\pyi.win32\p_tool\outPYZ1.pyz/logging.config"", line 76, in f
ileConfig
  File ""p_tool\build\pyi.win32\p_tool\outPYZ1.pyz/logging.config"", line 112, in
_create_formatters
  File ""p_tool\build\pyi.win32\p_tool\outPYZ1.pyz/ConfigParser"", line 532, in ge
t
ConfigParser.NoSectionError: No section: 'formatters'

My first idea was that the logging.conf file was missing, so I added it (and some other resource files) in the p_tool.spec file, but this is not better.
Python version: 2.6.6, under WinXP. I'm using pyinstaller as I will need it to package files for a Solaris workstation.
So, anyone did have this problem? The only topic related is the following question: PyInstaller Problem, really close to my problem, but hopelessly it got no answer.

Edit3: details about logging removed, as not really related to the problem.
",462
7838606,7878083,2,"The error message ConfigParser.NoSectionError: No section: 'formatters' suggests that it's not a missing file but a file with a missing section that you should be looking for.
",32
7838606,7878766,2,"I had a similar problem but couldn't find a elegant fix so far. The 'hack' I use that got me trough, say my project is located in '~/project/project_root', first in the .spec file:
excluded_sources = TOC([x for x in a.pure if not x[0].startswith('project_root')])

Here a is the Analysis object, basically I remove all of my projects files from the PYZ so no import is passed there and the logger's relative paths won't be computed from there. After this, create a Tree object from the project.
my_project_tree = Tree('~/project')

Then add this Tree to the list of TOC that is passed to COLLECT, so :
COLLECT( exe,
           a.binaries,
           a.zipfiles,
           a.datas,
           my_project_tree,
           ....)

You should have your project folder added to the dist folder. The problem is that you'll end up distributing the pyc's of your project too, but couldn't find a better way so far. Very interested in the valid solution. 
",196
7838606,7887341,2,"Firstly, it might be wise to do a print config_file / os.path.exists(config_file) before reading it, so you can be sure where the file is and if python can find it. 
As to actually accessing it, os.path.split(__file__) looks almost correct, but I'm not sure it works properly under pyinstaller - the proper way of packing files is to add them to the .spec file, pyinstaller will then load them at compile time and unpack them to $_MEIPASS2/ at run time. To get the _MEIPASS2 dir in packed-mode and use the local directory in unpacked (development) mode, I use this:
def resource_path(relative):
    return os.path.join(
        os.environ.get(
            ""_MEIPASS2"",
            os.path.abspath(""."")
        ),
        relative
    )


# in development
>>> resource_path(""logging.conf"")
""/home/shish/src/my_app/logging.conf""

# in deployment
>>> resource_path(""logging.conf"")
""/tmp/_MEI34121/logging.conf""

",172
7838629,7838629,1,"My possibilities are limited, as I do have a nice host but can just use the normal server plan. Which means, only a normal server on port 80.
I have tried to read up some on WebSockets and/or Comet, and they mostly seem to require a second server running on another port.
Is there a way to get a stable Comet-like behaviour that scales nicely. My solution up to now is a script that sends a GET request every 5 seconds, which is not a good way to make a web chat. And I am afraid it might kill my server when a few dozen people are online.
So how can I get a reliable comet-like behaviour?
",128
7838629,7839447,2,"I've had some success using socket.io for asynchronous web stuff (comet). For Django in particular, I don't have any personal experience, but I found a nice article about combining Gevent, Socket.io, and Django. Some other resources on Socket.io and Gevent can be found on my in a couple of my blog articles as well as a slideshare presentation.
",69
7838667,7838667,1,"I created a simple bookmarking app using django which uses sqlite3 as the database backend.
Can I upload it to appengine and use it? What is ""Django-nonrel""?
",32
7838667,7838935,2,"Unfortunately, no you can't. Google App Engine does not allow you to write files, and that is needed by SQLite.
Until recently, it had no support of SQL at all, preferring a home-grown solution (see the ""CAP theorem"" as for why). This motivated the creation of projects like ""Django-nonrel"" which is a version of Django that does not require a relational database.
Recently, they opened a beta service that proposes a MySQL database. But beware that it is fundamentally less reliable, and that it is probably going to be expensive.
EDIT: As Nick Johnson observed, this new service (Google Cloud SQL) is fundamentally less scalable, but not fundamentally less reliable.
",135
7838758,7838875,2,"In Fedora python is already installed. Just run in the command line:
 python ./yourProgram.py

",16
7838758,7838758,1,"How do I run a simple python program in fedora? 
I am very much new to fedora and i don't know how exactly to start in fedora. Do we need to install any software in fedora to make python programs work? Please tell me step by step how to do it. I am familiar with IDLE but not in fedora.
",66
7838758,7838878,2,"Put 
#!/usr/bin/env python

at the top of your file. Then on the command line do 
$ chmod u+x your_python_file.py

(I use $ to indicate a shell prompt, don't type that.) 
You can run the file with
$ ./yourpythonfile.py

You can also just do
$ python yourpythonfile.py

and don't need the #!/usr.... or $ chmod ... stuff, but the first approach is the natural way to do things in unix.
",84
7839723,7839723,1,"Version1
class ActionLog(db.Model):
    action = db.StringProperty()
    time_slice = db.IntegerProperty()
    trace_code = db.StringProperty()    # which profile this log belong to

    # Who
    facebook_id = db.StringProperty()   # the user's facebook id
    ip = db.StringProperty()            # the user's ip address

    # When
    time = db.DateTimeProperty(auto_now_add=True)   # the time of this event

    # What
    url = db.StringProperty()           # the imgurl
    secret = db.StringProperty()        # the secret of imgurl instance
    tag = db.StringProperty()           # the tag
    referurl = db.StringProperty()      # the tag's link

    # Where
    weburl = db.StringProperty()        # the user's refer url
    domain = db.StringProperty()        # the refer url's domain
    BSP = db.StringProperty()           # the refer url's BSP

#execute
log = ActionLog(action=action,
        trace_code=trace_code,
        facebook_id=facebook_id,
        ip=ip,
        time_slice=time_slice,
        url=url,
        secret=secret,
        tag=tag,
        referurl=referurl,
        weburl=weburl,
        domain=domain,
        BSP=BSP)

db.put(log)

Version 2
class ActionLog(db.Model):
    trace_code = db.StringProperty()
    url = db.StringProperty()
    secret = db.StringProperty()

    # use a dict like text property to store all implicit properties.
    desp = MyDictProperty() 
    time = db.DateTimeProperty(auto_now_add=True)   # the time of this event

#execute
log = ActionLog(
                    secret = secret,
                    url = url,
                    trace_code = trace_code,
                    desp = {
                            'action':action,
                            'facebook_id':facebook_id,
                            'ip':ip,
                            'tag':tag,
                            'referurl':referurl,
                            'weburl':weburl,                            
                            }
                    )

db.put(log)

These two versions of code basically do the same task, however, the version 1 code will use more than 800ms to perform a simple put operation (a yellow or red light) CPU time on google app engine. In the contract, the version 2 code only use about 300ms. (Both test on HRD datastore) 
On M/S Datastore, the version 1 code will use about 400ms and version 2 code will use about 150ms.
I can image that the version 1 will be slower compare to version 2, since it use more key index.  However, it is hard to believe that the difference is so huge. It is also surprising that Google app engine cannot handle such a easy task. 
Does that mean we cannot expect GAE to perform insert on data with more than 10 properties
or do I misunderstand anything?
thx
",445
7839723,7884415,2,"Your first model has 13 indexed properties, while your second one has only 5. It shouldn't be surprising that the first takes more time - you can reduce it by setting properties as unindexed, as Dave suggests.

Does that mean we cannot expect GAE to perform insert on data with
  more than 10 properties or do I misunderstand anything?

App Engine performs the insert just fine - you just have to be prepared for it to spend more time and cost more operations.
",91
7839723,7871059,2,"Set index=False on all properties that you don't need indexed (i.e., properties that you won't use in a query). This cuts down the number of index writes it takes to save an entity.
See http://code.google.com/appengine/docs/python/datastore/queries.html#Introduction_to_Indexes for an explanation.
",50
7839786,7843962,2,"Both execnet and Pyro mention PyPy <-> CPython communication. Other packages from Python Wiki's Parallel Processing page are probably suitable too.
",26
7839786,7840185,2,"Parallel Python might be worth a look, it works on Windows, OS X, and Linux (and I seem to recall I used it on a UltraSPARC Solaris 10 machine a while back).  I don't know if it works with PyPy, but it does seem to work with Psyco.
",57
7839786,7840047,2,"Native objects don't get shared between processes (due to reference counting).  
Instead, you can pickle them and share them using unix domain sockets, mmap, zeromq, or an intermediary such a sqlite3 that is designed for concurrent accesses.
",46
7839786,7839786,1,"What would be an inter-process communication (IPC) framework\technique with the following requirements:

Transfer native Python objects between two Python processes
Efficient in time and CPU (RAM efficiency irrelevant)
Cross-platform Win\Linux
Nice to have: works with PyPy

UPDATE 1: the processes are on the same host and use the same versions of Python and other modules
UPDATE 2: the processes are run independently by the user, no one of them spawns the others
",81
7839786,7839930,2,"Use multiprocessing to start with.
If you need multiple CPU's, look at celery.
",17
7843786,7843786,1,"I am using a package and it is returning me an array. When I print the shape it is (38845,). Just wondering why this ','.
I am wondering how to interpret this.
Thanks.
",43
7843786,7843805,2,"Python has tuples, which are like lists but of fixed size. A two-element tuple is (a, b); a three-element one is (a, b, c). However, (a) is just a in parentheses. To represent a one-element tuple, Python uses a slightly odd syntax of (a,). So there is only one dimension, and you have a bunch of elements in that one dimension.
",83
7843786,7843917,2,"
Just wondering why this ','.

Because (38845) is the same thing as 38845, but a tuple is expected here, not an int (since in general, your array could have multiple dimensions). (38845,) is a 1-tuple.
",50
7843786,7843818,2,"It seems you're talking of a Numpy array.
shape returns a tuple with the same size as the number of dimensions of the array. Each value of the tuple is the size of the array along the corresponding dimensions, or, as the tutorial says: 

An array has a shape given by the number of elements along each axis.

Here you have a 1D-array (as indicated with a 1-element tuple notation, with the coma (as @Amadan) said), and the size of the 1st (and only dimension) is 38845.
For example (3,4) would be a 2D-array of size 3 for the 1st dimension and 4 for the second.
You can check the documentation for shape here: http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html
",138
7843786,7843795,2,"It sounds like you're using Numpy. If so, the shape (38845,) means you have a 1-dimensional array, of size 38845.
",28
7843891,7843891,1,"I need to classify some values between two classes.
I have about 30 values that I can use as a training set and each value has 10 different dimensions.
I am using libSVM (in Python) and it seems that it works quite well.
I am trying also to give an interpretation to the model computed by libSVM, because I think that some dimensions are more ""important"" than others in the classification process.
For instance, consider the following example:
y, x = [1,1,1,-1,-1,-1],[[1,-1],[1,0],[1,1],[-1,-1],[-1,0],[-1,1]]
prob  = svm_problem(y, x)
param = svm_parameter()
param.kernel_type = LINEAR
param.C = 10
m = svm_train(prob, param)
svm_save_model('model_file', m)

It is clear that the second dimension of x list's elements is useless to classify this data set.
My question is:
is there any systematic way to detect these kind of situations analyzing the model generated by libSVM?
",208
7843891,10064716,2,"A little bit late, but:
It is your responsibility to check if a feature is important or not - so you have to choose your features manually that they meet your application's requirements. The SVM tries to get the best result with the features you put in - it wouldn't make much sense to ignore given data just because the choice will get clearer (but maybe more wrong).
Only you can know which features are good, and which not. You have to find them by hand/brain.
",97
7844118,31131547,2,"In the case of integers that are included at the string, if you want to avoid casting them to int individually you can do:
mList = [int(e) if e.isdigit() else e for e in mStr.split(',')]

It is called list comprehension, and it is based on set builder notation.
ex: 
>>> mStr = ""1,A,B,3,4""
>>> mList = [int(e) if e.isdigit() else e for e in mStr.split(',')]
>>> mList
>>> [1,'A','B',3,4]

",119
7844118,7844141,2,">>> some_string='A,B,C,D,E'
>>> new_tuple= tuple(some_string.split(','))
>>> new_tuple
('A', 'B', 'C', 'D', 'E')

",45
7844118,7844128,2,"You can use the str.split method.
>>> my_string = 'A,B,C,D,E'
>>> my_list = my_string.split("","")
>>> print my_list
['A', 'B', 'C', 'D', 'E']

If you want to convert it to a tuple, just
>>> print tuple(my_list)
('A', 'B', 'C', 'D', 'E')

If you are looking to append to a list, try this:
>>> my_list.append('F')
>>> print my_list
['A', 'B', 'C', 'D', 'E', 'F']

",133
7844118,7844118,1,"Given a string that is a sequence of several values separated by a commma:
mStr = 'A,B,C,D,E' 

How do I convert the string to a list?
mList = ['A', 'B', 'C', 'D', 'E']

",55
7844187,7864302,2,"After seeing the tenor of the answers here and remembering the Zen of Python, I'm going to answer my own dang question by saying, ""I probably should have just thought harder about it.""  
I will restate my own question as the answer.  Suppose I have this tiny program:
class Item(object):
    def __init__(self):
        self.broken = False

    def smash(self):
        print ""This object broke.""
        self.broken = True

class Person(object):
    def __init__(self, holding):
        self.holding = holding

    def using(self):
        if self.holding.broken != True:
            print ""Pass.""
        else:
            print ""Fail.""

Foo = Person(Item())
Bar = Person(Item())

Foo.holding.smash()
Foo.using()
Bar.using()

The program will return ""Fail"" for Foo.using() and ""Pass"" for Bar.using().  Upon actually thinking about what I'm doing, ""Foo.holding = Item()"" and ""Bar.holding = Item()"" are clearly different instances.  I even ran this dumpy program to prove it worked as I surmised it did, and no surprises to you pros, it does.  So I withdraw my question on the basis that I wasn't actually using my brain when I asked it.  The funny thing is, with the program I've been working on, I was already doing it this way but assuming it was the wrong way to do it.  So thanks for humoring me.
",287
7844187,7844236,2,"You have to create new instances of the Tool for each Student.
class Student(object):
    def __init__(self, tool):
        self.tool = tool

    def draw(self):
        if self.tool.broken != True:
            print ""I used my tool. Sweet.""
        else:
            print ""My tool is broken. Wah.""

class Tool(object):
    def __init__(self, name):
        self.name = name
        self.broken = False

    def break(self):
        print ""The %s busted."" % self.name
        self.broken = True

# Instead of instance, make it a callable that returns a new one
def Hammer():
    return Tool('hammer')

# Pass a new object, instead of the type
Billy = Student(Hammer())
Tommy = Student(Hammer())

",154
7844187,7844233,2,"Just call Tool('hammer') every time you want to create a new tool.
h1 = Tool('hammer')
h2 = Tool('hammer')
Billy = Student(h1)
Tommy = Student(h2)

",43
7844187,7844222,2,"You could change your lines like this:
Billy = Student(Tool('hammer'))
Tommy = Student(Tool('hammer'))

That'll produce a distinct instance of your Tool class for each instance of the Student class.  the trouble with your posted example code is that you haven't ""called the Tool into being"" (to use your words) more than once.
",75
7844187,7844187,1,"As an example, just a couple of dummy objects that will be used together.  FWIW this is using Python 2.7.2.
class Student(object):
    def __init__(self, tool):
        self.tool = tool

    def draw(self):
        if self.tool.broken != True:
            print ""I used my tool. Sweet.""
        else:
            print ""My tool is broken. Wah.""

class Tool(object):
    def __init__(self, name):
        self.name = name
        self.broken = False

    def break(self):
        print ""The %s busted."" % self.name
        self.broken = True

Hammer = Tool(hammer)
Billy = Student(Hammer)
Tommy = Student(Hammer)

That's probably enough code, you see where I'm going with this.  If I call Hammer.break(), I'm calling it on the same instance of the object; if Billy's hammer is broken, so is Tommy's (it's really the same Hammer after all).
Now obviously if the program were limited to just Billy and Tommy as instances of Students, the fix would be obvious - instantiate more Hammers.  But clearly I'm asking because it isn't that simple, heh.  I would like to know if it's possible to create objects which show up as unique instances of themselves for every time they're called into being.
EDIT: The kind of answers I'm getting lead me to believe that I have a gaping hole in my understanding of instantiation.  If I have something like this:
class Foo(object):
    pass

class Moo(Foo):
    pass

class Guy(object):
    def __init__(self, thing):
        self.thing = thing

Bill = Guy(Moo())
Steve = Guy(Moo())

Each time I use Moo(), is that a separate instance, or do they both reference the same object?  If they're separate, then my whole question can be withdrawn, because it'll ahve to make way for my mind getting blown.
",384
7844187,7845174,2,"Oh wait, I forgot, Python does have magic.
class Student:
    def __setattr__(self, attr, value):
        if attr == 'tool':
            self.__dict__[attr] = value.copy()
        else:
            self.__dict__[attr] = value

But I still say you should use magic sparingly.
",56
7844187,7844281,2,"
I'll try to be brief. Well.. I always try to be brief, but my level of success is pretty much random.randint(0, never). So yeah.

Lol. You even failed to be brief about announcing that you will try to be brief.
First, we need to be clear about what ""called into being"" means. Presumably you want a new hammer every time self.tool = object happens. You don't want a new instance every time, for example, you access the tool attribute, or you'd always a get a new, presumably unbroken, hammer every time you check self.tool.broken.
A couple approaches.
One, give Tool a copy method that produces a new object that should equal the original object, but be a different instance. For example:
class Tool:

    def __init__(self, kind):
        self.kind = kind
        self.broken = False

    def copy(self):
        result = Tool(self.kind)
        result.broken = self.broken
        return result

Then in Student's init you say
    self.tool = tool.copy()

Option two, use a factory function.
def makehammer():
    return Tool(hammer)

class Student:
    def __init__(self, factory):
        self.tool = factory()

Billy = Student(makehammer)

I can't think any way in Python that you can write the line self.tool = object and have object automagically make a copy, and I don't think you want to. One thing I like about Python is WYSIWYG. If you want magic use C++. I think it makes code hard to understand when you not only can't tell what a line of code is doing, you can't even tell it's doing anything special.
Note you can get even fancier with a factory object. For example:
class RealisticFactory:
    def __init__(self, kind, failurerate):
        self.kind = kind
        self.failurerate = failurerate

    def make(self):
        result = Tool(self.kind)
        if random.random() < self.failurerate:
            result.broken = True
        if (self.failurerate < 0.01):
            self.failurerate += 0.0001
        return result

factory = RealisticFactory(hammer, 0.0007)
Billy = Student(factory.make)
Tommy = Student(factory.make) # Tommy's tool is slightly more likely to be broken

",418
7845274,7845274,1,"I just found out about the following code I can enter into the command line:
python -m SimpleHTTPServer

Where can I find more documentation about the modules and what I can do with this?
",36
7845274,7845286,2,"Mostly in the source itself. There is almost no documentation on which can be used this way, and not all of them do anything useful.
",28
7845274,7845288,2,"In the main Python documentation, there is a note at the bottom of the SimpleHTTPServer documentation. Other modules such as pdb and timeit have similar notes.
",29
7845274,7845405,2,"The overall -m command line feature is documented here and here.  You can use it for you own modules. As noted, for individual standard lib modules it's best to consult their source code.
",38
7845274,7845599,2,"Try with:
python2 -m SimpleHTTPServer

Or in Python 3:
python -m http.server

",14
7849077,7849077,1,"I have a 3D (time, X, Y) numpy array containing 6 hourly time series for a few years. (say 5). I would like to create a sampled time series containing 1 instance of each calendar day randomly taken from the available records (5 possibilities per day), as follows.

Jan 01: 2006
Jan 02: 2011
Jan 03: 2009
...

this means I need to take 4 values from 01/01/2006, 4 values from 02/01/2011, etc.
I have a working version which works as follows:

Reshape the input array to add a ""year"" dimension (Time, Year, X, Y)
Create a 365 values array of randomly generated integers between 0 and 4
Use np.repeat and array of integers to extract only the relevant values:

Example:
sampledValues = Variable[np.arange(numberOfDays * ValuesPerDays), sampledYears.repeat(ValuesPerDays),:,:]

This seems to work, but I was wondering if this is the best/fastest approach to solve my problem? Speed is important as I am doing this in a loop, adn would benefit from testing as many cases as possible.
Am I doing this right?
Thanks
EDIT
I forgot to mention that I filtered the input dataset to remove the 29th of feb for leap years.
Basically the aim of that operation is to find a 365 days sample that matches well the long term time series in terms on mean etc. If the sampled time series passes my quality test, I want to export it and start again.
",286
7849077,7849200,2,"The year 2008 was 366 days long, so don't reshape.
Have a look at scikits.timeseries:
import scikits.timeseries as ts

start_date = ts.Date('H', '2006-01-01 00:00')
end_date = ts.Date('H', '2010-12-31 18:00')
arr3d = ... # your 3D array [time, X, Y]

dates = ts.date_array(start_date=start_date, end_date=end_date, freq='H')[::6]
t = ts.time_series(arr3d, dates=dates)
# just make sure arr3d.shape[0] == len(dates) !

Now you can access the t data with day/month/year objects:
t[np.logical_and(t.day == 1, t.month == 1)]

so for example:
for day_of_year in xrange(1, 366):
    year = np.random.randint(2006, 2011)

    t[np.logical_and(t.day_of_year == day_of_year, t.year == year)]
    # returns a [4, X, Y] array with data from that day

Play with the attributes of t to make it work with leap years too.
",186
7849077,7849448,2,"I don't see a real need to reshape the array, since you can embed the year-size information in your sampling process, and leave the array with its original shape.
For example, you can generate a random offset (from 0 to 365), and pick the slice with index, say, n*365 + offset.
Anyway, I don't think your question is complete, because I didn't quite understand what you need to do, or why.
",88
7849117,7854400,2,"Source: http://code.activestate.com/recipes/215418-watching-a-directory-tree-on-unix/
The watch_directories() function takes a list of paths and a callable object, and then repeatedly traverses the directory trees rooted at those paths, watching for files that get deleted or have their modification time changed. The callable object is then passed two lists containing the files that have changed and the files that have been removed.
from __future__ import nested_scopes

import os, time

def watch_directories (paths, func, delay=1.0):
    """"""(paths:[str], func:callable, delay:float)
    Continuously monitors the paths and their subdirectories
    for changes.  If any files or directories are modified,
    the callable 'func' is called with a list of the modified paths of both
    files and directories.  'func' can return a Boolean value
    for rescanning; if it returns True, the directory tree will be
    rescanned without calling func() for any found changes.
    (This is so func() can write changes into the tree and prevent itself
    from being immediately called again.)
    """"""

    # Basic principle: all_files is a dictionary mapping paths to
    # modification times.  We repeatedly crawl through the directory
    # tree rooted at 'path', doing a stat() on each file and comparing
    # the modification time.  

    all_files = {}
    def f (unused, dirname, files):
        # Traversal function for directories
        for filename in files:
            path = os.path.join(dirname, filename)

            try:
                t = os.stat(path)
            except os.error:
                # If a file has been deleted between os.path.walk()
                # scanning the directory and now, we'll get an
                # os.error here.  Just ignore it -- we'll report
                # the deletion on the next pass through the main loop.
                continue

            mtime = remaining_files.get(path)
            if mtime is not None:
                # Record this file as having been seen
                del remaining_files[path]
                # File's mtime has been changed since we last looked at it.
                if t.st_mtime > mtime:
                    changed_list.append(path)
            else:
                # No recorded modification time, so it must be
                # a brand new file.
                changed_list.append(path)

            # Record current mtime of file.
            all_files[path] = t.st_mtime

    # Main loop
    rescan = False
    while True:
        changed_list = []
        remaining_files = all_files.copy()
        all_files = {}
        for path in paths:
            os.path.walk(path, f, None)
        removed_list = remaining_files.keys()
        if rescan:
            rescan = False
        elif changed_list or removed_list:
            rescan = func(changed_list, removed_list)

        time.sleep(delay)

if __name__ == '__main__':
    def f (changed_files, removed_files):
        print changed_files
        print 'Removed', removed_files

    watch_directories(['.'], f, 1)

This recipe is useful where you'd like some way to send jobs to a daemon, but don't want to use some IPC mechanism such as sockets or pipes. Instead, the daemon can sit and watch a submission directory, and jobs can be submitted by dropping a file or directory into the submission directory.
Locking is not taken into account. The watch_directories() function itself doesn't really need to do locking; if it misses a modification on one pass, it'll notice it on the next pass. However, if jobs are written directly into a watched directory, the callable object might start running while a job file is only half-written. To solve this, you can use a lockfile; the callable must acquire the lock when it runs, and submitters must acquire the lock when they wish to add a new job. A simpler approach is to rely on the rename() system call being atomic: write the job into a temporary directory that isn't being watched, and once the file is complete use os.rename() to move it into the submission directory.
",715
7849117,7849117,1,"Ex, I need to catch remove and add files events on some directory on linux os. I found libs like inotify and python wrappers for them, but if I want to use clear python code should I watch for os.listdir(path) output every sec or are there some ways to accomplish such task?
",59
7849145,7877517,2,"You already have the proper 'João', methinks. The difference between >>> 'Jo\xc3\xa3o' and >>> print 'Jo\xc3\xa3o' is that the former calls repr on the object, while the latter calls str (or probably unicode, in your case). It's just how the string is represented.
Some examples might make this more clear:
>>> print 'Jo\xc3\xa3o'.decode('utf-8')
João
>>> 'Jo\xc3\xa3o'.decode('utf-8')
u'Jo\xe3o'
>>> print repr('Jo\xc3\xa3o'.decode('utf-8'))
u'Jo\xe3o'

Notice how the second and third result are identical. The original ldap_username currently is an ASCII string. You can see this on the Python prompt: when it is displaying an ACSII object, it shows as 'ASCII string', while Unicode objects are shown as u'Unicode string' -- the key being the leading u.
So, as your ldap_username reads as 'Jo\xc3\xa3o', and is an ASCII string, the following applies:
>>> 'Jo\xc3\xa3o'.decode('utf-8')
u'Jo\xe3o'
>>> print 'Jo\xc3\xa3o'.decode('utf-8') # To Unicode...
João
>>> u'João'.encode('utf-8')             # ... back to ASCII
'Jo\xc3\xa3o'

Summed up: you need to determine the type of the string (use type when unsure), and based on that, decode to Unicode, or encode to ASCII.
",249
7849145,7849145,1,"EDIT: 
The following print shows my intended value.
(both sys.stdout.encoding and sys.stdin.encoding are 'UTF-8').
Why is the variable value different than its print value? I need to get the raw value into a variable.
>>username = 'Jo\xc3\xa3o'
>>username.decode('utf-8').encode('latin-1')
'Jo\xe3o'
>>print username.decode('utf-8').encode('latin-1')
João

Original question:
I'm having an issue querying a BD and decoding the values into Python.
I confirmed my DB NLS_LANG using
select property_value from database_properties where property_name='NLS_CHARACTERSET';

'''AL32UTF8 stores characters beyond U+FFFF as four bytes (exactly as Unicode defines 
UTF-8). Oracle’s “UTF8” stores these characters as a sequence of two UTF-16 surrogate
characters encoded using UTF-8 (or six bytes per character)'''

os.environ[""NLS_LANG""] = "".AL32UTF8""

....
conn_data = str('%s/%s@%s') % (db_usr, db_pwd, db_sid)

sql = ""select user_name apex.users where user_id = '%s'"" % userid

...

cursor.execute(sql)
ldap_username = cursor.fetchone()
...

where 
print ldap_username
>>'Jo\xc3\xa3o'

I've both tried (which return the same)
ldap_username.decode('utf-8')
>>u'Jo\xe3o'
unicode(ldap_username, 'utf-8')
>>u'Jo\xe3o'

where
u'João'.encode('utf-8')
>>'Jo\xc3\xa3o'

how to get the queries result back to the proper 'João' ?
",270
7849169,7849169,1,"I am working with four tables:
class Product(models.Model):
    active = models.BooleanField(default=True)
    name = models.CharField(max_length=40, unique=True)
    acronym = models.CharField(max_length=3, unique=True)

class Alarm(models.Model):
    active = models.BooleanField(default=True)
    product = models.ForeignKey(Product) #NEW
    name = models.CharField(unique=True, max_length=35)
    image_file = models.CharField(max_length=30)

class File(models.Model):
    reference = models.CharField(max_length=40)
    product = models.ForeignKey(Product, related_name='item_product') #NEW
    created = models.DateTimeField(default=datetime.now)
    created_by = models.ForeignKey(User)

class FileAlarm(models.Model):
    file = models.ForeignKey(File)
    created = models.DateTimeField()
    created_by = models.ForeignKey(User, related_name=""alarm_created_by"")
    alarm_date = models.DateField(default=datetime.now)
    alarm_type = models.ForeignKey(Alarm)
    alarm_comment = models.CharField(max_length=80, blank=True, null=True)

Unfortunatly I need to pass an onChange event to the form element of FileAlarm.alarm_type (which means the queryset in __init__ wont work) so I need to populate the choices= variable of ChoiceField. I'm just wondering how to do that since I only want alarms that are active and linked to the same products as the File which the FileAlarm is linked.
class FileAlarmForm(ModelForm):

    def __init__(self,file,*args,**kwargs):
        super(FileAlarmForm, self).__init__(*args, **kwargs)
        self.fields['alarm'].queryset = Alarm.objects.filter(product__id=file.product_id)

    alarm_type = ChoiceField(required=True, widget=Select(attrs={'onChange':'updateAlarmImage'}))  # << ?!?

",285
7849169,7849859,2,"It turns out the best way round it was:
class FileAlarmForm(ModelForm):

    def __init__(self,file,*args,**kwargs):
        super(FileAlarmForm, self).__init__(*args, **kwargs)
        p = Product.objects.get(id=file.product_id)
        self.fields['alarm_type'].widget = Select(attrs={'onchange':'updateAlarmImage'})
        self.fields['alarm_type'].queryset = Alarm.objects.filter(product=p)

",75
7850908,7850908,1,"I'm going through and writing a setup doc for other developers at work for a python project and I've been reading up on the PYTHONPATH environment variable. I'm looking at my current development system and think I have a few things set wrong that is causing my IDE (IntelliJ) to behave incorrectly when looking up the python libraries.
I've looked at the documentation here and here and I'm still unsure of what should actually be in the PYTHONPATH environment variable. 
I have PYTHONHOME pointed to `C:\Python27'.
My current PYTHONPATH is set to PYTHONHOME. Should I also add the directories from sys.path?
UPDATE:
Based on the below information, PYTHONPATH does not need to be set unless there are non-standard libraries that you want python to be able to find by default. For instance, when I install wxPython from the installer it will add its libraries to PYTHONPATH. I do set PYTHONHOME to the root of the python installation so that I can add it to my system PATH environment variable so that I can run python from any where.
",200
7850908,7850960,2,"You don't have to set either of them.  PYTHONPATH can be set to point to additional directories with private libraries in them.  If PYTHONHOME is not set, Python defaults to using the directory where python.exe was found, so that dir should be in PATH.
",50
7850908,7851585,2,"For most installations, you should not set these variables since they are not needed for Python to run.  Python knows where to find its standard library.
The only reason to set PYTHONPATH is to maintain directories of custom Python libraries that you do not want to install in the global default location (i.e., the site-packages directory).
Make sure to read: http://docs.python.org/using/cmdline.html#environment-variables
",73
7854945,7854945,1,"I need to be able to manage the supervisord setup programmatically. Furthermore, not any user on the system should be able to to gain access to configuration of supervisord. For this reason, communication needs to be secured somehow.
I know that supervisord offers programmatic access in the form of XML-RPC. I read the documentation and attempted to work with it in several ways, but I keep running into problems.

On the Introduction page, the documentation recommends running an HTTP server for the XML-RPC interface and using the Python standard library xmlrpclib to communicate with it. There are two problems here:

The inet_http_server directive for supervisord.conf only includes username, password, and port as settings. There is no option to encrypt the connection.
xmlrpclib doesn't even support usernames and passwords. When I use the syntax username:password@host:port, I get IOError: unsupported XML-RPC protocol. As you can see in the example on the documentation page, no authentication occurs.

Since UNIX sockets are secure, I figured that connecting to the [unix_http_server] with xmlrpclib would be a good idea. Still, I don't know how authentication would work, and furthermore, xmlrpclib only supports network HTTP/HTTPS servers.
Another page in the documentation mentions a supervisor.rpcinterface module. I have no access to such a thing in Python, though. To glean more information as to why that is, I re-installed supervisord with Pip. sudo pip install --upgrade supervisor. In the pip output, I see the line Skipping installation of /usr/local/lib/python2.6/dist-packages/supervisor/__init__.py (namespace package). I don't know why it would skip installation of the namespace package.

How am I supposed to communicate programmatically and securely with supervisord?
",316
7854945,7932112,2,"Supervisor supports options to set permissions on the Unix domain socket.
http://supervisord.org/configuration.html#unix-http-server-section-example
I don't know the details but you should be able to call the xmlrpc interface over UNIX domain socket the same way that supervisorctl.py does.  It's calling options.getServerProxy() to get an xmlrpclib.ServerProxy object.
https://github.com/Supervisor/supervisor/blob/master/supervisor/supervisorctl.py#L188
",61
7855099,7855099,1,"How to define that?
What should be used instead of

get_by_key_name
SELECT __key__
key().name()

",20
7855099,7855122,2,"
get_by__id()
no change; it's still a key whether it has an ID or a name
key().id() 

",25
7855229,40452621,2,"To create the plot you want, we need to use matplotlib's plot_surface to plot Z vs (X,Y) surface, and then use the keyword argument facecolors to pass in a new color for each patch.
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm

# create some fake data
x = y = np.arange(-4.0, 4.0, 0.02)
# here are the x,y and respective z values
X, Y = np.meshgrid(x, y)
Z = np.sinc(np.sqrt(X*X+Y*Y))
# this is the value to use for the color
V = np.sin(Y)

# create the figure, add a 3d axis, set the viewing angle
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.view_init(45,60)

# here we create the surface plot, but pass V through a colormap
# to create a different color for each patch
ax.plot_surface(X, Y, Z, facecolors=cm.Oranges(V))


",189
7855229,7855229,1,"I am looking for a way to create four-dimensional plots (surface plus a color scale) using Python and matplotlib.  I am able to generate the surface using the first three variables, but I am not having success adding the color scale for the fourth variable.  Here is a small subset of my data below.  Any help would be greatly appreciated.  Thanks
Data Subset         
var1    var2    var3    var4
10.39   73.32   2.02    28.26
11.13   68.71   1.86    27.83
12.71   74.27   1.89    28.26
11.46   91.06   1.63    28.26
11.72   85.38   1.51    28.26
13.39   78.68   1.89    28.26
13.02   68.02   2.01    28.26
12.08   64.37   2.18    28.26
11.58   60.71   2.28    28.26
8.94    65.67   1.92    27.04
11.61   59.57   2.32    27.52
19.06   74.49   1.69    63.35
17.52   73.62   1.73    63.51
19.52   71.52   1.79    63.51
18.76   67.55   1.86    63.51
19.84   53.34   2.3     63.51
20.19   59.82   1.97    63.51
17.43   57.89   2.05    63.38
17.9    59.95   1.89    63.51
18.97   57.84   2       63.51
19.22   57.74   2.05    63.51
17.55   55.66   1.99    63.51
19.22   101.31  6.76    94.29
19.41   99.47   6.07    94.15
18.99   94.01   7.32    94.08
19.88   103.57  6.98    94.58
19.08   95.38   5.66    94.14
20.36   100.43  6.13    94.47
20.13   98.78   7.37    94.47
20.36   89.36   8.79    94.71
20.96   84.48   8.33    94.01
21.02   83.97   6.78    94.72
19.6    95.64   6.56    94.57

",206
7855235,8017058,2,"One way to achieve what you want to do is to use regex in your url and check for the attribute in your methods handler.
Example of urls to map
url_patterns = [
    # here we want to map url1 url2 and url
    (r""/url([1|2])/"", OneAndTwoHandler),
]

And an example of the corresponding handler
class OneAndTwoHandler(CustomRequestHandler):
    def get(self, my_param, *args, **kwargs):
        if my_param == '2':
            raise HTTPError(405)
        # code for only the first url here...

    def post(self, entry, *args, **kwargs):
        if my_param == '1':
            raise HTTPError(405)
        # code for only the first url here...

I would map 'my_param' options to a dictionary to keep things clear and to avoid me to dive into the handler if I need to change these values or if I want to add new urls.
my_dict_urls = {
    'get': (1,2,3,4),
    'post': (3,5)
}

if int(my_param) not in my_dict_urls.get('get'):
    # ...

",207
7855235,7865558,2,"You can use @ee_vin's answer to do this. However, in this situation, why not create two handlers? It's much simpler:
class OneHandler():
   def get(self):
       #only for the first url

class TwoHandler():
   def post(self):
       #only for the second url

handlers = [
   (r""/url1"",OneHandler), #only GET are allowed
   (r""/url2"",TwoHandler), #only POST are allowed
]

Anyone posting to the first URL or GETting the second would get a method not supported error.
",112
7855235,7855235,1,"I'm trying to implement some sort of API on tornado and I have such question:
is it possible to route two urls to one handler separating by method.
class Handler():
   def get(self):
       #only for the first url
   def post(self):
       #only for the secornd url
handlers = [
   (r""/url1"",Handler), #only GET are allowed
   (r""/url2"",Handler), #only POST are allowed
]

So if someone trying to send POST to the first url he should see error message
",108
7855237,7855326,2,"It's possible that the helper function better fits in at the module level rather than the class.
If you don't agree that this is the case, there is a staticmethod decorator that you can use on functions inside of the class. Simply put, a static method behaves the same between object instantiations of the same class. It does not rely on instance data.
For this reason, the staticmethod decorator renders behavior on the function such that it does not take an implicit first argument (typically self) as stated in the documentation).
",104
7855237,7855829,2,"When deciding where to put helper functions the question I ask is, ""Is it only for this class?""  If it can help in other places, then it goes at the module level; if it is indeed only for this class, then it goes in the class with either staticmethod (needs no class data to do its job) or classmethod (uses some class, but not instance, data to do its job).
Another python code checker is pyflakes.
",91
7855237,7855237,1,"In Python, if some methods of a class need a helper function, but the helper function itself doesn't use anything in the class, should I put the helper function inside or outside the class?
I tried putting it inside but PyLint was complaining that this function could have been put outside... 
Also, are there any good books talking about this kind of stuff (it doesn't have to be Python)? 
Thanks for your help!
Jack
@Karl:
The class is a software upgrader and the helper function creates a new folder if the folder doesn't exist yet. The class is in a module having pretty much only the code for the class as of now. Other classes may be added later on.
Thanks,
Jack
",141
7855293,7855379,2,"It will get serious. See PEP 4 for the details. But DeprecationWarnings tell you that some functionality is about to change in the next Python version.
",29
7855293,7855701,2,"No worries.  The warning is about something in the standard library that was already fixed in Python 2.7.  You can safely ignore it :-)
The sets.py is part of the standard library.  Line 85 is just a warning that the sets module is deprecated in favor of the set() builtin method but that won't disappear until Python3.0.
",66
7855293,7855293,1,"I ran some python code and got this error message:
C:\Python26\lib\sets.py:85: DeprecationWarning: functions overriding warnings.showwarning() must support the 'line' argument
  stacklevel=2)
I am not sure if this is some warning that I can just ignore or if this is serious? Any input will be appreciated. Thanks
",57
7855343,7855343,1,"I want to try playing around with gevent as a web server and application framework.  I don't see any way to ""restart"" the server or update the application code without killing and starting the whole python application again.
Is this just how it's done?  Maybe it's just a matter of me understanding a different paradigm to the apache way.
Also, as a semi-related question, is it even a good idea to run a web server AND the site/service itself through gevent.  I've seen other implementations using gunicorn for the server and gevent for the application but from the benchmarks I've seen, gevent far outperforms gunicorn as a server when it comes to scaling.
",129
7855343,7857201,2,"Gunicorn has 3 gevent workers:

-k gevent   (using gunicorn's HTTP parser)
-k gevent_pywsgi   (using gevent.pywsgi module)
-k gevent_wsgi     (using gevent.wsgi module)

gevent.wsgi is a fast HTTP server based on libevent.
gevent.pywsgi is WSGI server implemented in Python.
The reason for existence of gevent.pywsgi is libevent-http having a few limitations, such as not supporting keep-alive, streaming, ssl and websockets.
Note, that the new alpha version (1.0a3) of gevent uses libev and does not  include a WSGI server based on libevent-http. Currently, gevent.wsgi here is an alias for gevent.pywsgi.
The server classes in gevent don't have any features related to process management, restart, reload and so on. Those features are necessary for deployment though. Gunicorn provides that for gevent's WSGI servers. Use it.
",149
7855534,7855534,1,"I have a list of objects and for each of them I want to call their foo function, passing it the argument bar. I want to parallelize this operation, so right now I'm looking at using Pool.map from the multiprocessing package. I'm not sure how to use map to run an object method, though. How can I do that? Or is there a better way to do this in parallel?
",80
7855534,7855729,2,"Define a helper function and pass the object method and its arguments to that function via Pool.map. The helper function would look something like this:
def helper(*args):
    return args[0](*args[1:])

And you would use it like this:
pool = Pool()
results = pool.map(helper, [obj.method, arg1, arg2])

Note that the helper function must be directly importable from its containing module.
",86
7856843,7856843,1,"I've been following a tutorial but keep getting the following error
AttributeError: Worm instance has no attribute 'move'
I'm not sure exactly what it means or how to fix it. The error refers to line 44 towards the bottom the line is w.move()(this one's solved look below)
import pygame

class Worm:
    """"""A Worm.""""""
    def __init__(self, surface, x, y, length):
        self.surface = surface
        self.x = x
        self.y = y
        self.length = length
        self.dir_x = 0
        self.dir_y = -1
        self.body = []
        self.crashed = False

    def key_event(self, event):
        """"""Handle Key events that affect the worm.""""""
        if event.key == pygame.K_UP:
            self.dir_x = 0
            self.dir_y = -1
        elif event.key == pygame.K_DOWN:
            self.dir_x = 0
            self.dir_y = 1
        elif event.key == pygame.K_DOWN:
            self.dir_x = -1
            self.dir_y = 0
        elif event.key == pygame.K_DOWN:
            self.dir_x = 1
            self.dir_y = 0 
    def draw(self):
        for x, y in self.body:
            self.surface.set_at((x, y), (255, 255, 255))

width = 640
height = 400

screen = pygame.display.set_mode((width, height))
clock = pygame.time.Clock()
running = True

w = Worm(screen, width/2, height/2, 200)

while running:
    screen.fill((0, 0, 0))
    w.move()
    w.draw()

    if w.crashed or w.x <= 0 or w.x >= width -1 or w.y <= 0 or w.y >= height -1:
        print ""crash""
        running = False

    for event in  pygame.event.get():
        if event.type == pygame.QUIT:
            running = False
        elif event.type == pygame.KEYDOWN:
            w.key_event(event)

    pygame.display.flip()
    clock.tick(240)

----------Change --------
code:
import pygame

class Worm:
    """"""A Worm.""""""
    def __init__(self, surface, x, y, length):
        self.surface = surface
        self.x = x
        self.y = y
        self.length = length
        self.dir_x = 0
        self.dir_y = -1
        self.body = []
        self.crashed = False

    def key_event(self, event):
        """"""Handle Key events that affect the worm.""""""
        if event.key == pygame.K_UP:
            self.dir_x = 0
            self.dir_y = -1
        elif event.key == pygame.K_DOWN:
            self.dir_x = 0
            self.dir_y = 1
        elif event.key == pygame.K_DOWN:
            self.dir_x = -1
            self.dir_y = 0
        elif event.key == pygame.K_DOWN:
            self.dir_x = 1
            self.dir_y = 0 
    def draw(self):
        for x, y in self.body:
            self.surface.set_at((x, y), (255, 255, 255))
    def move(self):
        """"""move worm.""""""
        self.x += self.vx
        self.y += self.vy

        if (self.x, sel.y) in self.body:
            self.crashed = True

        self.body.insert(0, (self.x, self.y))

        if len(self.body) > self.length:
            self.body.pop()

    def draw(self):
        #for x, y self.body:
        #    self.surface.set_at((x, y),self.color)
        x, y = self.body[0]
        self.surface.set_at((x, y), self.color)
        x, y = self.body[-1]
        self.surface.set_at((x, y), (0, 0, 0))


width = 640
height = 400

screen = pygame.display.set_mode((width, height))
clock = pygame.time.Clock()
running = True

w = Worm(screen, width/2, height/2, 200)

while running:
    screen.fill((0, 0, 0))
    w.move()
    w.draw()

    if w.crashed or w.x <= 0 or w.x >= width -1 or w.y <= 0 or w.y >= height -1:
        print ""crash""
        running = False

    for event in  pygame.event.get():
        if event.type == pygame.QUIT:
            running = False
        elif event.type == pygame.KEYDOWN:
            w.key_event(event)

    pygame.display.flip()
    clock.tick(240)

and Error - 
Traceback (most recent call last):
  File ""C:/Users/Enrique/Dropbox/Public/snakegametutorial.py"", line 65, in <module>
    w.move()
  File ""C:/Users/Enrique/Dropbox/Public/snakegametutorial.py"", line 34, in move
    self.x += self.vx
AttributeError: Worm instance has no attribute 'vx'

",773
7856843,14293714,2,"dir_x and dir_y are vx and vy you should change them... to vx and vy...
",17
7856843,7856877,2,"AttributeError indicates that you attempted to access a property or method on an object that was not defined in the object's class definition.
It just seems like you have not progressed far enough in the tutorial code to have defined the Worm.move() method.  It occurs at line 43 of the tutorial, just before Worm.draw().  You are headed for another AttributeError on the draw() method, as you've not yet defined that one either. Just add both of these to the Worm class definition.
 43     def move(self):
 44         """""" Move the worm. """"""
 45         self.x += self.vx
 46         self.y += self.vy
 47 
 48         if (self.x, self.y) in self.body:
 49             self.crashed = True
 50 
 51         self.body.insert(0, (self.x, self.y))
 52 
 53         if (self.grow_to > self.length):
 54             self.length += 1
 55 
 56         if len(self.body) > self.length:
 57             self.body.pop()
 58
 59     def draw(self):
 60         #for x, y in self.body:
 61         #    self.surface.set_at((x, y), self.color)
 62         x, y = self.body[0]
 63         self.surface.set_at((x, y), self.color)
 64         x, y = self.body[-1]
 65         self.surface.set_at((x, y), (0, 0, 0))

Update
You're now receiving the AttributeError on Worm.vx  because you're missing that property (also vy) from Worm.__init__(). Compare your code to the code under the heading The improved game on the tutorial page. When you encounter further errors, compare your class definition to the tutorial's.
Add to __init__()
def __init__(self, surface):
    ...
    ...
    self.vx = 0
    self.vy = -1
    ...
    ...

",330
7856853,7857662,2,"Here's how I'd write it:
search = re.compile(r'^The.*said').search
match = search(input)
if match:
    match = match.group(0)

If input is ""The cat said my name"", match will be ""The cat said"".
If input is ""The cat never mentioned my name"", match will be None.
I really like the fact that Python makes it possible to compile a regular expression and assign the particular method of interest to a variable in one line.
",99
7856853,7856915,2,"Does this work for you?
instancesFound = compiledRegex.findall(origStr)
if instancesFound:
    parsedStr = parsedParts[0]

",21
7856853,7856926,2,"Use the group method on the match object:
>>> import re
>>> origStr = ""The cat said hi""
>>> compiledRegex = re.compile('The.*said')
>>> compiledRegex.match(origStr).group()
'The cat said'

",51
7856853,7856853,1,"Is there a more pythonic way than doing:
 parsedStr=origStr[compiledRegex.match(origStr).start():compiledRegex.match(origStr).end())

For exampile assume my original string is ""The cat said hi"" and my compiled regex is ""The.*said"" I would pull the text ""The cat said""
The above code looks ugly but that's how i've been doing it
",74
7856918,7856918,1,"I have just picked up Python to develop a tool and I am so far really enjoying the language, however have one issue I am not entirely sure how to solve.
I am looking to use a few external libraries in my project, at the moment cherryPy and Cheetah however I am not sure how to package up my application so that these libraries are included. Coming from a .NET world the compiler used to do pretty much everything for me.
Have done a bit of googling but have not been able to find any solution, so I must be missing something fundamental. Is this something I need to configure distutils for? Do I need to copy the libs in to my application folder structure anywhere? Both?
Appreciate any advice please. :)
",144
7856918,7862053,2,"I think what you're asking is how to create what Python calls a Built Distribution.  This can be done with distutils and it is tedious.  And again I'm guessing at the question, but I think you'd benefit from the docs about describing extension modules.
Besides that, I don't think a broad answer would be complete without pointing at py2exe and py2app -- utilities to create standalone executables for Windows and OS X, respectively.
",84
7856949,7876050,2,"It's documented in WTForms documentation of the SelectField quoted here for convenience:

Select fields keep a choices property which is a sequence of (value,
  label) pairs.

I'm not sure about form.parent.choices syntax but the code looks like:
form.parent.choices = [(1, 'parent name 1'), (2, 'parent name 2'), (3, 'parent name 3'), (4, 'parent name 4')]

",84
7856949,7856949,1,"I am testing out Python framework Flask and Flask-MongoAlchemy with MongoDB (of course). As I'm building multiple documents in my test app, I like to get the forms validated us WTForms.  
Can anyone share with me an example on how to create the object references in a SelectField()?
class Parent(db.Document):
    title = db.StringField()
    description = db.StringField()

class Object(db.Document):
    parent = db.DocumentField(Parent)
    title = db.StringField()

@app.route('/object/new', methods=['GET', 'POST'])
def new_object():
    form = ObjectForm(obj=Object)
    form.parent.choices = [(???) for p in Parent.query.all()]  #<-- #1 correct syntax I like to understand, '(t._id, t.title)' didn't work.
    if form.validate_on_submit():
        form.save()
        return redirect(url_for('...'))
    return ....

class ObjectForm(wtf.Form):
    parent = wtf.SelectField(u'Parent')  #<-- #2 do I need to add anything special?

Any suggestion would be great!  Or link to an online example.  Thanks!
",216
7857000,7857701,2,"(.*) doesn't mean anything specific in Python. However, it can mean specific things to certain functions when a part of a string. Hence '(.*)' might mean something to a function, although it means nothing to Python itself. Since
Two functions that do take strings containing (.*) are glob.glob, fnmatch.fnmatch and the re modules functions.
In glob and fnmatch it is '*' that has special meaning, it means ""anything"". You typically use it to match filenames:
>>> import glob
>>> glob.glob('/tmp/foobar.*')
['/tmp/foobar.tmp', '/tmp/foobar.txt', '/tmp/foobar.conf']

And you can also list everything with a specific ending:
>>> import glob
>>> glob.glob('/tmp/*.txt')
['/tmp/foobar.txt', '/tmp/frotz.txt', '/tmp/wfsh.txt']

Hence, in these modules '(.*)' would mean anything starts with (. and ends with ) with anything in between it.
In the re module you handle regular expressions. regular expressions is a highly magical text matching language. There '.' means ""any character (except newlines, unless you set a special flag to make it mean newlines as well)"", and '*' means ""zero to infinite amount of repetitions of the previous match"". Hence '.*' means ""pretty much anything"", and is a common thing to stick into regular expressions, as 'foobar.*' would mean anything that start with foobar.
Parenthesis means ""groups"", and to know what that mean you'll just have to read the documentation. You probably won't get it without some experimentation so you see what happens.
Basically '(.*)' matches anything. I'm assuming your regular expression has text before and after it as well?
",348
7857000,7857078,2,"This is a really weird way of explaining it but:    .* when used in files means a group of files... like if you used 
fruits.* instead of fruits.apple, it would be like saying fruits.apples, fruits.oranges, fruits.bananas, and any other files in the fruits group. 
",54
7857000,7857000,1,"I was reading a learning python book, and this was in one of the examples so I was wonder if this meant something.
",25
7861208,7861235,2,"decimal.Decimal allows you to use a specific precision.
>>> decimal.Decimal('12.000')
Decimal('12.000')

",22
7861208,7861208,1,"I have a string:
x = ""12.000""

And I want it to convert it to digits. However, I have used int, float, and others but I only get 12.0 and i want to keep all the zeroes. Please help!
I want x = 12.000 as a result.
",57
7861208,7861255,2,"You may be interested by the decimal python lib.
You can set the precision with getcontext().prec.
",21
7861208,7862381,2,"If you really want to perform calculations that take precision into account, the easiest way is to probably to use the uncertainties module.  Here is an example
>>> import uncertainties
>>> x = uncertainties.ufloat('12.000')
>>> x
12.0+/-0.001
>>> print 2*x
24.0+/-0.002

The uncertainties module transparently handles uncertainties (precision) for you, whatever the complexity of the mathematical expressions involved.

The decimal module, on the other hand, does not handle uncertainties, but instead sets the number of digits after the decimal point: you can't trust all the digits given by the decimal module.  Thus,
>>> 100*decimal.Decimal('12.1')
Decimal('1210.0')

whereas 100*(12.1±0.1) = 1210±10 (not 1210.0±0.1):
>>> 100*uncertainties.ufloat('12.1')
1210.0+/-10.0

Thus, the decimal module gives '1210.0' even though the precision on 100*(12.1±0.1) is 100 times larger than 0.1.

So, if you want numbers that have a fixed number of digits after the decimal point (like for accounting applications), the decimal module is good; if you instead need to perform calculations with uncertainties, then the uncertainties module is appropriate.
(Disclaimer: I'm the author of the uncertainties module.)
",235
7861299,7861299,1,"I tested out a simple hello world line to test out JPype:
jpype.java.lang.System.out.println(""hello world"")
It works great inside the context of a ""main"" python program.  However, inside the context of a running thread, I get the following response:
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f5272aeb226, pid=7888, tid=139991902578432
#
# JRE version: 7.0_01-b08
# Java VM: Java HotSpot(TM) 64-Bit Server VM (21.1-b02 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [_jpype.so+0x33226]  JPJavaEnv::FindClass(char const*)+0x36
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
#
# An error report file with more information is saved as:
# /home/imedia/NTsvm/src/server/nlp/hs_err_pid7888.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.sun.com/bugreport/crash.jsp
#
Aborted

",184
7861299,7861440,2,"citing from the JPype documentation:

Python Threads
For the most part, python threads based on OS level threads (i.e posix threads), will work without problem. The only thing to remember is to call jpype.attachThreadToJVM() in the thread body to make the JVM usable from that thread. For threads that you do not start yourself, you can call isThreadAttachedToJVM() to check.

HTH
",74
8022337,8022337,1,"I'm working with 3-dimensional arrays (for the purpose of this example you can imagine they represent the RGB values at X, Y coordinates of the screen).
>>> import numpy as np
>>> a = np.floor(10 * np.random.random((2, 2, 3)))
>>> a
array([[[ 7.,  3.,  1.],
        [ 9.,  6.,  9.]],

       [[ 4.,  6.,  8.],
        [ 8.,  1.,  1.]]])

What I would like to do, is to set to an arbitrary value the G channel for those pixels whose G channel is already below 5. I can manage to isolate the pixel I am interested in using:
>>> a[np.where(a[:, :, 1] < 5)]
array([[ 7.,  3.,  1.],
       [ 8.,  1.,  1.]])

but I am struggling to understand how to assign a new value to the G channel only. I tried:
>>> a[np.where(a[:, :, 1] < 5)][1] = 9
>>> a
array([[[ 7.,  3.,  1.],
        [ 9.,  6.,  9.]],

       [[ 4.,  6.,  8.],
        [ 8.,  1.,  1.]]])

...but it seems not to produce any effect. I also tried:
>>> a[np.where(a[:, :, 1] < 5), 1] = 9
>>> a
array([[[ 7.,  3.,  1.],
        [ 9.,  9.,  9.]],

       [[ 4.,  6.,  8.],
        [ 9.,  9.,  9.]]])

...(failing to understand what is happening). Finally I tried:
>>> a[np.where(a[:, :, 1] < 5)][:, 1] = 9
>>> a
array([[[ 7.,  3.,  1.],
        [ 9.,  6.,  9.]],

       [[ 4.,  6.,  8.],
        [ 8.,  1.,  1.]]])

I suspect I am missing something fundamental on how NumPy works (this is the first time I use the library). I would appreciate some help in how to achieve what I want as well as some explanation on what happened with my previous attempts.
Many thanks in advance for your help and expertise!
EDIT: The outcome I would like to get is:
>>> a
array([[[ 7.,  9.,  1.],     # changed the second number here
        [ 9.,  6.,  9.]],

       [[ 4.,  6.,  8.],
        [ 8.,  9.,  1.]]])   # changed the second number here

",583
8022337,8022415,2,">>> import numpy as np
>>> a = np.array([[[ 7.,  3.,  1.],
...         [ 9.,  6.,  9.]],
...
...        [[ 4.,  6.,  8.],
...         [ 8.,  1.,  1.]]])
>>> a
array([[[ 7.,  3.,  1.],
        [ 9.,  6.,  9.]],

       [[ 4.,  6.,  8.],
        [ 8.,  1.,  1.]]])

>>> a[:,:,1][a[:,:,1] <; 5 ] = 9
>>> a
array([[[ 7.,  9.,  1.],
        [ 9.,  6.,  9.]],

       [[ 4.,  6.,  8.],
        [ 8.,  9.,  1.]]])

a[:,:,1] gives you G channel, I subsetted it by a[:,:,1] < 5 using it as index.  then assigned value 9 to that selected elements.
",219
8022337,8022408,2,"there is no need to use where, you can directly index an array with the boolean array resulting from your comparison operator.
a=array([[[ 7.,  3.,  1.],
          [ 9.,  6.,  9.]],
         [[ 4.,  6.,  8.],
          [ 8.,  1.,  1.]]])


>>> a[a[:, :, 1] < 5]
array([[ 7.,  3.,  1.],
       [ 8.,  1.,  1.]])

>>> a[a[:, :, 1] < 5]=9

>>> a
array([[[ 9.,  9.,  9.],
        [ 9.,  6.,  9.]],
       [[ 4.,  6.,  8.],
        [ 9.,  9.,  9.]]])

you do not list the expected output in your question, so I am not sure this is what you want.
",193
8022342,8022342,1,"Counter objects are subclasses of dict so they have the method setdefault.
>>> from collections import Counter
>>> c = Counter(houses=5)
>>> print(c.setdefault.__doc__)
D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D

If I do:
>>> c.setdefault('castles')
>>> c.keys()
dict_keys(['castles', 'houses'])
>>> type(c)
<class 'collections.Counter'>

everything seems pretty nice. But:
>>> c
Traceback (most recent call last):
  File ""<pyshell#17>"", line 1, in <module>
    c
  File ""C:\Python32\lib\collections.py"", line 586, in __repr__
    items = ', '.join(map('%r: %r'.__mod__, self.most_common()))
  File ""C:\Python32\lib\collections.py"", line 477, in most_common
    return sorted(self.items(), key=_itemgetter(1), reverse=True)
TypeError: unorderable types: NoneType() < int()
>>> 

Is this a bug?. Should not give c.setdefault('castles') a value/key error instead of silently accept a key without value ? Or maybe a repr method taking into account None values?
",251
8022342,8022354,2,"The  c.setdefault('castles') line directly assigns c['castles'] = None.  This likely isn't what you intended.
If you intended to make castles show in the __repr__, use c['castles'] = 0 instead.
For a Counter to behave as designed, the keys can be anything you want to count and the values need to be a number.  As you've seen, the sort-step in __repr__ expects that the values are all numbers  and it won't work if one of the values is set to None.
It might seem that setdefault would be used to give the counter default values or assign a factory function, but that isn't what setdefault does.  And you don't need to do that step at all since Counter objects automatically return default value of zero for you.  No extra work is required.
Here is how it all works, simply and easily:
>>> from collections import Counter
>>> c = Counter(houses=5)
>>> c
Counter({'houses': 5})
>>> c['castles']           # counters automatically return zero for missing items, no work required
0
>>> c                      # but missing items won't show in the __repr__
Counter({'houses': 5})
>>> c['castles'] = 0       # unless you specifically add an entry for them
>>> c
Counter({'houses': 5, 'castles': 0})

",281
8022342,8022365,2,"Yes, looks like a bug. The problem is that setdefault without a value argument assumes the value is None, while in the case of a Counter it should really insert either one or zero, or fail by raising some exception.
In Python 2.7, your snippet works, btw., although it still inserts a None value, violating Counter's invariants.
Mind you, this isn't the first bug/design flaw that I encounter with collections.Counter.
",84
8121142,8135762,2,"In case anyone needs it, I created Whooshstore, which is essentially a Whoosh-based, pure Python clone of GNU id utils that provides incremental updates, pagination and a Python API.
The command line client works like this:
ws-update -b --index my.idx datadir  # build the index
ws-update -b --append --index my.idx datadir  # incremental update
ws --index my.idx hello world     # query the index

(-b is for batch updating, which is faster but requires more memory. For the full CLI syntax use --help.)
It does not come close to the speed of GNU id utils, but by updating the index using several incremental batch (in-memory) updates it's fast enough for us.
",131
8121142,8121142,1,"I am trying to create a web interface for searching through a large number of huge configuration files (approx 60000 files, each one with a size between 20 KByte to 50 MByte). Those files are also updated frequently (~3 times/day).
Requirements:

Concurrency
Must identify the line numbers for each matching line
Good update performance

What I have looked into:

Lucene: To identify a line number, each line must be stored in a separate Lucene document, each containing two fields (the line number and the line). This makes updates hard/slow.
SOLR and Sphinx: Both based on Lucene, they have the same problem and do not allow for identifying the line number.
SQL table with a fulltext index: Again, no way to show the line number.
SQL table with each line in a separate row: Tested this with SQLite or MySQL, and update performance was the worst of all options. Updating a 50 MB document took more than an hour.
eXist-db: We converted each text file to XML like this: <xml><line number=""1"">test</line>...</xml>. Updates take ~5 minutes, which somewhat works but we are still not happy with it.
Whoosh for Python: Pretty much like Lucene. I have implemented a prototype that sort-of works by dropping/re-importing all lines of a given file. Updating a 50MB document takes about 2-3 minutes using this method.
GNU id utils: Suggested by sarnold, this is blazingly fast (50MB document is updated in less then 10 seconds on my test machine) and would be perfect if it had pagination and an API.

How would you implement an alternative?
",316
8121142,8121230,2,"You might wish to investigate the GNU idutils toolkit. On a local copy of the Linux kernel sources, it can give output like this:
$ gid ugly
include/linux/hil_mlc.h:66:  * a positive return value causes the ""ugly"" branch to be taken.
include/linux/hil_mlc.h:101:    int         ugly;   /* Node to jump to on timeout       */

Rebuilding the index from a cold cache is reasonably quick:
$ time mkid

real    1m33.022s
user    0m17.360s
sys     0m2.730s

Rebuilding the index from a warm cache is much faster:
$ time mkid

real    0m15.692s
user    0m15.070s
sys     0m0.520s

The index only takes 46 megabytes for my 2.1 gigs of data -- which is tiny in comparison to yours, but the ratio feels good.
Finding 399 occurrences of foo took only 0.039 seconds:
$ time gid foo > /dev/null

real    0m0.038s
user    0m0.030s
sys     0m0.000s

Update
Larsmans was curious about the performance of git grep on the kernel sources -- which is an excellent way to show how much performance gain gid(1) provides.
On a cold cache, git grep foo (which returned 1656 entries, far more than idutils):
$ time git grep foo > /dev/null

real    0m19.231s
user    0m1.480s
sys     0m0.680s

Once the cache was warm, git grep foo runs much faster:
$ time git grep foo > /dev/null

real    0m0.264s
user    0m1.320s
sys     0m0.330s

Because my dataset fits entirely in RAM once the cache is warm, git grep is pretty amazing: it's only seven times slower than the gid(1) utility and certainly it would be more than fast enough for interactive use. If the dataset in question cannot be entirely cached (which is probably where things actually get interesting) then the performance benefit of the index is unmistakable.
The two complaints about idutils:

No pagination. This is definitely a downside, though in my experience it runs quickly enough to simply store the results of the search elsewhere. If the search is going to return an appreciable percentage of the original dataset, then storage of partial results is definitely going to be annoying.
No API: true enough, there's no API. But the source is available; src/lid.c function report_grep() takes a linked list of files that match the output. A little fiddling with this function should even offer pagination. (It would take some doing.) At the end of the day, you'd have a C API, which might still not be ideal. But customizing it doesn't look awful.

However, the weakness that is probably worst is the lack of an incremental database update. If all files are updated three times per day, this is not a big deal. If some files are updated three times a day, it is doing needless work. If a handful of files are updated three times a day, there must be a better solution.
",525
15670525,15693931,2,"Gensim has a semi-well-hidden function that can kind of do this for you:
http://radimrehurek.com/gensim/matutils.html#gensim.matutils.Sparse2Corpus
""class gensim.matutils.Sparse2Corpus(sparse, documents_columns=True)
    Convert a matrix in scipy.sparse format into a streaming gensim corpus.""
I've had some success with it using a corpus extracted with CountVectorizer, then loaded into gensim.
",59
15670525,15670525,1,"I have X as a csr_matrix that I obtained using scikit's tfidf vectorizer, and y which is an array
My plan is to create features using LDA, however, I failed to find how to initialize a gensim's corpus variable with X as a csr_matrix. In other words, I don't want to download a corpus as shown in gensim's documentation nor convert X to a dense matrix, since it would consume a lot of memory and the computer could hang.
In short, my questions are the following,

How do you initialize a gensim corpus given that I have a csr_matrix (sparse) representing the whole corpus?
How do you use LDA to extract features?

",129
15670559,15675120,2,"Installing a module from a git repository in the usual ways does require a setup.py, but it can be a bare minimum one rather than a fuller one like you'd use for a PyPI module. For example:
#!/usr/bin/env python

from distutils.core import setup

setup(
    name=""foo"",
)

If you drop something like that into each of your lib repositories then you can make your requirements file point to the git repo using -e git://git.example.com/foo.git#egg=foo. If you install this into a virtualenv then it will appear at src/foo inside the virtualenv directory, and it'll be added to sys.path automatically when running Python from that virtualenv.
The -e option also accepts a local directory as a parameter, so if you guarantee that the current working directory will always be the root of your project when you install from requirements.txt (so that the relative path resolves correctly) it should work just fine to write things like -e lib1 in there, and then you can still reference the libraries as git submodules if you want.
If you are determined not to create a setup.py then your task is to emulate what would happen when running python setup.py develop. This command does two things (as of Python 2.7, at least):

It creates an egg link in the site-packages lib directory in your virtualenv (or system-wide if you don't have a virtualenv) that points to your target source directory.
It adds a line to easy-install.pth, which is also in the site-packages directory, which also points to your target source directory.

You could choose to do both of the above actions by a means other than running setup.py editable and get the same effect for current versions of Python, but of course the implementation of editable distributions may change in newer versions of Python.
",334
15670559,15670559,1,"I've got a directory structure like this:
src/
--scripts/
----foo/a.py
----bar/b.py
--lib1
--lib2
--lib3

The intent if to have scripts/foo/ and scripts/bar/ be directories of runnable python scripts that make use of the modules in lib1, lib2, lib3. Scripts, lib1, lib2, and lib3 are all separate internal git repositories under active development. There isn't a static interface or published version to depend on. Basically, they are all being written nearly at the same time by a small team.
I've played with making lib[1-3] submodules and I really hate the workflow. What I think I would like is to be able to do ""import lib1"" from foo/a.py and have it use the current code in lib1. Once things mature we will likely version everything and work to product proper packages. 
One way to do this would to muck with sys.path in each of the scripts to explicitly look in '../../' or something. I was wondering if there was something more elegant. Could I get something like pip install -r requirements.txt to do this work for me? I don't want to make an official pypi setup.py, I just want to get a pointer to the current contents of the lib[1-3] directory. The reason I like the requirements.txt approach is that as the libs mature, I'll end up putting version and git URLs in there. 
Or, is there a completely different way to do this?
",277
15670586,15670672,2,"please see this path of how to setup the environment in order to use cpython. 

fully detailed tutorial for Linux
A small github project contains all the relevant files to use on MAC OSx.
use python-config in order to get the paths to Python.h and to the static lib of python, which you should link your code with.

",62
15670586,34204023,2,"Find the path with:
find / -iname python.h

Once you have the path, add it to:
Project > Properties > GCC C++ Compiler > Includes

If needed, add:
python2.7

to:
GCC C++ Linker > Libraries

",41
15670586,15670586,1,"I apologize if this is a silly question. But I tried to google this and I couldn't find anything to point me in the right direction. I'd just like to understand what I need to do to 'set-up' cdt to 'understand' my python.h include.
the erroneous statement is this:
#include <Python.h>

but I also tried
#include ""Python.h""

And CDT responds with an error sign on the side stating:
Multiple markers at this line
- fatal error: Python.h: No such file or 
 directory
- Unresolved inclusion: <Python.h>

I am using Eclipse CDT Juno on Ubuntu 12.04. Any pointers would be great!
",122
15670586,41693140,2,"after installing the python-devel, locate On Terminal(ctrl+shift+t) by locate Python.h > the result is the file , copy the path and add it to eclipse by the following instructions.
personaly i like to get in the gist of the code so im adding to the compiler includes. but you can add to the linker as well, as mentioned above.
On eclipse : 
Project > Properties > C/C++ Build > Settings > ...Compiler(*) > Includes > Include paths (-l)
Directory : paste the path you've located in terminal.
for example/usr/include/python2.7 
press OK
see it was added to the list
press OK
.
enjoy
(*)note: if you are compiling c project choose the include under '.. GCC Copmiler'
for C++ '..G++ Compiler'
",141
15670751,15670778,2,"Python will evaluate the arguments left to right in a function call, then the function itself is executed.
General evaluation order is left to right.
",28
15670751,15670818,2,"Python uses strict (eager) evaluation strategy: the arguments to a function are always evaluated completely before the function is applied. The evaluation order is left to right (except when evaluating assignment):
Not descending into attribute lookups:

Evaluate text.find (we'll name the result F1)
Evaluate 'zip' → A1
Evaluate text.find → F2
Evaluate 'zip' → A2
Call F2.__call__(A2) (we'll call the return value R1) (text.find('zip'))
Evaluate 1 → A3
Call R1.__add__(A3) (returns R2) (R1 + 1)
Call F1.__call__(A1, R2) (returns the final result) (text.find('zip', R2))

",129
15670751,15691577,2,"It is called operator precedence and evaluation order. Within an expression, operator precedence applies, and per expression the evaluation order is used.
The text.find() call comes before the + addition operator because it has a higher precedence.
For operators of equal priority, evaluation goes from left to right. In a function call, each argument is a separate expression and these are thus evaluated from left to right.
",78
15670751,15670751,1,"I'm learning python and here's one code I can't quite get:
text = 'zip is very zipped'
print text.find('zip', text.find('zip') + 1)

Now, I know this is a shortcut of accomplishing:
text = 'zip is very zipped'
occur_once = text.find('zip')
print text.find('zip', occur_once + 1)

I was wondering, how the does
print text.find('zip', text.find('zip') + 1)

works and in what order does Python evaluates these expressions? Is there a name for this kind of 'order' of execution?
",114
15670760,15670760,1,"I wish to check if in a text file of points (x,y,z, etc) where is an header (True) or not (False). I wish to know if there is a built-in function in Python or a better method respect my own function.
def check_header(filename, parse):
    with open(filename) as f:
        first = f.readline()
        line = first.rstrip().split(parse)
        try:
            float(line[0])
            return False
        except ValueError:
            return True

i wrote this function 
example
a b c d
449628.46 6244026.59 0.47 1
449628.55 6244033.12 0.30 2 
449628.75 6244046.31 0.37 3 
449628.81 6244049.63 0.44 1 
449628.81 6244049.88 0.39 5 
449628.81 6244050.66 0.30 1 
449628.96 6244060.67 0.38 2 
449629.18 6244075.61 0.39 2 
449629.24 6244078.72 0.47 4 
449629.24 6244078.96 0.41 8 
449629.23 6244079.19 0.34 4 

check_header(filename, "" "")
True

449628.46 6244026.59 0.47 1
449628.55 6244033.12 0.30 2 
449628.75 6244046.31 0.37 3 
449628.81 6244049.63 0.44 1 
449628.81 6244049.88 0.39 5 
449628.81 6244050.66 0.30 1 
449628.96 6244060.67 0.38 2 
449629.18 6244075.61 0.39 2 
449629.24 6244078.72 0.47 4 
449629.24 6244078.96 0.41 8 
449629.23 6244079.19 0.34 4

check_header(filename, "" "")
False 

",213
15670760,15671103,2,"If you can have columns named, e.g., ""3.5"", your code obviously won't work, so I'll assume you can't.
And that means the whole thing is a bit overcomplicated. Really, all you need to do is see if the first character is a valid float starting character for a float:
def check_header(filename):
    with open(filename) as f:
        first = f.read(1)
    return first not in '.-0123456789'

For an empty file, this will return True instead of raising an exception, but otherwise, it should work for exactly the same use cases as your original code.
I normally wouldn't even mention this, but since you tagged your question ""optimization"", I guess you care: This code is theoretically faster than yours for reasons that should be pretty obvious, but in real life, it will almost always make no difference. According to %timeit on my machine, the part after the read/readline takes 244ns instead of 2.6us. That's more than 10x as fast, as you'd expect. But the read/readline part takes 13.1us vs. 13.2us for a file is in the OS disk cache, or 39.7ms vs. 39.7ms for a file on a remote drive. The I/O cost of reading a block from a file into a buffer, even in the best case, swamps the cost of processing it (both the extra processing in readline, and the extra processing in your code).
",275
15670760,15670984,2,"Plaintext files don't really have headers in traditional sense. It's just a stream of characters.
If this were a binary format you could have a strict header and any reader would have to adhere to that format. I assume this is a custom format that you've created, if that's the case you've already got a good solution.
If you want to learn more about headers, you should look at the JPEG header specification, which is simple.
http://www.fastgraph.com/help/jpeg_header_format.html
See this post for an example of python code that reads the binary jpeg header.
Python: Check if uploaded file is jpg
",116
15674026,15674727,2,"x = [1,1,2,3,54,3,1]
y = [1,1,0,0,0,0,1]
any([i[0]==i[1] for i in zip(x,y)])

",32
15674026,15674322,2,"i think you want to search for a list whose pattern got matched.. .
x = [[1,1,2,3,54,3,1],[1,2,3,4,5,6,7],[2,4,6,8,10,12,14]]
y = [1,1,None,None,None,None,1] ## or [1,1,'n','n','n','n',1]

for l in x:
    if all(map(lambda x:x[0]==x[1],[x for x in zip(l,y) if x[1] and x[1]!='n'])):
        print l

output:
[1,1,2,3,54,3,1]

",116
15674026,15674235,2,"I think you are confused about what any means. It is used to check a sequence of values, and see if any of them is ""true"". That's not related to finding out if a value is ""any number"" or ""any of these possibilities"".
If you have a fixed, finite set of possibilities that you want to consider, then what you really want to know is whether your candidate value is in that set:
x in {1, 2, 3, 4, ""hi mom""} # returns whether x is any of those values

But ""any number"" is not a finite set. First off, you need to define what you mean by number; and then you need to perform the appropriate test. It sounds like what you are trying to do is check whether the value is an integer. In other words, you are concerned with the type of the values in the list.
If you already know they're all integers, then there's nothing to test; if you don't care what the value is, then just don't consider it when you make your checks. But if you need to be sure it's an integer, then the way to do that is
isinstance(x, int) # returns whether x is an `int`


But maybe you have confused me, by giving an example ""to-search list"" that happens to be the same length as your ""pattern"", when you actually want to look for the pattern at any point in a longer list.
In that case, you can make a function that does an exact match of the pattern against a list of the same length; and then use any to check whether any pattern-lengthed sublist matches. any is designed to be used with generator expressions, and it looks like this:
def match(a_sublist, the_pattern):
    # put your logic here

def search(the_full_list, the_pattern):
    pattern_length, full_length = len(the_pattern), len(the_full_list)
    return any(
        match(the_full_list[i:i+pattern_length], the_pattern)
        for i in range(full_length - pattern_length)
    )

There are more efficient ways to match, depending on the details of your pattern, that will be inspired by string search algorithms and regular expression engines. But that is getting into much more difficult material - the above should get you started.
",453
15674026,15674195,2,"from itertools import izip, islice
x = [2,1,3,1,1,2,3,54,3,1,5,6,7,1,1,0,0,0,0,1]
y = [1,1,None,None,None,None,1]

print [i for i in xrange(len(x)-len(y)+1) 
         if all(b is None or a==b for a,b in izip(islice(x, i, i+len(y)), y))]

Or more code for easy to understand:
def nwise(x, n):
    for i in xrange(len(x)-n+1):
        yield i, islice(x, i, i+n)

def match(x, y):
    return all(b is None or a==b for a,b in izip(x, y))

print [i for i, xs in nwise(x, len(y)) if match(xs, y)]

",165
15674026,15674186,2,">>> from operator import itemgetter
>>> x = [1, 1, 2, 3, 54, 3, 1]
>>> itemgetter(0,1,6)(x) == (1, 1, 1)
True

How is y really defined. Obviously you can't have n in there as a placeholder? Could your use None perhaps?
",70
15674026,15674026,1,"I'm trying to work out a function to find specific patterns in a list.  For example if we take the list 
x = [1,1,2,3,54,3,1]

I want to then check if the pattern y shows up in the list x:
y = [1,1,n,n,n,n,1]

where n represents any number.  So in my example it would return True.
I've looked into the any() function and I haven't been able to work much out. 
",92
15674026,15674541,2,"This type of problem is well suited to Numpy masked arrays:
import numpy.ma as ma

x = ma.array([1,1,2,3,54,3,1])
y = ma.array([1,1,1,1,1,1,1], mask=[0,0,1,1,1,1,0])

print x==y           # [True True -- -- -- -- True]
print ma.all(x==y)   # True

Of course, the use here may not merit installing and importing numpy, but it has advantages in some situations.
",78
15674037,15674037,1,"So I wrote this code a while back but now I have to write it recursively. This program takes the input and adds it up.
For example input=55 the answer should be 10. If the input=2645 the answer should be 17
def sumD(num):
    sumofdigits=0
    while num !=0:
        sumofdigits+=num%10
        num=num//10
    return sumofdigits

def main():
    num=int(input(""Enter number : ""))
    print(sumD(num))

main()

The def sumD(num) function has to call itself. I'm not sure how to do this. 
",109
15674037,15674057,2,"def sumD(num):
    if num == 0: return 0
    return (num % 10) + sumD(num // 10)

",26
15674410,15676432,2,"You wanted the while loop to end when the path length reached the number of squares on the board- using and instead of or in your while loop it will end when either this expression:
goal not in path

or this expression:
len(path) < self.boardSize ** 2

evaluates to False. Using or, as long as one of those expressions is true, the loop would keep running. So your fixed code would be:
def planPath(self, creature, goal, board):
        print(""in the path"")      
        path = [board[creature.x][creature.y]]       
        while goal not in path and len(path) < self.boardSize ** 2:
            print(""path length"")
            print(len(path))
            nextPossible = {}
            for neighbor in path[-1].neighbors:
                if type(neighbor) is not Land.Water:
                    nextPossible[neighbor] = abs(neighbor.location[0] - goal.location[0]) + abs(neighbor.location[1] - goal.location[1]) + abs(neighbor.elevation - goal.elevation)      
            path.append(min(nextPossible, key=nextPossible.get))
        return path

",212
15674410,15674410,1,"So I am trying to plan a path on a 9x9 grid, so boardSize is 9. The while loop should stop path list has a length of 81 or more so why is it possible that it can get to a length of 3531when the creature is at 7,5 and the goal is at 5,2 and elevations are 0? Is my while loop wrong or do you think it might be elsewhere?
def planPath(self, creature, goal, board):
        print(""in the path"")      
        path = [board[creature.x][creature.y]]       
        while goal not in path or len(path) < self.boardSize ** 2:
            print(""path length"")
            print(len(path))
            nextPossible = {}
            for neighbor in path[-1].neighbors:
                if type(neighbor) is not Land.Water:
                    nextPossible[neighbor] = abs(neighbor.location[0] - goal.location[0]) + abs(neighbor.location[1] - goal.location[1]) + abs(neighbor.elevation - goal.elevation)      
            path.append(min(nextPossible, key=nextPossible.get))
        return path

",206
15674412,15674412,1,"When I run this code, it responds with UnboundLocalError: local variable 'hhh' referenced before assignment. However, the global string 'temp' does not respond with such an error despite being defined in a similar manner. Any help would be fantastic, thank you.
    import random, os
def start():
    global level
    global hhh
    global temp
    level=1
    temp='     +-!'
    hhh='[X'
    os.system('CLS')
    actualcrawl()
def actualcrawl():
    print (temp)
    for a in range(2,128):
        hhh=hhh+temp[random.randrange(1,8)]
    hhh=hhh[:79]+'>'+hhh[80:]
    for i in range(1,3):
        a=random.randrange(3,8)
        b=random.randrange(6,15)
        hhh=hhh[:16*a+b-1]+'='+hhh[16*a+b:]
    for i in range(1,9):
        print (hhh[16*i-16:16*i])

",159
15674412,15674761,2,"Yes,you should take a look at this question Using global variables in a function other than the one that created them
Briefly speaking,if it is only reading from a name, and the name doesn't exist locally, it will try to look up the name in any containing scopes.That's what happens to temp,which will be found in the global scope.But with hhh,you do writing,which will make Python believe that hhh is a local variable.  
And another thing,but more important,it is not recomended using global.You could invoke actualcrawl() in start(),and pass in hhh,temp,which is the way most people do.
EDIT
It is simple:
import random,os
def start():
    level=1
    temp='     +-!'
    hhh='[X'
    os.system('CLS')
    actualcrawl(temp,hhh)

def actualcrawl(temp,hhh):
    print (temp)
    for a in range(2,128):
        hhh=hhh+temp[random.randrange(1,8)]
    hhh=hhh[:79]+'>'+hhh[80:]
    for i in range(1,3):
        a=random.randrange(3,8)
        b=random.randrange(6,15)
        hhh=hhh[:16*a+b-1]+'='+hhh[16*a+b:]
    for i in range(1,9):
        print (hhh[16*i-16:16*i])

I don't know what language you use before Python,but you really don't need to declare a variable like in C/C++.Because when you assign to a variable, you are just binding the name to an object.See this python variables are pointers?
",287
15674576,15674576,1,"I'm trying to implement an OAuth2 authentication server and for the client part i wanted to send a json request to the server (from a Django view) and i found several libraries to do that tho' the most common are httplib2 and urllib2 i was wondering which is the difference between them and which is the best library for this purpose.
Thanks in advance.
Edit:
After searching, i found an extremely useful library called Requests and i use this one since then. (http://docs.python-requests.org/en/latest/)
",96
15674576,15674649,2,"urllib2 handles opening and reading URLs. It also handles extra stuff like storing cookies.
httplib handles http requests, its what happens behind the curtain when you open a url. 
you can send json request with urllib2 so you should use that.
see this.
",49
15674601,15676278,2,"models.py
from django.db import models

class Upload(models.Model):
    name = models.CharField(max_length=100)
    file = models.FileField(upload_to=""images"")

forms.py
from django import forms
from app_name.models import Upload

class UploadForm(forms.ModelForm):
    class Meta:
        model = Upload

views.py
def upload_file(request):
    if request.method == 'POST':
        form = UploadForm(request.POST, request.FILES)
        if form.is_valid():
            form.save()
            return HttpResponseRedirect('/success/url/')
    else:
        form = UploadFileForm()
    return render_to_response('upload.html', {'form': form})

upload.html
<form enctype=""multipart/form-data"" action=""/upload/"" name=""test"" method=""post"">
    {% csrf_token %}
    {{form.as_p}}
    <input id=""signUpSubmit"" type=""submit"" value=""Submit"">
</form>

",150
15674601,15674601,1,"So I am trying to upload and save a csv file to a variable via a POST to a url.  I have looked through the django documentation on file uploads found here.  I just don't understand the use of a form?  What's the purpose in this situation?  
They use an example with a form: 
from django import forms
from django.http import HttpResponseRedirect
from django.shortcuts import render_to_response

# Imaginary function to handle an uploaded file.
from somewhere import handle_uploaded_file

class UploadFileForm(forms.Form):
    title = forms.CharField(max_length=50)
    file  = forms.FileField()

def upload_file(request):
    if request.method == 'POST':
        form = UploadFileForm(request.POST, request.FILES)
        if form.is_valid():
            handle_uploaded_file(request.FILES['file'])
            return HttpResponseRedirect('/success/url/')
    else:
        form = UploadFileForm()
    return render_to_response('upload.html', {'form': form})

Upload html:
<form enctype=""multipart/form-data"" action=""/upload/"" name=""test"" method=""post"">
    <input id=""file"" type=""file"" name=""test"" />
    <input id=""signUpSubmit"" type=""submit"" value=""Submit"">
</form>

",218
15674602,16370556,2,"I have never try MySQLdb on python 3k. You can read the dev blog:

Python 3 compatibility is needed now more than ever. I thought I could
  do this in 1.2.4, but I would have to sacrifice compatibility for
  Python < 2.7. So MySQLdb-1.2.4 will be a bugfix release and fully
  Python 2.7 compatible (and should be Python 2.8 compatible), and very
  soon thereafter there will be a 1.3.0 which will require Python 2.7 or
  newer and be compatible with Python 3.

",91
15674602,15674602,1,"I'm about to go nuts with python 3.2.3 
Do you see something wrong with the following statement? 
cur.execute('''SELECT hits FROM counters WHERE url = ?''', (page,)) 
data = cur.fetchone() 

because as you can see by visiting my webpage at http://superhost.gr it produces an error and I don't know why. 
I'm using MySQLdb.
Ι'm using '?' or '%s'; the latter used to work flawlessly with python 2.6, but it does not in python 3.2.3 
Both these commands fail in python 3.2.3
cur.execute('''SELECT hits FROM counters WHERE url = ?''', (page,)) 

cur.execute('''SELECT hits FROM counters WHERE url = %s''', (page,)) 

Any idea why?
",154
15674701,15674890,2,"Here are a few possibilities. Your question is quite vague and your code isn't even close to working, so it's difficult to understand the question
>>> test ={'line4': (4, 2), 'line3': (3, 2), 'line2': (2, 2), 'line1': (1, 2), 'line10': (10, 2)}
>>> for i in test.items():
...     print i
... 
('line4', (4, 2))
('line3', (3, 2))
('line2', (2, 2))
('line1', (1, 2))
('line10', (10, 2))
>>> for i in test:
...     print i
... 
line4
line3
line2
line1
line10
>>> for i in test.values():
...     print i
... 
(4, 2)
(3, 2)
(2, 2)
(1, 2)
(10, 2)
>>> for i in test.values():
...     for j in i:
...         print j
... 
4
2
3
2
2
2
1
2
10
2

",230
15674701,15674701,1,"I am trying to read all the elements in a dictionary one by one. my dictionary is as given below ""test"".
test ={'line4': (4, 2), 'line3': (3, 2), 'line2': (2, 2), 'line1': (1, 2), 'line10': (10, 2)}

i want to do as given in below sample code.
for i in range(1,len(test)+1):
    print test(1) # should print all the values one by one

thank you 
",114
15674701,15674767,2,"You can use a nested comprehension:
>>> test ={'line4': (4, 2), 'line3': (3, 2), 'line2': (2, 2), 'line1': (1, 2), 'line10': (10, 2)}
>>> print '\n'.join(str(e) for t in test.values() for e in t)
4
2
3
2
2
2
1
2
10
2

Since dictionaries are unsorted in Python, your tuples will be unsorted as well. 
",104
15674701,15674713,2,"Try this:
for v in test.values():
    for val in v:
        print val

if you need a list:
print [val for v in test.values() for val in v ]

If you want to print each record from dict than:
for k, v in test.iteritems():
    print k, v

",61
15674701,15674719,2,"#Given a dictionary
>>> test ={'line4': (4, 2), 'line3': (3, 2), 'line2': (2, 2), 'line1': (1, 2), 'line10': (10, 2)}

#And if you want a list of tuples, what you need actually is the values of the dictionary
>>> test.values()
[(4, 2), (3, 2), (2, 2), (1, 2), (10, 2)]

#Instead if you want a flat list of values, you can flatten using chain/chain.from_iterable
>>> list(chain(*test.values()))
[4, 2, 3, 2, 2, 2, 1, 2, 10, 2]
#And to print the list 
>>> for v in chain.from_iterable(test.values()):
    print v


4
2
3
2
2
2
1
2
10
2

Analyzing your code
for i in range(1,len(test)+1):
    print test(1) # should print all the values one by one


You can't index a dictionary. A dictionary is not a sequence like a list
You don;t use parenthesis to index. It turns to be a function call
To iterate a dictionary, you can either iterate the keys or the values. 

for key in test to iterate a dictionary by keys
for key in test.values() to iterate a dictionary by values


",292
15675469,15675469,1,"I am writing a simple program that pulls up an image (BackgroundFinal.png) and displays it in a window. I want to be able to press a button on the window to move the picture down by 22 pixels. Everything works except the button does not do anything. 
import Tkinter
import Image, ImageTk
from Tkinter import Button


a = 0       #sets inital global 'a' and 'b' values
b = 0

def movedown():             #changes global 'b' value (adding 22)
    globals()[b] = 22
    return

def window():               #creates a window 
    window = Tkinter.Tk();
    window.geometry('704x528+100+100');

    image = Image.open('BackgroundFinal.png');      #gets image (also changes image size)
    image = image.resize((704, 528));
    imageFinal = ImageTk.PhotoImage(image);

    label = Tkinter.Label(window, image = imageFinal);   #creates label for image on window 
    label.pack();
    label.place(x = a, y = b);      #sets location of label/image using variables 'a' and 'b'

    buttonup = Button(window, text = 'down', width = 5, command = movedown()); #creates button which is runs movedown()
    buttonup.pack(side='bottom', padx = 5, pady = 5);

    window.mainloop();

window()

If I am not mistaken, the button should change the global 'b' value, therefore changing the y position of the label. I really appreciate any help, sorry for my god-awful conventions. Thanks in advance!
",295
15675469,15736052,2,"Thanks for the reply but, It was not really what I was looking for. I'll post what I found worked best here for anybody else with the same problem. 
Essentially, It is much better, in this case, to use a Canvas instead of a label. With canvases, you can move objects with canvas.move, here is a simple example program 
# Python 2
from Tkinter import *

# For Python 3 use:
#from tkinter import *

root = Tk()
root.geometry('500x500+100+100')

image1 = PhotoImage(file = 'Image.gif')

canvas = Canvas(root, width = 500, height = 400, bg = 'white')
canvas.pack()
imageFinal = canvas.create_image(300, 300, image = image1)

def move():
    canvas.move(imageFinal, 0, 22)  
    canvas.update()

button = Button(text = 'move', height = 3, width = 10, command = move)
button.pack(side = 'bottom', padx = 5, pady = 5)

root.mainloop()

my code may not be perfect (sorry!) but that is the basic idea. Hope I help anybody else with this problem 
",220
15675469,15675742,2,"You have a few problems here.  
First, you're using pack and place.  In general, you should only use 1 geometry manager within a container widget.  I don't recommend using place.  That's just too much work that you need to manage.
Second, you're calling the callback movedown when you construct your button.  That's not what you want to do -- You want to pass the function, not the result of the function:
buttonup = Button(window, text = 'down', width = 5, command = movedown)

Third, globals returns a dictionary of the current namespace -- It's not likely to have an integer key in it.  To get the reference to the object referenced by b, you'd need globals()[""b""].  Even if it did, changing the value of b in the global namespace won't change the position of your label because the label has no way of knowing that change.  And in general, if you need to use globals, you probably need to rethink your design.
Here's a simple example of how I would do it...
import Tkinter as tk

def window(root):
    buf_frame = tk.Frame(root,height=0)
    buf_frame.pack(side='top')
    label = tk.Label(root,text=""Hello World"")
    label.pack(side='top')
    def movedown():
        buf_frame.config(height=buf_frame['height']+22)

    button = tk.Button(root,text='Push',command=movedown)
    button.pack(side='top')

root = tk.Tk()
window(root)
root.mainloop()

",298
15678988,15678988,1,"So I have the following case:
I have a module that has several function definitions. I want to create a class that has those functions available to it.
I just wanted to ask which way is best?
Option1
import ModuleWithFunctions

class bla(object):

    ModuleWithFunctions = ModuleWithFunctions

Option2
class bla(object):

    import ModuleWithFunctions

I know option 2 is against PEP 8 so I assume people will say option1, however, what if I wanted everything to be available as:
bla.(function) 

instead of:
bla.ModuleWithFunctions.(function)

How can I do that with option1?
",111
15678988,15679105,2,"You generally wouldn't do this.
You normally would create a base class in baseclass_module instead:
# baseclass_module.py

class BaseClass(object):
    def method1(self):
        # ...

    def method2(self):
        # ...

Then use that in other modules:
from baseclass_module import BaseClass

class Bla(BaseClass):
    # ...

",61
15679067,15679819,2,"With standard PyDev
It seems that this option is only available in Eclipse's Java Editor.
The Java editor allows you to create ""profiles"" for the code formatter, while PyDev's options for the code formatter are very limited.
However,
You can hack this. PythonTidy.py is an awesome script that cleans up Python code to make it follow PEP8 conventions, and that can be tweaked with your own settings. 
PythonTidy (code cleanup & formatting)
Get here (homepage) the source for PythonTidy. 
You will see inside the file, at the beginning of the code and just after the comments, that many settings are defined. 
The first one of these is COL_LIMIT with its default value set to 72. Now you can use PythonTidy to format your code the way you want.
Integration with PyDev
Now you have to make this work with PyDev's formatting. This blog post will explain it really better than me, but still I'll sum up the steps :

Write a Jython interface betwenn PyDev's editor (PyEdit) and PythonTidy. This blog's author already wrote a wrapper script in the public domain available in the above link or here in case the link goes 404.
Save this file anywhere you want, with the name pyedit_pythontidy.py, along with the PythonTidy.py file.
Configure PyDev to use this script as its Code Formatter. You can do this in Preferences > PyDev > Scripting PyDev

Note #1: I really recommend reading the original blog post to have a better understanding
Note #2: The wrapper script author did not implement Code Block formatting, so this means you can only format a full file. This should not be that hard to implement, up to you.
Note #3: All credits goes to bear330 for the PyDev integration part.


",336
15679067,15679067,1,"I'm using Pydev for Python programming in Eclipse.
I would like to know if there is a way for auto-formatting the code so that a maximum number of characters per line is respected.
Thanks
",37
15679162,15689093,2,"There is a module for openerp 6.1 to remove the create and edit option(in the openerp apps site search for web remove) from the default selection of many2one field. You can use this as an example and create you own module. or you can modify the base codes goto your server, then navigate to openerp/addons/web/static/src/js/view_form.js and remove the quick create functionality defined from the line number 2860.
This is the Same answer that I have given in  openerp help site.
",88
15679162,20212921,2,"In v7 you can use the answer as suggested in http://help.openerp.com/question/16498/how-to-disable-create-and-edit-from-from-a-menu/
<form string=""My form"" create=""false"">

I had this problem in v6.1 though, so I created a new option so that I could apply it to only some fields (not all fields as suggested by @Bipin)
<form string=""My form"" options='{""no_create"": true}'>

and changed web/static/src/js/view_form.js
     // Hack: check for new ""no_create"" option:
     if (self.get_definition_options().no_create === undefined || !self.get_definition_options().no_create) {
     // the rest of the code stays asis:

        // quick create
        var raw_result = _(data.result).map(function(x) {return x[1];});
        if (search_val.length > 0 &&
            !_.include(raw_result, search_val) &&
            (!self.value || search_val !== self.value[1])) {
            values.push({label: _.str.sprintf(_t('<em>   Create ""<strong>%s</strong>""</em>'),
                    $('<span />').text(search_val).html()), action: function() {
                self._quick_create(search_val);
            }});
        }
        // create...
        values.push({label: _t(""<em>   Create and Edit...</em>""), action: function() {
            self._change_int_value(null);
            self._search_create_popup(""form"", undefined, {""default_name"": search_val});
        }});

     } // here endith the hack

I want to make this into a module, as editing the source code isn't very maintainable.
",317
15679162,15679162,1,"Friends,
Need to remove this option from pop up manyone fields. (not in all fields.some fields need to remove this feature).i used widget=""selection"".then my domain filter not working.so please help me to find a solution.

",45
15679162,19246498,2,"I have faced the same problem, but I solved it easily.
You need to change your web add-ons.
Please follow the step:

Go to: web/static/src/js
open the file: view_form.js
Go to line number 2958 or you can find label: _t (""Create and
Edit...""),
comment it

Enjoy, you can now see in your many2one fields don't have 'Create and Edit'
Note : This will affect every many2one field.
",83
15679261,15679261,1,"I have a directory containing 450 folders, all unique names. Inside this folder I need to create a sub-folder called Metadata, so it looks like the following:
Folder1/Metadata
Folder2/Metadata
Folder3/Metadata
Is there a way using Python to create this example?
",45
15679261,15679335,2,"The functions you are looking for are all in the os module:
import os
for item in os.listdir('.'):
    if os.path.isdir(item):
        newdir = os.path.join(item, 'Metadata')
        if not os.path.exists(newdir):
            os.makedirs(newdir)

",51
15679272,15679272,1,"By ""reasonable"" environment I mean that it should not require the user to manually install any dependencies of the application, but a working Python installation can be required. Additionally I would like the application to work on Windows, OSX, and popular Linux distributions. If I can package a Python interpreter as well, that's better. Size is not really a concern. A good example of what I want to accomplish is the SublimeText editor. 
Is there an established way of doing this?
",94
15679272,15679428,2,"Yes, python comes with setup utilities, and there are packages which will put your complete application in a platform specific binary(exe on windows, .app on osx).
Some of the packages I would recommend looking at would be:
cx_freeze
py2app
py2exe
",48
15679359,15679476,2,"urllib does not support parsing a .pac file. The page you see is probably the Apache page for the server serving that .pac configuration file instead.
.pac files contain javascript code that present your browser with proxy rules. You can try and open the file directly and see what proxy would be configured for the Python Challenge site instead. See http://en.wikipedia.org/wiki/Proxy_auto-config for more details on the file format.
Once you figured out what proxy server would be used, configure that as server in the proxies mapping instead.
",96
15679359,15679359,1,"I'm currently working my way through the excellent Python Challenge (http://www.pythonchallenge.com/). The current problem I'm tackling involves the use of the urllib library but I'm having issues.  I'm attempting to use this library to connect to the site through my company's firewall.  Let's start with some code:
proxy = {'http':'http://my.companys.proxy/proxy.pac'}
urllib.urlopen('http://www.pythonchallenge.com', proxies=proxy).read()

This yields an http response, but strangely its the Apache HTTP server test page:

...Red Hat Enterprise Linux Test Page... This page is used to test the proper operation of the Apache HTTP server after it has been installed, etc...

So, I appear to be successfully acheiving an http connection outside our firewall but getting a different http resposne than my browser.  Another clue (or not) is when I try to connect to the about.php page:
urllib.urlopen('http://www.pythonchallenge.com/about.php', proxies=proxy).read()

This, however, yields:

404 Not found... Apache 2.2.3 Red Hat Server at www.pythonchallenge.com Port 80

Both addresses above work just fine in my browser (using the same proxy).  Any ideas where I'm going wrong?
",226
15679406,15679406,1,"I try to put a listing of subdirectories of a directory into a list.
I use that code :
import os

for dirname, dirnames, filenames in os.walk(""W:/test""):
  # print path to all subdirectories first.
  for subdirname in dirnames:
      a= os.path.join(dirname, subdirname)

liste = []
liste.append(a)
print liste

The problem is that I have not all my subdirectories into my list ""liste""
Would you have any solutions ?
Thanks
",91
15679406,15679423,2,"You need to call liste.append inside the loop.
import os
liste = []
for root, dirs, files in os.walk(path):
    for subdir in dirs:
        liste.append(os.path.join(root, subdir))

print(liste)        

",45
15679454,15680848,2,"Use sys.argv. Which gives you a list of the items passed on the command line
x = 10
ssh = paramiko.SSHClient()
ssh.connect(server, username=username, password=password)
ssh_stdin, ssh_stdout, ssh_stderr = ssh.exec_command(""./scriptB.py "" + str(x))

Script b
import sys

y = int(sys.argv[1])
print y

sys.argv will be a list that contains ['./scriptB.py', '10'] in this case.
",82
15679454,15679454,1,"Hey guys I've got a problem. 
I'm trying to send variable x that is found in script a to script b and then execute script b with that variable. 
Example:
Script a
x = 10
ssh = paramiko.SSHClient()
ssh.connect(server, username=username, password=password)
ssh_stdin, ssh_stdout, ssh_stderr = ssh.exec_command(x >> scriptB.py)
ssh_stdin, ssh_stdout, ssh_stderr = ssh.exec_command(./scriptB.py)

Script b
y = x
print y

Any ideas on how I could do this?
Thanks
",93
15679467,15679531,2,"Just create a loop over os.listdir():
import os

path = '/path/to/directory'
for filename in os.listdir(path):
    if not filename.endswith('.xml'): continue
    fullname = os.path.join(path, filename)
    tree = ET.parse(fullname)

",45
15679467,15679467,1,"I'm parsing XML in python by ElementTree
import xml.etree.ElementTree as ET 
tree = ET.parse('try.xml')
root = tree.getroot()

I wish to parse all the 'xml' files in a given directory. The user should enter only the directory name and I should be able to loop through all the files in directory and parse them one by one. Can someone tell me the approach. I'm using Linux. 
",78
15679526,15680860,2,"If you want to make list2 identical to list1, you don't need to mess with order or re-arrange anything, just replace list2 with a copy of list1:
list2 = list(list1)

list() takes any iterable and produces a new list from it, so we can use this to copy list1, thus creating two lists that are exactly the same.
It might also be possible to just do list2 = list1, but do note that this will cause any changes to either to affect the other (as they point to the same object), so this is probably not what you want.
If list2 is referenced elsewhere, and thus needs to remain the same object, it's possible to replace every value in the list using list2[:] = list1.
In general, you probably want the first solution.
",159
15679526,15679667,2,"or may be just check everything is perfect then..copy list a for b
if all(x in b for x in a) and len(a)==len(b):
    b=a[:]

",38
15679526,15679526,1,"I have two lists in python.
a=[1,4,5]
b=[4,1,5]

What i need is to order b according to a. Is there any methods to do it so simply without any
loops?
",39
15679526,39667373,2,"Sort b based on items' index in a, with all items not in a at the end.
>>> a=[1,4,5,2]
>>> b=[4,3,1,5]
>>> sorted(b, key=lambda x:a.index(x) if x in a else len(a))
[1, 4, 5, 3]

",67
15679526,15679634,2,"The easiest way to do this would be to use zip to combine the elements of the two lists into tuples:
a, b = zip(*sorted(zip(a, b)))

sorted will compare the tuples by their first element (the element from a) first; zip(*...) will ""unzip"" the sorted list.
",68
15679672,15680677,2,"I'm posting this without further comments, for learning purposes (in the real life please do use a library). Note that there's no error checking (a homework for you!)
Feel free to ask if there's something you don't understand.
# PART 1. The Lexer

symbols = None

def read(input):
    global symbols
    import re
    symbols = re.findall(r'\w+|[()]', input)

def getsym():
    global symbols
    return symbols[0] if symbols else None

def popsym():
    global symbols
    return symbols.pop(0)

# PART 2. The Parser
# Built upon the following grammar:
#  
#     program = expr*
#     expr    = '(' func args ')'
#     func    = AND|OR|NOT
#     args    = arg*
#     arg     = string|expr
#     string  = [a..z]

def program():
    r = []
    while getsym():
        r.append(expr())
    return r

def expr():
    popsym() # (
    f = func()
    a = args()
    popsym() # )
    return {f: a}

def func():
    return popsym()

def args():
    r = []
    while getsym() != ')':
        r.append(arg())
    return r

def arg():
    if getsym() == '(':
        return expr()
    return string()

def string():
    return popsym()

# TEST = Lexer + Parser

def parse(input):
    read(input)
    return program()

print parse('(AND a b (OR c d)) (NOT foo) (AND (OR x y))')
# [{'AND': ['a', 'b', {'OR': ['c', 'd']}]}, {'NOT': ['foo']}, {'AND': [{'OR': ['x', 'y']}]}]

",383
15679672,15679672,1,"im trying to parse lines in the form:
(OP something something (OP something something ) ) ( OP something something )

Where OP is a symbol for a logical gate (AND, OR, NOT) and something is the thing i want to evaluate.
The output im looking for is something like:
{ 'OPERATOR': [condition1, condition2, .. , conditionN] }

Where a condition itself can be a dict/list pair itself (nested conditions). So far i tried something like:
        tree = dict()
        cond = list()
        tree[OP] = cond


    for string in conditions:
        self.counter += 1
        if string.startswith('('):
            try:
                OP = string[1]
            except IndexError:
                OP = 'AND'
            finally:
                if OP == '?':
                    OP = 'OR'
                elif OP == '!':
                    OP = 'N'

            # Recurse
            cond.append(self.parse_conditions(conditions[self.counter:], OP))
            break

        elif not string.endswith("")""):
            cond.append(string)

        else:
            return tree

    return tree

I tried other ways aswell but i just can't wrap my head around this whole recursion thing so im wondering if i could get some pointers here, i looked around the web and i found some stuff about recursive descent parsing but the tutorials were all trying to do something more complicated than i needed.
PS: i realize i could do this with existing python libraries but what would i learn by doing that eh?
",278
15679719,15688085,2,"You are basically reinventing the indexing scheme of a multidimensional array. It is relatively easy to code, but you can use the two functions unravel_index and ravel_multi_index to your advantage here.
If your grid is of M rows and N columns, to get the idx and idy of a single item you could do:
>>> M, N = 12, 10
>>> np.unravel_index(4, dims=(M, N))
(0, 4)

This also works if, instead of a single index, you provide an array of indices:
>>> np.unravel_index([15, 28, 32, 97], dims=(M, N))
(array([1, 2, 3, 9], dtype=int64), array([5, 8, 2, 7], dtype=int64))

So if cells has the indices of several cells you want to find neighbors to:
>>> cells = np.array([15, 28, 32, 44, 87])

You can get their neighbors as:
>>> idy, idx = np.unravel_index(cells, dims=(M, N))
>>> neigh_idx = np.vstack((idx-1, idx+1, idx, idx))
>>> neigh_idy = np.vstack((idy, idy, idy-1, idy+1))
>>> np.ravel_multi_index((neigh_idy, neigh_idx), dims=(M,N))
array([[14, 27, 31, 43, 86],
       [16, 29, 33, 45, 88],
       [ 5, 18, 22, 34, 77],
       [25, 38, 42, 54, 97]], dtype=int64)

Or, if you prefer it like that:
>>> np.ravel_multi_index((neigh_idy, neigh_idx), dims=(M,N)).T
array([[14, 16,  5, 25],
       [27, 29, 18, 38],
       [31, 33, 22, 42],
       [43, 45, 34, 54],
       [86, 88, 77, 97]], dtype=int64)

The nicest thing about going this way is that ravel_multi_index has a mode keyword argument you can use to handle items on the edges of your lattice, see the docs.
",441
15679719,15679719,1,"Let's suppose I have a set of 2D coordinates that represent the centers of cells of a 2D regular mesh. I would like to find, for each cell in the grid, the two closest neighbors in each direction.
The problem is quite straightforward if one assigns to each cell and index defined as follows:
idx_cell = idx+N*idy
where N is the total number of cells in the grid, idx=x/dx and idy=y/dx, with x and y being the x-coordinate and the y-coordinate of a cell and dx its size.
For example, the neighboring cells for a cell with idx_cell=5 are the cells with idx_cell equal to 4,6 (for the x-axis) and 5+N,5-N (for the y-axis).
The problem that I have is that my implementation of the algorithm is quite slow for large (N>1e6) data sets.
For instance, to get the neighbors of the x-axis I do
[x[(idx_cell==idx_cell[i]-1)|(idx_cell==idx_cell[i]+1)] for i in cells]
Do you think there's a fastest way to implement this algorithm?
",203
15679734,15679734,1,"I am using python minidom to parse a xml, but not able to get it working for below xml. I want to select the first server tag and want the value of name tag , in this case ""Server1""
<class>
<name>MyClass</name>
<security>
<name>MyClass</name>
</security>
<server>
<name>Server1</name>
<ssl>
 <name>Server1</name>
</ssl>
<server-info>
 <name>Server1</name>
</server-info>
</server>
<server>
<name>Server2</name>
<ssl>
 <name>Server2</name>
</ssl>
<server-info>
 <name>Server2</name>
</server-info>
</server>
<server>
<name>Server3</name>
<ssl>
 <name>Server3</name>
</ssl>
<server-info>
 <name>Server3</name>
</server-info>
</server>
</class>

",186
15679734,15680192,2,"You'll have to reference the XML DOM documentation and grit your teeth.
To get the first <server> element, then its <name>:
from xml.dom import minidom

document = minidom.parse(inputfilename)

def findChildNodeByName(parent, name):
    for node in parent.childNodes:
        if node.nodeType == node.ELEMENT_NODE and node.localName == name:
            return node
    return None

def getText(nodelist):
    rc = []
    for node in nodelist:
        if node.nodeType == node.TEXT_NODE:
            rc.append(node.data)
    return ''.join(rc)

# Get the first of all `<server>` nodes
server = document.getElementsByTagName('server')[0]

# Get the first child node that is a `<name>` element
name = findChildNodeByName(server, 'name')
if name is not None:
    print getText(name.childNodes)

",151
15679762,15679762,1,"In python 3 I have a line asking for input that will then look in an imported dictionary and then list all their inputs that appear in the dictionary. My problem is when I run the code and put in the input it will only return the last word I input.
For example
the dictionary contains (AIR, AMA)
and if I input (AIR, AMA) it will only return AMA.
Any information to resolve this would be very helpful!
The dictionary: 
EXCHANGE_DATA = [('AIA', 'Auckair', 1.50),
                 ('AIR', 'Airnz', 5.60),
                 ('AMP', 'Amp',3.22), 

The Code:
import shares
a=input(""Please input"")
s1 = a.replace(' ' , """")
print ('Please list portfolio: ' + a)
print ("" "")
n=[""Code"", ""Name"", ""Price""]
print ('{0: <6}'.format(n[0]) + '{0:<20}'.format(n[1]) + '{0:>8}'.format(n[2]))
z = shares.EXCHANGE_DATA[0:][0]
b=s1.upper()
c=b.split()
f=shares.EXCHANGE_DATA
def find(f, a):
    return [s for s in f if a.upper() in s]
x= (find(f, str(a)))
toDisplay = []
a = a.split()
for i in a:
    temp = find(f, i)
    if(temp):
        toDisplay.append(temp)
for i in toDisplay:
    print ('{0: <6}'.format(i[0][0]) + '{0:<20}'.format(i[0][1]) + (""{0:>8.2f}"".format(i[0][2])))

",365
15679762,15680564,2,"Ok, the code seems somewhat confused. Here's a simpler version that seems to do what you want:
#!/usr/bin/env python3

EXCHANGE_DATA = [('AIA', 'Auckair', 1.50),
                 ('AIR', 'Airnz', 5.60),
                 ('AMP', 'Amp',3.22)]

user_input = input(""Please Specify Shares: "")

names = set(user_input.upper().split())

print ('Listing the following shares: ' + str(names))
print ("" "")

# Print header
n=[""Code"", ""Name"", ""Price""]
print ('{0: <6}{1:<20}{2:>8}'.format(n[0],n[1],n[2]))

#print data
for i in [data for data in EXCHANGE_DATA if data[0] in names]:
  print ('{0: <6}{1:<20}{2:>8}'.format(i[0],i[1],i[2]))

And here's an example of use:
➤ python3 program.py 
Please Specify Shares: air amp
Listing the following shares: {'AMP', 'AIR'}

Code  Name                   Price
AIR   Airnz                    5.6
AMP   Amp                     3.22

The code sample you provided actually does what was expected, if you gave it space separated quote names.
Hope this helps.
",274
15679782,15679782,1,"What is the difference between these fetching.?
please give me a examples for reference site to get clear idea.still i'm confuse with it
res = cr.dictfetchall()

res2 = cr.dictfetchone()

res3 = cr.fetchall()

res4 = cr.fetchone()

cr is the current row, from the database cursor  (OPENERP 7 )
ex : 
def _max_reg_no(self, cr, uid, context=None):
    cr.execute(""""""
    select register_no as reg_no
    from bpl_worker
    where id in (select max(id) from bpl_worker)
    """""")
    res = cr.fetchone()[0]
    print (res)
    return res

",114
15679782,15679995,2,"cr.dictfetchall() will give you all the matching records in the form of ** list of dictionary** containing key, value. 
cr.dictfetchone() works same way as cr.dictfetchall() except it returns only single record.
cr.fetchall() will give you all the matching records in the form of list of tupple.
cr.fetchone() works same way as cr.fetchall() except it returns only single record.
In your given query, if you use:

cr.dictfetchall() will give you [{'reg_no': 123},{'reg_no': 543},].
cr.dictfetchone() will give you {'reg_no': 123}.
cr.fetchall() will give you '[(123),(543)]'.
cr.fetchone() will give you '(123)'.

",150
15680583,15680787,2,"This could be due to network congestion in your LAN. If it is just happening for a little while like 1 minute. 
",24
15680583,15680583,1,"I've a problem: I created a small Python script to read data from an Omron PLC memory on LAN. Delphi program run batch file that run Python script periodically (every 6 seconds).
This script runs on 2 Win 7 PCs and 1 Win XP PC.
My problem is: no data transfer between Win XP PC and PLC for a random period of time (around 1 minute, sometimes more), but Win 7 PCs have no problem to communicate with same PLC.
I use UDP protocol.
def main():
    udpSock = util_socket.utilSocket()
    command = udpSock.read_from_file(""udpCommand"")
    command = int(command)
    messaggio = mex()
    if command==1:
        msg = messaggio.get_messaggio_lettura()
        udpSock.send_command(msg)
        ricevi_risposta(udpSock)
    if command==2:
        msg = messaggio.get_messaggio_azzeramento()
        udpSock.send_command(msg)
        msg = messaggio.get_messaggio_lettura()
        udpSock.send_command(msg)
        ricevi_risposta(udpSock)
    if command<1 or command>2:
        udpSock.write_to_file(""ERROR !!!"", ""numPezzi"")

def ricevi_risposta(udpSock):        
    data, addr = udpSock.recv_socket()
    contaPezzi = udpSock.get_dato_pulito(data)
    udpSock.write_to_file(contaPezzi, ""numPezzi"")
    stringa = str(datetime.datetime.now().strftime( ""%d/%m/%Y %H:%M"" )) + "" - dato ricevuto: "" + str(data) + "" // dato convertito: "" + str(contaPezzi) + ""\n"" 
    udpSock.append_to_file(stringa, ""numPezzi_Log_"" + str(datetime.datetime.now().strftime( ""%d/%m/%Y"")) )

def recv_socket(self):

    # the public network interface
    #HOST = socket.gethostbyname(socket.gethostname())

    # create a raw socket and bind it to the public interface
    s = socket.socket(socket.AF_INET, socket.SOCK_RAW, socket.IPPROTO_IP)
    self.get_parametri()
    s.bind((self.ipAddressPc, 0))
    # Include IP headers
    s.setsockopt(socket.IPPROTO_IP, socket.IP_HDRINCL, 1)
    data, addr = s.recvfrom(65536)

    return data, addr

def send_command(self, msg):
    self.get_parametri()
    udp_ip = self.ipAddressPlc
    udp_port=9600 #5575

    #print ""UDP target IP:"", udp_ip
    #print ""UDP target port:"", udp_port
    #print ""message:"", msg

    sock = socket.socket( socket.AF_INET, # Internet
                          socket.SOCK_DGRAM ) # UDP
    sock.sendto( msg, (udp_ip, udp_port) )

LAN seems ""fall to sleep"". This is LOG File: (08:41:13 -> 08:42:30. Expectation 08:41:13 -> 08:41:19, 08:41:25, 08:41:31, ...)
27/03/2013 08:41:13 - dato ricevuto (dati non riportabili) dato convertito: 252
27/03/2013 08:42:30 - dato ricevuto (dati non riportabili) dato convertito: 260
27/03/2013 08:42:30 - dato ricevuto (dati non riportabili) dato convertito: 260
27/03/2013 08:42:30 - dato ricevuto (dati non riportabili) dato convertito: 260
27/03/2013 08:42:30 - dato ricevuto (dati non riportabili) dato convertito: 260
27/03/2013 08:42:30 - dato ricevuto (dati non riportabili) dato convertito: 260
27/03/2013 08:42:30 - dato ricevuto (dati non riportabili) dato convertito: 260
27/03/2013 08:42:30 - dato ricevuto (dati non riportabili) dato convertito: 260
27/03/2013 08:42:30 - dato ricevuto (dati non riportabili) dato convertito: 260
27/03/2013 08:42:30 - dato ricevuto (dati non riportabili) dato convertito: 260
27/03/2013 08:42:30 - dato ricevuto (dati non riportabili) dato convertito: 260
27/03/2013 08:42:30 - dato ricevuto (dati non riportabili) dato convertito: 260
27/03/2013 08:42:30 - dato ricevuto (dati non riportabili) dato convertito: 260
27/03/2013 08:42:36 - dato ricevuto (dati non riportabili) dato convertito: 261

What could be the problem?
",672
15680593,15680781,2,"Notice you are not only working with 1D arrays:
In [6]: a.ndim
Out[6]: 1

In [7]: b.ndim
Out[7]: 2

So, b is a 2D array.
You also see this in the output of b.shape: (1,3) indicates two dimensions as (3,) is one dimension.
The behaviour of np.dot is different for 1D and 2D arrays (from the docs):

For 2-D arrays it is equivalent to matrix multiplication, and for 1-D
  arrays to inner product of vectors

That is the reason you get different results, because you are mixing 1D and 2D arrays. Since b is a 2D array, np.dot(b, b) tries a matrix multiplication on two 1x3 matrices, which fails.

With 1D arrays, np.dot does a inner product of the vectors:
In [44]: a = np.array([1,2,3])

In [45]: b = np.array([1,2,3])

In [46]: np.dot(a, b)
Out[46]: 14

In [47]: np.inner(a, b)
Out[47]: 14

With 2D arrays, it is a matrix multiplication (so 1x3 x 3x1 = 1x1, or 3x1 x 1x3 = 3x3):
In [49]: a = a.reshape(1,3)

In [50]: b = b.reshape(3,1)

In [51]: a
Out[51]: array([[1, 2, 3]])

In [52]: b
Out[52]:
array([[1],
       [2],
       [3]])

In [53]: np.dot(a,b)
Out[53]: array([[14]])

In [54]: np.dot(b,a)
Out[54]:
array([[1, 2, 3],
       [2, 4, 6],
       [3, 6, 9]])

In [55]: np.dot(a,a)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-55-32e36f9db916> in <module>()
----> 1 np.dot(a,a)

ValueError: objects are not aligned

",466
15680593,15680593,1,"I try to understand how to handle 1D array (vector in linear algebra) with numpy. In the following example, I generate two numpy.array a and b:
>>> import numpy as np
>>> a = np.array([1,2,3])
>>> b = np.array([[1],[2],[3]]).reshape(1,3)
>>> a.shape
(3,)
>>> b.shape
(1, 3)

For me, a and b have the same shape according linear algebra definition: 1 row, 3 columns, but not for numpy.
Now, the numpy dot product:
>>> np.dot(a,a)
14
>>> np.dot(b,a)
array([14])
>>> np.dot(b,b)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ValueError: objects are not aligned

I have three different output. What's the difference between dot(a,a) and dot(b,a)? Why dot(b,b) doesn't work?
I also have some differencies with those dot products:
>>> c = np.ones(9).reshape(3,3)
>>> np.dot(a,c)
array([ 6.,  6.,  6.])
>>> np.dot(b,c)
array([[ 6.,  6.,  6.]])

",287
15680809,15682527,2,"Personally, I'd store an rrule object from python-dateutil (http://labix.org/python-dateutil) rather than inventing your own recurrence format. Then you can just define some methods that use rrule. between(after, before) to generate instances of your event object for a given range.
One catch though, dateutil's rrule object doesn't pickle correctly, so you should define your own mechanism of serialising the object to the database. I've generally gone with a JSON representation of the keyword arguments for instantiating the rrule. The annoying edge case is that if you want to store stuff like '2nd Monday of the month', you have to do additional work with MO(2), because the value it returns isn't useful. It's hard to explain, but you'll see the problem when you try it.
I'm not aware of any efficient way to find all eligible events within a range though, you'll have to load in all the Event models that potentially overlap with the range. So you'll always be loading in potentially more data than you'll eventually use. Just make sure relatively smart about it to reduce the burden. Short of someone adding recurrence handling to databases themselves, I'm not aware of any way to improve this.
",239
15680809,15680809,1,"I've been working on a event based AJAX application that stores recurring events in the a table in the following format (Django models):
event_id = models.CharField(primary_key=True, max_length=24)
# start_date - the start date of the first event in a series
start_date = models.DateTimeField()
# the end date of the last event in a series
end_date = models.DateTimeField()
# Length of the occurence
event_length = models.BigIntegerField(null=True, blank=True, default=0)
rec_type = models.CharField(max_length=32)

The rec_type stores data in the following format:
[type]_[count]_[day]_[count2]_[days]#[extra]

type - the type of repeation: 'day','week','month','year'.
count - the interval between events in the “type” units.
day and count2 - define a day of a month ( first Monday, third Friday, etc ).
days - the comma-separated list of affected week days.
extra - the extra info that can be used to change presentation of recurring details.

For example:
day_3___ - each three days
month _2___ - each two month
month_1_1_2_ - second Monday of each month
week_2___1,5 - Monday and Friday of each second week 

This works fine, and allows many events to be transmitted concisely, but I now have the requirement to extract all events that occur during a given range. For example on a specific date, week or month and I am a bit lost as to how best to approach. 
In particular, I am stuck with how to check if an event with a given recurrence pattern is eligible to be in the results.
What is the best approach here?
",316
15680836,15680836,1,"I am trying to run a piece of code using netrc lib in python. I got examples from the Internet but they have all failed at the first line. 
import netrc

info = netrc.netrc()

Traceback (most recent call last):
  File ""./netrc.py"", line 2, in <module>
    import netrc
  File ""/usr/local/etc/xxx/netrc.py"", line 5, in <module>
    info = netrc.netrc()
TypeError: 'module' object is not callable

",85
15680836,16031745,2,"The problem is the name of your script file: netrc.py which is the same as the module name. Rename it.
Providing filename to netrc.netrc() causes only to use the specific netrc file in place of the default ~\.netrc.
",44
15684178,15684178,1,"I count lines in python for a text like this:
count = 0
for ch in text:
    if( ch == ""\n"" ):
        count += 1
return count

The same text goes into a textBox control.
I would like to scroll to the last line but the line count does not help , because the textBox wraps long lines making them 2 or more lines.
I can go to the last line by scrolling to -1, but then I cannot scroll up anymore. I need to know the (actual) max line count so I can scroll to any position i want.
lastLine = self.count_lines( text )
self.getControl( 100 ).setText( text )
self.getControl( 100 ).scroll( lastLine )

",136
15684178,32752419,2,"A little late but just in-case someone else could use this, here is my solution. This example will dynamically size the height of the curses textpad widget.
#!/usr/bin/env python 
# -*- coding: utf-8 -*-

# Purpose: Test Scrolling
#
# File test.py
# Author: Dan Huckson
# Date: 20150922
#

import curses

def main(stdscr):
    vpwidth = 1
    vpheight = 30
    string = ""A\n BB\n  CCC\n   DDDD\n    EEEEE\n     FFFFFF\n      GGGGGGG\n       HHHHHHHH\n        JJJJJJJJJ\n         KKKKKKKKKK\n          LLLLLLLLLLL\n           MMMMMMMMMMMM\n            NNNNNNNNNNNNN\n             OOOOOOOOOOOOOO END""

    height = w = 0  
    stdscr.refresh() 
    for c in string:
        if c == '\n' and not w: height += 1
        else:
            if c != '\n':
                w += 1
                if w < vpwidth: continue
            elif w == vpwidth: height += 1
            w = 0     
            height += 1
    if w: height += 1

    pad = curses.newpad(height+1, vpwidth)
    pad.addstr(string)

    top = 0
    left = 3
    offset = key = 0  
    while True:
        if key == curses.KEY_UP and offset: offset -= 1
        elif  key == curses.KEY_DOWN and (offset + vpheight) < height:
            offset += 1

        pad.refresh(offset, 0, top, left, vpheight+top, vpwidth+left)

        for i in range(vpheight):
            stdscr.addstr(i,0, (' %s:' % i)[-3:])

        stdscr.addstr(vpheight,0,   '   ----~----+----~----+----~----+----~----+----~----+----~----+----~----+----~----')
        stdscr.addstr(vpheight+1,0, '            10        20        30        40        50        60        70        ')
        stdscr.addstr(vpheight+2,0, '                                                                               ')
        stdscr.addstr(vpheight+4,0, '%s %s %s %s %s %s %s %s  ' % (offset, 0, 0, 0, vpheight,  vpwidth, height, len(string)))

        key = stdscr.getch()


curses.wrapper(main)

",375
15684178,15735445,2,"you said a long line is divided in 2 lines or more,right?
you could count lines this way
count=0
for a in string.split():
    count+= 1+a//MaxLen

where string is the text you are dealing with, and MaxLen is the maximum number of characters that the textbox can show in a line
but this just doesn't solve the problem if you don't know how to get MaxLen,and i actually don't...
",81
15684335,15684335,1,"I'm trying to use tastypie filtering but when I try to get a resource through filtering I receive a 404.
code
class UserResource(ModelResource):
   class Meta:
       queryset = UsersCouchDb.objects.all()
       resource_name = 'users/list'
       fields = ['firstName', 'lastName', 'gender','status','date']
       always_return_data = True
       authorization= Authorization()
       filtering = {
           ""firstName"": ('exact', 'startswith'),
       }

I'm using urlopen to access the resource:
    info= urllib2.urlopen('http://127.0.0.1:8000/api/users/list/&firstName__exact=David').read()

How do I make it so I don't get a 404 when trying to get a resource through filtering with Tastypie?
",125
15684335,15685030,2,"If there are no results in a list that you query on, TastyPie would just send back Json with zero elements (something like the following): 
{
    meta: {
        limit: 20,
        next: null,
        offset: 0,
        previous: null,
        total_count: 0
    },
}

So it seems that if you're getting a 404, you don't have something set up correctly. 
The following things could resolve your issue:

Make sure you have ?format=json appended to your url before the &
Make sure you've registered the APIResource
Make sure you've set up the appropriate urls.py if anything is different.

",117
15684473,15684473,1,"I have the following function that takes 3 pieces of information (name, age, hometown) for 3 people and saves it in a txt file.
def peopleInfo():
    txtFile = open(""info.txt"", ""w"")
    i = 0
    for i in range(0, 3):
        name = input(""Enter name "")
        age = input(""Enter age "")
        hometown = input(""Enter hometown "")
        txtFile.write(name + ""\n"" + age + ""\n"" + hometown + ""\n"")
    txtFile.close()

I am now trying to create a function that will read the text file and print a persons name if their hometown is 'Oxford'. So far I have the following just to read the text from the file but im not sure how to skip back a line and print the name if the town is Oxford.
def splitLine():
    txtFile = open(""info.txt"", ""r"")
    for line in txtFile:
        line = line.rstrip(""\n"")
        print(line)

Thanks for any help!
",208
15684473,15686380,2,"I used the following for anyone that is interested:
def peopleInfo():
    txtFile = open(""info.txt"", ""w"")
    i = 0
    for i in range(0, 3):
        name = input(""Enter name "")
        age = input(""Enter age "")
        hometown = input(""Enter hometown "")
        txtFile.write(name + ""\n"" + age + ""\n"" + hometown + ""\n"")
    txtFile.close()

def splitLine():
    txtFile = open(""info.txt"", ""r"")
    lineList = []
    i = 0
    for line in txtFile:
        lineList.append(line.rstrip(""\n""))
        if ""Oxford"" in lineList[i]:
            print(lineList[i - 2])
        i += 1

",150
15684557,15684557,1,"I am trying to use Form Wizard but I can't figure out where to set the choices for the fields.
#views.py
class QuizWizard(SessionWizardView):
    def done(self, form_list, **kwargs):
        return render_to_response('done.html', {
            'form_data':[form.cleaned_data for form in form_list],
        })

#forms.py
class QuestionForm(forms.ModelForm):
    #selection = forms.ChoiceField()
    class Meta:
        model = Question

I see an empty form that looks like the admin panel for adding an object.
I would like to be able to pass a question to the form and have the question field filled out and not editable and preferable not submitted.
If I do 
(r'^(?P<quiz_id>\d+)', QuizWizard.as_view(get_form_list)),

the function get_form_list has no length
(r'^(?P<quiz_id>\d+)', QuizWizard.as_view(get_form_list(quiz_id))),

Quiz_id is unknown.
so now I am trying to pass quiz_id to the view function and generate the list of question forms to be used in the form wizard
urls.py
url(r'^(?P<quiz_id>\d+)', 'quiz.views.get_form_list'),

views.py
class QuizWizard(SessionWizardView):
    def done(self, form_list, **kwargs):
        return render_to_response('done.html', {
            'form_data':[form.cleaned_data for form in form_list],
        })

def get_form_list(request, quiz_id):
    quiz = Quiz.objects.get(id=quiz_id)

    question_forms = []

    for question in quiz.questions.all():        
        choices = []
        for choice in question.choices.all():
            choices.append(choice)
        f = QuestionForm(instance=question)        
        question_forms.append(f)

    return QuizWizard.as_view(question_forms)(request)

I am getting the error message
issubclass() arg 1 must be a class

Update based on Rohan's answer:
def get_form_list(request, quiz_id):
    quiz = Quiz.objects.get(id=quiz_id)

    question_forms = []

    for question in quiz.questions.all():        
        choices = []
        for choice in question.choices.all():
            choices.append(choice)
        f = QuestionForm(instance=question)        
        question_forms.append(f)

    inst_dict = {}
    for idx, question in enumerate(question_forms):
        inst_dict[str(idx)] = question
    print inst_dict
    #inst_dict = { str(index(x)) : x for x in question_forms}

    QuestFormList = []
    for i in range(len(question_forms)):    
        QuestFormList.append(QuestionForm)

    QuizWizard.as_view(QuestFormList, instance_dict=inst_dict)(request)

With this code I am getting an error 
'ModelFormOptions' object has no attribute 'many_to_many'

Here is my models.py
class Choice(models.Model):
    choice = models.CharField(max_length=64)
    def __unicode__(self):
        return self.choice

#create a multiple choice quiz to start
class Question(models.Model):
    question = models.CharField(max_length=64)
    answer = models.CharField(max_length=64)
    choices = models.ManyToManyField(Choice)
    module = models.CharField(max_length=64)

    def __unicode__(self):
        return self.question

class Quiz(models.Model):
    name = models.CharField(max_length=64)
    questions = models.ManyToManyField(Question)

    def __unicode__(self):
        return self.name

",569
15684557,15684711,2,"You should call it using the class instead of object. So change your call to
QuizWizard.as_view(question_forms)(request)

Update:
The wizard view takes form class list as parameters not the form instance. You are creating form instances in question_forms and passing it to view.
If you want to pass instance for the form in each step, you can pass instance_dict.
Something like ...
inst_dict = { '0': question_forms[0], #step 0 instance
              '1': question_forms[1], #step 1 instance
            }
QuestFormList = [QuestionForm, QuestionForm ...]
QuizWizard.as_view([QuestFormList, instance_dict=inst_dict)(request)

",119
15684605,15684605,1,"I am writing a simple Python for loop to prnt the current character in a string. However, I could not get the index of the character. Here is what I have, does anyone know a good way to get the current index of the character in the loop?
 loopme = 'THIS IS A VERY LONG STRING WITH MANY MANY WORDS!'

 for w  in loopme:
    print ""CURRENT WORD IS "" + w + "" AT CHARACTER "" 

",85
15684605,15684860,2,"Do you want to iterate over characters or words?
For words, you'll have to split the words first, such as
for index, word in enumerate(loopme.split("" "")):
    print ""CURRENT WORD IS"", word, ""AT INDEX"", index

This prints the index of the word.
For the absolute character position you'd need something like
chars = 0
for index, word in enumerate(loopme.split("" "")):
    print ""CURRENT WORD IS"", word, ""AT INDEX"", index, ""AND AT CHARACTER"", chars
    chars += len(word) + 1

",119
15684605,15684617,2,"Use the enumerate() function to generate the index along with the elements of the sequence you are looping over:
for index, w in enumerate(loopme):
    print ""CURRENT WORD IS"", w, ""AT CHARACTER"", index 

",47
15684662,15684679,2,"You'd read the lines into memory, into a list and then index into that list:
with open('somefile') as fileobj:
    lines = list(fileobj)

for index in indices:
    print lines[index]

",43
15684662,15684662,1,"I have a list which contains a series of numbers. This list of numbers corresponds to a line in a .dat file. How can I use the list say [0,1,2,3,4,5,6,9,4] and then print out the line in the .dat file which each number corresponds to.
",50
15684662,15685001,2,"file = [ l for l in open('file.name') ]
for i in list:
   print file[i]

",23
15684687,15684687,1,"I am working on inserting a local image into a table on my mysql server. I can insert data but when I download it, it isn't a valid jpg. 
Here is a sample like what I am using. Am I doing something wrong when I format the data?
printFileLoc = QtGui.QFileDialog.getOpenFileName(self,  caption = 'Open Print', filter = '*.jpg')
with open(printFileLoc,  'r') as f:
    printBin = re.escape(f.read())
newQry = QtSql.QSqlQuery()
qry = ""Insert into prints set print = '{0}'"".format(printBin)
newQry.exec_(qry)

The code completes as expected but the jpg is no good.
",130
15684687,15702263,2,"I figured out how to make it work. From this:
qry = ""Insert into prints set print = '{0}'"".format(printBin)

To this:
qry = ""Insert into prints set print = '0x{0}'"".format(printBin)

",53
15684842,15688667,2,"Assignment itself does nothing. rc here is your client. Doing rc[0], or any sort of indexing, generates and returns a DirectView object that's a view with whatever engines you specify in []. This is a shorthand for generating the views: it's not actually just getting a specific object.
Thus, those views aren't unique. The best way to explain it, I think, is with an example. Say you have 2 engines. You want to run some tasks on only engine one, and want the tasks to block. You want to run others on only engine one, but don't want them to block. You want to run yet more on engines 1 and 2, and don't want them to block. Then you could do:
view_1_block = rc[0]
view_1_block.block = True
view_2_noblock = rc[0]
view_2_noblock.block = False
view_3_noblock = rc[[0,1]]
view_3_noblock.block = False

Then, you can use these to run tasks in whatever way you'd like, eg
view_1_block.map(lambda x:x**10, range(32)) # blocks, returns results, runs only on 1
view_3_noblock.map(lambda x:x**10, range(32)) # does not block, returns AsyncResult, runs on 1 and 2

There's no actual magic being used here. When you run rc[0] twice, it generates two views. The second view is not the same as the first. When you assign rc[0] to a variable, and then use that variable, you're working with one view, and not creating a new one. 
iPython, like Numpy and Scipy, has quite a few shorthand notations that don't necessarily fit Python's idioms perfectly. This is especially the case with [] and getitem. A purer Python way of writing this could would be to use the much more unwieldy rc.direct_view(1), and so on, which would make clear that this wasn't just getting an item, and was actually creating a view.
",388
15684842,15684842,1,"I'm playing around with IPython and remote control:
in:  from IPython Import parallel as p
in:  rc=p.client(profile=myprofile)

There is a behavior difference between
in:  rc[0].block
out: False
in:  rc[0].block=True
in:  rc[0].block
out: False

and
in:  view=rc[0]
in:  view.block
out: False
in:  view.block=True
in:  view.block
out: True

I realy can't understand what append ? Why is that ? what do assignement do ?
",96
15684881,15685014,2,"The python copy module can reuse the pickle module interface for letting classes customize copy behaviour.
The default for instances of custom classes is to create a new, empty class, swap out the __class__ attribute, then for shallow copies, just update the __dict__ on the copy with the values from the original. A deep copy recurses over the __dict__ instead.
Otherwise, you specify a __getstate__() method to return internal state. This can be any structure that your class __setstate__() can accept again.
You can also specify the __copy__() and/or __deepcopy__() methods to control just copy behaviour. These methods are expected to do all the copying themselves, the __deepcopy__() method is passed a memo mapping to pass on to recursive deepcopy() calls.
An example could be:
from copy import deepcopy

class Foo(object):
    def __init__(self, bar):
        self.bar = bar
        self.spam = expression + that * generates - ham   # calculated

    def __copy__(self):
        # self.spam is to be ignored, it is calculated anew for the copy
        # create a new copy of ourselves *reusing* self.bar
        return type(self)(self.bar)

    def __deepcopy__(self, memo):
        # self.spam is to be ignored, it is calculated anew for the copy
        # create a new copy of ourselves with a deep copy of self.bar
        # pass on the memo mapping to recursive calls to copy.deepcopy
        return type(self)(deepcopy(self.bar, memo))

This example defines custom copy hooks to prevent self.spam being copied too, as a new instance will calculate it anew.
",301
15684881,15684881,1,"It is in most of the situations easy to implement copy constructors (or overloaded assignment operator) in C++ since there is a concept of pointers. However, I'm quite confused about how to implement shallow and deep copy in Python.
I know that there are special commands in one of the libraries but they don't work on classes that you have written by yourself. So what are the common ways to implement?
P.S. Showing process on some basic data structures (linked list or tree) will be appreciated.
EDIT: Thanks, they worked, it was my mistake in syntax.
I am very interested in overwriting these functions with __copy__() and __deep_copy()__. For example. how can I make a deep copy without knowing which type of information is in a data structure?
",153
15685631,15685898,2,"I think it certainly sounds like a viable option. As far as I know, the only ""interaction"" the official Python installer has with Windows is to add registry keys associating .py and .pyw files with the proper executables and possibly modifying the PATH variable. As long as the user has the correct .dll files to which the .exe's are linked, you could just zip up c:\Python33 or whichever version you're using and distribute that with your application. Before you do that, though, clone the directory and go through c:\clonedPython\libs\site-packages and get rid of any modules that aren't required for your application. Don't delete any necessary dependencies!
Portable Python is a possibility, but there may be some issues with certain modules not working properly, and it's not available yet for Python 3.3 (3.2.1 is the latest version, as well as 2.7.3), so if you have version-dependent syntax that might not be the best choice.
",180
15685631,15685854,2,"When you install python on Windows, select ""for current user only"" rather than ""for all users of this system"" when you're asked. And select the installation target to some custom directory, e.g. D:\mypython\
This kind of installation will package all necessary binaries and DLL files (e.g. msvcr90.dll) to this specified dir, with which you can deploy easily to another system (with same CPU-bit and operating system).
(I got this solution from a Chinese website http://www.oschina.net/question/23734_13481 - comment 1)
",100
15685631,15685631,1,"I'd like to ""install"" a version of Python locally, which doesn't touch anything on the system (Windows in this case) except the directory I extract it into. I would run it by specifying that particular python.exe. 
This is for the end-user. Essentially, I want to be able to extract a Python into a directory and start using it immediately, without requiring the user to even know that my program is using Python. I'm looking into py2exe and PyInstaller as well, but I'd like to know if this option is viable.
",107
15685631,15686095,2,"Use Portable Python - it is a version of python modified to do exactly what you want.
",18
15685682,15685682,1,"I have a main object with a Pyglet window as an attribute. Pylget's window class has a method called push handlers, which lets me push methods to the event stack. The following code works:
import pyglet

class main:
    win = None
    gameItems = {}

    def __init__(self, window):
        self.win = window
        gameItem.win = self.win
        self.gameItems[""menu""] = menu()
        self.gameItems[""menu""].add()
        pyglet.app.run()

class gameItem:
    win = None

    def add(self):
        self.win.push_handlers(self)

class menu(gameItem): ##I actually have multiple objects inheriting from gameItem, this is just one of them.
    def on_mouse_press(self, x, y, button, modifier):
        '''on_mouse_press() is an accepted handler for the window object.'''
        print(x)
        print(y)

    def on_draw(self):
        '''With a quick draw function, so I can see immediately
        that the handlers are getting pushed.'''
        pyglet.graphics.draw(4, pyglet.gl.GL_QUADS, ('v2i', (256,350,772,350,772,450,256,450)))

m = main(pyglet.window.Window())

The above code will spawn a new window at the default size and attach the on_mouse_press() and on_draw event handlers to it. That works well and good - however, trying to call on the push_handlers() method in other classes doesn't seem to work.
import pyglet

class Main:
    win = None
    gameItems = {}

    def __init__(self, window):
        self.win = window
        GameItem.game = self
        GameItem.win = self.win
        self.gameItems[""main""] = MainMenu()
        pyglet.app.run()

    def menu(self):
        self.gameItems[""main""].add()

class GameItem:
    win = None

    def add(self):
        self.win.push_handlers(self)

class MainMenu(GameItem): ##I actually have multiple objects inheriting from gameItem, this is just one of them.
    def on_mouse_press(self, x, y, button, modifier):
        '''on_mouse_press() is an accepted handler for the window object.'''
        print(x)
        print(y)

    def on_draw(self):
        '''With a quick draw function, so I can see immediately
        that the handlers are getting pushed.'''
        pyglet.graphics.draw(4, pyglet.gl.GL_QUADS, ('v2i', (256,350,772,350,772,450,256,450)))

m = Main(pyglet.window.Window(width=1024, height=768))
m.menu()

The above code spawns a new window, but it doesn't attach the menu class's handlers. Is there a reason for this, or a workaround I can use? Thanks!
",494
15685682,17002763,2,"When you call pyglet.app.run(), you enter the pyglet loop and it doesn't come back until the pyglet window is closed, so your m.menu() is called only when the pyglet loop ends. If you remove the pyglet.app.run line from Main and call it like this:
m = Main(pyglet.window.Window(width=1024, height=768))
m.menu()
pyglet.app.run()
print ""Window is closed now.""

It works.
",81
15688744,15688744,1,"i have to read a line in which i looking for pattern like 
width:40
height :50
left : 60
right: 70

following  found the required pattern 
line = ""width:40""
match = re.search(r'width\s*:\s*\d+', line)

in above code i have hard-coded the regex pattern for width
i have stored all four variables in array key_word = ['width', 'height', 'left', 'right']
i want to search for pattern for all these variable like
for key in key_word:
        match = re.search(key, line)

the problem is how can i make this key a raw string which will be a pattern like
r'width\s*:\s*\d+'
r'height\s*:\s*\d+'
r'left\s*:\s*\d+'
r'right\s*:\s*\d+'

",132
15688744,15688986,2,"You don't need regular expressions for this task. See other answers.
However if you insist, you can create one dynamically using re.escape:
import re

key_word = ['width', 'height', 'left', 'right']

myre = r'({})\s*:\s*(\d+)'.format('|'.join(map(re.escape, key_word)))

",70
15688744,15688879,2,"You can also just use a generic regex:
matches = re.findall(r'(.*?)\s*:\s*(\d+)', text)

matches will be a list of (key, value) tuples.
",42
15688744,15688847,2,"I would do something like the following:
key_word = ['width', 'height', 'left', 'right']
regex_template = r'{}\s*:\s*\d+'
for key in key_word:
    print re.search(regex_template.format(key), line)

",47
15688744,15688783,2,"Why not use split (or partition) and strip?
for line in lines:
    key, sep, value = line.partition(':')
    key = key.strip()
    value = value.strip()

If you're really needing to use regular expressions, you can format them, too:
r'%s\s*:\s*\d+' % 'width'

Or for each key:
regexes = [r'%s\s*:\s*\d+' % key for key in ['width', 'height', ...]]

",94
15688887,15688887,1,"It sounds reasonable that the os/rtos would schedule an ""Idle task"". In that case, wouldn't it be power consuming? (it sounds reasonable that the idle task will execute: while (true) {} )
",43
15688887,15690680,2,"This answer is specific to Windows NT-based OS.
Idle thread functioality
Tasks may vary between architectures, but generally these are the tasks performed by idle threads:

Enable interrupts to allow pending interrupts be delivered
Disable interrupts (using STI or CLI instructions, more on wiki)
On the DEBUG (or checked) builds, query if a kernel debugger is attached and allow breakpoints if been requested
Handle deferred procedure calls
Check if there are any runnable threads ready for execution. If there is one, update the idle processor control block with a pointer to the thread
Check the queues of other processors, if possible schedule thread awaiting execution on the idle processor
Call a power management routine, which may halt a processor or downgrade CPU tick rate and do other similar power saving activities

Additional info
When there are no runnable threads for a logical processor, Windows executes a kernel-mode idle thread. There is only 1 Idle process that has as many idle threads as there are logical processors. So on a Quad core machine with 4 logical/physical processors, there will be 1 Idle process and 4 idle threads.
In Windows, Idle process has ID = 0, so do all the Idle threads. These objects are represented by standard EPROCESS/KPROCESS and ETHREAD/KTHREAD data structures. But they are not executive manager processes and threads objects. There are no user-land address space and no user-land code is executed..
Idle process is statically allocated at system boot time before the process manager and object manager are set up. Idle thread structures are allocated dynamically as logical processors are brought live.
Idle thread priority is set to 0. However, this value doesn't actually matter as this thread only gets executed when there are no other threads available to run. Idle thread priority is never compared with priority of any other threads.
Idle threads are also special cases for preemption. The idle thread main routine KiIdleLoop (implementation from reactos) performs several tasks that are not interrupted by other threads. When there are no runnable threads available to run on a processor, that processor is marked as idle in a processor control block. Then if a runnable threads arrives to the queue scheduled for execution, that thread's address pointer is stored in the NextThread pointer of the idle processor control block. During the run of an idle thread, this pointer address gets checked on every iteration inside a while loop.
Source: Windows Internals. M. Russinovich. 6-th edition. Part 1, p.453 - 456.
",460
15688887,15689206,2,"Historically it's been a lot of different schemes, especially before reducing power consumption in idle was an issue.
Generally there is an ""idle"" process/task that runs at the lowest priority and hence always gets control when there's nothing else to do.  Many older systems would simply have this process run a ""do forever"" loop with nothing of consequence in the loop body.  One OS I heard of would run machine diagnostics in the idle process.  A number of early PCs would run a memory refresh routine (since memory needed to be cycled regularly or it would ""evaporate"").
(A benefit of this scheme is that 100% minus the % CPU used by the idle process gives you the % CPU utilization -- a feature that was appreciated by OS designers.)
But the norm on most modern systems is to either run a ""halt"" or ""wait"" instruction or have a special flag in the process control block that even more directly tells the processor to simply stop running and go into power-saving mode.
",196
15688887,15688958,2,"There's always code to run, the idle task is the code if there's nothing else. It may execute a special CPU instruction to power down the CPU until a hardware interrupt arrives. On x86 CPUs it's hlt (halt).
",47
15688887,15688937,2,"This depends on the OS and the CPU architecture. On x86 (Intel compatible) the operating system might execute HLT instructions, making the CPU wait until something interesting happens, such as a hardware interrupt. This supposedly consumes very little power. Operating systems report the time spent doing this as ""idle"" and may even assign it to a fictional ""idle"" process.
So, when in Windows task manager you see that the System Idle Process is consuming 90% CPU what it really means is that the CPU does not have an actual a program to run 90% of the time.
Here's a good article on the subject: What does an idle CPU do?
",129
15688889,15688943,2,"It's equivalent to 
status = app.exec_()   # run app, show window, wait for input
sys.exit(status)       # terminate program with a status code returned from app

",33
15688889,15688889,1,"I'm following this tutorial:
http://zetcode.com/tutorials/pyqt4/firstprograms/
and on the very first example I don't understand why does the application show only after the command:
sys.exit(app.exec_())

Also, I modified the code slightly to test things
import sys
from PyQt4 import QtGui


if __name__ == ""__main__"":
    app = QtGui.QApplication(sys.argv)
    w = QtGui.QWidget()
    w.resize(250,150)
    w.move(300,300)
    w.setWindowTitle(""Title"")
    w.show()
    sys.exit(app.exec_())

If I run this in the terminal and comment out the last line it will only show the window after running the last line.
I don't quite understand what the last line does.
EDIT:
I'm also confused as to why there isn't a reference of app and w i'd expect some kind of indication that w is a child or something of app.
I'm running it in Spyder and an IPython interpreter.
",174
15688889,15688946,2,"The last line is two functions. First it runs app.exec_() (which shows the main window of your application), then when that function ends, it passes the return value to as a parameter to sys.exit, which ends the program and sends the return value to the operating system (you can see this on *nix systems with echo $? after the program ends).
The reason it doesn't immediately exit is that Python can't execute sys.exit until it knows the value of the parameter to it, and it won't know that until app.exec_() finishes.
",110
15688898,15689294,2,"Buildpacks are the mechanism Heroku uses to build your application including installing dependencies. Pip is not installed by default, the Python buildpack itself pulls this dependency in. You could use the multi-buildpack which allows you to include several buildpacks. 
Multi buildpack - https://github.com/ddollar/heroku-buildpack-multi
Python buildpack - https://github.com/heroku/heroku-buildpack-python
Ruby buildpack - https://github.com/heroku/heroku-buildpack-ruby
",61
15688898,15688898,1,"I have an rails app deployed on heroku. This app depends on one python module, https://github.com/clips/pattern. Based on documentation of pattern, I could install by two ways:
1: cd pattern-2.5; python setup.py install

creating /usr/local/lib/python2.7/site-packages/pattern error: could
  not create '/usr/local/lib/python2.7/site-packages/pattern': Read-only
  file system

2: pip install pattern

bash: pip: command not found

Please, advise me how to install pattern. I found a similar question, How to install python module on Heroku cedar stack with Rails, but it just doesn't work for me.
",103
15688954,15692895,2,"You can't unload C extension modules at all.  There is just no way to do it, and I know for sure that most of the standard extension modules would leak like crazy if there was.
",39
15688954,15688954,1,"The CPython headers define a macro to declare a method that is run to initialize your module on import: PyMODINIT_FUNC
My initializer creates references to other python objects, what is the best way to ensure that these objects are properly cleaned up / dereferenced when my module is unloaded?
",52
15689151,15689238,2,"You can set the root to use your FE url patterns like this:
urlpatterns = patterns('',
    url(r'^', include('frontend.urls', namespace=""frontend"")),
)

If you wanna forcibly redirect to /frontend/ then you will need a view to handle the redirect.
Maybe look at the Redirect Generic view: https://docs.djangoproject.com/en/1.1/ref/generic-views/#django-views-generic-simple-redirect-to
",70
15689151,15689151,1,"I'trying to redirect the / of my domain to point to a index in my ""frontend"" app.
I tried a lot of ways and all of them work. 
The problem is that my index_view is being called twice for every redirect.
Here is my top urls.py
urlpatterns = patterns('',
     url(r'^$', lambda x: HttpResponseRedirect('/frontend/')),
     url(r'^frontend/', include('frontend.urls', namespace=""frontend"")),
)

And here is my frontend/urls.py
urlpatterns = patterns('',
    url(r'^$', views.index, name='index'),
    url(r'^alert/create/$', views.create_alert, name=""create_alert""),
    url(r'^alert/edit/(\w+)', views.edit_alert, name=""edit_alert""),
)

Every time I go to / is calling my views.index twice and I can't see why =/
Am I doing the redirecting wrong ?
Thanks in advance for any help!
",177
15689237,15689944,2,"Generally speaking, thinking of the ""readability counts"" mantra, the actual operator should always be your preferred choice. Using the operator versions has a place, when you can replace lambda a, b: a < b with the more compact operator.lt, but not much outside of that. And you really shouldn't be using explicit calls to the corresponding ufunc, unless you want to use the out parameter to store the calculated values directly in an existing array.
That said, if what you are worried is performance, you should do fair comparisons, because as you say, all your calls are eventually handled by numpy's less ufunc.
If your data is already in a numpy array, then you have already shown that they are all performing similarly, so go with the < operator for clarity.
What if your data is in a python object, say a list? Well, here are some timings for you to ponder:
In [13]: x = range(10**5)

In [19]: %timeit [j < 5000 for j in x]
100 loops, best of 3: 5.32 ms per loop

In [20]: %timeit np.less(x, 5000)
100 loops, best of 3: 11.3 ms per loop

In [21]: %timeit [operator.lt(j, 5000) for j in x]
100 loops, best of 3: 16.2 ms per loop

Not sure why operator.lt is so slow, but you clearly want to stay away from it. If you want to get a numpy array as output from a Python object input, then this will probably be the fastest:
In [22]: %timeit np.fromiter((j < 5000 for j in x), dtype=bool, count=10**5)
100 loops, best of 3: 7.91 ms per loop

Note that ufuncs operating on numpy arrays are much faster than any of the above:
In [24]: y = np.array(x)

In [25]: %timeit y < 5000
10000 loops, best of 3: 82.8 us per loop

",394
15689237,15689237,1,"I'm curious about the benefits and tradeoffs of using numpy ufuncs vs. the built-in operators vs. the 'function' versions of the built-in operators.
I'm curious about all ufuncs.  Maybe there are times when some are more useful than others.  However, I'll use < for my examples just for simplicity.
There are several ways to 'filter' a numpy array by a single number to get a boolean array.  Each form gives the same results, but is there a preferred time/place to use one over the other?  This example I'm comparing an array against a single number, so all 3 will work.
Consider all examples using the following array:
>>> x = numpy.arange(0, 10000)
>>> x
array([   0,    1,    2, ..., 9997, 9998, 9999])

'<' operator
>>> x < 5000
array([ True,  True,  True, ..., False, False, False], dtype=bool)
>>> %timeit x < 5000
100000 loops, best of 3: 15.3 us per loop

operator.lt
>>> import operator
>>> operator.lt(x, 5000)
array([ True,  True,  True, ..., False, False, False], dtype=bool)
>>> %timeit operator.lt(x, 5000)
100000 loops, best of 3: 15.3 us per loop

numpy.less
>>> numpy.less(x, 5000)
array([ True,  True,  True, ..., False, False, False], dtype=bool)
>>> %timeit numpy.less(x, 5000)
100000 loops, best of 3: 15 us per loop

Note that all of them achieve pretty much the equivalent performance and exactly the same results.  I'm guessing that all of these calls actually end up in the same function anyway since < and operator.lt both map to __lt__ on a numpy array, which is probably implemented using numpy.less or the equivalent?
So, which is more 'idiomatic' and 'preferred'?
",382
15689237,15689361,2,"In this case, the preferred form is x < 5000 because it is simpler and you are already using a numpy array.
ufuncs are meant to allow these operations to be done on any type of data (not only numpy arrays)
>>> numpy.less([1, 2, 3, 4, 6, 8], 5)
array([ True,  True,  True,  True, False, False], dtype=bool)

>>> [1, 2, 3, 4, 6, 8] < 5
False

On Python 3, this last comparison will raise an error.
",115
15689260,15689260,1,"I have a large application that has many small windows.  I wanted to use traitsui's file dialog to open some files from these windows.  However, when I do, the file dialog correctly spawns and picks a file, but it also consistently switches the active window to an undesirable window after it is finished.  I am really confused why.
Here is a simplified test which displays the same problem:
from traitsui.api import *
from traits.api import *
from traitsui.file_dialog import *

class BigApplication(HasTraits):
  subwindow=Instance(HasTraits)
  open_subwindow=Button('clickme')

  traits_view=View(Item(name='open_subwindow'),height=500,width=500)

  def _subwindow_default(self):
    return Subwindow()

  def _open_subwindow_fired(self):
    self.subwindow.edit_traits()

class Subwindow(HasTraits):
  f=File
  some_option = Bool
  openf=Button('Browse for file')

  traits_view=View(Item(name='f',style='text'),
                Item(name='some_option'),
                Item(name='openf'),buttons=OKCancelButtons)

  def _openf_fired(self):
    self.f=open_file()

BigApplication().configure_traits()

When open_file returns and selects the desired file, the active window is switched to the BigApplication window and not back to the Subwindow window (so that the user can select some additional options before clicking OK).
",232
15689260,17602727,2,"I found a hacky workaround, as per usual.  But this behavior is still a bug.
The workaround is to dispose() of the old window and then to call edit_traits() on it.  This edits the File trait and also happens to make it the active window.  Disposing of the window manually has to be done inside the handler and is a little trickier than might be expected.
from traits.api import *
from traitsui.api import *
from traitsui.file_dialog import *

class BigApplication(Handler):
  subwindow=Instance(Handler)
  open_subwindow=Button('clickme')

  traits_view=View(Item(name='open_subwindow'),height=200,width=200)

  def _subwindow_default(self):
    return Subwindow()

  def _open_subwindow_fired(self):
    self.subwindow.edit_traits()

class Subwindow(Handler):
  f=File
  some_additional_option=Bool
  openf=Button('Browse')

  traits_view=View(Item(name='f',style='text'),
    Item(name='some_additional_option'),
    Item(name='openf'),
    buttons=OKCancelButtons)

  def _openf_fired(self):
    self.f=open_file()
    self.do_dispose(self.info)
    self.edit_traits()

  #handler methods
  def init_info(self,info):
    self.info=info
  def do_dispose(self,info):
    info.ui.dispose()

BigApplication().configure_traits()

",217
15689290,15689331,2,"You're looking for:
for x in (str(y) for y in range(0, 5)):

This doesn't come with the overhead of instantiating the str list (though with 5 elements it's hardly an issue). Whenever I encounter this, though, I generally just use str(y) within the body of the loop.
",70
15689290,15689290,1,"Real simple question. I'm trying to do this in python: 
for x = str(y) in range(0,5):

so that y will hold an integer, and x will hold the string representation of said integer. I realize I could 2 line it pretty easily - just wanting a shortcut.
Also, I'm curious as to what exactly I'm asking. I'm sure this has been answered a bazillion times throughout the forum, but I have no idea what the heck to search for to find it.
",102
15689290,15689317,2,"You can use list comprehension for achieving this:
str_list = ['%s'%(x) for x in range(0, 5)]

",30
15689290,15689346,2,"An easy way is to use the map() function:
for y in map(str, range(0, 5)):
    # ...

The map() function takes an iterable and passes each elem through the first arg which is a callable.
",50
15689290,15689351,2,"You want to search for List Comprehensions most likely.
[(str(y),y) for y in range(0,5)]

This will give you tuples of strings and ints as a list, then iterate through that list like you would any other
",50
15689290,15689400,2,"Your question is unclear, since it relies upon a piece of code that doesn't actually compile.
for x,y in ((str(z), z) for z in range(0, 5)):
  print x, y

The above construct will bind x and y to a str and an int, respectively, for each loop iteration. The output is:
0 0
1 1
2 2
3 3
4 4

",84
15690071,15690071,1,"How do I save the list of items if 'boa' is in href to a list? I don't want to print them using get() but instead convert them to a list in their own variable (it seems these are in a dictionary?), preferably to boat_links. Thanks!
import urllib2
from bs4 import BeautifulSoup

#Open Craigslist with BeautifulSoup and save to file

url = 'http://losangeles.craigslist.org/boo/'

response = urllib2.urlopen(url)
webContent = response.read()

f = open('C:\Users\dell\Desktop\python\\boat_crawler\craigslist.html', 'w')
f.write(webContent)
f.close

html_doc = open('C:\Users\dell\Desktop\python\\boat_crawler\craigslist.html')

soup = BeautifulSoup(html_doc)

boat_links = []

for a in soup.find_all('a'):
    if 'boa' in a['href']:
    print a.get('href')

",149
15690071,17243193,2,"I am not sure if you want a list or dictionary or a dictionary of lists so here are all of them
if a.get('href').find('boa')>-1:
    boat_links.append(a.get('href'))

Here is a dictionary with the a tags text as the key and the href as the value
boat_links = {}
for a in soup.find_all('a'):
    if a.get('href').find('boa')>-1:
         boat_links[a.text] = a.get('href')

Here is a dictionary of lists based on the a.tags (what if you have multiple links with the same text)
boat_links = {}
for a in soup.find_all('a'):
    if a.get('href').find('boa')>-1
         if boat_links.has_key(a.text):
              boat_links[a.text].append(a.get('href'))
         else:
              boat_links[a.text] = [a.get('href')]

",179
15690185,15690185,1,"I know this question is pretty basic, and I'm sorry for posting it, but I literally have spent the last hour googling and I still havent found an answer.
I coded a blackjack game using pygame, and want to convert it to wxpython mostly as a way to learn wxpython. In pygame the graphics are easy, I have a base image I display (basically just the table), and then I can just display other images on top of it as the action unfolds, for example the cards. Each time the player hits I just draw a new card in a different position. When The hand is over and I want to go to the next hand, I just display the base image again, and it covers everything up and viola! Its really simple all I have to use is blit() and pygame.display.update().
I cant seem to find anyway to do this in wxpython. All the examples I find are for drawing items in new widgets, or drawing vector graphics over an image, or opening new frames with images, etc.
any help is greatly appreciated, thank you very much.
--Daniel
",218
15690185,15690383,2,"The AlphaDrawing example in the wxPython demo shows how to overlap multiple drawings. This uses wx.GraphicContext / wx.GCDC. If you look at the documentation, you'll note that it has CreateBitmapFromImage and CreateBitmap methods that probably apply to what you want to do. See also the DragImage demo and the following links for related information:

wx python card game
http://rummy-py.sourceforge.net/ (look at the original that was done in wx)

",78
15690185,15724125,2,"I found this post, which reading the comments, does a good job of showing me want I was looking for. 
Delete image in wxpython?
",28
15690201,15690201,1,"I have a server that I'd like to use to maintain persistent connections with a set of devices, just so that they can pass simple messages back and forth. It's a trivial task, but selecting a server-side platform has been suprrisingly difficult (especially since I have no administrative privileges - it's a dedicated commercial server).
My best idea so far is to write a TCP server in Python. The Twisted platform seems suitable for the task, and has a lot of good reviews. However, my server has Python 2.7 but not Twisted, and the admins have been reluctant to install it for me.
Is there any way that I can just upload a Twisted package to the server and reference it in my libraries without installing it as a framework?
",146
15690201,15705793,2,"Use virtualenv to create your private Python libraries installation.
",10
15690201,15692480,2,"I'm not sure what you mean by ""installing it as a framework"".  If you are using an OS X server hosting environment, then maybe you're talking about Framework with a Capital F.  However, OS X server hosting isn't a very common environment so I'm guessing that's not it.
If you just want to know how to install a Python library in your home directory, then the general answer is:
$ python setup.py install --user

This Just Works™ on Python 2.7 (assuming the package uses distutils, which Twisted does, and you unpack the source .tar.gz and change your working directory to the directory that is the root of the contents of that .tar.gz), so you should be done after that.
",139
15690224,15690224,1,"I've recently decided to start using .format() instead of % (see this question). Instead of the {0}, {1} syntax, I'm wondering if the following is an acceptable use:
import os
def get_filename(player_name):
    for ext in ('jpg', 'jpeg', 'png'):
        filename = ""data/avatars/{player_name}.{ext}"".format(**locals())
        if os.path.exists(filename):
            return filename
    return None

I like the straightforwardness of it - local variables go into the string - but am wondering if there's any reason I shouldn't do the above.
",121
15690224,15690479,2,"As I commented above, this should not be used on untrusted sources. Also it may not me explicit enough to be Pythonic.
One can also define a function to do that, but to access the right locals, it needs to do some frame handling
def format_locals(string):
    return string.format(**sys._getframe().f_back.f_locals)

This kind of pattern is not nice and things like Pypy can't optmize this kind of code.
I would use this code (unless you need Python 2.6 support so you must add the indexes):
filename = 'data/avatars/{}.{}'.format(player_name, ext)

",116
15690224,15690429,2,"The major problem with passing locals() (or globals()) to format (or %) is that often, format strings can come from untrusted sources, and you risk exposing variables you didn't want to. If you're just formatting a literal string, that isn't an issue, but if you ever may have untrusted format strings, you have to think very carefully about what you're doing—and it's easier to just not do it.
The more minor problem is that some of your code's readers won't understand locals, or the ** syntax, and will have a hard time figuring out what it does or why it works. This isn't much of an argument. In fact, you could even say that a lot of Python's design decisions come down to making sure this is almost never a good argument—the language is exactly big enough that it's reasonable to expect your readers to understand/learn anything pythonic you write. But it's still worth thinking about.
Then there's the style issue. Every few months, someone comes along suggesting that the language should make it easier to do this, and it starts an argument on the mailing lists. Some people definitely think that this feels ""implicit rather than explicit"". Others disagree. I think it's pretty well settled that magic locals here would not be pythonic… but if you have to explicitly pass locals(), maybe that's fine and maybe it isn't. Like most style arguments that haven't gathered a consensus, it's really up to you. (By the way, the format API ultimately came out of an argument like this, where the original suggestion was for more-perl-like string interpolation with an implicit locals.)
But ultimately, you have to consider what you're saving. Compare this:
filename = ""data/avatars/{player_name}.{ext}"".format(**locals())

to:
filename = ""data/avatars/{0}.{1}"".format(player_name, ext)

Your version isn't clearer, more explicit, easier to type, or even shorter. So, I'd say the risk of making it a little harder for novices to read, and annoying to some segment of the community (even if it's for bad reasons), isn't worth it if there's no benefit.
",444
15693448,15701844,2,"The response to your login request will contain a Set-Cookie header looking something like this:
Set-Cookie:session=<encoded session>; Path=/; HttpOnly

You need to send that cookie with your curl request so that the session data is available for processing, you can add additional headers to curl requests with -H, or specify the cookie explicitly:
curl --cookie ""session=<encoded session>"" http://localhost:5000/address/

Browsers will handle this for you of course, but curl is totally stateless and wont parse and store the Set-Cookie header for you by default, though if you're performing the login using curl, you can tell it to store the cookie in a cookie jar with -c <file>, and then you can read from it on your next request with -b file
HTTP Cookie wiki page
Curl cookie docs
Curl man page
",156
15693448,15693448,1,"I am building a RESTful application with Flask which will use sessions to track the logged in user. Here is the login code which I adapted from this Flask tutorial
@mod.route('/login/', methods=['GET', 'POST'])
def login():
  user = User.query.filter_by(email=request.json['email']).first()
  # we use werkzeug to validate user's password
  if user and check_password_hash(user.password, request.json['password']):
    # the session can't be modified as it's signed, 
    # it's a safe place to store the user id
    session['user_id'] = user.id
    resp = jsonify({'status':'authenticated'})
  else:
    resp = jsonify({'status':'Invalid usernam/password'})
    resp.status_code = 401
  return resp

When a user first logs in, I store their userid in the session, so that when the same user requests a resource, the data is tailored to them:
@mod.route('/address/')
@requires_login
def user_data():
  user = User.query.filter_by(id=session['usr_id']).first
  resp = jsonify(user.address)
  return resp

If I issue this command after logging in:
curl http://localhost:5000/address/

I receive:
{""status"": 401, ""message"": ""Unauthorized""}

instead of the address information for my logged in user.
Can anyone tell me how I can use the session in subsequent curl calls to return data that is specific to the userid stored in the cookie?
",283
15693529,16034387,2,"There are a number of ways you can achieve what you want. For example, one way is to make a custom initialiser for your handler:
import os
import yaml

def logmaker():
    path = os.path.dirname(os.path.realpath(__file__))
    path = os.path.join(path, 'bot.log')
    return logging.FileHandler(path)

def main():
    # The file's path
    path = os.path.dirname(os.path.realpath(__file__))

    # Config file relative to this file
    loggingConf = open('{0}/logging.yml'.format(path), 'r')
    logging.config.dictConfig(yaml.load(loggingConf))
    loggingConf.close()
    logger = logging.getLogger('cloaked_chatter')
    logger.debug('Hello, world!')

if __name__ == '__main__':
    main()

Note that I moved the logging.yml to be adjacent to the script. The logmaker is the custom initialiser. Specify it in the YAML as follows:
version: 1
formatters:
  default:
    format: '%(asctime)s %(levelname)s %(name)s %(message)s'
handlers:
  console:
    class: logging.StreamHandler
    level: DEBUG
    formatter: default
    stream: ext://sys.stdout
  file:
    () : __main__.logmaker
    formatter: default
loggers:
  cloaked_chatter:
    level: DEBUG
    handlers: [console, file]
    propagate: no

If you run the Python script, you should find that the bot.log is created adjacent to the script and YAML file. The same message is printed to the console and bot.log:
2013-04-16 11:08:11,178 DEBUG cloaked_chatter Hello, world!

N.B. The script could be a little tidier, but it illustrates my point.
Update: As per the documentation, the use of () as a key in dictionary indicates that the value is a callable which is essentially a custom constructor for the handler.
",330
15693529,15693529,1,"In my Python program I have the following code:
def main():
    # The file's path
    path = os.path.dirname(os.path.realpath(__file__))
    ...
    # Config file relative to this file
    loggingConf = open('{0}/configs/logging.yml'.format(path), 'r')
    logging.config.dictConfig(yaml.load(loggingConf))
    loggingConf.close()
    logger = logging.getLogger(LOGGER)
    ...

and this is my logging.yml configuration file:
version: 1
formatters:
  default:
    format: '%(asctime)s %(levelname)s %(name)s %(message)s'
handlers:
  console:
    class: logging.StreamHandler
    level: DEBUG
    formatter: default
    stream: ext://sys.stdout
  file:
    class : logging.FileHandler
    formatter: default
    filename: bot.log
loggers:
  cloaked_chatter:
    level: DEBUG
    handlers: [console, file]
    propagate: no

The problem is that the bot.log file is created where the program is launched. I want it to always be created in the project's folder, i.e. in the same folder as my Python program.
For an example, launching the program with ./bot.py would create the log file in the same folder. But launching it with python3 path/bot.py would create the log file a level above the Python program in the file hierarchy.
How should I write the filename in the config file to solve this? Or do I need to write a custom handler? If so, how? Or is this not possible to solve using dictConfig?
",273
15693565,15953727,2,"I solved my own problem and even though it's a bit of a corner case, I'll still answer it here. The script that Jenkins was launching itself launched a threaded Python server using ThreadedTCPServer implemented more or less exactly from here. That threaded server wasn't properly exiting, so it left some pipes open. Although the server process died, the leaky pipes made it impossible for Jenkins to properly determine that the process was over (Jenkins waits for an EOF from the child process to determine if the process completed). The solution was a reimplementation of the socket server that exited properly. Hope this helps someone in the future!
",122
15693565,15693722,2,"To debug this:

Add set -x towards the top of your shell script.
Set a PS4 which prints the line number of each line when it's invoked: PS4='+ $BASH_SOURCE:$FUNCNAME:$LINENO:'
Look in particular for any places where your scripts assume environment variables which aren't set when Hudson is running.

If your Python scripts redirect stderr (where logs from set -x are directed) and don't pass it through to Hudson (and so don't log it), you can redirect it to a file from within the script: exec 2>>logfile
There are a number of tools other than Jenkins for kicking off jobs across a number of machines, by the way; MCollective (which works well if you already use Puppet), knife ssh (which you'll already have if you use Chef -- which, in my not-so-humble opinion, you should!), Rundeck (which has a snazzy web UI, but shouldn't be used by anyone until this security bug is fixed), Fabric (which is a very good choice if you don't have mcollective or knife already), and many more.
",219
15693565,15693565,1,"I have a ton of scripts I need to execute, each on a separate machine. I'm trying to use Jenkins to do this. I have a Python script that can execute a single test and handles time limits and collection of test results, and a handful of Jenkins jobs that run this Python script with different args. When I run this script from the command line, it works fine. But when I run the script via Jenkins (with the exact same arguments) the test times out. The script handles killing the test, so control is returned all the way back to Jenkins and everything is cleaned up. How can I debug this? The Python script is using subprocess.popen to launch the test.
As a side note, I'm open to suggestions for how to do this better, with or without Jenkins and my Python script. I just need to run a bunch of scripts on different machines and collect their output.
",180
15693597,15693597,1,"I have three lists (L1, L2, L3), something like :
L1 = [1,2]
L2 = ['a','b']
L3 = ['A','B']

I want to compute the product of L1*L2*L3, id est,
itertools.product(L1,L2,L3) = [ [1,'a','A'], [1,'a','B'], ... ]

but I want to take or not in account some lists; hence,[1,], [ 1, 'a' ] would be a part of the result, like ['a',], ['a', 'B'] and so on.
Any idea to help me ? Thanks !
",141
15693597,15693791,2,"Use the powerset function given in the itertools examples. powerset([L1,L2,L3]) will give you all subsets of the set of 3 lists. For each subset you can take the cartesian product, and then chain them all together.
>>> from itertools import chain, product
>>> result = chain.from_iterable(product(*lists) for lists in powerset([L1,L2,L3]))        
>>> list(result)

[(), (1,), (2,), ('a',), ('b',), ('A',), ('B',), (1, 'a'), (1, 'b'), (2, 'a'), 
(2, 'b'), (1, 'A'), (1, 'B'), (2, 'A'), (2, 'B'), ('a', 'A'), ('a', 'B'), 
('b', 'A'), ('b', 'B'), (1, 'a', 'A'), (1, 'a', 'B'), (1, 'b', 'A'), 
(1, 'b', 'B'), (2, 'a', 'A'), (2, 'a', 'B'), (2, 'b', 'A'), (2, 'b', 'B')]

",296
15693605,15707814,2,"A more conventional way to store ""meta"" data along with a dict would be either:

to maintain two dicts with same set of keys, one for the actual data, one for ""meta""
to have a dict with (""raw"") keys, and values are 2-tuples: ( value, item-meta-data )

Both are simple and require no special magic.  You would also avoid problems like the one you describe in your question (and others to come).
",90
15693605,15707644,2,"I've made a slight modification to your base code:
def __repr__(self):
    return 'DictKey(' + self.member + ')'

Then if you want to retrieve the instance of DictKey in the set of keys you can do the following:
index_of_instance = d.keys().index('hello')
my_instance_of_dict_key = d.keys()[index_of_instance]

Hope it helps.
",70
15693605,15693605,1,"I have created a class that will allow me to store meta-data with an arbitrary dictionary key and still pass the in test with the original object type:
class DictKey:

    def __init__(self, key):
        self.hashkey = hash(key)
        self.member = key

    def __hash__(self):
        return self.hashkey

    def __repr__(self):
        return 'DictKey(' + self.strkey + ')'

    def __cmp__(self, o):
        return cmp(self.member, o)

d = {}
key = DictKey('hello')
d[key] = 'world'

print key.hashkey
print hash('hello')
print key in d
print 'hello' in d
print DictKey('hello') in d

produces the output:
840651671246116861
840651671246116861
True
True
True

Now, given the string 'hello', I need to get the instance of DictKey that was created from said string in constant time:
if 'hello' in d:
    #need some way to return the instance of DictKey so I can get at it's member
    tmp = d.getkey('hello') 
    tmp.member

",197
15693633,15693633,1,"I'm trying to create a sqlite3 table using python. My code is given below:
def initDb():
    database = 'index.db'
    conn = sqlite3.connect(database)

    cur = conn.cursor()

    # Initialize database
    cur.execute('PRAGMA foreign_keys = ON')

    cur.execute('DROP TABLE IF EXISTS modules')
    cur.execute('DROP TABLE IF EXISTS files')
    cur.execute('DROP TABLE IF EXISTS modulesfiles')

    cur.execute(
        '''CREATE TABLE modules (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            label TEXT UNIQUE NOT NULL
        )'''
    )
    cur.execute(
        '''CREATE TABLE files (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            filename TEXT UNIQUE NOT NULL
        )'''
    )
    cur.execute(
        '''CREATE TABLE modulesfiles (
        module INTEGER UNIQUE NOT NULL,
        file INTEGER UNIQUE NOT NULL,
        PRIMARY KEY (module,file),
        FOREIGN KEY (module) REFERENCES modules(id) ON UPDATE CASCADE ON DELETE CASCADE,
        FOREIGN KEY (file) REFERENCES files(id) ON UPDATE CASCADE ON DELETE CASCADE
        )'''
    )

    cur.close()

    return conn

if __name__ == '__main__':
    conn = initDb()
    conn.commit()
    conn.close()

This code runs fine the first time I run it and my database is created. However, if I run it a second time, I get the following error:
    cur.execute('DROP TABLE IF EXISTS files')
sqlite3.OperationalError: no such table: main.modules

I have no idea what's going wrong. Can anybody help?
",268
15693633,15699000,2,"Dropping modules first makes the foreign key constraint in modulesfiles invalid.
Drop the child table first.
",18
15693703,15693703,1,"I am using os.system() to run a python program and am trying to log its output to the file. This works fine. 
os.system(""myprogram.py -arg1 -arg2 > outputfile.txt"")
#something here to kill/cleanup whatever is left from os.system()
#read outputfile1.txt- this output file got all the data I needed from os.system()

The problem is that myprogram.py calls another python program which gives me the output i need but doesn't finish- I can even see the prompt become different as in the picture below

Is there a way to kill the child process when I get to the next line of my program 
I tried using os.system(""quit()"") and subprocess.popen(""quit()"", shell=False) and that didn't do anything.
I can't really use exit() because that just kills python all together.
Btw this stalls 
f=subprocess.Popen(""myprogram.py -arg1 > outputfile.txt"") and then 
f.communicate() #this just stalls because the child program does not end. 

",190
15693703,15694075,2,"The program that is being called by myprogram.py lands you in the python prompt. Why that happens we cannot tell you unless you show us the code.
Using the subprocess module (which is more versatile) is preferred to using os.system.
But you're not using subprocess correctly. Try it like this:
with open('outputfile.txt', 'w+') as outf:
    rc = subprocess.call(['python', 'myprogram.py', '-arg1'], stdout=outf)

The with statement will close the file once subprocess.call is done. The program and its arguments should be given as a list of strings. Redirection is achieved by using the std... arguments.
After myprogram.py finishes, rc contains its return code.
If you want to capture the output of the program, use subprocess.check_output() instead.
",151
15693735,15693735,1,"How can I generate all the paths to text strings in a HTML document, preferably using BeautifulSoup? 
I have f.e. this code:
<DIV class=""art-info""><SPAN class=""time""><SPAN class=""time-date"" content=""2012-02-28T14:46CET"" itemprop=""datePublished"">
             28. february 2012
            </SPAN>
            14:46
           </SPAN></DIV><DIV>
           Something,<P>something else</P>continuing.
          </DIV>

I'd like to divide HTML code into paths to text strings, like
str1 >>>  <DIV class=""art-info""><SPAN class=""time""><SPAN class=""time-date"" content=""2012-02-28T14:46CET"" itemprop=""datePublished"">28. february 2012</SPAN></SPAN></DIV>
str2 >>>  <DIV class=""art-info""><SPAN class=""time"">14:46</SPAN></DIV>
str3 >>>  <DIV>Something,continuing.</DIV>
str4 >>>  <DIV><P>something else</P></DIV>

or
str1 >>>  <DIV><SPAN><SPAN>28. february 2012</SPAN></SPAN></DIV>
str2 >>>  <DIV><SPAN>14:46</SPAN></DIV>
str3 >>>  <DIV>Something,continuing.</DIV>
str4 >>>  <DIV><P>something else</P></DIV>

or  
str1 >>>  //div/span/span/28. february
str2 >>>  //div/span/14:46
str3 >>>  //div/Something,continuing.
str4 >>>  //div/p/something else

I've studied BeautifulSoup documentation, but I can't figure out how to do it. Do you have any ideas?
",327
15693735,15694498,2,"from bs4 import BeautifulSoup
import re
file=open(""input"")
soup = BeautifulSoup(file)
for t in soup(text=re.compile(""."")):
  path = '/'.join(reversed([p.name for p in t.parentGenerator() if p]))
  print path+""/""+ t.strip()

Output
[document]/html/body/div/span/span/28. february 2012
[document]/html/body/div/span/14:46
[document]/html/body/div/Something,
[document]/html/body/div/p/something else
[document]/html/body/div/continuing.

",86
15693756,15693756,1,"How can I convert Extended ASCII characters such as: ""æ, ö or ç"" into non-extended ASCII characters (a,o,c) using python? The way it works should be that if it takes ""A, Æ ,Ä"" as input, It returns A for all of them.
",59
15693756,15693802,2,"Unidecode might be of use to you.
Python 3.2.3 (default, Jun  8 2012, 05:36:09) 
[GCC 4.7.0 20120507 (Red Hat 4.7.0-5)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from unidecode import unidecode
>>> unidecode(""æ, ö or ç"")
'ae, o or c'

",77
15693808,15693808,1,"I know that in order to install a package I need to execute:
sudo pip install package_name

But how can I know what is the name of package -  I should give as an argument.
I found in pypi a package I want to install - pcapy:
https://pypi.python.org/pypi/pcapy/0.10.3
I tried:
sudo pip install pcapy

It didn't work...
What is the right way to install this package?
Thank you very much!
",80
15693808,15693858,2,"That package isn't in the PyPI. There's a page, but the source code is hosted elsewhere for some reason (this is the first time I've seen it):
Ubuntu has a (probably old) package:
$ sudo apt-get install python-pcapy

You can also build it from source:
$ sudo pip install ""http://corelabs.coresecurity.com/index.php?module=Wiki&action=attachment&type=tool&page=Pcapy&file=pcapy-0.10.8.tar.gz""

",76
15693859,15693859,1,"I have a bash script myscript.sh:
#!/bin/bash
while read line; do
    myprog.py
done

calling a python program myprog.py
#!/usr/bin/env python
import subprocess
output = subprocess.check_output(['ssh', 'user@host', 'cmd'])

The ssh command that is called by subprocess executes without error, the output is correct. But when called like this the loop in myscript.sh only runs through the first line of input and then exits with status 0.  If I replace the subprocess.check_output(...) call with a subprocess.Popen(...) and don't subsequently call Popen.wait() then the outer loop works as expected and the output from the ssh command is dumped to standard out some time after any output from the bash script.  With the Popen.wait() behavior is the same as with check_output: bash loop only goes through one iteration before exiting without error.  
If instead of ssh another command, e.g. ls, is called with check_output then the bash loop works as expected.
Can anyone help me understand why the code as shown isn't working as expected?
Note: this is a simplified version of what I am trying to do, though I do experience the same behavior with this code.  In reality I am doing something with ""$line"" in the bash script and the subprocess call is wrapped in a try/except block.
",254
15693859,15693954,2,"As @larsmans guessed the ssh call was consuming stdin, breaking the outer bash loop.  Adding the -n option to the ssh command resolved the issue:
output = subprocess.check_output(['ssh', '-n', 'user@host', 'cmd'])

",49
15693880,15707208,2,"You need to use the Text widget's get method to get all of the text from '1.0' (line 1, character 0) to END. 
Here's a modified version of your code that does this in a write_text function. I also added scroll bars and switched to using grid instead of pack.
from tkinter import *
from tkinter import ttk

def write_text():
    text = edit.get('1.0', END)
    with open(""DATA/Test.txt"", ""w"") as f:
        f.write(text)

root = Tk()
root.title(""This May Help"")
root.geometry(""600x600"")

edit = Text(root, width=80, height=25, wrap=NONE)
edit.insert('1.0', '[enter text]')
edit.grid(column=0, row=0, sticky=(N,W,E,S))

yscroll = ttk.Scrollbar(root, orient=VERTICAL, command=edit.yview)
yscroll.grid(column=1, row=0, sticky=(N,S))
edit['yscrollcommand'] = yscroll.set

xscroll = ttk.Scrollbar(root, orient=HORIZONTAL, command=edit.xview)
xscroll.grid(column=0, row=1, sticky=(W,E))
edit['xscrollcommand'] = xscroll.set

write_button = Button(text=""Write"", command=write_text)
write_button.grid(column=0, row=2)

",236
15693880,15693880,1,"Hi so i'm having a hard time getting Text from my program on Python to convert to a string so i can write it to a file without it just writing numbers in the file. i put this code :
from tkinter import *

a = Tk()
a.title(""i need help"")
a.geometry(""600x600"")

entryText = StringVar(None)

codeEdit = Text(a)
codeEdit.insert(END, """")
codeEdit.pack(side='top')

text = str(codeEdit)

def setLoc():
    saveFile = open(""DATA\Test.txt"", ""w"")
    saveFile.write(text)
    saveFile.close()


    return 

writeButton = Button(text=""Write"",command=setLoc)
writeButton.pack(side='bottom')

so thats the code to write the obj locFile to the file Test.txt but when i type something in the Text box on the program and hit the writButton it will write to the file just not what i typed in it puts .50132192 so i wanted to know how i could convert it to a String? 
",194
15693900,15694036,2,"Seeing that the text is the only large blob, and everything else is barely larger than a pixel, a simple morphological opening should suffice
You can do this in opencv
or with imagemagic
Afterwards the white rectangle should be the only thing left in the image. You can find it with opencvs findcontours, with the CvBlobs library for opencv or with the imagemagick -crop function
Here is your image with 2 steps of erosion followed by 2 steps of dilation applied:

You can simply plug this image into the opencv findContours function as in the Squares tutorial example to get the position
",107
15693900,15699181,2,"Using your test image I was able to remove all the noises with a simple erosion operation.

After that, a simple iteration on the Mat to find for the corner pixels is trivial, and I talked about that on this answer. For testing purposes we can draw green lines between those points to display the area we are interested at in the original image:

At the end, I set the ROI in the original image and crop out that part. 
The final result is displayed on the image below:

I wrote a sample code that performs this task using the C++ interface of OpenCV. I'm confident in your skills to translate this code to Python. If you can't do it, forget the code and stick with the roadmap I shared on this answer.
#include <cv.h>
#include <highgui.h>

int main(int argc, char* argv[])
{
    cv::Mat img = cv::imread(argv[1]);
    std::cout << ""Original image size: "" << img.size() << std::endl;

    // Convert RGB Mat to GRAY
    cv::Mat gray;
    cv::cvtColor(img, gray, CV_BGR2GRAY);
    std::cout << ""Gray image size: "" << gray.size() << std::endl;

    // Erode image to remove unwanted noises
    int erosion_size = 5;
    cv::Mat element = cv::getStructuringElement(cv::MORPH_CROSS,
                                       cv::Size(2 * erosion_size + 1, 2 * erosion_size + 1),
                                       cv::Point(erosion_size, erosion_size) );
    cv::erode(gray, gray, element);

    // Scan the image searching for points and store them in a vector
    std::vector<cv::Point> points;
    cv::Mat_<uchar>::iterator it = gray.begin<uchar>();
    cv::Mat_<uchar>::iterator end = gray.end<uchar>();
    for (; it != end; it++)
    {
        if (*it) 
            points.push_back(it.pos()); 
    }

    // From the points, figure out the size of the ROI
    int left, right, top, bottom;
    for (int i = 0; i < points.size(); i++)
    {
        if (i == 0) // initialize corner values
        {
            left = right = points[i].x;
            top = bottom = points[i].y;
        }

        if (points[i].x < left)
            left = points[i].x;

        if (points[i].x > right)
            right = points[i].x;

        if (points[i].y < top)
            top = points[i].y;

        if (points[i].y > bottom)
            bottom = points[i].y;
    }
    std::vector<cv::Point> box_points;
    box_points.push_back(cv::Point(left, top));
    box_points.push_back(cv::Point(left, bottom));
    box_points.push_back(cv::Point(right, bottom));
    box_points.push_back(cv::Point(right, top));

    // Compute minimal bounding box for the ROI
    // Note: for some unknown reason, width/height of the box are switched.
    cv::RotatedRect box = cv::minAreaRect(cv::Mat(box_points));
    std::cout << ""box w:"" << box.size.width << "" h:"" << box.size.height << std::endl;

    // Draw bounding box in the original image (debugging purposes)
    //cv::Point2f vertices[4];
    //box.points(vertices);
    //for (int i = 0; i < 4; ++i)
    //{
    //    cv::line(img, vertices[i], vertices[(i + 1) % 4], cv::Scalar(0, 255, 0), 1, CV_AA);
    //}
    //cv::imshow(""Original"", img);
    //cv::waitKey(0);

    // Set the ROI to the area defined by the box
    // Note: because the width/height of the box are switched, 
    // they were switched manually in the code below:
    cv::Rect roi;
    roi.x = box.center.x - (box.size.height / 2);
    roi.y = box.center.y - (box.size.width / 2);
    roi.width = box.size.height;
    roi.height = box.size.width;
    std::cout << ""roi @ "" << roi.x << "","" << roi.y << "" "" << roi.width << ""x"" << roi.height << std::endl;

    // Crop the original image to the defined ROI
    cv::Mat crop = img(roi);

    // Display cropped ROI
    cv::imshow(""Cropped ROI"", crop);
    cv::waitKey(0);

    return 0;
}

",913
15693900,15693900,1,"I asked a similar question here but that is focused more on tesseract. 
I have a sample image as below. I would like to make the white square my Region of Interest and then crop out that part (square) and create a new image with it. I will be working with different images so the square won't always be at the same location in all images. So I will need to somehow detect the edges of the square. 

What are some pre-processing methods I can perform to achieve the result?
",99
15693938,17378072,2,"I shouldn't give full answers as JES is an application designed for students, but I think that three months later one can give a full working sample which can be used as reference for the others...
This should be close to what you attempted to do:
Note : your approach of a simple double loop over x and y was the right one.
def crazyPic(pic, newRed, newGreen, newBlue):

    w = getWidth(pic)
    h = getHeight(pic)
    new_w = w * 2
    new_h = h * 2
    newPic = makeEmptyPicture(w * 2, h * 2)

    for x in range(new_w):
      for y in range(new_h):
          new_px = getPixel(newPic, x, y)

          # Top-left: B&W
          if (x < w) and (y < h):
            px = getPixel(pic, x, y)
            nRed = getRed(px) * newRed #0.299
            nGreen = getGreen(px) * newGreen #0.587
            nBlue = getBlue(px) * newBlue #0.114
            luminance = nRed + nGreen + nBlue
            new_col = makeColor(luminance, luminance, luminance)

          # Top-right
          elif (y < h):
            px = getPixel(pic, x - w, y)
            nRed = getRed(px) * newRed
            new_col = makeColor(nRed, getGreen(px), getBlue(px))

          # Bottom-left
          elif (x < w):
            px = getPixel(pic, x, y - h)
            nGreen = getGreen(px) * newGreen
            new_col = makeColor(getGreen(px), nGreen, getBlue(px))

          # Bottom-right
          else:
            px = getPixel(pic, x - w, y - h)
            nBlue = getBlue(px) * newBlue
            new_col = makeColor(getGreen(px), getBlue(px), nBlue)

          setColor(new_px, new_col)

    return newPic

file = pickAFile()
picture = makePicture(file)
#picture = crazyPic(picture, 0.299, 0.587, 0.114)
# Here, with my favorite r, g, b weights
picture = crazyPic(picture, 0.21, 0.71, 0.07)

writePictureTo(picture, ""/home/quartered.jpg"")

show(picture)


Output (Painting by Antoni Tapies):

......From......

Here is a more detailed thread about greyscale.

",438
15693938,15693938,1,"My goal is to double the picture size then change the left half to grayscale, then change the green value of the top right half and the blue value of the bottom right half. I have values that I found in my textbook for the grayscale but im not sure if thats what I actually use. And I also am unsure if I program each of these different values using for loops or just something different 
So far my code is:
 def crazyPic(newGreen,newBlue,pic,file):
      show(pic)
      newPic = makeEmptyPicture(getWidth(pic)*2,getHeight((pic)*2
           for x in range(width):
              for y in range(height):
                  for px in getPixel(pic,0,100):
                  nRed = getRed(px) * 0.299
                  nGreen = getGreen(px) * 0.587
                  nBlue = getBlue(px) * 0.114
                  luminance = nRed + nGreen + nBlue
                  setColor(px,makeColor(luminance,luminance,luminance)

",184
15695846,15695918,2,"Your add function doesn't have anything that will increment self.size. So it's whatever you set it to in __init__, which is presumably 0.
So, when the list actually is empty, __len__ returns 0 because self.head is None.
And after you add an element, it still returns 0 because self.size is 0.

Also, your code has at least one other problem in it. Look at this:
elif value > self.tail.data:
    self.tail = newNode
    newNode.prev = self.tail
    newNode.next = None
    return newNode

Clearly, newNode.prev is going to end up pointing at itself, rather than the previous tail.
There are lots of things that can help judge the correctness of code—unit tests, code reviews by someone who didn't work on it, stepping through it with an interactive visualizer, formal proofs, etc.—but the number of hours you worked on it is not one of those things.
",165
15695846,15695846,1,"I cant seem to get my len function to work, ive been trying heaps of stuff but im a complete beginner so im pretty sure im missing something completely obvious. This is my code...
 def __len__(self):
    if self.head is None:
        return 0
    else:
        return self.size

My thinking behind this is simple. If the head of the doubly linked list is None, then it must be empty so return 0, otherwise, just return the size of the list.
However, i get an asssertion error saying...
AssertionError: List should contain 1 element, but length is 0

Any help is appreciated, thanks in advance.
EDIT: This is the code thats running my function...
testList.add(14)
assert len(testList) == 1, ""List should contain 1 element, but length is %r"" % len(testList)

EDIT2: this is my add function, im pretty sure its right, i spend 2 hours on it...
def add(self, value):
    newNode = DoubleListNode(value)
    if self.head is None:
        self.head = newNode
        self.tail  = newNode
        newNode.prev = None
        newNode.next = None
        return newNode
    elif value < self.head.data:
        self.head = newNode
        newNode.next = self.head
        newNode.prev = None
        return newNode
    elif value > self.tail.data:
        self.tail = newNode
        newNode.prev = self.tail
        newNode.next = None
        return newNode
    else:
        node = self.head
        node2 = node
        while node is not None and node.data < value :
            node = node.next
            node2 = node.prev
        newNode.next = node.prev
        newNode.prev = node2.next
        return newNode

",280
15699551,15699587,2,"These are format specifiers. The %d specifier refers specifically to a decimal (base 10) integer. The %s specifier refers to a Python string.
",30
15699551,15699580,2,"It is used for string formatting. The random number will be converted to an integer (since %d is being used), then it will be converted to a string and inserted into the given string.
For example:
>>> print ""blah blah blah %d!"" % 4
blah blah blah 4!

There are several other letters that correspond to different types. They can be found here (in the second table of that section).
",88
15699551,15699578,2,"%d stands for ""decimal"". ! is there with the rest of the string. %d means that a decimal number will be outputted there. Synonim for %d is also %i. same goes for instance with %s and strings
",48
15699551,15699551,1,"I was looking at this python tutorial and in the generator section they had something like this:
print ""blah blah blah %d!""%random_number

What I want to know is exactly what does that mean and is there any other letters because I've seen %s before. I've googled everything I could think of and nothing gave me an answer.
",69
15699566,15774990,2,"The cleanest solution would be inotify in many ways - this is more or less exactly what it's intended for, after all. If the log file was changing extremely rapidly then you could potentially risk being woken up almost constantly, which wouldn't necessarily be particularly efficient - however, you could always mitigate this by adding a short delay of your own after the inotify filehandle returns an event. In practice I doubt this would be an issue on most systems, but I thought it worth mentioning in case your system is very tight on CPU resources.
I can't see how the sleep() approach would miss file updates except in cases where the file is truncated or rotated (i.e. renamed and another file of the same name created). These are tricky cases to handle however you do things, and you can use tricks like periodically re-opening the file by name to check for rotation. Read the tail man page because it handles many such cases, and they're going to be quite common for log files in particular (log rotation being widely considered to be good practice).
The downside of sleep() is of course that you'd end up batching up your reads with delays in between, and also that you have the overhead of constantly waking up and polling the file even when it's not changing. If you did this, say, once per second, however, the overhead probably isn't noticeable on most systems.
I'd say inotify is the best choice unless you want to remain compatible, in which case the simple fallback using sleep() is still quite reasonable.
EDIT:
I just realised I forgot to mention - an easy way to check for a file being renamed is to perform an os.fstat(fd.fileno()) on your open filehandle and a os.stat() on the filename you opened and compare the results. If the os.stat() fails then the error will tell you if the file's been deleted, and if not then comparing the st_ino (the inode number) fields will tell you if the file's been deleted and then replaced with a new one of the same name.
Detecting truncation is harder - effectively your read pointer remains at the same offset in the file and reading will return nothing until the file content size gets back to where you were - then the file will read from that point as normal. If you call os.stat() frequently you could check for the file size going backwards - alternatively you could use fd.tell() to record your current position in the file and then perform an explicit seek to the end of the file and call fd.tell() again. If the value is lower, then the file's been truncated under you. This is a safe operation as long as you keep the original file position around because you can always seek back to it after the check.
Alternatively if you're using inotify anyway, you could just watch the parent directory for changes.
Note that files can be truncated to non-zero sizes, but I doubt that's likely to happen to a log file - the common cases will be being deleted and replaced, or truncated to zero. Also, I don't know how you'd detect the case that the file was truncated and then immediately filled back up to beyond your current position, except by remembering the most recent N characters and comparing them, but that's a pretty grotty thing to do. I think inotify will just tell you the file has been modified in that case.
",669
15699566,15699566,1,"I'm writing a Python script that needs to tail -f a logfile.
The operating system is RHEL, running Linux 2.6.18.
The normal approach I believe is to use an infinite loop with sleep, to continually poll the file.
However, since we're on Linux, I'm thinking I can also use something like pyinotify (https://github.com/seb-m/pyinotify) or Watchdog (https://github.com/gorakhargosh/watchdog) instead?
What are the pros/cons of the this?
I've heard that using sleep(), you can miss events, if the file is growing quickly - is that possible? I thought GNU tail uses sleep as well anyhow?
Cheers,
Victor
",122
15699666,15699666,1,"I'm using gevent to build a server which do some redis stuff and return the result to client. But the performance is bad. After some research I found that there is only one connection to redis. It looks like there is only one greenlet spawned. Here is my program:
#!/usr/bin/env python
from gevent import monkey
monkey.patch_all()
import gevent
from gevent.wsgi import WSGIServer
from gevent.pool import Pool
import gevent.socket
from cgi import parse_qs, escape
import json
import redis

p = redis.ConnectionPool()

def app(environ, start_response):
    status = '200 OK'
    body = ''

    path = environ.get('PATH_INFO', '').lstrip('/')
    parameters = parse_qs(environ.get('QUERY_STRING', ''))

    r = redis.Redis(connection_pool=p)
    if path == 'top':
        l = r.zrevrange('online', 0, 50, True)
        body = json.dumps({'onlinetime':map(lambda (pid, online): {'pid':pid, 'onlinetime':online}, l)}) + '\n'

    headers = [
        ('Content-Type', 'text/html'),
        ('Content-Length', str(len(body))),
    ]

    print 'in_use_conn:', len(p._in_use_connections), 'created_connections:', p._created_connections
    start_response(status, headers)

    yield body

def main():
    pool = Pool(1000)
    WSGIServer(('', 7332), app, spawn=pool, log=None).serve_forever()

if __name__ == '__main__':
    main()

Is there something wrong with my program? Why there is only one connection to redis?
",296
15699666,19117266,2,"Have a look at http://gehrcke.de/2013/01/highly-concurrent-connections-to-redis-with-gevent-and-redis-py/
I'm not 100% is your monkey-patching is doing the trick but I'd replace it with:
import gevent
import redis.connection
redis.connection.socket = gevent.socket

You could also go and create your own pool with gevent supported connection to redis...
",49
15699666,19120904,2,"What makes you think your only have one connection to redis? Actually my little test shows that your server is indeed opening lots of connections to redis.
To make the test more clear, I modified your print statement a bit:
print '%s' % parameters['index'], 'in_use_conn:', len(p._in_use_connections), 'created_connections:', p._created_connections, 'available_conn:', len(p._available_connections)

Then run this script to make some requests:
for i in {1..20}
do
    wget http://127.0.0.1:7332/top?index=$i > /dev/null 2>&1 &
done

And here's what I got:
['1'] in_use_conn: 1 created_connections: 2 available_conn: 1
['2'] in_use_conn: 4 created_connections: 5 available_conn: 1
['3'] in_use_conn: 3 created_connections: 5 available_conn: 2
['4'] in_use_conn: 5 created_connections: 6 available_conn: 1
['6'] in_use_conn: 4 created_connections: 6 available_conn: 2
['5'] in_use_conn: 3 created_connections: 6 available_conn: 3
['7'] in_use_conn: 2 created_connections: 6 available_conn: 4
['10'] in_use_conn: 1 created_connections: 6 available_conn: 5
['8'] in_use_conn: 0 created_connections: 6 available_conn: 6
['14'] in_use_conn: 10 created_connections: 11 available_conn: 1
['11'] in_use_conn: 9 created_connections: 11 available_conn: 2
['12'] in_use_conn: 8 created_connections: 11 available_conn: 3
['16'] in_use_conn: 7 created_connections: 11 available_conn: 4
['15'] in_use_conn: 6 created_connections: 11 available_conn: 5
['13'] in_use_conn: 5 created_connections: 11 available_conn: 6
['20'] in_use_conn: 4 created_connections: 11 available_conn: 7
['19'] in_use_conn: 3 created_connections: 11 available_conn: 8
['9'] in_use_conn: 2 created_connections: 11 available_conn: 9
['17'] in_use_conn: 1 created_connections: 11 available_conn: 10
['18'] in_use_conn: 0 created_connections: 11 available_conn: 11

It can be seen that at peek time you have 10 greenlets running simultaneously, waiting for sockets. Your code looks perfectly fine to me. Why 'the performance is bad' is another story. It could be your sorted set of 'online' is tooo large. Or more likely you are using a blocking client to test the server, in which case you'll see only one connection to redis.
",454
15699814,15704532,2,"For the template part everything looks absolutely ok and there should not be problem if what you do is what you showed.
Is Your list is dict() or actually list()? 
Because your problem is here:
{% for x in List %}
    {% set User = List[x] %}

This syntax will work only if List is dictionary.
In case of list you should write:
{% for x in List %}
    {% set User = x %}

",95
15699814,15699814,1,"Here is what i want 
tmpl1.jinja
{% for x in List %}
    {% set User = List[x] %}
    {% include 'tmpl2.jinja' %}
{% endfor %}

tmpl2.jinja
{% extends ""tmpl3.jinja"" %}    
{% block link %}
   <a>share</a>
{% endblock link %}

tmpl3.jinja
User.name
{% block link %}
{% endblock link %}

Basically i have a user block that exists across site with only the action(one or more link but with quiet a few html like image etc) changing. What can i do. 
Thanks 
",116
15699836,15699836,1,"I have seen a number of questions relating to writing files & creating new directories using Python and GAE, but a number of them conclude (not only on SO) by saying that Python cannot write files or create new directories. Yet these commands exist and plenty of other people seem to be writing files and opening directories no problem.
I'm trying to write to .txt files and create folders and getting the following errors:
Case #1:
with open(""aardvark.txt"", ""a"") as myfile:
    myfile.write(""i can't believe its not butter"")

produces ""IOError: [Errno 30] Read-only file system: 'aardvark.txt'"".  But i've checked and it's def-o not a read only file.
Case #2:
folder = r'C:\project\folder\' + str(name)
os.makedirs(folder)

produces ""OSError: [Errno 38] Function not implemented: 'C:\project\folder'""
What am i missing?
",183
15699836,31651345,2,"AppEngine can now write to a local ""ephemeral"" disk storage when using Managed-VM which is not supported when using the sandbox method as specified on this documentation:
https://cloud.google.com/appengine/docs/managed-vms/tutorial/step3
",33
15699836,15699948,2,"Appengine does not support any write operations to the filesystem (amongst other restrictions).
The BlobStore does have a file like api, but you cannot rewrite/append to existing blob store entities.  The dev server also presents these restrictions to emulate production environment.
You should probably have a read of the some of the docs about appengine.
The overview doc https://developers.google.com/appengine/docs/python/overview explicitly states you can't write.  
",76
15699964,15699964,1,"I am facing a strange error while executing a python code. The following code is a small snippet of the python code I am executing:
#samplecode.py
    import time 
    from datetime import datetime 
    import sys 
    import os 
    import inspect
    sys.path.append(os.path.dirname('C:\Users\qksr\Desktop\work\kako\logging.py'))
    import logging
    from logging import Dynamic

While executing samplecode.py I am facing an error showing the following:
Traceback (most recent call last):
  File ""C:\Users\qksr\Desktop\work\Fire\samplecode6.py"", line 8, in <module>
    from logging import Dynamic
ImportError: cannot import name Dynamic

My logging.py which contains the code that needs to be imported while execution. The following is the code:
class Dynamic(object):
    pfile3=open('C:\Users\qksr\Desktop\work\sample3.txt','w')

we can see that the class Dynamic is created yet the import error is thrown.
The strangest thing is I did few examples of importing files and it worked well. I have tried hard but still cannot figure it out. I would like to know why this error was thrown and why suddenly for this and not in previous samples?
",203
15699964,15700053,2,"Python already has a built-in logging module, which is being located before yours (you're appending your folder to the end of the path).
Rename your logging.py file to something else.
",36
15700109,15700109,1,"I am trying to parse a large xml file and print the tags to an output file. I am using minidom, my code is working fine for 30Mb files but for larger ones it is getting memory error. So I used bufferred reading the on file but now I am unable to get the desired output.
    XML File
> <File> <TV>Sony</TV> <FOOD>Burger</FOOD> <PHONE>Apple</PHONE> </File>   
> <File> <TV>Samsung</TV> <FOOD>Pizza</FOOD> <PHONE>HTC</PHONE> </File>  
> <File> <TV>Bravia</TV> <FOOD>Pasta</FOOD> <PHONE>BlackBerry</PHONE> </File>  

Desired Output
Sony, Burger, Apple
Samsung, Pizza, HTC
Bravia, Pasta, BlackBerry  
When reading with buffer its giving me an output saying :-
Sony, Burger, Apple
Samsung,Piz
Bravia, Pasta, BlackBerry  
while 1:
    content = File.read(2048)
        if not len(content):
            break
         else:
             for lines in StringIO(content):
                lines = lines.lstrip(' ')
                if lines.startswith(""<TV>""):
                   TV =  lines.strip(""<TV>"")
                   tvVal = TV.split(""</TV>"")[0]
                   #print tvVal
                   w2.writelines(str(tvVal)+"","")
                elif lines.startswith(""<FOOD>""):
                   FOOD =  lines.strip(""<FOOD>"")
                   foodVal = FOOD.split(""</FOOD>"")[0]
                   #print foodVal
                   w2.writelines(str(foodVal)+"","")
                   ............................
                   ...........................

I tried with seek() but still I was unable to get the desired output. 
",352
15700109,15700565,2,"You're reading in 2048 byte at once, which put the reading cursor in the middle of a line. In the next read, the rest of that line is discard because it doesn't start with a tag.
Instead of rolling your own parser, consider using iterparse. An even faster version of iterparse is included with lxml
Here's an example
import cStringIO
from xml.etree.ElementTree import iterparse

fakefile = cStringIO.StringIO(""""""<temp>
  <email id=""1"" Body=""abc""/>
  <email id=""2"" Body=""fre""/>
  <email id=""998349883487454359203"" Body=""hi""/>
</temp>
"""""")
for _, elem in iterparse(fakefile):
    if elem.tag == 'email':
        print elem.attrib['id'], elem.attrib['Body']
    elem.clear()

",157
15700109,15883599,2,"Thanks for your support and i have finally written my code and its working great here it is   
import lxml import etree    
for event, element in etree.iterparse(the_xml_file):
    if 'TV' in element.tag:
        print element.text

",40
15700128,15700128,1,"I have a dataframe and I grouped it by two keys df.groupby(['key1',key2']). For each key2 entry, how do I display the its percent of key1 values?
",38
15700128,15702317,2,"call groupby twice for ""k1"" and (""k1"", ""k2""), and then do div:
import pandas as pd
k1 = [""a"", ""a"", ""a"", ""a"", ""b"", ""b"", ""b""]
k2 = [""x"", ""x"", ""y"", ""y"", ""x"", ""y"", ""y""]
df = pd.DataFrame({""k1"":k1, ""k2"":k2})

df.groupby([""k1"", ""k2""]).k2.count().div(
    df.groupby(""k1"").k1.count().astype(float), level=0)

output:
k1  k2
a   x     0.500000
    y     0.500000
b   x     0.333333
    y     0.666667

",154
15700128,15703898,2,"Here's an alternative method using one groupby statement.
Group by k1, select column k2 and apply a lambda function.  The lambda gets frequency counts for each level of k2 within k1 and then we divide by the count of k1:
In [1]: df.groupby('k1')['k2'].apply(lambda x: pd.value_counts(x)/x.count().astype(float))

Out[1]:
k1
a   x    0.500000
    y    0.500000
b   y    0.666667
    x    0.333333

Performance:
HYRY's method:
100 loops, best of 3: 3.07 ms per loop

My method:
1000 loops, best of 3: 1.98 ms per loop

",123
15701312,15701312,1,"I'm trying to get celery's official tutorial work but kept getting this error:

D:\test>celery -A tasks worker --loglevel=info
   -------------- celery@BLR122S v3.0.17 (Chiastic Slide)
  ---- **** -----
  --- * *  * -- [Configuration]
  -- * - **** --- . broker:      amqp://guest@localhost:5672//
  - ** ---------- . app:         tasks:0x2a76850
  - ** ---------- . concurrency: 2 (processes)
  - ** ---------- . events:      OFF (enable -E to monitor this worker)
  - ** ----------
  - * --- * --- [Queues]
  -- ******* ---- . celery:      exchange:celery(direct) binding:celery
  --- ***** -----
  [Tasks]
    . tasks.add
  [2013-03-29 17:50:52,533: WARNING/MainProcess] celery@BLR122S ready.
  [2013-03-29 17:50:52,568: INFO/MainProcess] consumer: Connected to amqp://guest@
  127.0.0.1:5672//.
  [2013-03-29 17:51:32,496: INFO/MainProcess] Got task from broker: tasks.add[8345
  9233-ce54-40ed-a2a8-ee0d60768006]
  [2013-03-29 17:51:32,562: ERROR/MainProcess] Task tasks.add[83459233-ce54-40ed-a
  2a8-ee0d60768006] raised exception: Task of kind 'tasks.add' is not registered,
  please make sure it's imported.
  Traceback (most recent call last):

File ""C:\Python27\lib\site-packages\billiard\pool.py"", line 293, in worker

result = (True, func(*args, **kwds))
      File ""C:\Python27\lib\site-packages\celery\task\trace.py"", line 320, in _fast_trace_task
      return _tasks[task].__trace__(uuid, args, kwargs, request)[0]
      File ""C:\Python27\lib\site-packages\celery\app\registry.py"", line 20, in __missing__
      raise self.NotRegistered(key)
      NotRegistered: 'tasks.add'



I installed celery==3.0.17 and rabbitMQ.
Then start celery by ""D:\test>celery -A tasks worker --loglevel=info""
tasks.add seems to be in [Tasks], but calling by:
>>> from tasks import add
>>> add.delay(1,1)
# Out: AsyncResult: 83459233-ce54-40ed-a2a8-ee0d60768006

got the failure above. Does anyone have the same problem?
Edit:
Here is my tasks.py copying from tutorial.
from celery import Celery

celery = Celery('tasks', broker='amqp://guest@localhost//')

@celery.task
def add(x, y):
    return x + y

",423
15701312,15701595,2,"try to import tasks first, I recommend you implement your work in a interactive python environment, like a python IDE, and then you do this:

import tasks

before you write tasks.add
",35
15701356,15702117,2,"Ok, i just had to inherit the account_invoice class and define a new invoice_validate method
@user1576199 is this what you meant ?
from account import account_invoice

class account_invoice(osv.osv):
    _inherit=""account.invoice""
    def invoice_validate(self, cr, uid, ids, context=None):
        super(account_invoice,self).invoice_validate()

",61
15701356,15701356,1,"I want to add a small process after an user validate an invoice in Openerp 7 (just some recording for the salesperson) but i dont know how to 'intercept' the action ?
",35
15705434,15705434,1,"Right now I'm having trouble with the code restarting. It restarts but it doesn't go back to the beginning. It just keeps asking me if I want to restart.
For example it says 
The player has cards [number, number, number, number, number] with a total value of (whatever the numbers add up too.)

--> Player is busted!

Start over? Y/N

I type in Y and it keeps saying
The player has cards [number, number, number, number, number] with a total value of (whatever the numbers add up too.)

--> Player is busted!

Start over? Y/N

Can anyone please fix it so that it will restart. - or tell me how to my code is below.
from random import choice as rc
def playAgain():
# This function returns True if the player wants to play again, otherwise it returns False.
print('Do you want to play again? (yes or no)')
return input().lower().startswith('y')
def total(hand):
# how many aces in the hand
aces = hand.count(11)
t = sum(hand)
# you have gone over 21 but there is an ace
if t > 21 and aces > 0:
    while aces > 0 and t > 21:
        # this will switch the ace from 11 to 1
        t -= 10
        aces -= 1
return t
cards = [2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10, 11]
c2win = 0 # computer2 win
cwin = 0  # computer win 
pwin = 0  # player win 
while True:
player = []
player.append(rc(cards))
player.append(rc(cards))
pbust = False  # player busted 
cbust = False  # computer busted
c2bust = False # computer2 busted
while True:
    tp = total(player)
    print (""The player has cards %s with a total value of %d"" % (player, tp))
    if tp > 21:
        print (""--> Player is busted!"")
        pbust = True
        print('Start over? Y/N')
        answer = input()
        if answer == 'n':
            done = True
            break
    elif tp == 21:
        print (""\a BLACKJACK!!!"")
        print(""do you want to play again?"")
        answer = input()
        if answer == 'y':
            done = False
        else:
            break
    else:
        hs = input(""Hit or Stand/Done (h or s): "").lower()
        if 'h' in hs:
            player.append(rc(cards))
        if 's' in hs:
            player.append(rc(cards))
while True:
    comp = []
    comp.append(rc(cards))
    comp.append(rc(cards))
while True:
    comp2 = []
    comp.append(rc(cards))
    comp.append(rc(cards))
    while True:
        tc = total(comp)                
        if tc < 18:
            comp.append(rc(cards))
        else:
            break
    print (""the computer has %s for a total of %d"" % (comp, tc))
    if tc > 21:
        print (""--> Computer is busted!"")
        cbust = True
        if pbust == False:
            print (""Player wins!"")
            pwin += 1
            print('Start over? Y/N')
        answer = input()
        if answer == 'y':
            playAgain()  
        if answer == 'n':
            done = True
    elif tc > tp:
        print (""Computer wins!"")
        cwin += 1
    elif tc == tp:
        print (""It's a draw!"")
    elif tp > tc:
        if pbust == False:
            print (""Player wins!"")
            pwin += 1
        elif cbust == False:
            print (""Computer wins!"")
            cwin += 1
    break
print
print (""Wins, player = %d  computer = %d"" % (pwin, cwin))
exit = input(""Press Enter (q to quit): "").lower()
if 'q' in exit:
    break
print
print
print (""Thanks for playing blackjack with the computer!"")

",812
15705434,15708619,2,"fun little game, I removed the second dealer for simplicity, but it should be easy enough to add back in.  I changed input to raw_input so you could get a string out of it without entering quotes.  touched up the logic a bit here and there, redid formating and added comments.
from random import choice as rc

def play_again():
    """"""This function returns True if the player wants to play again,
    otherwise it returns False.""""""
    return raw_input('Do you want to play again? (yes or no)').lower().startswith('y')

def total(hand):
    """"""totals the hand""""""
    #special ace dual value thing
    aces = hand.count(11)
    t = sum(hand)
    # you have gone over 21 but there is an ace
    while aces > 0 and t > 21:
        # this will switch the ace from 11 to 1
        t -= 10
        aces -= 1
    return t

cards = [2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10, 11]
cwin = 0  # computer win
pwin = 0  # player win
while True:
    # Main Game Loop (multiple hands)
    pbust = False  # player busted
    cbust = False  # computer busted
    # player's hand
    player = []
    player.append(rc(cards))
    player.append(rc(cards))
    pbust = False  # player busted
    cbust = False  # computer busted
    while True:
        # Player Game Loop (per hand)
        tp = total(player)
        print (""The player has cards %s with a total value of %d"" % (player, tp))
        if tp > 21:
            print (""--> Player is busted!"")
            pbust = True
            break
        elif tp == 21:
            print (""\a BLACKJACK!!!"")
            break
        else:
            hs = raw_input(""Hit or Stand/Done (h or s): "").lower()
            if hs.startswith('h'):
                player.append(rc(cards))
            else:
                break
    #Dealers Hand
    comp = []
    comp.append(rc(cards))
    comp.append(rc(cards))
    tc = total(comp)
    while tc < 18:
        # Dealer Hand Loop
        comp.append(rc(cards))
        tc = total(comp)
    print (""the computer has %s for a total of %d"" % (comp, tc))
    if tc > 21:
        print (""--> Computer is busted!"")
        cbust = True

    # Time to figure out who won
    if cbust or pbust:
        if cbust and pbust:
            print (""both busted, draw"")
        elif cbust:
            print (""Player wins!"")
            pwin += 1
        else:
            print (""Computer wins!"")
            cwin += 1
    elif tc < tp:
        print (""Player wins!"")
        pwin += 1
    elif tc == tp:
        print (""It's a draw!"")
    else:
        print (""Computer wins!"")
        cwin += 1

    # Hand over, play again?
    print (""\nWins, player = %d  computer = %d"" % (pwin, cwin))
    exit = raw_input(""Press Enter (q to quit): "").lower()
    if 'q' in exit:
       break

print (""\n\nThanks for playing blackjack with the computer!"")

",653
15705439,41648497,2,"I found that using os.seteuid and os.setegid didn't actually drop the root privileges.  After calling them I was still able to do things that required root privileges.  The solution I found that worked was to use os.setresuid and os.setresgid instead:
sudo_uid = int(os.getenv(""SUDO_UID""))
sudo_gid = int(os.getenv(""SUDO_GID""))

# drop root privileges
os.setresgid(sudo_gid, sudo_gid, -1)
os.setresuid(sudo_uid, sudo_uid, -1)

subprocess.call(""mkdir /foo1"", shell = True) # should fail

# regain root privileges
os.setresgid(0, 0, -1)
os.setresuid(0, 0, -1)

subprocess.call(""mkdir /foo2"", shell = True) # should succeed

",134
15705439,15705439,1,"In my Python script, I perform a few operations that need root privileges. I also create and write to files that I don't want to be owned exclusively by root but by the user who is running my script.
Usually, I run my script using sudo. Is there a way to do the above?
",61
15705439,15706858,2,"http://linux.die.net/man/8/sudo quote: 
The real and effective uid and gid are set to match those of the target user 
So, your only option of knowing which user to use is to read the target user from either a config file or a cmdline option, or someway of heuristical guessing.
A good idea is the so called rights shedding: Start with root privilegs, then do what you nedd them for. Then become a less privileged user. 
You would use the os module for that:
http://docs.python.org/2/library/os.html#os.setuid
",98
15705439,15707075,2,"You can switch between uid's using os.seteuid(). This differs from os.setuid() in that you can go back to getting root privileges when you need them.
For example, run the following as root:
import os

open('file1', 'wc')

# switch to userid 501
os.seteuid(501)
open('file2', 'wc')

# switch back to root
os.seteuid(0)
open('file3', 'wc')

This creates file1 and file3 as root, but file2 as the user with uid 501.
If you want to determine which user is calling your script, sudo sets two environment variables:
SUDO_USER
SUDO_UID

Respectively the username and the uid of the user who called sudo. So you could use int(os.environ['SUDO_UID']) to use with os.seteuid().
",154
15705491,15718449,2,"You are attempting to use both grid and pack for the same containing widget. You cannot do that. You either need to use grid for the text and scrollbars or use pack for the buttons.
",39
15705491,15705491,1,"the follwing code in a serparate file is working fine. It is creating a text area and adding a scrollbar to it.
root = Tkinter.Tk()
text=Text(root,height=10,width=50,background='pink')
scroll=Scrollbar(root)
text.configure(yscrollcommand=scroll.set)
scroll.config(command=text.yview)
text.pack(side=LEFT)
scroll.pack(side=RIGHT,fill=Y)

But exactly same code is not woking when it was merged with other code (main.py)
//================ other code
root = Tkinter.Tk()
root.geometry(""800x600+100+0"") # width, height, x ,y
button_1 =  Button(root,text=""iphone file"")
button_1.pack()
button_1.grid(row=0, column=0)
button_1.configure(command=openFile)

//------------------ following is the same code
text=Text(root,height=10,width=50,background='pink')
scroll=Scrollbar(root)
text.configure(yscrollcommand=scroll.set)
scroll.config(command=text.yview)
text.pack(side=LEFT)
scroll.pack(side=RIGHT,fill=Y)

and when i running main.py file from cmd prompt, it just hanging. what is going wrong here ?
",194
15705511,15707037,2,"you can modify query._whereclause directly, but I'd seek to find a way to not have this issue in the first place - whereever it is that the Query is generated should be factored out so that the non-whereclause version is made available.
",45
15705511,15705511,1,"If I've been given a Query object that I didn't construct, is there a way to directly modify its WHERE clause?  I'm really hoping to be able remove some AND statements or replace the whole FROM clause of a query instead of starting from scratch.  
I'm aware of the following methods to modify the SELECT clause:
Query.with_entities(), Query.add_entities(), Query.add_columns(), Query.select_from()
which I think will also modify the FROM.  And I see that I can view the WHERE clause with Query.whereclause, but the docs say that it's read-only.  
I realize I'm thinking in SQL terms, but I'm more familiar with those concepts than the ORM, at this point.  Any help is very appreciated.
",141
15705546,15724609,2,"Here's a way of doing it without collections.Counter, as requested in chat:
def countLetters(word):
    d = {}
    for l in word:
        d[l] = d.get(l,0) + 1
    return d

def checkSubset(answer,letters):
    a, l = countLetters(answer), countLetters(letters)
    return all(l.get(x,0) >= a.get(x) for x in a.keys())

print(checkSubset('dog','odr'))

",95
15705546,15705622,2,"Use collections.Counter() to convert x and y to multi-sets, then subtract to see if all of y's letters can be found in x:
from collections import Counter

def checkYinX(y, x):
    return not (Counter(y) - Counter(x))

Subtracting multi-sets removes characters when their count falls to 0. If this results in an empty multi-set, it becomes False in a boolean context, like all 'empty' python types. not turns that into True if that is the case.
Demo:
>>> x = ""dsjcosnag""
>>> y = ""dog""
>>> print(checkYinX(y,x))
True
>>> print(checkYinX('cat',x))
False

",143
15705546,15705546,1,"For instance I have 
x = ""dsjcosnag""
y = ""dog""

print(checkYinX(y,x))
>>true

So I think I would need to use a while loop as a counter for each of the letter in y, and then I can use itetools to cycle through each of x, each cycle It would check to see if x == y, if it is it would remove it then check the next letter in o.
Is there a more simple way to do this?
",98
15705577,15705955,2,"Pass in your lists and the key that you want to check values on. 
def getsubset(set, index):
    hash = {}
    for list in set:
        if not list[index] in hash:
            set.remove(list)
            hash[list[index]]  = list

    return set

",56
15705577,15705577,1,"I have a dataset like this:
[[0,1],
 [0,2],
 [0,3],
 [0,4],
 [1,5],
 [1,6],
 [1,7],
 [2,8],
 [2,9]]

I need to delete the first elements of each subview of the data as defined by the first column. So first I get all elements that have 0 in the first column, and delete the first row: [0,1]. Then I get the elements with 1 in the first column and delete the first row [1,5], next step I delete [2,8] and so on and so forth. In the end, I would like to have a dataset like this:
[[0,2],
 [0,3],
 [0,4],
 [1,6],
 [1,7],
 [2,9]]


EDIT: Can this be done in numpy? My dataset is very large so for loops on all elements take at least 4 minutes to complete.
",189
15705577,15705717,2,"You want to use itertools.groupby() with a dash of itertools.islice() and itertools.chain:
from itertools import islice, chain, groupby
from operator import itemgetter

list(chain.from_iterable(islice(group, 1, None)
                         for key, group in groupby(inputlist, key=itemgetter(0))))


The groupby() call groups the input list into chunks where the first item is the same (itemgetter(0) is the grouping key). 
The islice(group, 1, None) call turns the groups into iterables where the first element will be skipped.
The chain.from_iterable() call takes each islice() result and chains them together into a new iterable, which list() turns back into a list.

Demo:
>>> list(chain.from_iterable(islice(group, 1, None) for key, group in groupby(inputlist, key=itemgetter(0))))
[[0, 2], [0, 3], [0, 4], [1, 6], [1, 7], [2, 9]]

",209
15705577,15705725,2,"a = [[0,1],
 [0,2],
 [0,3],
 [0,4],
 [1,5],
 [1,6],
 [1,7],
 [2,8],
 [2,9]]

a = [y for x in itertools.groupby(a, lambda x: x[0]) for y in list(x[1])[1:]]

print a

",75
15705577,15706171,2,"As requested, a numpy solution:
import numpy as np
a = np.array([[0,1], [0,2], [0,3], [0,4], [1,5], [1,6], [1,7], [2,8], [2,9]])
_,i = np.unique(a[:,0], return_index=True)

b = np.delete(a, i, axis=0)


(above is edited to incorporate @Jaime's solution, here is my original masking solution for posterity's sake)
m = np.ones(len(a), dtype=bool)
m[i] = False
b = a[m]


Interestingly, the mask seems to be faster:
In [225]: def rem_del(a):
   .....:     _,i = np.unique(a[:,0], return_index=True)
   .....:     return np.delete(a, i, axis = 0)
   .....: 

In [226]: def rem_mask(a):
   .....:     _,i = np.unique(a[:,0], return_index=True)
   .....:     m = np.ones(len(a), dtype=bool)
   .....:     m[i] = False
   .....:     return a[m]
   .....: 

In [227]: timeit rem_del(a)
10000 loops, best of 3: 181 us per loop

In [228]: timeit rem_mask(a)
10000 loops, best of 3: 59 us per loop

",280
15705577,15705828,2,"My answer is :  
from operator import itemgetter
sorted(l, key=itemgetter(1))  # fist sort by fist element of inner list 
nl = []
[[0, 1], [0, 2], [0, 3], [0, 4], [1, 5], [1, 6], [1, 7], [2, 8], [2, 9]]
j = 0;
for i in range(len(l)): 
    if(j == l[i][0]):
        j = j + 1   # skip element 
    else:
        nl.append(l[i])  # otherwise append  in new list

output is:
>>> nl
[[0, 2], [0, 3], [0, 4], [1, 6], [1, 7], [2, 9]]

",180
15705630,15705630,1,"I hope I can find help for my question. I am searching for a solution for the following problem:
I have a dataFrame like:
 Sp  Mt Value  count
0  MM1  S1   a      **3**
1  MM1  S1   n      2
2  MM1  S3   cb     5
3  MM2  S3   mk      **8**
4  MM2  S4   bg     **10**
5  MM2  S4   dgd      1
6  MM4  S2  rd     2
7  MM4  S2   cb      2
8  MM4  S2   uyi      **7**

My objective is to get the result rows whose count is max between the groups, like :
0  MM1  S1   a      **3**
1 3  MM2  S3   mk      **8**
4  MM2  S4   bg     **10** 
8  MM4  S2   uyi      **7**

Somebody knows how can I do it in pandas or in python?
UPDATE
I didn't give more details for my question. For my problem, I want to group by ['Sp','Mt'].  Let take a second example like this :
   Sp   Mt   Value  count
4  MM2  S4   bg     10
5  MM2  S4   dgd    1
6  MM4  S2   rd     2
7  MM4  S2   cb     8
8  MM4  S2   uyi    8

For the above example, I want to get ALL the rows where count equals max in each group e.g :
MM2  S4   bg     10
MM4  S2   cb     8
MM4  S2   uyi    8

",226
15705630,44960833,2,"Easy solution would be to apply : idxmax() function to get indices of rows with max values. 
This would filter out all the rows with max value in the group.
In [365]: import pandas as pd

In [366]: df = pd.DataFrame({
'sp' : ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4','MM4'],
'mt' : ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],
'val' : ['a', 'n', 'cb', 'mk', 'bg', 'dgb', 'rd', 'cb', 'uyi'],
'count' : [3,2,5,8,10,1,2,2,7]
})

In [367]: df                                                                                                       
Out[367]: 
   count  mt   sp  val
0      3  S1  MM1    a
1      2  S1  MM1    n
2      5  S3  MM1   cb
3      8  S3  MM2   mk
4     10  S4  MM2   bg
5      1  S4  MM2  dgb
6      2  S2  MM4   rd
7      2  S2  MM4   cb
8      7  S2  MM4  uyi


### Apply idxmax() and use .loc() on dataframe to filter the rows with max values:
In [368]: df.loc[df.groupby([""sp"", ""mt""])[""count""].idxmax()]                                                       
Out[368]: 
   count  mt   sp  val
0      3  S1  MM1    a
2      5  S3  MM1   cb
3      8  S3  MM2   mk
4     10  S4  MM2   bg
8      7  S2  MM4  uyi

### Just to show what values are returned by .idxmax() above:
In [369]: df.groupby([""sp"", ""mt""])[""count""].idxmax().values                                                        
Out[369]: array([0, 2, 3, 4, 8])

",362
15705630,40629420,2,"You can sort the dataFrame by count and then remove duplicates. I think it's easier:
df.sort_values('count', ascending=False).drop_duplicates(['Sp','Mt'])

",35
15705630,31185210,2,"For me, the easiest solution would be keep value when count is equal to the maximum. Therefore, the following one line command is enough : 
df[df['count'] == df.groupby(['Mt'])['count'].transform(max)]

",52
15705630,21709413,2,"Having tried the solution suggested by Zelazny on a relatively large DataFrame (~400k rows) I found it to be very slow.  Here is an alternative that I found to run orders of magnitude faster on my data set.
df = pd.DataFrame({
    'sp' : ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],
    'mt' : ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],
    'val' : ['a', 'n', 'cb', 'mk', 'bg', 'dgb', 'rd', 'cb', 'uyi'],
    'count' : [3,2,5,8,10,1,2,2,7]
    })

df_grouped = df.groupby(['sp', 'mt']).agg({'count':'max'})

df_grouped = df_grouped.reset_index()

df_grouped = df_grouped.rename(columns={'count':'count_max'})

df = pd.merge(df, df_grouped, how='left', on=['sp', 'mt'])

df = df[df['count'] == df['count_max']]

",227
15705630,15705958,2,"In [1]: df
Out[1]:
    Sp  Mt Value  count
0  MM1  S1     a      3
1  MM1  S1     n      2
2  MM1  S3    cb      5
3  MM2  S3    mk      8
4  MM2  S4    bg     10
5  MM2  S4   dgd      1
6  MM4  S2    rd      2
7  MM4  S2    cb      2
8  MM4  S2   uyi      7

In [2]: df.groupby(['Mt'], sort=False)['count'].max()
Out[2]:
Mt
S1     3
S3     8
S4    10
S2     7
Name: count

To get the indices of the original DF you can do:
In [3]: idx = df.groupby(['Mt'])['count'].transform(max) == df['count']

In [4]: df[idx]
Out[4]:
    Sp  Mt Value  count
0  MM1  S1     a      3
3  MM2  S3    mk      8
4  MM2  S4    bg     10
8  MM4  S2   uyi      7

Note that if you have multiple max values per group, all will be returned.
Update
On a hail mary chance that this is what the OP is requesting:
In [5]: df['count_max'] = df.groupby(['Mt'])['count'].transform(max)

In [6]: df
Out[6]:
    Sp  Mt Value  count  count_max
0  MM1  S1     a      3          3
1  MM1  S1     n      2          3
2  MM1  S3    cb      5          8
3  MM2  S3    mk      8          8
4  MM2  S4    bg     10         10
5  MM2  S4   dgd      1         10
6  MM4  S2    rd      2          7
7  MM4  S2    cb      2          7
8  MM4  S2   uyi      7          7

",303
15705719,15705719,1,"I installed lxml on a mac and trying to use it in my code and I get errors importing tostring and tounicode. Python cannot see it at all. Any ideas what I am doing wrong?
Here is the code that is causing problems - 
from lxml.etree import tostring
from lxml.etree import tounicode

I get an unresolved import error 
Also my IDE (eclipse) is able to see the init.py file for lxml.etree module. Here is what it sees ---
# this is a package

def get_include():
    """"""
    Returns a list of header include paths (for lxml itself, libxml2
    and libxslt) needed to compile C code against lxml if it was built
    with statically linked libraries.
    """"""
    import os
    lxml_path = __path__[0]
    include_path = os.path.join(lxml_path, 'includes')
    includes = [include_path, lxml_path]

    for name in os.listdir(include_path):
        path = os.path.join(include_path, name)
        if os.path.isdir(path):
            includes.append(path)

    return includes

Thanks for any help.
EDIT:
The only log I see is 
    Unresolved import: tostring
    Unresolved import: tounicode
And when I add the following line before the import of tostring it works no errors --
    import etree from lxml
Also to give you some more background on what I am trying to do. I got the readability code from here (https://github.com/buriy/python-readability) and trying to use it in my project. 
EDIT2: I fixed the problem but still do not understand why. I wanted to use the code from the readability project directly without installing the package using easy_install. The idea was step into the code so that I understand what its doing. But when I copy the code into my project I get the above mentioned error in the readability code. If I install the package using easy_install then all I can simply import the class exported by readability and use it.
So can some one tell me what the difference is between using the code directly and the package installed? What is a .egg file? How to create one?
",382
15705719,46108414,2,"In lxml's code, it dynamically load modules. That makes IDE fails to analyze reference as IDE just analyzes raw code.
",24
15705745,15787278,2,"This is based off of Oblivion's answer but I edited it so it worked for me.
I made a new listbox class that was based off of the original one but had a new function that I got from Oblivion's code. I then call that function and it makes the listbox an appropriate size.
class Listbox(tk.Listbox):
    def autowidth(self,maxwidth):
        f = font.Font(font=self.cget(""font""))
        pixels = 0
        for item in self.get(0, ""end""):
            pixels = max(pixels, f.measure(item))
        # bump listbox size until all entries fit
        pixels = pixels + 10
        width = int(self.cget(""width""))
        for w in range(0, maxwidth+1, 5):
            if self.winfo_reqwidth() >= pixels:
                break
            self.config(width=width+w)

master = tk.Tk()
listbox = Listbox(master, selectmode=tk.SINGLE)

keys = serverDict.keys()
for key in sorted(keys):
    listbox.insert(tk.END, key)

button = tk.Button(master, text=""Execute"", command=execute)
listbox.autowidth(250)
listbox.pack()
button.pack()
tk.mainloop()

",217
15705745,26504193,2,"Resetting the listbox width worked for me. I used the Oblivion's answer and noticed that only width is always zero.
listbox = tk.Listbox(master, selectmode=tk.SINGLE)
listbox.config(width=0)

I also recommend to reset root window geometry after reloading content of the list. If user manually extends the window the window would stop accommodate size of its content.
root.winfo_toplevel().wm_geometry("""")

",74
15705745,15705745,1,"I am writing a program to run batch files for various servers and so far everything is going fine. I mean the programs works and uses a simple GUI and all is well. Apart from when I give it a slightly longer name to display in the listbox it clips the end off. The code that Tkinter uses is below.
master = tk.Tk()
listbox = tk.Listbox(master, selectmode=tk.SINGLE)

keys = serverDict.keys()
for key in sorted(keys):
    listbox.insert(tk.END, key)

button = tk.Button(master, text=""Execute"", command=execute)

listbox.pack()
button.pack()
tk.mainloop()

So basically it all works perfectly fine, I'm not getting any errors but it is a bit annoying the way it doesn't properly fit and I can't scroll. I know I can add scroll bars and make the window re-sizable and make the listbox fit the space it's given, but I would like it to work without having to resize stuff. I know it's not that important but it's just one of those things that I would love to work but can't figure out :/ .

",216
15705745,15712127,2,"tkListAutoWidth.py shows one way to do it:
http://svn.effbot.org/public/stuff/sandbox/tkinter/
edit:
So you might have something along the lines of,
import tkinter as tk
from tkinter import font


class NewListbox(tk.Listbox):

    def autowidth(self, maxwidth=100)
        autowidth(self, maxwidth)


def autowidth(list, maxwidth=100):
    f = font.Font(font=list.cget(""font""))
    pixels = 0
    for item in list.get(0, ""end""):
        pixels = max(pixels, f.measure(item))
    # bump listbox size until all entries fit
    pixels = pixels + 10
    width = int(list.cget(""width""))
    for w in range(0, maxwidth+1, 5):
        if list.winfo_reqwidth() >= pixels:
            break
        list.config(width=width+w)


if __name__ == ""__main__"":

    master = tk.Tk()
    listbox = NewListbox(master, selectmode=tk.SINGLE)

    # ...
    # ...
    keys = serverDict.keys()
    for key in sorted(keys):
        listbox.insert(""end"", key)

    listbox.pack()

    button = tk.Button(master, text=""Execute"", command=execute)
    button.pack()

    listbox.autowidth()

    master.mainloop()

",214
16131208,16131255,2,"np.argsort(A) is sorting each row of A separately. For example,
In [21]: np.argsort([[6,5,4],[3,2,1]])
Out[21]: 
array([[2, 1, 0],
       [2, 1, 0]])

Instead, you want to flatten your array into a 1-dimensional array of values, then argsort that. That can be done by setting the axis parameter to None (thanks to @Akavall for pointing this out):
In [23]: np.argsort(A, axis=None)
Out[23]: array([3, 0, 1, 2])

Then use np.unravel_index to recover the associated index in A.

In [14]: import numpy as np

In [15]: A = np.array([[7, 8], [9, 5]])   

In [4]: np.column_stack(np.unravel_index(np.argsort(A, axis=None)[::-1], A.shape))
Out[4]: 
array([[1, 0],
       [0, 1],
       [0, 0],
       [1, 1]])

Note, for NumPy version 1.5.1 or older, np.unravel_index raises a ValueError if passed an array-like object for its first argument. In that case, you could use a list comprehension:
In [17]: [np.unravel_index(p, A.shape) for p in np.argsort(A, axis=None)[::-1]]
Out[17]: [(1, 0), (0, 1), (0, 0), (1, 1)]

",315
16131208,16131208,1,"I'm aware of numpy.argsort(), but what it does is return indices of elements in an array that would be sorted along a certain axis.
What I need is to sort all the values in an N-dimensional array and have a linear list of tuples as as result.
Like this:
>>> import numpy
>>> A = numpy.array([[7, 8], [9, 5]])
>>> numpy.magic(A)
[(1, 0), (0, 1), (0, 0), (1, 1)]

P.S. I don't even understand what the output of argsort is trying to tell me for this array.
",135
16301356,16301356,1,"I'm using Cygwin and Python2.7 and I have already installed some third party modules (PySide, PyGame) on my computer and when I run them using the normal GUI they work fine. But when I run python inside Cygwin it doesn't recognize the modules I already have installed. I'm using Windows 7 for this. Is there anyway I can get Cygwin to recognize these libraries? 
",74
16301356,16640744,2,"You're question is not clear (e.g. what do you mean by ""the normal GUI""), but it sounds like you installed Windows versions of Python and those extensions; those binaries won't work with the Cygwin version of Python.
",47
23500040,23500083,2,"You could use the built-in sum function:
sum(v for v in color_codes.itervalues() if v > x)

The argument to sum is a generator expression, and the result is the sum of the values that are greater than x.
",46
23500040,23500040,1,"I was wondering what is a nice, elegant pythonic way to run a sumif on values of keys in a dictionary that adhere to a certain condition. For example, this dict:
color_codes = {'red':1,'yellow':2, 'green':3, 'brown':4, 'blue':5, 'pink':6, 'black': 7}

lets say I want the sum of all the values of the keys, given that the value is >= a certain number x.
How would you go about it? Anonymous function maybe?
Thanks in advance for your help
",98
23500173,23501543,2,"The documentation says:
""Returns the approximate processor time used by the process since the beginning of an implementation-defined era related to the program's execution. To convert result value to seconds divide it by CLOCKS_PER_SEC.""
That's pretty vague. CLOCK_PER_SEC is set to 10^6 and the approximate stands for poor resolution, not that the current clocks tick over 1000 faster and the results are rounded. That might be not a very technical term, but it is appropriate. The actual resolution everywhere I tested was about 100Hz = 0,01s. It's been like that for years. Note date here http://www.guyrutenberg.com/2007/09/10/resolution-problems-in-clock/. 
Then the doc follows with: ""On POSIX-compatible systems, clock_gettime with clock id CLOCK_PROCESS_CPUTIME_ID offers better resolution.""
So:

It's CPU time only. But 2 threads = 2*CPU time. See the example on cppreference.
It is not suited for fine grain measurements at all, as explained above. You were on the verge of its accuracy.
IMO measuring wall-clock is the only sensible thing, but its a rather personal opinion. Especially with multithreaded applications and multiprocessing in general. Otherwise results of system+user should be similar anyways.

EDIT: At 3. This of course holds for computational tasks. If your process uses sleep or give up execution back to system, it might be more feasible measuring CPU time. Also regarding the comment that clock resolution is erm... bad. It is, but to be fair one could argue you should not measure such short computations. IMO its too bad, but if you measure times over few seconds I guess its fine. I would personally use others available tools.
",305
23500173,23500173,1,"I'm interested in comparing CPU times some code portions written C++ vs Python (running on Linux). Will the following methods produce a ""fair"" comparison between the two? 
Python
Using the resource module:
import resource
def cpu_time():
    return resource.getrusage(resource.RUSAGE_SELF)[0]+\ # time in user mode
        resource.getrusage(resource.RUSAGE_SELF)[1] # time in system mode

which allows for timing like so:
def timefunc( func ):
    start=cpu_time()
    func()
    return (cpu_time()-start)

Then I test like:
def f():
    for i in range(int(1e6)):
        pass

avg = 0
for k in range(10):
    avg += timefunc( f ) / 10.0
print avg
=> 0.002199700000000071

C++
Using the ctime lib:
#include <ctime>
#include <iostream>

int main() {
    double avg = 0.0;
    int N = (int) 1e6;
    for (int k=0; k<10; k++) {
        clock_t start;
        start = clock();
        for (int i=0; i<N; i++) continue;
        avg += (double)(clock()-start) / 10.0 / CLOCKS_PER_SEC;
    }
    std::cout << avg << '\n';
    return 0;
}

which yields 0.002.
Concerns:

I've read that C++ clock() measures CPU time which is what I'm after, but I can't seem to find if it includes both user and system times.
Results from C++ are much less precise. Why is that?
Overall fairness of comparison as mentioned.

Update
updated the c++ code as per David's suggestion in the comments:
#include <sys/resource.h>
#include <iostream>

int main() {
    double avg = 0.0;
    int N = (int) 1e6;
    int tally = 0;

    struct rusage usage;
    struct timeval ustart, ustop, sstart, sstop;

    getrusage(RUSAGE_SELF, &usage);
    ustart = usage.ru_utime;
    sstart = usage.ru_stime;

    for (int k=0; k<10; k++) {
        ustart = usage.ru_utime;
        sstart = usage.ru_stime;

        for (int i=0; i<N; i++) continue;

        getrusage(RUSAGE_SELF, &usage);
        ustop = usage.ru_utime;
        sstop = usage.ru_stime;

        avg += (
            (ustop.tv_sec+ustop.tv_usec/1e6+
            sstop.tv_sec+sstop.tv_usec/1e6)
            -
            (ustart.tv_sec+ustart.tv_usec/1e6+
            sstart.tv_sec+sstart.tv_usec/1e6)
        ) / 10.0; 
    }

    std::cout << avg << '\n';

    return 0;
}

Running:
g++ -O0 cpptimes.cpp ; ./a.out
=> 0.0020996
g++ -O1 cpptimes.cpp ; ./a.out
=> 0

So I suppose getrusage gets me a little bit better resolution, but I'm not sure how much I should read into it. Setting the optimization flag certainly makes a big difference.
",515
23500173,23504062,2,"
Setting the optimization flag certainly makes a big difference.

C++ is a language that begs to be compiled optimized, particularly so if the code in question uses containers and iterators from the C++ standard library. A simple ++iterator shrinks from a good-sized chain of function calls when compiled unoptimized to one or two assembly statement when optimization is enabled.
That said, I knew what the compiler would do to your test code. Any decent optimizing compiler will make that for (int i=0; i<N; i++) continue; loop vanish. It's the as-if rule at work. That loop does nothing, so the compiler is free to treat it as if it wasn't even there.
When I look at the CPU behavior of a suspect CPU hog, I write a simple driver (in a separate file) that calls the suspect function a number of times, sometimes a very large number of times. I compile the functionality to be tested with optimization enabled, but I compile the driver with optimization disabled. I don't want a too-smart optimizing compiler to see that those 100,000 calls to function_to_be_tested() can be pulled out of the loop and then further optimize the loop away.
There are a number of solid reasons for calling the test function a number of times between the single call to start timer and stop timer. This is why python has the timeit module.
",261
23500202,23500202,1,"I'm trying to find a blendshape deformer from a target mesh in the python maya api. I'm pretty sure I have to iterate through the dependency graph to get the blendshape.
This is what i'm trying:
import maya.OpenMaya as OpenMaya
import maya.OpenMayaAnim as OpenMayaAnim

#Name of our targetmesh.
targetMesh = ""pSphere1""

#Add selection.
mSel = OpenMaya.MSelectionList()
mSel.add(targetMesh, True)

#Get MObj
mObj = OpenMaya.MObject()
mSel.getDependNode(0, mObj)

#Make iterator.
itDG = OpenMaya.MItDependencyGraph(mObj,
                                   OpenMaya.MFn.kBlendShape, 
                                   OpenMaya.MItDependencyGraph.kUpstream)

while not itDG.isDone():
    oCurrentItem = itDG.currentItem()
    blndSkin = OpenMayaAnim.MFnBlendShapeDeformer(oCurrentItem)
    print blndSkin
    break

Unfortunately I get no blendshape deformer.
The same example with maya.cmds:
import maya.cmds as cmds

targetMesh = ""pSphere1""    

history = cmds.listHistory(targetMesh, future=True)
blndshape = cmds.ls(history, type=""blendShape"")

print blndshape

Any help would be greatly appreciated!
",174
23500202,23501184,2,"you don't want the future flag if you're working from the deformable object:
targetMesh = ""pSphere1""    
blendshapes = cmds.ls(*cmds.listHistory(targetMesh) or [], type= 'blendShape')

To get the actual shapes, you'd add
source_shapes =  cmds.ls(*cmds.listHistory(*blendshapes) or [], type= 'mesh', ni=True)

",64
23500202,23502118,2,"So here's the solution I got working i believe:
def getBlendShape(shape):
    '''
    @param Shape: Name of the shape node.
    Returns MFnBlendShapeDeformer node or None.
    '''
    # Create an MDagPath for our shape node:
    selList = OpenMaya.MSelectionList()
    selList.add(shape)
    mDagPath = OpenMaya.MDagPath()
    selList.getDagPath(0, mDagPath)

    #Create iterator.
    mItDependencyGraph = OpenMaya.MItDependencyGraph(
        mDagPath.node(),
        OpenMaya.MItDependencyGraph.kPlugLevel)

    # Start walking through our shape node's dependency graph.
    while not mItDependencyGraph.isDone():
        # Get an MObject for the current item in the graph.
        mObject = mItDependencyGraph.currentItem()
        # It has a BlendShape.
        if mObject.hasFn(OpenMaya.MFn.kBlendShape):
            # return the MFnSkinCluster object for our MObject:
            return OpenMayaAnim.MFnBlendShapeDeformer(mObject)
        mItDependencyGraph.next()

if __name__ == '__main__':
    #TargetMesh
    targetMesh = ""pSphereShape1""

    #Get Blendshape.
    blndShpNode = getBlendShape(targetMesh)

    if blndShpNode:
        #Get base objects.
        mObjArr = OpenMaya.MObjectArray()
        blndShpNode.getBaseObjects(mObjArr)
        mDagPath = OpenMaya.MDagPath()
        OpenMaya.MFnDagNode(mObjArr[0]).getPath(mDagPath)
        print(mDagPath.fullPathName())

    else:
        print(""No Blendshape found."")

The trick is that I needed to pass the shape node and to only use OpenMaya.MItDependencyGraph.kPlugLevel). In this example it finds the base object of the blendshape.
",245
23503326,23503414,2,"Seems you're using an incorrect import, try:
from PIL import Image

instead.
",16
23503326,23503326,1,"I have this code:
import Image

import pygame, sys
from pygame.locals import *

pygame.init()

catImg = Image.open(""cat.jpg"")

I am using Python 3.3 and latest version of Pillow. But when I run this code I get:
ImportError: No module named 'Image'

I uninstall PIL. Can somebody help me?
",61
23503486,23503575,2,"This should be a unicode string:
>>> 'KRöger'.title()
'KröGer'
>>> u'KRöGer'.title()
u'Kröger'

Edit: A simple python script as an example:
# -- coding: utf-8 --
print 'KRöger'.title()  # 'KröGer'
print u'KRöGer'.title()  # 'Kröger'
print 'KRöger'.decode('utf-8').title()  # 'Kröger'

",61
23503486,23503486,1,"I have a list of surnames and it isn't formatted the right way. Every name is written in cAmEl sTyLe -.-
I'm trying to make it look more clean with title() method.
s = 'KroGer'
s = s.title()
print s
>Kroger

This one works fine. But when I have non-ascii letter in the name:
s = 'KRöGer'
s = s.title()
print s
>KröGer

the letter that follows this non-ascii remains in Upper-case. Even if I change the string:
s = 'KRöger'
s = s.title()
print s
>KröGer

I still get the wrong result. 
Why does it behave this way? How can I make this string become 'Kröger'?
",130
23503486,23616980,2,"I finally found a way to do what I want. God bless generators:
name = 'KRöGer'
name = ' '.join(name[0].upper() + name[1:].lower() for n in name.split())
print name
>>Kröger

",51
23503486,23503597,2,"You could decode UTF 8 before the title:
print s.decode('utf-8').title()

",18
23503667,23503667,1,"I can test the rank of a matrix using np.linalg.matrix_rank(A) . But how can I test if all the rows of A are orthogonal efficiently?
I could take all pairs of rows and compute the inner product between them but is there a better way?
My matrix has fewer rows than columns and the rows are not unit vectors.
",65
23503667,23504241,2,"This answer basically summarizes the approaches mentioned in the question and the comments, and adds some comparison/insights about them

Approach #1 -- checking all row-pairs
As you suggested, you can iterate over all row pairs, and compute the inner product. If A.shape==(N,M), i.e. you have N rows of size M each, you end up with a O(M*N^2) complexity.
Approach #2 -- matrix multiplication
As suggested in the comments by @JoeKington, you can compute the multiplication A.dot(A.T), and check all the non-diagonal elements. Depending on the algorithm used for matrix multiplication, this can be faster than the naive O(M*N^2) algorithm, but only asymptotically better. Unless your matrices are big, they would be slower.

The advantages of approach #1:

You can ""short circuit"" -- quit the check as soon as you find the first non-orthogonal pair
requires less memory. In #2, you create a temporary NxN matrix.

The advantages of approach #2:

The multiplication is fast, as it is implemented in the heavily-optimized linear-algebra library (BLAS of ATLAS). I believe those libraries choose the right algorithm to use according to input size (i.e. they won't use the fancy algorithms on small matrices, because they are slower for small matrices. There's a big constant hidden behind that O-notation).
less code to write

My bet is that for small matrices, approach #2 would prove faster due to the fact the LA libraries are heavily optimized, and despite the fact they compute the entire multiplication, even after processing the first pair of non-orthogonal rows.
",309
23503667,23512809,2,"It seems that this will do
product = np.dot(A,A.T)
np.fill_diagonal(product,0)
if (product.any() == 0):

",27
23503667,23552362,2,"Approach #3: Compute the QR decomposition of AT
In general, to find an orthogonal basis of the range space of some matrix X, one can compute the QR decomposition of this matrix (using Givens rotations or Householder reflectors). Q is an orthogonal matrix and R upper triangular. The columns of Q corresponding to non-zero diagonal entries of R form an orthonormal basis of the range space.
If the columns of X=AT, i.e., the rows of A, already are orthogonal, then the QR decomposition will necessarily have the R factor diagonal, where the diagonal entries are plus or minus the lengths of the columns of X resp. the rows of A.
Common folklore has it that this approach is numerically better behaved than the computation of the product A*AT=RT*R. This may only matter for larger matrices. The computation is not as straightforward as the matrix product, however, the amount of operations is of the same size.
",175
23503725,23503725,1,"I am new to Python, I have seen many examples but not a good summary of examples. Here are the 4 things that I want to get done.  Thanks for your help.
mydict = {'carl':40,
          'alan':2,
          'bob':1,
          'danny':3}

I wanted to use the following to access the dict elements:
for key in sorted(mydict.keys()):
    print(key, mydict[key])

How do I get the following output:
Case 1: (by key ascending)
alan   2
bob    1
carl  40
danny  3

Case 2: (by key descending) 
danny  3
carl   40
bob    1
alan   2

Case 3: (by value ascending)
bob   1
alan  2
danny 3
carl 40

Case 4:  (by value descending)
carl   40
danny   3
alan    2
bob     1

",150
23503725,23504182,2,"d = {'carl':40,
      'alan':2,
      'bob':1,
      'danny':3}

ks = sorted(d.keys()) #  sort keys
for key in ks:# key ascending
print(key, d[key])


for key in ks[::-1]: #key descending
    print(key, d[key])


vs= sorted(d, key=d.get) # sort keys by values
for key in vs: # values ascending
    print(key, d[key])


for key in vs[::-1]: # values descending
    print(key, d[key])

",110
23503725,23503832,2,"See the documentation for sorted.
Ascending by key:
for key, value in sorted(mydict.items()):
    pass

Descending by key:
for key, value in sorted(mydict.items(), reverse=True):
    pass

Ascending by value:
def get_value(item):
    return item[1]

for key, value in sorted(mydict.items(), key=get_value):
    pass

Descending by value:
for key, value in sorted(mydict.items(), key=get_value, reverse=True):
    pass

You could also use key=lambda x: x[1] instead of defining a get_key function if you wanted.
",114
23507320,23507320,1,"Say I have:
path1 = [0,3,1]
path2 = [0, 3, 2, 1]

and I want 
splitsOfPath1 = [(0,3), (3,1)]
splitsOfPath2 = [(0,3), (3, 2), (2, 1)]

How can this be achieved? the way I read paths is to go from 0 to 1, you need to visit 3. But to break that down, to go from 0 to 1. You need to go from 0 to 3 (0,3) and then from 3 to 1 (3, 1)
",113
23507320,23507328,2,"You can use zip and Explain Python's slice notation:
>>> path1 = [0, 3, 1]
>>> splitsOfPath1 = zip(path1, path1[1::])
>>> splitsOfPath1
[(0, 3), (3, 1)]
>>>
>>> path2 = [0, 3, 2, 1]
>>> splitsOfPath2 = zip(path2, path2[1::])
>>> splitsOfPath2
[(0, 3), (3, 2), (2, 1)]
>>>

",115
23507365,23507365,1,"Why is it that I cannot access the attribute of an object as seen in the following example? (raises an error)
class X(object):
def __init__(self):
    self.y = Y(self)
    self.variable = 5

class Y(object):
    def __init__(self,parent):
        self.parent = parent
        print self.parent.variable

my_instance = X()

Error raised: AttributeError: 'X' object has no attribute 'variable'
Shouldn't variable be accessible to the instance of Y that is created when X is initialized?
",100
23507365,23513195,2,"You're instantiating before adding/initializing the variable attribute. It will have no knowledge of the variable unless again instantiated or swapped as the other answer mentioned.
",28
23507365,23507444,2,"Move self.variable = 5 to before self.y = Y(self). When Y is initialized, variable has yet to be set in the parent. Since it is not a class attribute (it is defined only for the instance) it does not exist until then.
",51
23507408,28179286,2,"It took me a long time to get this working for PHP, after a lot of communication with Google it was finally revealed to me that in your app.yaml file you need to have a line that reads:
threadsafe: false

In order for the pipeline to successfully pick up and deploy your git push (I use sourcetree, but command line git has the same end result) that line must be present.  If it's omitted or set to true the pipeline won't be able to deploy it.
I wanted to throw this answer on here in case anyone stumbled on this thread looking for help.  One of my projects has ""randomly broken"" and after 3 months of successfully using my release pipeline for multiple commits per day it suddenly no longer deploys when I push.  Ultimately giving the extremely helpful error message of ""Unable to get deployment status"" - and now none of my changes can be applied to the live site.  Copying the entire source code, changing the app name, and pushing to a new GAE project with release pipeline works fine, but I need the original site to start working again.
",213
23507408,26578198,2,"Google App Engine pipelines do not like .gitignore file. Try if it works without that file. It fixed the problem for me.
",25
23507408,23507408,1,"So since last week suddenly git push origin master doesn't work anymore to ""push to deploy"". It sure pushes the sources to remote repository at Google, and the code is there but it never deploys. Read about it here: GAE: Trouble with push to deploy
It seems things are changing over at Google and this week there is new stuff in the Google Developer Console, in the ""Cloud Development/Releases"" section; ""Configure Release Pipeline""
There are three settings: the pipeline name, pipeline tasks, and then an optional setting to have deploy notifications sent by email.
I just enter a random name like ""mydevpipeline"", select ""Deploy source only"", and check the email box. But I just get this error: ""Failed to create the pipeline."". I also tried unchecking the email box, still same error. Tried it over and over.
No where to go from there...
Anyone been able to create this pipeline and get it all working?
It seems that this pipeline configuration must go through in order for push to deploy from now. I haven't seen any news or notification about this change...
Fwiw, the documentation https://developers.google.com/appengine/docs/push-to-deploy states nothing about pipelines. It's just outdated I guess.
Update:
What do you know... I went on trying to configure this pipeline on the live GAE project (the one described above is the dev GAE project I'm using)... and it worked. I could configure a pipeline ok. After that, I could once more push-to-deploy, alas only on the live version so far. I might try creating a new dev project, it seems existing projects ""break"" from time to time... I have had similar problems before and creating a new project DOES solve things from time to time.....
",342
23507481,23507481,1,"Trying to install ez_setup.py on windows xp. Python2.7 is installed, and proper path variables are established.
I tried following this, but get the following error below:
 copying build\lib\setuptools\command\bdist_rpm.py -> build\bdist.win32\egg\setup
 tools\command
 copying build\lib\setuptools\command\bdist_wininst.py -> build\bdist.win32\egg\s
 etuptools\command
 copying build\lib\setuptools\command\build_ext.py -> build\bdist.win32\egg\setup
 tools\command
 copying build\lib\setuptools\command\build_py.py -> build\bdist.win32\egg\setupt
 ools\command
 copying build\lib\setuptools\command\develop.py -> build\bdist.win32\egg\setupto
 ols\command
 copying build\lib\setuptools\command\easy_install.py -> build\bdist.win32\egg\se
 tuptools\command
 copying build\lib\setuptools\command\egg_info.py -> build\bdist.win32\egg\setupt
 ools\command
 copying build\lib\setuptools\command\install.py -> build\bdist.win32\egg\setupto
 ols\command
 copying build\lib\setuptools\command\install_egg_info.py -> build\bdist.win32\eg
 g\setuptools\command
 copying build\lib\setuptools\command\install_lib.py -> build\bdist.win32\egg\set
 uptools\command
 copying build\lib\setuptools\command\install_scripts.py -> build\bdist.win32\egg
 \setuptools\command
 copying build\lib\setuptools\command\launcher manifest.xml -> build\bdist.win32\
 egg\setuptools\command
 copying build\lib\setuptools\command\register.py -> build\bdist.win32\egg\setupt
 ools\command
 copying build\lib\setuptools\command\rotate.py -> build\bdist.win32\egg\setuptoo
 ls\command
 copying build\lib\setuptools\command\saveopts.py -> build\bdist.win32\egg\setupt
 ools\command
 copying build\lib\setuptools\command\sdist.py -> build\bdist.win32\egg\setuptool
 s\command
 copying build\lib\setuptools\command\setopt.py -> build\bdist.win32\egg\setuptoo
 ls\command
 copying build\lib\setuptools\command\test.py -> build\bdist.win32\egg\setuptools
 \command
 copying build\lib\setuptools\command\upload_docs.py -> build\bdist.win32\egg\set
 uptools\command
 copying build\lib\setuptools\command\__init__.py -> build\bdist.win32\egg\setupt
 ools\command
 copying build\lib\setuptools\compat.py -> build\bdist.win32\egg\setuptools
 copying build\lib\setuptools\depends.py -> build\bdist.win32\egg\setuptools
 copying build\lib\setuptools\dist.py -> build\bdist.win32\egg\setuptools
 copying build\lib\setuptools\extension.py -> build\bdist.win32\egg\setuptools
 copying build\lib\setuptools\gui-32.exe -> build\bdist.win32\egg\setuptools
 copying build\lib\setuptools\gui-64.exe -> build\bdist.win32\egg\setuptools
 copying build\lib\setuptools\gui-arm-32.exe -> build\bdist.win32\egg\setuptools
 copying build\lib\setuptools\gui.exe -> build\bdist.win32\egg\setuptools
 copying build\lib\setuptools\lib2to3_ex.py -> build\bdist.win32\egg\setuptools
 copying build\lib\setuptools\package_index.py -> build\bdist.win32\egg\setuptool
 s
 copying build\lib\setuptools\py26compat.py -> build\bdist.win32\egg\setuptools
 copying build\lib\setuptools\py27compat.py -> build\bdist.win32\egg\setuptools
 copying build\lib\setuptools\py31compat.py -> build\bdist.win32\egg\setuptools
 copying build\lib\setuptools\sandbox.py -> build\bdist.win32\egg\setuptools
 copying build\lib\setuptools\script template (dev).py -> build\bdist.win32\egg\s
 etuptools
 copying build\lib\setuptools\script template.py -> build\bdist.win32\egg\setupto
 ols
 copying build\lib\setuptools\site-patch.py -> build\bdist.win32\egg\setuptools
 copying build\lib\setuptools\ssl_support.py -> build\bdist.win32\egg\setuptools
 copying build\lib\setuptools\svn_utils.py -> build\bdist.win32\egg\setuptools
 creating build\bdist.win32\egg\setuptools\tests
 copying build\lib\setuptools\tests\doctest.py -> build\bdist.win32\egg\setuptool
 s\tests
 copying build\lib\setuptools\tests\environment.py -> build\bdist.win32\egg\setup
 tools\tests
 copying build\lib\setuptools\tests\py26compat.py -> build\bdist.win32\egg\setupt
 ools\tests
 copying build\lib\setuptools\tests\script-with-bom.py -> build\bdist.win32\egg\s
 etuptools\tests
 copying build\lib\setuptools\tests\server.py -> build\bdist.win32\egg\setuptools
 \tests
 copying build\lib\setuptools\tests\test_bdist_egg.py -> build\bdist.win32\egg\se
 tuptools\tests
 copying build\lib\setuptools\tests\test_build_ext.py -> build\bdist.win32\egg\se
 tuptools\tests
 copying build\lib\setuptools\tests\test_develop.py -> build\bdist.win32\egg\setu
 ptools\tests
 copying build\lib\setuptools\tests\test_dist_info.py -> build\bdist.win32\egg\se
 tuptools\tests
 copying build\lib\setuptools\tests\test_easy_install.py -> build\bdist.win32\egg
 \setuptools\tests
 copying build\lib\setuptools\tests\test_egg_info.py -> build\bdist.win32\egg\set
 uptools\tests
 copying build\lib\setuptools\tests\test_find_packages.py -> build\bdist.win32\eg
 g\setuptools\tests
 copying build\lib\setuptools\tests\test_markerlib.py -> build\bdist.win32\egg\se
 tuptools\tests
 copying build\lib\setuptools\tests\test_packageindex.py -> build\bdist.win32\egg
 \setuptools\tests
 copying build\lib\setuptools\tests\test_resources.py -> build\bdist.win32\egg\se
 tuptools\tests
 copying build\lib\setuptools\tests\test_sandbox.py -> build\bdist.win32\egg\setu
 ptools\tests
 copying build\lib\setuptools\tests\test_sdist.py -> build\bdist.win32\egg\setupt
 ools\tests
 copying build\lib\setuptools\tests\test_svn.py -> build\bdist.win32\egg\setuptoo
 ls\tests
 copying build\lib\setuptools\tests\test_test.py -> build\bdist.win32\egg\setupto
 ols\tests
 copying build\lib\setuptools\tests\test_upload_docs.py -> build\bdist.win32\egg\
 setuptools\tests
 copying build\lib\setuptools\tests\__init__.py -> build\bdist.win32\egg\setuptoo
 ls\tests
 copying build\lib\setuptools\version.py -> build\bdist.win32\egg\setuptools
 copying build\lib\setuptools\__init__.py -> build\bdist.win32\egg\setuptools
 creating build\bdist.win32\egg\_markerlib
 copying build\lib\_markerlib\markers.py -> build\bdist.win32\egg\_markerlib
 copying build\lib\_markerlib\__init__.py -> build\bdist.win32\egg\_markerlib
 byte-compiling build\bdist.win32\egg\easy_install.py to easy_install.pyc
 byte-compiling build\bdist.win32\egg\pkg_resources.py to pkg_resources.pyc
 byte-compiling build\bdist.win32\egg\setuptools\archive_util.py to archive_util.
 pyc
 byte-compiling build\bdist.win32\egg\setuptools\command\alias.py to alias.pyc
 byte-compiling build\bdist.win32\egg\setuptools\command\bdist_egg.py to bdist_eg
 g.pyc
 byte-compiling build\bdist.win32\egg\setuptools\command\bdist_rpm.py to bdist_rp
 m.pyc
 byte-compiling build\bdist.win32\egg\setuptools\command\bdist_wininst.py to bdis
 t_wininst.pyc
 byte-compiling build\bdist.win32\egg\setuptools\command\build_ext.py to build_ex
 t.pyc
 byte-compiling build\bdist.win32\egg\setuptools\command\build_py.py to build_py.
 pyc
 byte-compiling build\bdist.win32\egg\setuptools\command\develop.py to develop.py
 c
 byte-compiling build\bdist.win32\egg\setuptools\command\easy_install.py to easy_
 install.pyc
 byte-compiling build\bdist.win32\egg\setuptools\command\egg_info.py to egg_info.
 pyc
 byte-compiling build\bdist.win32\egg\setuptools\command\install.py to install.py
 c
 byte-compiling build\bdist.win32\egg\setuptools\command\install_egg_info.py to i
 nstall_egg_info.pyc
 byte-compiling build\bdist.win32\egg\setuptools\command\install_lib.py to instal
 l_lib.pyc
 byte-compiling build\bdist.win32\egg\setuptools\command\install_scripts.py to in
 stall_scripts.pyc
 byte-compiling build\bdist.win32\egg\setuptools\command\register.py to register.
 pyc
 byte-compiling build\bdist.win32\egg\setuptools\command\rotate.py to rotate.pyc
 byte-compiling build\bdist.win32\egg\setuptools\command\saveopts.py to saveopts.
 pyc
 byte-compiling build\bdist.win32\egg\setuptools\command\sdist.py to sdist.pyc
 byte-compiling build\bdist.win32\egg\setuptools\command\setopt.py to setopt.pyc
 byte-compiling build\bdist.win32\egg\setuptools\command\test.py to test.pyc
 byte-compiling build\bdist.win32\egg\setuptools\command\upload_docs.py to upload
 _docs.pyc
 byte-compiling build\bdist.win32\egg\setuptools\command\__init__.py to __init__.
 pyc
 byte-compiling build\bdist.win32\egg\setuptools\compat.py to compat.pyc
 byte-compiling build\bdist.win32\egg\setuptools\depends.py to depends.pyc
 byte-compiling build\bdist.win32\egg\setuptools\dist.py to dist.pyc
 byte-compiling build\bdist.win32\egg\setuptools\extension.py to extension.pyc
 byte-compiling build\bdist.win32\egg\setuptools\lib2to3_ex.py to lib2to3_ex.pyc
 byte-compiling build\bdist.win32\egg\setuptools\package_index.py to package_inde
 x.pyc
 byte-compiling build\bdist.win32\egg\setuptools\py26compat.py to py26compat.pyc
 byte-compiling build\bdist.win32\egg\setuptools\py27compat.py to py27compat.pyc
 byte-compiling build\bdist.win32\egg\setuptools\py31compat.py to py31compat.pyc
 byte-compiling build\bdist.win32\egg\setuptools\sandbox.py to sandbox.pyc
 byte-compiling build\bdist.win32\egg\setuptools\script template (dev).py to scri
 pt template (dev).pyc
 byte-compiling build\bdist.win32\egg\setuptools\script template.py to script tem
 plate.pyc
 byte-compiling build\bdist.win32\egg\setuptools\site-patch.py to site-patch.pyc
 byte-compiling build\bdist.win32\egg\setuptools\ssl_support.py to ssl_support.py
 c
 byte-compiling build\bdist.win32\egg\setuptools\svn_utils.py to svn_utils.pyc
 byte-compiling build\bdist.win32\egg\setuptools\tests\doctest.py to doctest.pyc
 byte-compiling build\bdist.win32\egg\setuptools\tests\environment.py to environm
 ent.pyc
 byte-compiling build\bdist.win32\egg\setuptools\tests\py26compat.py to py26compa
 t.pyc
 byte-compiling build\bdist.win32\egg\setuptools\tests\script-with-bom.py to scri
 pt-with-bom.pyc
 byte-compiling build\bdist.win32\egg\setuptools\tests\server.py to server.pyc
 byte-compiling build\bdist.win32\egg\setuptools\tests\test_bdist_egg.py to test_
 bdist_egg.pyc
 byte-compiling build\bdist.win32\egg\setuptools\tests\test_build_ext.py to test_
 build_ext.pyc
 byte-compiling build\bdist.win32\egg\setuptools\tests\test_develop.py to test_de
 velop.pyc
 byte-compiling build\bdist.win32\egg\setuptools\tests\test_dist_info.py to test_
 dist_info.pyc
 byte-compiling build\bdist.win32\egg\setuptools\tests\test_easy_install.py to te
 st_easy_install.pyc
 byte-compiling build\bdist.win32\egg\setuptools\tests\test_egg_info.py to test_e
 gg_info.pyc
 byte-compiling build\bdist.win32\egg\setuptools\tests\test_find_packages.py to t
 est_find_packages.pyc
 byte-compiling build\bdist.win32\egg\setuptools\tests\test_markerlib.py to test_
 markerlib.pyc
 byte-compiling build\bdist.win32\egg\setuptools\tests\test_packageindex.py to te
 st_packageindex.pyc
 byte-compiling build\bdist.win32\egg\setuptools\tests\test_resources.py to test_
 resources.pyc
 byte-compiling build\bdist.win32\egg\setuptools\tests\test_sandbox.py to test_sa
 ndbox.pyc
 byte-compiling build\bdist.win32\egg\setuptools\tests\test_sdist.py to test_sdis
 t.pyc
 byte-compiling build\bdist.win32\egg\setuptools\tests\test_svn.py to test_svn.py
 c
 byte-compiling build\bdist.win32\egg\setuptools\tests\test_test.py to test_test.
 pyc
 byte-compiling build\bdist.win32\egg\setuptools\tests\test_upload_docs.py to tes
 t_upload_docs.pyc
 byte-compiling build\bdist.win32\egg\setuptools\tests\__init__.py to __init__.py
 c
 byte-compiling build\bdist.win32\egg\setuptools\version.py to version.pyc
 byte-compiling build\bdist.win32\egg\setuptools\__init__.py to __init__.pyc
 byte-compiling build\bdist.win32\egg\_markerlib\markers.py to markers.pyc
 byte-compiling build\bdist.win32\egg\_markerlib\__init__.py to __init__.pyc
 creating build\bdist.win32\egg\EGG-INFO
 copying setuptools.egg-info\PKG-INFO -> build\bdist.win32\egg\EGG-INFO
 copying setuptools.egg-info\SOURCES.txt -> build\bdist.win32\egg\EGG-INFO
 copying setuptools.egg-info\dependency_links.txt -> build\bdist.win32\egg\EGG-IN
 FO
 copying setuptools.egg-info\entry_points.txt -> build\bdist.win32\egg\EGG-INFO
 copying setuptools.egg-info\requires.txt -> build\bdist.win32\egg\EGG-INFO
 copying setuptools.egg-info\top_level.txt -> build\bdist.win32\egg\EGG-INFO
 copying setuptools.egg-info\zip-safe -> build\bdist.win32\egg\EGG-INFO
 creating dist
 creating 'dist\setuptools-3.5.1-py2.7.egg' and adding 'build\bdist.win32\egg' to
  it
 removing 'build\bdist.win32\egg' (and everything under it)
 Processing setuptools-3.5.1-py2.7.egg
 Removing c:\python27\lib\site-packages\setuptools-3.5.1-py2.7.egg
 Copying setuptools-3.5.1-py2.7.egg to c:\python27\lib\site-packages
 setuptools 3.5.1 is already the active version in easy-install.pth
 Installing easy_install-script.py script to C:\Python27\Scripts
 Installing easy_install.exe script to C:\Python27\Scripts
 Installing easy_install.exe.manifest script to C:\Python27\Scripts
 Installing easy_install-2.7-script.py script to C:\Python27\Scripts
 Installing easy_install-2.7.exe script to C:\Python27\Scripts
 Installing easy_install-2.7.exe.manifest script to C:\Python27\Scripts

 Installed c:\python27\lib\site-packages\setuptools-3.5.1-py2.7.egg
 Processing dependencies for setuptools==3.5.1
 Traceback (most recent call last):
   File ""setup.py"", line 217, in <module>
     dist = setuptools.setup(**setup_params)
   File ""C:\Python27\lib\distutils\core.py"", line 152, in setup
     dist.run_commands()
   File ""C:\Python27\lib\distutils\dist.py"", line 953, in run_commands
     self.run_command(cmd)
   File ""C:\Python27\lib\distutils\dist.py"", line 972, in run_command
     cmd_obj.run()
   File ""c:\docume~1\jason\locals~1\temp\tmpqpn4qe\setuptools-3.5.1\setuptools\co
 mmand\install.py"", line 65, in run
     self.do_egg_install()
   File ""c:\docume~1\jason\locals~1\temp\tmpqpn4qe\setuptools-3.5.1\setuptools\co
 mmand\install.py"", line 115, in do_egg_install
     cmd.run()
   File ""c:\docume~1\jason\locals~1\temp\tmpqpn4qe\setuptools-3.5.1\setuptools\co
 mmand\easy_install.py"", line 360, in run
     self.easy_install(spec, not self.no_deps)
   File ""c:\docume~1\jason\locals~1\temp\tmpqpn4qe\setuptools-3.5.1\setuptools\co
 mmand\easy_install.py"", line 576, in easy_install
     return self.install_item(None, spec, tmpdir, deps, True)
   File ""c:\docume~1\jason\locals~1\temp\tmpqpn4qe\setuptools-3.5.1\setuptools\co
 mmand\easy_install.py"", line 627, in install_item
     self.process_distribution(spec, dist, deps)
   File ""c:\docume~1\jason\locals~1\temp\tmpqpn4qe\setuptools-3.5.1\setuptools\co
 mmand\easy_install.py"", line 673, in process_distribution
     [requirement], self.local_index, self.easy_install
   File ""c:\docume~1\jason\locals~1\temp\tmpqpn4qe\setuptools-3.5.1\pkg_resources
 .py"", line 633, in resolve
     requirements.extend(dist.requires(req.extras)[::-1])
   File ""c:\docume~1\jason\locals~1\temp\tmpqpn4qe\setuptools-3.5.1\pkg_resources
 .py"", line 2291, in requires
     dm = self._dep_map
   File ""c:\docume~1\jason\locals~1\temp\tmpqpn4qe\setuptools-3.5.1\pkg_resources
 .py"", line 2277, in _dep_map
     for extra, reqs in split_sections(self._get_metadata(name)):
   File ""c:\docume~1\jason\locals~1\temp\tmpqpn4qe\setuptools-3.5.1\pkg_resources
 .py"", line 2715, in split_sections
     for line in yield_lines(s):
   File ""c:\docume~1\jason\locals~1\temp\tmpqpn4qe\setuptools-3.5.1\pkg_resources
 .py"", line 1989, in yield_lines
     for ss in strs:
   File ""c:\docume~1\jason\locals~1\temp\tmpqpn4qe\setuptools-3.5.1\pkg_resources
 .py"", line 2305, in _get_metadata
     for line in self.get_metadata_lines(name):
   File ""c:\docume~1\jason\locals~1\temp\tmpqpn4qe\setuptools-3.5.1\pkg_resources
 .py"", line 1369, in get_metadata_lines
     return yield_lines(self.get_metadata(name))
   File ""c:\docume~1\jason\locals~1\temp\tmpqpn4qe\setuptools-3.5.1\pkg_resources
 .py"", line 1361, in get_metadata
     return self._get(self._fn(self.egg_info, name))
   File ""c:\docume~1\jason\locals~1\temp\tmpqpn4qe\setuptools-3.5.1\pkg_resources
 .py"", line 1425, in _get
     return self.loader.get_data(path)
 zipimport.ZipImportError: bad local file header in c:\python27\lib\site-packages
 \setuptools-3.5.1-py2.7.egg
 Something went wrong during the installation.
 See the error message above.

 C:\Python27>python ez_setup.py install

",1260
23507481,23513725,2,"You can try that same without the 'install', i have never used that install -command. Or one likely option is that you ez_setup.py broken or bad, i have been using this: https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py 
",39
23507481,23598378,2,"For anyone who happens upon this question: I used anaconda installer, and now everything works perfectly. I would avoid the path I took and go directly to anaconda.
",32
23507492,23507492,1,"So I have this line of code in views.py
def display_agao(request):
    query_agao = Butuan_Parcel.objects.filter(newpin=""162-01-0001-002-29"")
    djf = Django.Django(geodjango='geom',properties=['id','newpin'])
    geoj = GeoJSON.GeoJSON()
    butuan_agao = geoj.encode(djf.decode(query_agao.transform(3857)))

    query_subclass = tbl_subclass.objects.all()
    return render(request,""index1.html"",{'butuan_agao': butuan_agao, 'query_agao':query_agao,'query_subclass':query_subclass})

I want to get the value from the html file and so that I can get a newpin depends on the users query and able to display the layer from the map..
    <form action=""."" method=""GET"">
            <select name=""q"">
             {% for parcel in query_agao %}
            <option type=""text""  value=""{{ parcel.newpin }}"">{{ parcel.newpin }}</option>
             {% endfor %}
              </select>
              <input type=""submit"" value=""Search"">

        </form>

just sample form above...
",196
23507492,23507634,2,"You can get the value from html form to view use request arg,
def my_view(request, ):
   search_term = request.GET.get(""q"", None) #--> q means ""name"" of select box in html.
   # use search_term whatever you wants.

",52
23507666,23507666,1,"Given a Python list whose elements are either integers or lists of integers (only we don't know how deep the nesting goes), how can we find the sum of each individual integer within the list?
It's fairly straightforward to find the sum of a list whose nesting only goes one level deep, 
but what if the nesting goes two, three, or more levels deep?
I know the best approach is recursion, but this is a challenge wherein I have to do it without recursion.
Please help!!
",100
23507666,23507786,2,"L = [...]
while any(isinstance(i, list) for i in L):
   L = [j for i in L for j in (i if isinstance(i, list) else [i])]

result = sum(L)

Basically you iterate over the outer list and unpack the first level of any inner lists until there are no inner lists left
",75
23507666,23507904,2,"Here is one solution:
from copy import deepcopy

def recursive_sum(int_list):
    #int_list = deepcopy(int_list)    use this line if don't want to modify original list
    ret = 0
    while len(int_list) > 0:
        elem = int_list.pop(0)
        if type(elem) == int:
            ret += elem
        elif type(elem) == list:
            int_list.extend(elem)
        else:
            raise ValueError
    return ret

testcase = [1,2,3,[4,5,[6,7,8,[9,10]]]]
print recursive_sum(testcase)    # print 55

Basically, it pops first element of input list. If it's Int, add into sum; if it's List, extend to the end of input list
",135
23507666,23508122,2,"One mostly-readable (and presumably performant, though I haven't tested it) way to iteratively flatten a list:
from collections import deque

def iterative_flatten(li):
    nested = deque(li)
    res = []
    dq = deque()
    while nested or dq:
        x = dq.pop() if dq else nested.popleft()
        dq.extend(reversed(x)) if isinstance(x, list) else res.append(x)
    return res

Uses deques to avoid nasty O(n**2) behavior from list.pop(0).  You can get equivalent results by making a reversed copy and popping from the end, but I find the code a little easier to follow if you just use deques and popleft.  On a similar note, it's a line or two less code if you want to mutate the list in-place but way slower (for the same reason; popping from the head of the list is O(n) since every element in the underlying array has to be shifted).
nested = [1,[[2,3],[[4,5],[6]]],[[[[7]]]]]

iterative_flatten(nested)
Out[116]: [1, 2, 3, 4, 5, 6, 7]

sum(iterative_flatten(nested))
Out[117]: 28

After it's flat, summing is (hopefully) trivial :-)
",270
23507731,23507848,2,"Create a simple algorithm that would generate those characters, by using the math patterns that would be necessary to create those characters. For that example, notice you can use a pattern for the digits you gave.
For example (say the user input is i):
The first line XXXXXX is six characters. The example with an input of i=8 is eight characters. So why not output the the character X for i times?
print ""X""*i
That's your first line. Now continue with the rest of the lines.
The second line is X    X. There are four spaces for an input of i=6, six spaces for an input of i=8. So there's a pattern there, clearly. So why output an X, then i-2 spaces, then another X?
print ""X"" + "" ""*(i-2) + ""X""
Complete the third line the same way:
print ""X""*(i-2)/2 + "" ""*(i/2-2) + ""X""*(i/2)/2
You should be able to complete the rest from here, to reverse the character and try out various inputs. Think about what happens for other integer values that aren't i=6, 8, 10, 12, etc. You can use round, floor, or a variation on these patterns in order to handle inputs such as i=7 and 9.
",264
23507731,23507731,1,"I have a question regarding Python ASCII Art.
User inputs 6.
XXXXXX
X    X
XX  XX
XX  XX
X    X
XXXXXX

User inputs 8.
XXXXXXXX
X      X
X      X
XXX  XXX
XXX  XXX
X      X
X      X
XXXXXXXX

How would I craft something like this in Python 3, using input from the user?
Could someone point me in the right direction. I just started learning python at home. This is one of the questions in the bonus quiz section of a book I purchased from Amazon. 
",94
23507779,23511975,2,"I assume your purpose is to create a program that starts two threads, one (client thread) receives keyboard input and sends to the other (server thread), the server thread prints out everything it received.
Based on my assumption, you first need to start a ServerThread listen to a port (it's not like what your 'ClientThread' did). Here's an example:
import socket
import threading

def main():
    host = 'localhost'
    port = 5000
    size = 1024

    thread1 = ServerThread(host, port, size) 
    thread1.start() 

    #open a socket for client
    try:
        clientSock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        clientSock.connect((host,port))
    except socket.error, (value,message):
        if clientSock:
            clientSock.close()
            print ""Could not connect to server: "" + message
        sys.exit(1) 

    while True:
        #wait for keyboard input
        line = raw_input()
        #send the input to the server unless its only a newline
        if line != ""\n"":
            clientSock.send(line)
            # Is server supposed to send back any response?
            #data = clientSock.recv(size)
            #print data
        if line == ""Quit"":
            clientSock.close()
            break

class ServerThread(threading.Thread): 
    def __init__(self, host, port, size): 
        super(ServerThread, self).__init__() 
        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.sock.bind((host, port))
        self.sock.listen(1)
        self.data_size = size
        self.stopped = False 

    def run(self): 
        conn, addr = self.sock.accept()
        print 'Connected by', addr
        while not self.stopped: 
            data = conn.recv(self.data_size)
            if data == 'Quit':
                print 'Client close the connection'
                self.stopped = True
            else:
                print 'Server received data:', data
                # Is server supposed to send back any response?
                #conn.sendall('Server received data: ' + data)
        conn.close()

if __name__ == '__main__':
    main()

And these are the output:
Connected by ('127.0.0.1', 41153)
abc
Server received data: abc
def
Server received data: def
Quit
Client close the connection

You may check here for more details about Python socket: https://docs.python.org/2/library/socket.html?#example
",414
23507779,23507779,1,"so right now in order to receive your message you need to receive one 
my teachers instructions are (in the main)""Modify the loop so that it only listens for keyboard input and then sends it to the server.""
I did the rest but don't understand this, ... help?
import socket
import select
import sys
import threading

'''
Purpose:  Driver
parameters: none
returns: none
'''

def main():
    host = 'localhost'
    port = 5000
    size = 1024
    #open a socket to the client.
    try:
        clientSock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        clientSock.connect((host,port))
        #exit on error
    except socket.error, (value,message):
        if clientSock :
            clientSock.close()
            print ""Could not make connection: "" + message
        sys.exit(1) 

    thread1 = ClientThread() 
    thread1.start() 

    while True:
        #wait for keyboard input
        line = raw_input()
        #send the input to the server unless its only a newline
        if line != ""\n"":
            clientSock.send(line)
        #wait to get something from the server and print it
        data = clientSock.recv(size)
        print data
class ClientThread(threading.Thread): 

 ''' 
 Purpose: the constructor 
 parameters: the already created and connected client socket 
 returns: none 
 ''' 

def __init__(self, clientSocket): 
    super(ClientThread, self).__init__() 
    self.clientSocket = clientSocket 
    self.stopped = False 



def run(self): 
    while not self.stopped: 
        self.data = self.clientSocket.recv(1024) 
        print self.data 



main()

",285
23507883,23507935,2,"The most straightforward approach would be to get the first item from mock.call_args_list and check if it is called with 1:

call_args_list
This is a list of all the calls made to the mock object in sequence
  (so the length of the list is the number of times it has been called).

assert m.call_args_list[0] == call(1)

where call is imported from mock: from mock import call.
Also, mock_calls would work in place of call_args_list too.
Another option would be to use assert_any_call():
m.assert_any_call(1)

",103
23507883,23507883,1,"eg in t.py
def a(obj):
  print obj

def b():
  a(1)
  a(2)

then:
from t import b

with patch('t.a') as m:
  b()
  m.assert_called_with(1)

I get:
AssertionError: Expected call: a(1)
Actual call: a(2)

",65
23508577,23509541,2,"xlabel and ylabel are on my plot when I run your example, just a bit outside the display depending on how it is sized.
Try adding the line:
plt.subplots_adjust(bottom=.25, left=.25)

or simply stretching the display window.
",44
23508577,23508577,1,"How is it possible to make xlabel and ylabel visible in the following plot?
import pandas as pd
import matplotlib.pyplot as plt
a = {'Test1': {1: 21867186, 4: 20145576, 10: 18018537},
'Test2': {1: 23256313, 4: 21668216, 10: 19795367}}

d = pd.DataFrame(a).T
#print d

f = plt.figure()
plt.xlabel(""xTEST"")
plt.ylabel(""yTEST"")

plt.ticklabel_format(style = 'plain')

plt.title('Title here!', color='black')
d.plot(kind='bar', ax=f.gca())
plt.show()

",116
23508582,23508582,1,"With a row of QListWidgets I wonder if there is a simple way to label each so the user would be able to say which List is which.
Here is a dialog's screenshot a code posted below results.
I avoid using any widgets outside of QListWidgets(). Ideal solution would be solved utilizing QListWidgets itself. It would be great if there is a way to place a text line-label similar to those available for QGroupBox with .setTitle('myString'). At very least an ability to place a label as a first list item would be sufficient too... 


from PyQt4 import QtGui, QtCore

class MyApp(object):
    def __init__(self):
        super(MyApp, self).__init__()
        app = QtGui.QApplication(sys.argv)
        self.mainWidget = QtGui.QWidget()
        self.mainLayout = QtGui.QVBoxLayout()
        self.mainWidget.setLayout(self.mainLayout)

        self.groupbox = QtGui.QGroupBox()
        self.groupbox.setTitle('My Groupbox')
        self.layout = QtGui.QVBoxLayout()
        self.groupbox.setLayout(self.layout)

        self.listGroupbox = QtGui.QGroupBox()
        self.listLayout = QtGui.QHBoxLayout()
        self.listGroupbox.setLayout(self.listLayout)
        self.listA=QtGui.QListWidget()
        self.listB=QtGui.QListWidget()

        self.listLayout.addWidget(self.listA)
        self.listLayout.addWidget(self.listB)
        self.layout.addWidget(self.listGroupbox) 

        self.okButton = QtGui.QPushButton('OK')
        self.okButton.clicked.connect(self.OK) 
        self.layout.addWidget(self.okButton)                      
        self.mainLayout.addWidget(self.groupbox)
        self.mainWidget.show()
        sys.exit(app.exec_())

    def OK(self):
        print 'Ok'    
if __name__ == '__main__':
    MyApp()



",252
23508582,23509644,2,"Here is my attempt to achieve it without abandoning QListWidget()... utilizing layout's .insertLayout() method to attach QLabel without losing GUI space usually taken by QGroupBox()...


from PyQt4 import QtGui, QtCore

class MyApp(object):
    def __init__(self):
        super(MyApp, self).__init__()
        app = QtGui.QApplication(sys.argv)
        self.mainWidget = QtGui.QWidget()
        self.mainLayout = QtGui.QVBoxLayout()
        self.mainWidget.setLayout(self.mainLayout)

        self.groupbox = QtGui.QGroupBox()
        self.groupbox.setTitle('My Groupbox')
        self.layout = QtGui.QVBoxLayout()
        self.groupbox.setLayout(self.layout)

        self.listGroupbox = QtGui.QGroupBox()
        self.listLayout = QtGui.QHBoxLayout()
        self.listGroupbox.setLayout(self.listLayout)
        self.listA=QtGui.QListWidget()
        self.listB=QtGui.QListWidget()

        self.subLayoutA=QtGui.QVBoxLayout()        
        self.listLayout.insertLayout(0,self.subLayoutA)
        self.subLayoutA.addWidget(QtGui.QLabel('Label A') )
        self.subLayoutA.addWidget(self.listA)

        self.subLayoutB=QtGui.QVBoxLayout()
        self.listLayout.insertLayout(1,self.subLayoutB)
        self.subLayoutB.addWidget(QtGui.QLabel('Label B') )
        self.subLayoutB.addWidget(self.listB)


        self.layout.addWidget(self.listGroupbox) 

        self.okButton = QtGui.QPushButton('OK')
        self.okButton.clicked.connect(self.OK) 
        self.layout.addWidget(self.okButton)                      
        self.mainLayout.addWidget(self.groupbox)
        self.mainWidget.show()
        sys.exit(app.exec_())

    def OK(self):
        print 'Ok'    
if __name__ == '__main__':
    MyApp()

",215
23508582,23509288,2,"Unfortunately there's no (Qt-native) way to label a QListView (on which QListWidget is based). If your really don't want additional widgets, I would instead use a single-column QTableWidget, and put the list title in the column header. QTableWidget and QListWidget work pretty similarly, so this probably won't break too much of your existing code.
An example based on yours:
class MyApp(object):
    def __init__(self):

        # snipped

        self.listA = self.prepareTableWidget('List A')
        self.listB = self.prepareTableWidget('List B')

        # snipped

    def prepareTableWidget(self, name):
        table = QtGui.QTableWidget()
        table.setColumnCount(1)
        table.setHorizontalHeaderLabels([name])
        table.horizontalHeader().setResizeMode(QtGui.QHeaderView.Stretch)
        return table

    # snipped


",139
23508596,23508834,2,"interesting! Let's take a look.
The design is pretty simple. Read the file into a dictionary and perform manipulation on the dict, then write out the files.
with open('file1.txt') as in_:
    mapping = {}
    for line in in_:
        key,value = line.strip().split(',')
        mapping[key] = int(value)

mapping is now {""a"":1, ""b"":3, ""c"":5, ""d"":-4} Let's read in our files.
values = {}
with open('file2.txt') as in_:
    for _ in range(3):
        # This is ugly, but it's a quick hack. I'd improve it later.
        cur_dict = next(in_).strip()
        values[cur_dict] = {}
        for __ in range(4):
            key, value = next(in_).strip().split(',')
            values[cur_dict][key] = int(value)

Sheesh that's probably the ugliest code I've ever written, but values is now {""sample1"": {""a"":12, ""b"":10, ""c"":4, ""d"":6}, ""sample2"": ...}
Now for the manipulation. This is actually easy. Let's tack file write onto it, since this step is rather elementary
for dataset in values:
    for key, value in mapping.items():
        values[dataset][key] += value
    with open(dataset + "".txt"") as out:
        out.write(dataset)
        for key,value in values[dataset]:
            out.write(""{},{}\n"".format(key,value))

",336
23508596,23508596,1,"I have two text files:
file1.txt:
a,1
b,3
c,5
d,-4

and file2.txt:
sample1
a,12 
b,10
c,4
d,6

sample2
a,5 
b,8
c,6
d,12

sample3
a,3 
b,6
c,9
d,10

what I want to do is to subtract a value for a given letter in file1.txt from the corresponding letter in all the samples in file2.txt and create multiple files so the output looks like:
First file for sample1, sample1.txt
sample1.txt
a,11 # 12-1 as 1 from file1.txt was subtracted from 12 in file2.txt
b,7 # 10-3
c,-1 # 4-5
d,10 # 6-(-4)

and then separate file for sample2, sample2.txt:
sample2.txt
a,4 # 5-1 as 1 from file1.txt was subtracted from 5 in file2.txt
b,5 # 8-3
c,1 # 6-5
d,16 # 12-(-4)

and the same for sample3.
I tried looping over the file2.txt, but as my original file2.txt has over 1000 samples it takes a long time, is there a quicker pythonic way to do so?
Cheers,
Kate
",178
23508732,23521792,2,"You could this something like this :
# -*- coding: utf-8 -*-

soup = BeautifulSoup(html)
title = soup.find('td', class_='station').text.strip()

spans = soup.find_all('span', class_='ul')

# create the root of the XML file
root = ET.Element(""counter"")
root.set(""name"", title)

for item in spans:
    # retrieve the text inside the <td class=""station"">
    text = list(list(item.parents)[2].previous_siblings)[1].text.strip()
    if text == u'Entrée':
        break

    dish = ET.SubElement(root, 'dish')
    name = ET.SubElement(dish, 'name')
    name.text = item.text.rstrip()

tree = ET.ElementTree(root)
tree.write(""filename.xml"")

And this is the content of desired xml file :
<counter name=""Deli"">
    <dish>
        <name>Made to Order Deli Core</name>
    </dish> 
    <dish>
        <name>Turkey Chipotle Petite Wrap</name>
    </dish>
</counter>

Is very important include the following line # -*- coding: utf-8 -*- line above in the beginning of your file to avoid problems  with the accent , see SyntaxError: Non-ASCII character '\xa3' in file when function returns '£' for more details.
",244
23508732,23509208,2,"Actually i used both beautiful soup and element tree(for xml parsing)
fetch all elements in <span> 
# -*- coding: UTF-8 -*-

from bs4 import *
import xml.etree.ElementTree as ET

html='''<html>
<head>
    <title></title>
</head>

<body>
    <table class=""dayinner"">
        <tr class=""lun"">
            <td class=""mealname"" colspan=""3"">LUNCH</td>
        </tr>

        <tr class=""lun"">
            <td class=""station"">&nbsp;Deli</td>

            <td class=""menuitem"">
                <div class=""menuitem"">
                    <input class=""chk"" id=""S1L0000010000047598_35356"" onclick=
                    ""rptlist(this);"" onmouseout=""wschk(0);"" onmouseover=
                    ""wschk(1);"" type=""checkbox""> <span class=""ul"" onclick=
                    ""nf('0000047598_35356');"" onmouseout=""pcls(this);""
                    onmouseover=""ws(this);"">Made to Order Deli Core</span>
                </div>
            </td>

            <td class=""price""></td>
        </tr>

        <tr class=""lun"">
            <td class=""station"">&nbsp;</td>

            <td class=""menuitem"">
                <div class=""menuitem"">
                    <input class=""chk"" id=""S1L0000020000047933_06835"" onclick=
                    ""rptlist(this);"" onmouseout=""wschk(0);"" onmouseover=
                    ""wschk(1);"" type=""checkbox""> <span class=""ul"" onclick=
                    ""nf('0000047933_06835');"" onmouseout=""pcls(this);""
                    onmouseover=""ws(this);"">Turkey Chipotle Petite Wrap</span>
                </div>
            </td>

            <td class=""price""></td>
        </tr>

        <tr class=""lun"">
            <td colspan=""3"" style=""height:3px;""></td>
        </tr>

        <tr class=""lun"">
            <td colspan=""3"" style=""background-color:#c0c0c0; height:1px;""></td>
        </tr>

        <tr class=""lun"">
            <td class=""station"">&nbsp;Entrée</td>

            <td class=""menuitem"">
                <div class=""menuitem""><input class=""chk"" id=
                ""S1L0000030000044794_08943"" onclick=""rptlist(this);""
                onmouseout=""wschk(0);"" onmouseover=""wschk(1);"" type=""checkbox"">
                <span class=""ul"" onclick=""nf('0000044794_08943');"" onmouseout=
                ""pcls(this);"" onmouseover=""ws(this);"">Steamed
                Corn</span><img alt=""Vegan"" class=""icon"" src=
                ""images/g_062.gif""><img alt=""Mindful Item"" class=""icon"" src=
                ""images/m_051.gif""></div>
            </td>

            <td class=""price""></td>
        </tr>

        <tr class=""lun"">
            <td class=""station"">&nbsp;</td>

            <td class=""menuitem"">
                <div class=""menuitem"">
                    <input class=""chk"" id=""S1L0000040000033087_22244"" onclick=
                    ""rptlist(this);"" onmouseout=""wschk(0);"" onmouseover=
                    ""wschk(1);"" type=""checkbox""> <span class=""ul"" onclick=
                    ""nf('0000033087_22244');"" onmouseout=""pcls(this);""
                    onmouseover=""ws(this);"">Cuban Mojo Roasted Pork Loin</span>
                </div>
            </td>

            <td class=""price""></td>
        </tr>
    </table>
</body>
</html> '''

soup = BeautifulSoup(html)

counter = ET.Element('counter')
counter.set(""name"", ""#Deli"")





for i in soup.findAll('span'):
    dish = ET.SubElement(counter, 'dish')
    name = ET.SubElement(dish, 'name')
    name.text= i.text.replace('\n',' ')

print ET.dump(counter)

",776
23508732,23508732,1,"I am trying to get the deli title, and then under the deli title get the two menu items Made to Order Deli Core and Turkey Chipotle Petite Wrap? I'm using beautiful soup 4 to do this and its not working. And the same is true for the entree times?
<html>
<head>
    <title></title>
</head>

<body>
    <table class=""dayinner"">
        <tr class=""lun"">
            <td class=""mealname"" colspan=""3"">LUNCH</td>
        </tr>

        <tr class=""lun"">
            <td class=""station"">&nbsp;Deli</td>

            <td class=""menuitem"">
                <div class=""menuitem"">
                    <input class=""chk"" id=""S1L0000010000047598_35356"" onclick=
                    ""rptlist(this);"" onmouseout=""wschk(0);"" onmouseover=
                    ""wschk(1);"" type=""checkbox""> <span class=""ul"" onclick=
                    ""nf('0000047598_35356');"" onmouseout=""pcls(this);""
                    onmouseover=""ws(this);"">Made to Order Deli Core</span>
                </div>
            </td>

            <td class=""price""></td>
        </tr>

        <tr class=""lun"">
            <td class=""station"">&nbsp;</td>

            <td class=""menuitem"">
                <div class=""menuitem"">
                    <input class=""chk"" id=""S1L0000020000047933_06835"" onclick=
                    ""rptlist(this);"" onmouseout=""wschk(0);"" onmouseover=
                    ""wschk(1);"" type=""checkbox""> <span class=""ul"" onclick=
                    ""nf('0000047933_06835');"" onmouseout=""pcls(this);""
                    onmouseover=""ws(this);"">Turkey Chipotle Petite Wrap</span>
                </div>
            </td>

            <td class=""price""></td>
        </tr>

        <tr class=""lun"">
            <td colspan=""3"" style=""height:3px;""></td>
        </tr>

        <tr class=""lun"">
            <td colspan=""3"" style=""background-color:#c0c0c0; height:1px;""></td>
        </tr>

        <tr class=""lun"">
            <td class=""station"">&nbsp;Entrée</td>

            <td class=""menuitem"">
                <div class=""menuitem""><input class=""chk"" id=
                ""S1L0000030000044794_08943"" onclick=""rptlist(this);""
                onmouseout=""wschk(0);"" onmouseover=""wschk(1);"" type=""checkbox"">
                <span class=""ul"" onclick=""nf('0000044794_08943');"" onmouseout=
                ""pcls(this);"" onmouseover=""ws(this);"">Steamed
                Corn</span><img alt=""Vegan"" class=""icon"" src=
                ""images/g_062.gif""><img alt=""Mindful Item"" class=""icon"" src=
                ""images/m_051.gif""></div>
            </td>

            <td class=""price""></td>
        </tr>

        <tr class=""lun"">
            <td class=""station"">&nbsp;</td>

            <td class=""menuitem"">
                <div class=""menuitem"">
                    <input class=""chk"" id=""S1L0000040000033087_22244"" onclick=
                    ""rptlist(this);"" onmouseout=""wschk(0);"" onmouseover=
                    ""wschk(1);"" type=""checkbox""> <span class=""ul"" onclick=
                    ""nf('0000033087_22244');"" onmouseout=""pcls(this);""
                    onmouseover=""ws(this);"">Cuban Mojo Roasted Pork Loin</span>
                </div>
            </td>

            <td class=""price""></td>
        </tr>
    </table>
</body>
</html>

or if I could get it into a XML format like this:
<counter name=""Deli"">
    <dish>
        <name>Made to Order Deli Core</name>
    </dish>
    <dish>
        <name>Turkey Chipotle Petite Wrap</name>
    </dish>
</counter>

Thank you very much in advance, I really appreciate you taking the time to help me.
",800
23512010,23522076,2,"I use pyinstaller to create stand alone exe for windows.  http://www.pyinstaller.org/.
",15
23512010,23512010,1,"I have been developing an application using Python 3.3 and PyQt4 and I would like to be able to distribute it as a standalone app. My development environment is OS X and I have been able to create a standalone OS X app using cx_Freeze and py2app.
My question is, how would I go about creating an executable file for windows, considering I do not have access to a Windows operating system for development? 
I have tried using cx_Freeze's build_exe on OS X and running it on windows but it will not run. 
Thanks in advance :)
",105
23512030,23520198,2,"Have you considered just creating the certificate for the IP address? That wouldn't be much more fragile (probably less fragile, actually) than having to manually add the domain name to hosts files. See https://stackoverflow.com/a/11710762/138772 and https://stackoverflow.com/a/8444863/372643 for more info on that.
An alternative, but one probably requiring more work, would be to include a local DNS server in your app that redirects the domain name to your IP address. I can't really say what the best one to use for that would be, though.
",101
23512030,23512030,1,"I am using windows 7 and python 2.7 I created local https server with redirect url to server as its IP address. I created cert file for https using openssl.
Then I mapped my local system IP(172.16.17.84) to myapp.nobies.in in hosts file of windows.
So my server redirect url becomes https://myapp.nobies.in:443.
By doing this IP mapping in host file, I am not getting SSL error. 
But, I want to distribute my app to others, so, writing in host file through python code is not desirable, as it needs administrative privileges.
So, is there any way to assign/map this IP with hostname instead of making an entry in hosts file.
",127
23512095,23512095,1,"Hi i am new to GAE and python. I am trying to create small web application in GAE using python. I intend to use Unirest module in application.
I followed instruction in this post and got following error. 

no module named poster.encode

How can i install unirest successfully in GAE  ?
Thanks
",56
23512095,23513568,2,"Add import sys to your main.py and add these lines.
# inject './lib' dir in the path so that we can simply do ""import ndb"" or whatever there's in the app lib dir.
if 'lib' not in sys.path:
    sys.path[0:0] = ['lib']

Create the lib folder in the root of your project and copy the unirest folder in lib folder.
You can now import unirest and use your library.
But, why not use the standard method and library provide by Google App Engine ?
",100
23512160,23512160,1,"I am trying to pass class element to method. Element is formed dynamically inserting current time in it. Mine class looks something like this:
class MineContact(dict):
    def __init__(self, **kwargs):
        # set your default values
        import time
        curr_time = repr(time.time()).replace('.', '') 
        self['givenName'] = ['name%s' % curr_time[10:]]
        ...

So, I create object of this class and now I want to insert it as method argument:
    contact = MineContact()
    extra_text = ""-%d"" % (self.iteration)
    new_contact.insert_given_name(contact.givenName + extra_text)

When I run this script, I get this type of error:

TypeError: can only concatenate list (not ""str"") to list

So, does anyone knows where am I getting it wrong?
",163
23512160,23512230,2,"givenName seems to be a list. You can append another list like this:     
new_contact.insert_given_name(contact.givenName + [extra_text])

",23
23512160,23512250,2,"contact.givenName is a list, and extra_text is a string. in python you can add string to string, or list to list. you can't add string to list.
if you'd like to add string to a list, use list.append method.
mylist.append(mystr)

",52
23512276,23512276,1,"I want to find out if there is a way to count the number of iterations that have occurred with the code below:
with open(filename1) as file1, open(filename2) as file2:
    for line1, line2 in zip(file1, file2):

",51
23512276,23512299,2,"You can do that using enumerate:
with open(filename1) as file1, open(filename2) as file2:
    for i, (line1, line2) in enumerate(zip(file1, file2)):

Here i will be the number of iterations that you have run. More correctly, i will be the index of line1 and line2 in the zipped list which for your purpose is essentially the same. Note however that on the first iteration, i will be 0 not 1. More generally, on the nth iteration, the value of i will be n-1
",109
23512339,23512339,1,"When using the drop_duplicates() method I reduce duplicates but also merge all NaNs into one entry. How can I drop duplicates while preserving rows with an empty entry (like np.nan, None or  '')?
import pandas as pd
df = pd.DataFrame({'col':['one','two',np.nan,np.nan,np.nan,'two','two']})

Out[]: 
   col
0  one
1  two
2  NaN
3  NaN
4  NaN
5  two
6  two


df.drop_duplicates(['col'])

Out[]: 
   col
0  one
1  two
2  NaN

",110
23512339,23512486,2,"Well, one workaround that is not really beautiful is to first save the NaN and put them back in:
temp = df.iloc[pd.isnull(df).any(1).nonzero()[0]]
asd = df.drop_duplicates('col')
pd.merge(temp, asd, how='outer')
Out[81]: 
   col
0  one
1  two
2  NaN
3  NaN
4  NaN

",72
23512339,35860429,2,"Try
df[(~df.duplicated()) | (df['col'].isnull())]

The result is :
col
0   one
1   two
2   NaN
3   NaN     
4   NaN

",35
23512452,23512759,2,"You're over-thinking this and confusing yourself. This appears to be a representation of a directed graph: it looks like edges is a list of Edges, and each Edge has a headNodeId and a tailNodeId. Each of those IDs refers to a Node in the nodes list.
So all that's happening when you say nodes[ edges[-1].headNodeId ] is ""give me the item from 'nodes' whose index is the head node ID of the last item in 'edges'"". Then, the code is simply calling addInEdge on that Node.
A much more verbose version of this would be:
last_edge = edges[-1]
head_node_id = last_edge.headNodeId
head_node = nodes[head_node_id]
last_edge.addInEdge(head_node)
tail_node_id = last_edge.tailNodeId
tail_node = nodes[tail_node_id]
last_edge.addOutEdge(tail_node)

",147
23512452,23512452,1,"I came across these two lines while reading a file. I don't quite understand what it means exactly. Here nodes, edges, headnodeId, tailnodeId are lists. I really appreciate your help
def addInEdge( self, edge ):
    self.inEdges.append( edge )     
    self.totalEdgeCapacity += edge.capacity

def addOutEdge( self, edge ):
    self.outEdges.append( edge )        
    self.totalEdgeCapacity += edge.capacity

def addEdge(self, edge):
    self.edges.append( edge )
    self.nodes[ edge.headNodeId ].addInEdge( edge )
    self.nodes[ edge.tailNodeId ].addOutEdge( edge ) 

nodes[ edges[-1].headNodeId ].addInEdge( edges[-1] )
nodes[ edges[-1].tailNodeId ].addOutEdge( edges[-1] )

",125
23512469,23512469,1,"Consider the following minimum working example:
from pysvg.text import *
from pysvg.builders import *

doc = svg()
doc.addElement(text(""hello\nWorld"", 150, 50))
doc.save('HelloWorld2.svg')

When the resulting svg is viewed graphically, the newline has been transformed into a single space, because the XML does not respect the newline. 
<?xml version=""1.0"" encoding=""ISO-8859-1"" standalone=""no""?><svg xmlns=""http://www.w3.org/2000/svg"" version=""1.1"" xmlns:xlink=""http://www.w3.org/1999/xlink""  >
<text font-size=""12"" y=""50"" x=""150""  >
hello
World</text>
</svg>

How can I get a newline to display in the SVG?
",138
23512469,23512836,2,"Section 10.1 of the SVG spec gives three options:

Each ‘text’ element causes a single string of text to be rendered. SVG
  performs no automatic line breaking or word wrapping. To achieve the
  effect of multiple lines of text, use one of the following methods:

The author or authoring package needs to pre-compute the line breaks and use multiple ‘text’ elements (one for each line of text).
The author or authoring package needs to pre-compute the line breaks and use a single ‘text’ element with one or more ‘tspan’ child
  elements with appropriate values for attributes ‘x’, ‘y’, ‘dx’ and
  ‘dy’ to set new start positions for those characters which start new
  lines. (This approach allows user text selection across multiple lines
  of text -- see Text selection and clipboard operations.)
Express the text to be rendered in another XML namespace such as XHTML embedded inline within a ‘foreignObject’ element. (Note:
  the exact semantics of this approach are not completely defined at
  this time.)


",201
23512479,23512479,1,"I'm trying to write an alembic migration to add a datetime column to a table. I want all existing rows to have a default time of right now and future rows to default to the time they were created.  I've tried server_default='now()', which sets all existing rows to right now, but new rows seem to get this same time.  How do I set the default so new rows get the current time?
",83
23512479,24624328,2,"The issue is that the default is set to the result of now(), not the execution of it, so the default will be the exact time it was set, rather than the current time at insert.  Use sa.func.current_timestamp() to set it to that function, rather than the result of that function.
def upgrade():
    op.add_column('my_table', sa.Column('my_column', sa.DateTime,
            server_default=sa.func.current_timestamp()))

",83
23512509,23533589,2,"that would work:
from ABC import ABC
class MMMM(QDialog, Ui_MMMM):

  def __init__(self, iface):
    ...

  def graph(self):
    c = ABC()
    x, y = c.defineABC()

otherwise, you can set x and y in ABC (by doing self.x = ...) and then access them by c.x
",66
23512509,23512509,1,"I am developing a plugin in qgis. I have one interface (MMMM.py) with several buttons and one of them opens a new interface (ABC.py) where I introduce values. My objective is to read these values in the main interface (the first one). So I have a script to each interface but when I import the variables, I have several errors. I have troubles to import these variables.
second script named ABC.py
class ABC(QDialog, Ui_ABC):

    def __init__(self, iface):
       ...     

    def defineABC(self):

        x = self.input_x.text()
        y = self.input_y.text()
        return x, y

first (main) script named MMMM.py
class MMMM(QDialog, Ui_MMMM):

    def __init__(self, iface):
       ...

    def graph(self):
       import ABC
       x = ABC.ABC()
       xc = x.defineABC()

I tried some ways to import the values x and y to main interface but I have always errors. I am working in qgis.
What I am doing wrong?
",194
23512525,23512525,1,"I am trying to pass class element to method. Element is formed dynamically inserting current time in it. Mine class looks something like this:
class MineContact(dict):
    def __init__(self, **kwargs):
        # set your default values
        import time
        curr_time = repr(time.time()).replace('.', '') 
        self['tel'] = [{
            'type': ['Mobile'],
            'value': '555%s' % curr_time[8:]}]
        ...

So, I create object of this class and now I want to insert it as method argument:
contact = MineContact()
extra_text = ""-%d"" % (self.iteration)
new_contact.insert_phone(contact.tel['value'])

When I run this script, I get this type of error:

TypeError: list indices must be integers, not str

So, does anyone knows where am I getting it wrong?
",174
23512525,23512551,2,"You have a list of one dictionary [{}] instead of {}. The following will work:
contact = MineContact()
extra_text = ""-%d"" % (self.iteration)
new_contact.insert_phone(contact.tel[0]['value'])

Alternatively, you could change your self['tel'] to a dictionary instead of a list of a dictionary. Here is what it would look like:
self['tel'] = {'type': ['Mobile'], 'value': '555%s' % curr_time[8:]}

Then, your original new_contact.insert_phone(contact.tel['value']) would work
",121
23513305,23513305,1,"I have made an egg from django (1.6) project (lets say pialon) with application (web_interface_app).
And I have Scrapy project, with virtualenv. I install django app via pip install /path/to/egg.tar.gz
All I want to do is - to use Django ORM in Scrapy pipeline. Basically, I do something like this:
pipelines.py:
from pialon.web_interface_app import models
models.Posts.objects.all()

But this thorws me an error:
django.core.exceptions.ImproperlyConfigured: Requested setting DEFAULT_INDEX_TABLESPACE, but settings are not configured. You must either define the environment variable DJANGO_SETTINGS_MODULE or call settings.configure() before accessing settings.

Any ideas how to fix this error?
",115
23513305,23513720,2,"If you want to use Django's ORM without using the full Django suite, then you need to configure it before you run.
You can create a django settings file and point the DJANGO_SETTINGS_MODULE environment variable to it, or you can call settings.configure.
The documentation explains this here. It's pretty straightforward:
from django.conf import settings

settings.configure(DEBUG=True, ONE_SETTING=1, ANOTHER_SETTING=2, SOME_SETTING=3)

Note that if you're only interested in default settings, you probably don't actually need to pass any settings in there.
",96
23516515,23516515,1,"This question regards a homework assignment I'm doing for my programming class. I'm a total beginner when it comes to programming and up until now I've been fine but, in this instance, I'm lost. So, while answering this question, please assume that I'm clueless/stupid as hell because that's how  I feel right now lol.
So I'm making a game of ""In-Between"" and I'm having an issue.
Example of what happens:

Would you like to play in-between [y|n]? y
Die 1: 1   Die 2: 1
Number of chips: 100
  Place your bet: 50
Even-steven!
Even-steven! Higher or lower [h|l]? h
Die 3: 10
* You win! *
Die 3: 10
* Sorry - You lose! *
You now have 100 chips!
  Would you like to play in-between [y|n]? 

Here's my code:
if die1 == die2:
    print('\nEven-steven!')
    guess = input('\nEven-steven! Higher or lower [h|l]? ')

    print('\nDie 3:', die3)

    if guess == 'h':
        if die3 > die1:
            print('\n*** You win! ***')
            chipBalance = chipBalance + bet
        elif die3 < die1:
            print('\n*** Sorry - You lose! ***')
            chipBalance = chipBalance - bet
        elif die3 == die1:
            print('\n*** You hit the post - You lose! ***')
            chipBalance = chipBalance - bet

    elif guess == 'l':
        if die3 > die1:
            print('\n*** Sorry - You lose! ***')
            chipBalance = chipBalance - bet
        elif die3 < die1:
            print('\n*** You win! ***')
            chipBalance = chipBalance + bet
        elif die3 == die1:
             print('\n*** You hit the post - You lose! ***')
             chipBalance = chipBalance - bet

I'm sorry that it's really long but I'm just so clueless r/n and would really appreciate ANY help you can give. Allow me to reiterate that I'm very new to this and I don't understand much programming terminology at all so please give me the simplest answers you can. Thanks so much!!!
",408
23516515,23516788,2,"A common pattern with these kind of restarts is to assume that the player wants to play the first time and then ask if he/she wants to continue before ending.
play = 'y'
while play == 'y'

    Do your game logic here

    play = input('Would you like to play in-between [y|n]? ')

Hopefully that should show you how to get back on track.
Edit: sorry I saw now that this functionality was already in place. The issue you are having is that you get stuck in loops when you look at what value guess is, instead of looping you should use a if statement when checking what the guess is.
",122
23516520,23516633,2,"You need to find all script tags that apply; you only looked for the first. Use soup.find_all():
for script in soup.find_all('script', src=False):
    script.decompose()

This finds all <script> tags that do not have a src attribute.
",51
23516520,23516520,1,"The following is the a simple BeautifulSoup code which has the two internal JavaScript( dont blame about the JavaScript it is just for testing purpose).
from bs4 import BeautifulSoup
html = """"""
<html><head><title>The Dormouse's story</title>
<script>

var x = 5;
var y = 6;
document.getElementById(""demo"").innerHTML = x + y;
//document.getElementById(""demo"").innerHTML = x;
//document.getElementById(""demo"").innerHTML = y;

</script>
<script>

var x = 5;
var y = 6;
document.getElementById(""demo"").innerHTML = x + y;
//document.getElementById(""demo"").innerHTML = x;
//document.getElementById(""demo"").innerHTML = y;

</script>

</head>
<body>
<p class=""title""><b>The Dormouse's story</b></p>

<p class=""story"">Once upon a time there were three little sisters; and their names were
<a href=""http://example.com/elsie"" class=""sister"" id=""link1"">Elsie</a>,
<a href=""http://example.com/lacie"" class=""sister"" id=""link2"">Lacie</a> and
<a href=""http://example.com/tillie"" class=""sister"" id=""link3"">Tillie</a>;
and they lived at the bottom of a well.</p>

<p class=""story"">...</p>
""""""
soup = BeautifulSoup(html)
soup.script.decompose()
print soup.prettify()

when I ran this code it remove only one <script>...</script> from the document(Dom tree) but it don't remove all other script tags. How can we remove all the <script>, <style>(internal and inline) tags which are existing in the document 
",359
23516520,23544567,2,"for element in soup.findAll('script'):
            element.extract()

another alternative, you can replace 'script' with ['script', 'style'] to get rid of styles as well.
",36
23516527,23516595,2,"__EVENTVALIDATION is probably not static, you need to load the login page in python, get the __EVENTVALIDATION field and then do the login.
Something like this should work:
import requests
from bs4 import BeautifulSoup

s = requests.session()

def get_eventvalidation():
    r = s.get(""http://url.to.login.page"")
    bs = BeautifulSoup(r.text)

    return bs.find(""input"", {""name"":""__EVENTVALIDATION""}).attrs['value']

authentication_url = '<URL I am trying to log into>'

payload = {
  '__EVENTVALIDATION': get_eventvalidation(),
  'txtUsername': '<USERNAME>',
  'txtPassword': '<PASSWORD>',
  }

login = s.post(authentication_url, data=payload)

print login.text

You need the requests module and beautifulsoup4. Or you can just rewrite it to not use libraries.
Edit:
You probably need __VIEWSTATE as a POST value.
",168
23516527,23516527,1,"I have searched all over the Internet, looking at many examples and have tried every one I've found, yet none of them are working for me, so please don't think this is a duplicate - I need help with my specific case.
I'm trying to log into a website using Python (in this instance I'm trying with v2.7 but am not opposed to using a more recent version, it's just I've been able to find the most info on 2.7).
I need to fill out a short form, consisting simply of a username and password.
The form of the webpage I need to fill out and log in to is as follows (it's messy, I know):
<form method=""post"" action=""login.aspx?ReturnUrl=..%2fwebclient%2fstorepages%2fviewshifts.aspx"" id=""Form1"">
<div class=""aspNetHidden"">
<input type=""hidden"" name=""__VIEWSTATE"" id=""__VIEWSTATE"" value=""/wEPDwUKMTU4MTgwOTM1NWRkBffWXYjjifsi875vSMg9OVkhxOQYYstGTNcN9/PFb+M="" />
</div>

<div class=""aspNetHidden"">

    <input type=""hidden"" name=""__EVENTVALIDATION"" id=""__EVENTVALIDATION"" value=""/wEdAAVrmuRkG3j6RStt7rezNSLKVK7BrRAtEiqu9nGFEI+jB3Y2+Mc6SrnAqio3oCKbxYY85pbWlDO2hADfoPXD/5td+Ot37oCEEXP3EjBFcbJhKJGott7i4PNQkjYd3HFozLgRvbhbY2j+lPBkCGQJXOEe"" />
</div>
            <div><span></span>
                <table style=""BORDER-COLLAPSE: collapse"" borderColor=""#000000"" cellSpacing=""0"" cellPadding=""0""
                    width=""600"" align=""center"" border=""1"">
                    <tr>
                        <td>
                            <table cellSpacing=""0"" cellPadding=""0"" width=""100%"" align=""center"" border=""0"">
                                <tr>
                                    <td width=""76%""><span id=""centercontentTitle""></span>
                                        <H1 align=""center""><br>
                                            <span>
                                                <IMG height=""52"" src=""../images/logo-GMR.jpg"" width=""260""></span><span><br>
                                            </span></H1>
                                        <div id=""centercontentbody"">
                                            <div align=""center"">
                                                <TABLE width=""350"">
                                                    <TR>
                                                        <TD class=""style7"">Username:</TD>
                                                        <TD>
                                                            <div align=""right""><input name=""txtUsername"" type=""text"" id=""txtUsername"" style=""width:250px;"" /></div>
                                                        </TD>
                                                    </TR>
                                                    <TR>
                                                        <TD class=""style7"">Password:</TD>
                                                        <TD>
                                                            <div align=""right""><input name=""txtPassword"" type=""password"" id=""txtPassword"" style=""width:250px;"" /></div>
                                                        </TD>
                                                    </TR>
                                                    <TR>
                                                        <TD></TD>
                                                        <TD align=""right""><input type=""submit"" name=""btnSubmit"" value=""Submit"" id=""btnSubmit"" /><input type=""submit"" name=""btnCancel"" value=""Cancel"" id=""btnCancel"" /></TD>
                                                    </TR>
                                                    <TR>
                                                        <TD colspan=""2"" align=""center""></TD>
                                                    </TR>
                                                </TABLE>
                                            </div>
                                        </div>
                                    </td>
                                    <td>
                                        <div align=""center"" style='height:250px'></div>
                                    </td>
                                </tr>
                            </table>
                        </td>
                    </tr>
                </table>
                <br>
                <br>
                <p>&nbsp;</p>
        </form>

From searching around online, the best Python code I have found to fill out this form and log into the website is as follows:
Note: This is not my code, I got it from this question/example, where many people have said they've found it to work well.
import cookielib
import urllib
import urllib2


# Store the cookies and create an opener that will hold them
cj = cookielib.CookieJar()
opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))

# Add our headers
opener.addheaders = [('User-agent', 'LoginTesting')]

# Install our opener (note that this changes the global opener to the one
# we just made, but you can also just call opener.open() if you want)
urllib2.install_opener(opener)

# The action/ target from the form
authentication_url = '<URL I am trying to log into>'

# Input parameters we are going to send
payload = {
  '__EVENTVALIDATION': '/wEdAAVrmuRkG3j6RStt7rezNSLKVK7BrRAtEiqu9nGFEI+jB3Y2+Mc6SrnAqio3oCKbxYY85pbWlDO2hADfoPXD/5td+Ot37oCEEXP3EjBFcbJhKJGott7i4PNQkjYd3HFozLgRvbhbY2j+lPBkCGQJXOEe""',
  'txtUsername': '<USERNAME>',
  'txtPassword': '<PASSWORD>',
  }

# Use urllib to encode the payload
data = urllib.urlencode(payload)

# Build our Request object (supplying 'data' makes it a POST)
req = urllib2.Request(authentication_url, data)

# Make the request and read the response
resp = urllib2.urlopen(req)
contents = resp.read()

Unfortunately, this is not working for me and I'm unable to figure out why. If someone could please please please look over the code and tell me how I could improve it so as it works as it should. It would be so greatly appreciated!
Thanks in advance for all help I receive :)
",942
23516574,23559559,2,"Note: I'm making assumptions here since I'm unfamiliar with .H5 files and the Python code the accesses them.
I think that what is happening is that when you ""load"" the array, you're not actually loading an array. Instead, I think that an object is constructed on top of the file. It probably reads in dimensions and information related to how the file is organized, but it doesn't read the whole file.
That object mimicks an array so good that when you later on perform the slice operation, the normal Python slice operation can be executed, but at this point the actual data is being read. That's why the slice takes so long time compared to ""loading"" all the data.
I arrive at this conclusion because of the following.
If you're reading 75 frames of 3584x3584 pixels, I'm assuming they're uncompressed (H5 seems to be just raw dumps of data), and in that case, 75 * 3.584 * 3.584 = 963.379.200, this is around 918MB of data. Couple that with you ""reading"" this in 180ms, we get this calculation:
918MB / 180ms = 5.1GB/second reading speed

Note, this number is for 1-byte pixels, which is also unlikely.
This speed thus seems highly unlikely, as even the best SSDs today reach way below 1GB/sec.
It seems much more plausible that an object is just constructed on top of the file and the slice operation incurs the cost of reading at least 1 frame worth of data.
If we divide the speed by 75 to get per-frame speed, we get 68MB/sec speed for 1-byte pixels, and with 24 or 32-bit pixels we get up to 270MB/sec reading speeds. Much more plausible.
",324
23516574,23516574,1,"I am working with .h5 files with little experience.
In a script I wrote I load in data from an .h5 file. The shape of the resulting array is: [3584, 3584, 75]. Here the values 3584 denotes the number of pixels, and 75 denotes the number of time frames. Loading the data and printing the shape takes 180 ms. I obtain this time using os.times().
If I now want to look at the data at a specific time frame I use the following piece of code:
data_1 = data[:, :, 1]

The slicing takes up a lot of time (1.76 s). I understand that my 2D array is huge but at some point I would like to loop over time which will take very long as I'm performing this slice within the for loop.
Is there a more effective/less time consuming way of slicing the time frames or handling this type of data?
Thank you!
",182
23516649,23516666,2,"You should do it as:
def printv(*prargs):
    if vflag:
        print ' '.join(prargs)

>>> printv(""hello"", ""there"", ""world"")
hello there world

The string.join(iterable) returns a string of all the elements in the list separated by the specified string, in this case ' ' (a whitespace).
",72
23516649,23516649,1,"My apologies if this has been answered - I suspect it's very simple - but I can't see how to do it.
It's easier to demonstrate what I want to do.
vflag=True

def printv(*prargs):
    if vflag:
        print prargs
#       print *prargs gives a syntax error, unsurprisingly


printv(""hello"", ""there"", ""world"")
printv(""hello"", ""again"")

I want the output to be 
hello there world
hello again

and I get (of course)
('hello', 'there', 'world')
('hello', 'again')

",116
23516664,23516748,2,"import xml.etree.ElementTree as ET
e = ET.parse('test.txt')
root = e.getroot()
print(ET.tostring(root.find('test')))

yields
<test>The tag &lt;StackOverflow&gt; is good to bring up at parties.</test>

Alternatively, you could escape the text with saxutils.escape:
import xml.sax.saxutils as saxutils
print(saxutils.escape(root.find('test').text))

yields
The tag &lt;StackOverflow&gt; is good to bring up at parties.

",94
23516664,23516664,1,"I'm using python version 2.7.3.
test.txt:
<?xml version=""1.0"" encoding=""UTF-8""?>
<root>
    <test>The tag &lt;StackOverflow&gt; is good to bring up at parties.</test>
</root>

Result:
>>> import xml.etree.ElementTree as ET
>>> e = ET.parse('test.txt')
>>> root = e.getroot()
>>> print root.find('test').text
The tag <StackOverflow> is good to bring up at parties.

As you can see, the parser must have changed the &lt;'s to <'s etc.
What I'd like to see:
The tag &lt;StackOverflow&gt; is good to bring up at parties.
Untouched, raw text. Sometimes I really like it raw. Uncooked.
I'd like to use this text as-is for display within HTML, therefore I don't want an XML parser to mess with it.
Do I have to re-escape each string or can there be another way?
",197
23516680,23517776,2,"You can generate the truth table using a powerset,
def power_set(items):
    n = len(items)
    for i in xrange(2**n):
        combo = []
        for j in xrange(n):
            if (i >> j) % 2 == 1:
                combo.append(1)
            else:
                combo.append(0)
        yield combo    # if you want tuples, change to yield tuple(combo)


In [13]: list(power_set(l))
Out[13]: [[0, 0], [1, 0], [0, 1], [1, 1]]

In [14]: l=['B','C','E']

In [15]: list(power_set(l))
Out[15]: 
[[0, 0, 0],
[1, 0, 0],
 [0, 1, 0],
 [1, 1, 0],
 [0, 0, 1],
 [1, 0, 1],
 [0, 1, 1],
 [1, 1, 1]]

If you want to  make a dict of the data, change yield combo to yield tuple(combo)
Then you can store key value pairings  like:
d={}
for data in power_set(l):
    d[data]=""your_calc_prob""
print d
{(0, 1): 'your_calc_prob', (1, 0): 'your_calc_prob', (0, 0): 'your_calc_prob', (1, 1): 'your_calc_prob'}

If you want the output sorted you can use sorted() which makes a copy of the list and returns a list:
 sorted(list(power_set(l)))
 Out[21]: 
 [[0, 0, 0],
 [0, 0, 1],
 [0, 1, 0],
 [0, 1, 1],
 [1, 0, 0],
 [1, 0, 1],
 [1, 1, 0],
 [1, 1, 1]]

Or you can use the list method sort() which sorts the list  in place:
In [22]: data = list(power_set(l))  
In [23]: data.sort()
In [24]: data
Out[24]: 
[[0, 0, 0],
[0, 0, 1],
[0, 1, 0],
[0, 1, 1],
[1, 0, 0],
[1, 0, 1],
[1, 1, 0],
[1, 1, 1]]

",525
23516680,23517153,2,"You can use itertools.product() to generate the truth table and then depending on the logical operation, determine the probability. I don't know which logical operation you would like to use so let's just create a dictionary each row:
>>> l = ['B', 'C']
>>> truth_table = [dict(zip(l, x)) for x in product((0, 1), repeat=2)]
>>> print(truth_table)
[{'B': 0, 'C': 0}, {'B': 0, 'C': 1}, {'B': 1, 'C': 0}, {'B': 1, 'C': 1}]

For calculating the probability, you'll probably need a separate function to do that. For example a logical disjunction for two keys with 0 and 1 being the values is basically equivalent to max().
>>> l.append('Prob')
>>> truth_table = [dict(zip(l, x + (max(x), )) for x in product((0, 1), repeat=2)]
>>> print(truth_table)
[{'B': 0, 'C': 0, 'Prob': 0},
 {'B': 0, 'C': 1, 'Prob': 1},
 {'B': 1, 'C': 0, 'Prob': 1},
 {'B': 1, 'C': 1, 'Prob': 1}]

",301
23516680,23516680,1,"I have this list and number:
list = ['B','C']

The outcome that I need for my table is:
B    C    Prob
0    0    x
0    1    x
1    0    x
1    1    x

How can I build this truth table (there can be more vairables, not only 3) and assign a number to that row's probability?
I need to build it with a dictionary, I tried with some list comprehension but I don't know how to generate dynamically the truth table, considering that there can be more/less than 3 variables.
EDIT: to be more clear my goal is to have a dictionary like this:
dict = {""B"":0/1,""C"":0/1,""Prob"":arbitraryNumber}

and I need to insert all these dictionaries into a list to represent the structure of a table, is it clearer now?
Thank you very much
",170
23516703,23516703,1,"I doing a simple python GUI using tkinter to do screen recording.Basically, I am using ffmpeg commands at the backend with tkinter as the front end triggering the ffmpeg commands.There is something that I stuck with.I dont know why my time is unable to trigger off if I program in this way.
The code below is basically the recording method.You will notice that I am actually trying to update my tkinter GUI in the while loop.This method is actually in my class named Gui_Rec() which contains other methods I need for my screen recording program.
def rec(self):
    global videoFile
    mydate = datetime.datetime.now()
    videoFile = mydate.strftime(""\%d%b_%Hh%Mm.avi"")

    self.l['text']=os.path.expanduser('~')+""\Videos""
    self.l1['text']=videoFile
    self.b.config(state=DISABLED)
    self.b1.config(state=ACTIVE)

    t = Thread(target=self.rec_thread)#trigger another method using thread which will run ffmpeg commands here
    t.start()


    while True:
        if self.count_flag == False:
            break

        self.label['text'] = str(""%02dm:%02ds"" % (self.mins,self.secs))

        if self.secs == 0:
            time.sleep(0)
        else:
            time.sleep(1)

        if(self.mins==0 and self.secs==1):
            self.b1.config(fg=""white"")
            self.b1.config(bg=""red"")
            self.b.config(fg=""white"")
            self.b.config(bg=""white"")

        if self.secs==60:
            self.secs=0
            self.mins+=1
            self.label['text'] = str(""%02dm:%02ds"" % (self.mins,self.secs))

        main.gui.update()               
        self.secs = self.secs+1

other method in the class Gui_Rec() then this below
def main():
   gui = Gui_Rec()
   gui.minsize(300,155)
   gui.maxsize(390,195)
   gui.title(""Desktop REC"")
   gui.attributes(""-topmost"", 1)
   gui.mainloop() #start mainloop of program


if __name__ == '__main__':
       main()

Strangely, if I don't put the above section of code in the the def main(), the GUI will be update with the duration of the time running when rec button is pressed.I don't really know how to go about solving this.Tried putting it in another thread yet it doesn't  work as well.Thank you everyone for your help.
",418
23516703,23517360,2,"The while loop is creating a conflict with Tkinter's mainloop. Threading or multiprocessing are solutions, but I'd recommend looking into Tkinter's after() method. Here's a simplified example of how to handle a timer using after:
from Tkinter import *

class App(Frame):
    def __init__(self, parent):
        Frame.__init__(self, parent)

        self.mins = 0
        self.secs = 0

        # make a stringvar instance to hold the time
        self.timer = StringVar()
        self.timer.set('%d:%d' % (self.mins, self.secs))

        Label(self, textvariable=self.timer).pack()
        Button(self, text='Start', command=self._start_timer).pack()
        Button(self, text='Stop', command=self._stop_timer).pack()

    def _start_timer(self):
        self.secs += 1      # increment seconds
        if self.secs == 60: # at every minute,
            self.secs = 0   # reset seconds
            self.mins += 1  # and increment minutes

        self.timer.set('%d:%d' % (self.mins, self.secs))

        # set up the after method to repeat this method
        # every 1000 ms (1 second)
        self.repeater = self.after(1000, self._start_timer)

    def _stop_timer(self):
        self.after_cancel(self.repeater)

root = Tk()
App(root).pack()
mainloop()

",240
23516828,23516828,1,"I am having some trouble attempting to manipulate a CSV file and appending the results to a new column.
Essentially I have a csv file (delimited ;) with 5 columns currently (of Cartesian coords [X, Y] and components [dX, dY], and magnitude/ length). I wish to add the result of some equations, which differ depending on the value of my Cartesian components, to a 6th column in this csv file (the angle).
Thus far my code is this (the maths is correct [hopefully], it's just the appending that I'm having trouble with):
import csv, math
with open(""mydata.csv"", ""rb"") as f:
vectors = csv.reader(f, delimiter="";"")

    for col in vectors:
        x = float(col[0])
        y = float(col[1])
        dX = float(col[2])
        dY = float(col[3])
        magnitude = float(col[4])

        if dX > 0 and dY > 0:
            comp = dY/dX
            theta = math.degrees(math.atan(comp))
            angle = 90 - theta
        elif dX > 0 and dY < 0:
            comp = dY/dX
            theta = math.degrees(math.atan(comp))
            angle = 90 + theta
        elif dX < 0 and dY > 0:
            comp = dX/dY
            theta = math.degrees(math.atan(comp))
            angle = 360 - theta
        elif dX < 0 and dY < 0:
            comp = dY/dX
            theta = math.degrees(math.atan(comp))
            angle = 270 - theta

So essentially, I want to add the angle variable to a 6th column, for the correct line of my csv file.
I tried to create a new list and append (e.g.):
angles = []
...
angles.append(col)
angles.append(angle)

However, as you may have guessed I ended up with a line like this:
[[x, y, dX, dY, magnitude], angle]

Thanks for your help in advance.
",390
23516828,23516876,2,"col is a list itself, so you'd extend angles:
angles.extend(col)
angles.append(angle)

where list.extend() copies over the elements into the angles list, rather than add a single reference to the col list object.
If all you do is produce a new row with one value added, just re-use col and append to it directly:
col.append(angle)

and write that to your output CSV file.
col is misnamed, really, I'd call it row instead.
",94
23516828,23516935,2,"since col is list . u can just copy list items to angles and append it
angles=col[:]
angles.append(angle)

",24
23516828,23519668,2,"This answer is too late as a solution has already been accepted, but the simplest way to solve the  problem is to write the newly-constructed row directly to the output csv file without creating an intermediate list.
You could write something like:
import csv, math

with open(""mydata.csv"", ""rb"") as f,\
     open(""newdata.csv"", ""wb"") as g:
        vectors = csv.reader(f, delimiter="";"")
        writer = csv.writer(g, delimiter="";"")
        for row in vectors:
            # use destructuring
            x, y, dX, dY, magnitude = map(float, row)

            if dX > 0 and dY > 0:
            #<snip>

            # at this stage you can write directly to the output
            # file.
            writer.writerow([x, y, dX, dY, magnitude, angle])

",166
23516914,23516914,1,"I have a vector D of length N and a matrix A of shape N*M. Vector D has some zero elements. I'm doing this operation:
D = D.reshape(-1,1)
A / D

However I'm getting a division by zero error because of some elements in D that are zero. What I need is to put zero when there's a division by zero instead of raising an error. How to do this?
E.g. my try:
A = [ [0,1,0,0,0,0], 
          [0,0,1,1,0,0],
          [1,0,0,1,1,0],
          [0,0,0,0,1,0],
          [0,0,0,0,0,0],
          [0,0,0,0,1,0] 
          ]
A = np.array(A, dtype='float')

D = np.sum(A, axis=1)
D = D.reshape(-1, 1)

A = np.where(D != 0, A / D, 0)

RuntimeWarning: invalid value encountered in divide
  A = np.where(D != 0, A / D, 0)

",175
23516914,23518410,2,"Why not use try-catch block? Something like
try:
    some_var = A/D
except ZeroDivisionError:
    some_var = 0
",19
23516914,23519204,2,"You could use a masked array for D, like:
D = np.ma.array(D, mask=(D==0))

and when you perform the calculations with the masked array only the non-masked values will be considered.
",40
23518088,23518088,1,"def data_from_file(filename):

    list1 = []

    infile = open(filename, 'r', encoding=""utf-8"")

    lines = infile.read().split()
    lines = "" "".join(lines)
    lines1 = lines.replace(""."" , """")
    lines2 = lines1.replace("","", """")
    lines3 = lines2.replace(""\n"", """")
    lines4 = lines3.replace(""\"""", """")
    lines5 = lines4.replace(""\\"", """")
    lines6 = lines5.replace(""\"""", """")
    lines7 = lines6.replace("":"", """")
    lines8 = lines7.replace("";"", """")

    lines9 = lines8.split()

    for i in lines9:
        if i.isalpha():
            list1.append(i)
    return list1

Hi code newbie here,
Basically what I need to do is read  data from a certain file. I then need to remove the characters such as ("" ; : . , \n ' )  but only if they are at the start or end of the word. Currently my program removes every instance of these characters. For example I want to be able to turn ""cars"" into cars but ca""rs would stay as ca""rs
The next part of the program involves only selecting the words which have an alphanumeric character in every position.This part works perfectly.
Any help with this would be greatly appreciated.
",268
23518088,23518157,2,"str.strip does what you want:
>>> 'potato'.strip('o')
'potat'

There are also str.lstrip and str.rstrip if you only want to take off the left or right, respectively.  
",35
23521273,23521273,1,"I have read and searched all stack overflow .. I also found JPype class not found but it didn't help me although it is solved! I have the same problem ! I am using Mac , python 2.7.6
My both python code and A.java are on desktop. But I keep receiving this error :

Traceback (most recent call last):   File
  ""/Users/jeren/Desktop/aa.py"", line 13, in 
      A = jpype.JClass(""A"")   File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/jpype/_jclass.py"",
  line 54, in JClass
      raise _RUNTIMEEXCEPTION.PYEXC(""Class %s not found"" % name) java.lang.ExceptionPyRaisable: java.lang.Exception: Class A not found

aa.py :
    import jpype
import os

jpype.startJVM(jpype.getDefaultJVMPath(), ""-ea"", ""-Djava.class.path=/Users/jeren/Desktop/"")

A = jpype.JClass(""A"")

a = A()

jpype.shutdownJVM()

A.java :
class A

{

    public A()

    {

        super();

    }

    public String sayHi()

    {

        return(""Hello"");

    }



    public static void main(String[] argv)

    {

        System.out.println (""Hello "");

    }



    public static int add(int a, int b)

    {

        return(a+b);

    }

}

My mac , java and python are all 64bit ! where the problem can be? 
",232
23521273,23608516,2,"everything was ok just needed to add a 'public' to the beginning of class A:
    public class A
    {       
        public A()       
        {
            super();
        }
        public String sayHi()

        {
            return(""Hello"");
        }

",45
23521345,23521467,2,"Specify the branch, commit hash, or tag name after an @ at the end of the url:
pip install git+https://github.com/django/django.git@1.7b3

This will install the version tagged with 1.7b3.
Reference: https://pip.pypa.io/en/latest/reference/pip_install.html#git
",43
23521345,23521345,1,"I want to install Django 1.7 via pip. It is currently a development version, so not in pips repositories. 
So I have installed packages from github before using:
pip install git+[url here]

Now looking at github, I get the clone url on the django page:
https://github.com/django/django.git

But this mentions nothing of the branch. How do I specify that I want version 1.7? 
Is it somewhere obvious on the github page? 
",84
23521361,35158283,2,"The simpliest way for python2 is to use the repr():
>>> key_unicode = u'uuuu\xf6\x9f_\xa1\x05\xeb9\xd4\xa3\xd1'
>>> key_ascii = 'uuuu\xf6\x9f_\xa1\x05\xeb9\xd4\xa3\xd1'
>>> print(key_ascii)
uuuu��_��9ԣ�
>>> print(key_unicode)
uuuuö_¡ë9Ô£Ñ
>>>
>>> # here is the save method for both string types:
>>> print(repr(key_ascii).lstrip('u')[1:-1])
uuuu\xf6\x9f_\xa1\x05\xeb9\xd4\xa3\xd1
>>> print(repr(key_unicode).lstrip('u')[1:-1])
uuuu\xf6\x9f_\xa1\x05\xeb9\xd4\xa3\xd1
>>> # ____________WARNING!______________
>>> # if you will use jsut `str.strip('u\'\""')`, you will lose
>>> # the ""uuuu"" (and quotes, if such are present) on sides of the string:
>>> print(repr(key_unicode).strip('u\'\""'))
\xf6\x9f_\xa1\x05\xeb9\xd4\xa3\xd1

For python3 use str.encode() to get the bytes type.
>>> key = 'l\xf6\x9f_\xa1\x05\xeb9\xd4\xa3\xd1q\xf5L\xa9\xdd0\x90\x8b\xf5ht\x86za\x0e\x1b\xed\xb6(\xaa+'
>>> key
'lö\x9f_¡\x05ë9Ô£ÑqõL©Ý0\x90\x8bõht\x86za\x0e\x1bí¶(ª+'
>>> print(key)
lö_¡ë9Ô£ÑqõL©Ý0õhtzaí¶(ª+
>>> print(repr(key.encode()).lstrip('b')[1:-1])
l\xc3\xb6\xc2\x9f_\xc2\xa1\x05\xc3\xab9\xc3\x94\xc2\xa3\xc3\x91

",227
23521361,23521361,1,"I know this may sounds like a duplicate question, but that's because I don't know how to describe this question properly. 
For some reason I got a bunch of unicode string like this:
a = u'\xcb\xea'

As you can see, it's actually bytes representation of a Chinese character, encoding in gbk
>>> print(b'\xcb\xea'.decode('gbk'))
岁

u'岁' is what I need, but I don't know how to convert u'\xcb\xea' to b'\xcb\xea'.
Any suggestions?
",95
23521361,23521424,2,"It's not really a bytes representation, it's still unicode codepoints. They are the wrong codepoints, because it was decoded from bytes as if it was encoded to Latin-1.
Encode to Latin 1 (whose codepoints map one-on-one to bytes), then decode as GBK:
a.encode('latin1').decode('gbk')

Demo:
>>> a = u'\xcb\xea'
>>> a.encode('latin1').decode('gbk')
u'\u5c81'
>>> print a.encode('latin1').decode('gbk')
岁

",99
23521463,23737310,2,"It looks like you are using PyMC2, and as far as I know, you must use some Python approach to parallel computation, like IPython.parallel.  There are many ways to do this, but all the ones I know are a little bit complicated.  Here is an example of one, which uses PyMC2, IPCluster, and Wakari.
In PyMC3, parallel sampling is implemented in the psample method, but your reference code will need to be updated to the PyMC3 format:
with pm.Model() as model:
    beta1 = pm.Laplace('beta1', mu=0, b=b)
    beta2 = pm.Laplace('beta2', mu=0, b=b)
    beta3 = pm.Laplace('beta3', mu=0, b=b)

    y_hat = beta1 * x1 + beta2 * x2 + beta3 * x3
    y_obs = pm.Normal('y_obs', mu=y_hat, tau=1.0, observed=y)

    trace = pm.psample(draws=20000, step=pm.Slice(), threads=3)

",168
23521463,26410417,2,"PYMC3 has merged the psample into sample.
To run in parallel set the parameter njobs > 1.
The usage for the pymc.sample function is:
sample(draws, step, start=None, trace=None, chain=0, njobs=1, tune=None,
           progressbar=True, model=None, random_seed=None)

Note if you set njobs=None, it will default to Number of CPUs - 2.
I hope this helps.
",70
23521463,23521463,1,"Could someone give some general instructions on how one can parallelize the PyMC MCMC code.  I am trying to run LASSO regression following the example given here. I read somewhere that parallel sampling is done by default, but do I still need to use something like Parallel Python to get it to work?
Here is some reference code that I would like to be able to parallelize on my machine.
x1 = norm.rvs(0, 1, size=n)
x2 = -x1 + norm.rvs(0, 10**-3, size=n)
x3 = norm.rvs(0, 1, size=n)

X = np.column_stack([x1, x2, x3])
y = 10 * x1 + 10 * x2 + 0.1 * x3

beta1_lasso = pymc.Laplace('beta1', mu=0, tau=1.0 / b)
beta2_lasso = pymc.Laplace('beta2', mu=0, tau=1.0 / b)
beta3_lasso = pymc.Laplace('beta3', mu=0, tau=1.0 / b)

@pymc.deterministic
def y_hat_lasso(beta1=beta1_lasso, beta2=beta2_lasso, beta3=beta3_lasso, x1=x1, x2=x2, x3=x3):
    return beta1 * x1 + beta2 * x2 + beta3 * x3

Y_lasso = pymc.Normal('Y', mu=y_hat_lasso, tau=1.0, value=y, observed=True)

lasso_model = pymc.Model([Y_lasso, beta1_lasso, beta2_lasso, beta3_lasso])
lasso_MCMC = pymc.MCMC(lasso_model)
lasso_MCMC.sample(20000,5000,2)

",240
23521511,23531250,2,"I guess anther way, possibly faster, to achieve this is 
1) Use dict comprehension to get desired dict (i.e., taking 2nd col of each array)
2) Then use pd.DataFrame to create an instance directly from the dict without loop over each col and concat.
Assuming your mat looks like this (you can ignore this since your mat is loaded from file):
In [135]: mat = {'a': np.random.randint(5, size=(4,2)),
   .....: 'b': np.random.randint(5, size=(4,2))}

In [136]: mat
Out[136]: 
{'a': array([[2, 0],
        [3, 4],
        [0, 1],
        [4, 2]]), 'b': array([[1, 0],
        [1, 1],
        [1, 0],
        [2, 1]])}

Then you can do:
In [137]: df = pd.DataFrame ({name:mat[name][:,1] for name in mat})

In [138]: df
Out[138]: 
   a  b
0  0  0
1  4  1
2  1  0
3  2  1

[4 rows x 2 columns]

",248
23521511,31097813,2,"Here is how to create a DataFrame where each series is a row.
For a single Series (resulting in a single-row DataFrame):
series = pd.Series([1,2], index=['a','b'])
df = pd.DataFrame([series])

For multiple series with identical indices:
cols = ['a','b']
list_of_series = [pd.Series([1,2],index=cols), pd.Series([3,4],index=cols)]
df = pd.DataFrame(list_of_series, columns=cols)

For multiple series with possibly different indices:
list_of_series = [pd.Series([1,2],index=['a','b']), pd.Series([3,4],index=['a','c'])]
df = pd.concat(list_of_series, axis=1).transpose()

To create a DataFrame where each series is a column, see the answers by others. Alternatively, one can create a  DataFrame where each series is a row, as above, and then use df.transpose(). However, the latter approach is inefficient if the columns have different data types.
",206
23521511,23521511,1,"My current code is shown below - I'm importing a MAT file and trying to create a DataFrame from variables within it:
mat = loadmat(file_path)  # load mat-file
Variables = mat.keys()    # identify variable names

df = pd.DataFrame         # Initialise DataFrame

for name in Variables:

    B = mat[name]
    s = pd.Series (B[:,1])

So within the loop I can create a series of each variable (they're arrays with two columns - so the values I need are in column 2)
My question is how do I append the series to the dataframe? I've looked through the documentation and none of the examples seem to fit what I'm trying to do.
Best Regards,
Ben
",137
23521511,23522030,2,"No need to initialize an empty DataFrame (you weren't even doing that, you'd need pd.DataFrame() with the parens). Instead make a list of Series and concat those together with df = pd.concat(series, axis=1)
Something like:
series = [pd.Series(mat[name][:, 1]) for name in Variables]
df = pd.concat(series, axis=1)

",76
23521600,23521600,1,"I have two classes that loosely take the form below:
class Foo:

    def __init__(self, foo):
        self.__foo = foo


class Bar(Foo):

    def bar(self):
        print self.__foo

When I try to invoke the bar method on an instance of Bar, it fails.
b = Bar('foobar')    
b.bar()

Result:
Traceback (most recent call last):
  File ""foobar.py"", line 14, in <module>
    b.bar()
  File ""foobar.py"", line 10, in bar
    print self.__foo
AttributeError: Bar instance has no attribute '_Bar__foo'

My understanding is that this code should work based on two other questions, why doesn't it?
",130
23521600,23521763,2,"Simple. __foo contains 2 underscores in the beginning, so it's assumed to be class-private method and it's transformed into _Classname__method. 
When you request access to the attribute named as such on Bar object it asks Bar class if it has this method (not Foo class), so self.__foo is always the same as self._Bar__foo.
From the documentation:

When an identifier that textually occurs in a class definition begins
  with two or more underscore characters and does not end in two or more
  underscores, it is considered a private name of that class. Private
  names are transformed to a longer form before code is generated for
  them. The transformation inserts the class name, with leading
  underscores removed and a single underscore inserted, in front of the
  name. For example, the identifier __spam occurring in a class named
  Ham will be transformed to _Ham__spam.

If you modify your code slightly
class Foo:
    def __init__(self, foo):
        self.__foo = foo
        assert hasattr(self, '_Foo__foo'), 'Attribute has been just created'


class Bar(Foo):
    def bar(self):
        assert hasattr(self, '_Foo__foo'), 'No errors, thanks to inheritance'

assert statements will not cause any AssertionErrors.
Add __getattribute__ method to Bar class to capture all requests to Bar objects:
class Bar(Foo):

    def bar(self):
        print('Accessing __foo from bar')
        print(self.__foo)

    def __getattribute__(self, name):
        print('Requested', name)
        return super().__getattribute__(name)

b = Bar('foobar')
b.bar()

There will be 3 lines (apart from AttributeError) in the output:
Requested bar
Accessing __foo from bar
Requested _Bar__foo # AttributeError follows

As you can see, if attribute you are requesting has 2 leading underscores, Python is renaming it on the fly.
",347
23521652,23522628,2,"Make sure, action point to proper url
I think you render the form with wrong action for submitting the form.
Your version is using action="""" and I guess, it shall be action=""/search""
So your template shall be changed like:
{% extends ""hello.html"" %}
{% block content %}
<div class=""search"">
<form action=""/search"" method=post>
    <input type=text name=search value=""{{ request.form.search}}""></br>
    <div class=""actions""><input type=submit value=""Search""></div>
</form>
</div>
{% for message in get_flashed_messages() %}
<div class=flash>
    {{ message }}
</div>
{% endfor %}
{% endblock %}

Do not redirect out of your result
Your existing code is processing POST, but within first loop it ends up returning with redirect
@app.route('/search', methods=['GET', 'POST'])
def search():
    if request.method == ""POST"":
        db = MySQLdb.connect(user=""root"", passwd="""", db=""cs324"", host=""127.0.0.1"")
        c=db.cursor()
        c.executemany('''select * from student where name = %s''', request.form['search'])
        for r in c.fetchall():
            print r[0],r[1],r[2]
            return redirect(url_for('search')) # <- Here you jump away from whatever result you create
    return render_template('search.html')

Do render your template for final report
Your code does not show in POST branch any attempt to render what you have found in the database.
Instead of print r[0], r[1]... you shall call render_template()
Something like this
@app.route('/search', methods=['GET', 'POST'])
def search():
    if request.method == ""POST"":
        db = MySQLdb.connect(user=""root"", passwd="""", db=""cs324"", host=""127.0.0.1"")
        c=db.cursor()
        c.executemany('''select * from student where name = %s''', request.form['search'])
        return render_template(""results.html"", records=c.fetchall())
    return render_template('search.html')

",438
23521652,23521652,1,"I want to make some kind of search engine for student's information by entering their first name in html input field, but I have some troubles with my code. I am using Flask with Python though. 
Here is my project.py code:
@app.route('/search', methods=['GET', 'POST'])
def search():
    if request.method == ""POST"":
        db = MySQLdb.connect(user=""root"", passwd="""", db=""cs324"", host=""127.0.0.1"")
        c=db.cursor()
        c.executemany('''select * from student where name = %s''', request.form['search'])
        for r in c.fetchall():
            print r[0],r[1],r[2]
            return redirect(url_for('search'))
    return render_template('search.html')

Here is my search.html code:
{% extends ""hello.html"" %}
{% block content %}
<div class=""search"">
<form action="""" method=post>
    <input type=text name=search value=""{{ request.form.search}}""></br>
    <div class=""actions""><input type=submit value=""Search""></div>
</form>
</div>
{% for message in get_flashed_messages() %}
<div class=flash>
    {{ message }}
</div>
{% endfor %}
{% endblock %}

When I hit Search button nothing happens, I checked database it has some data in it so it is not empty, I can't find where am I making a mistake, please help?
",301
23521847,23521890,2,"The error message in the question does not come from the subprocess. It was generated before the subprocess execution. You cannot capture that error using stderr option.
Make sure there's ding program in the path.
",41
23521847,23521847,1,"Based on the answer provided here, I wanted to save the error as a string:
p = subprocess.Popen(['ding', 'dong'], stderr=subprocess.PIPE, stdout=subprocess.PIPE)
output, errors = p.communicate()

However, it seems that redirecting stderr is not working:
>>> p = subprocess.Popen(['ding', 'dong'], stderr=subprocess.PIPE, stdout=subprocess.PIPE)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in ?
  File ""/usr/lib64/python2.4/subprocess.py"", line 550, in __init__
    errread, errwrite)
  File ""/usr/lib64/python2.4/subprocess.py"", line 993, in _execute_child
    raise child_exception
OSError: [Errno 2] No such file or directory

What is the correct way to capture the error as a string?
",140
23522570,23523042,2,"To answer this specific question:

How do I take object-based inputs and turn them into passable
  variables?

You access the controls and call the appropriate methods:
self.resize_txt.GetValue()
self.file_input.GetPath()
self.outfile_input.GetPath()

To pass them to your resize, function, just pass those values to your resize_file function.
resize_file(self.file_input.GetPath(),
            self.outfile_input.GetPath())

You're asking too many questions here TBH.  You should start with one, and create others as you progress.
What the heck, I have time to kill before lunch!  I'll give you a few pointers.
Let's look at this function:
def resize_file(filename, filename2 = None)

Looking at this function, I really have no idea what the inputs are to the function.  It says filename and an optional filename2.  Which of these is being resized?  How do I know what each parameter does?  You did not document this function, which made me dig off into your code to try to determine what it does.
So, I dug off into your code...  And it appears that this particular function resizes and possibly appends pdfs.  Notice how in your code, you execute the same code twice?
fIn1 = file(os.path.join(inputDir,filename), 'rb')

inp1 = PdfFileReader(fIn1)
p1 = inp1.getPage(0)
p1.scale(.5,.5)
output.addPage(p1)

if filename2 is not None:
    fIn2 = file(os.path.join(inputDir,filename2), 'rb')
    inp2 = PdfFileReader(fIn2)
    p2 = inp2.getPage(0)
    p2.scale(.50,.50)
    output.addPage(p2)

Don't do this.  Use the DRY principle.  You should have a loop, as the algorithm is basically the same.  (Can't illustrate the loop atm, running out of time before lunch, maybe when I get back :P)
You could even get fancy and have your function take an indefinite amount of PDF files.  Check this snippet out:
def resize_file(*args):
    output = PdfFileWriter()

    for filename in args:
        fIn1 = file(os.path.join(inputDir, filename), 'rb')
        inp1 = PdfFileReader(fIn1)
        p1 = inp1.getPage(0)
        p1.scale(.5,.5)
        output.addPage(p1)

    outputStream = file(os.path.join(validateDir,str(fileout) + '.pdf'),""wb"")
    output.write(outputStream)
    outputStream.close()

Ok, I lied, I tried to squeeze this in before lunch.  The code above may not work out of the box, but it should point you in the general direction.  You should add error catching to check for when no arguments are passed (among other things).
Hope it helps!
",509
23522570,23522754,2,"    #initGUIStuff
    ....
    self.resize_btn.Bind(wx.EVT_BUTTON,self.OnResize)

 def OnResize(self,evt):
    resize(self.resize_txt.GetValue(),self.file_input.GetPath(),self.outfile_input.GetPath())

maybe? 
not sure what you are asking to be honest ...
",44
23522570,23522570,1,"I have a working script that I am attempting to port over to a GUI.  I'm new to programming so a lot of this code may be hack-ish.  I'm open to suggestions on general practices and methods!  Below is my working text-based version of the function I'd like to port:
def InitUI(self):
    self.pdf = None
    sizer = wx.BoxSizer(wx.VERTICAL)
    btnSizer = wx.BoxSizer(wx.HORIZONTAL)
    self.pdf = PDFWindow(self, style=wx.SUNKEN_BORDER)
    panel = wx.Panel(self)

    sizer.Add(self.pdf, proportion=1, flag=wx.EXPAND)

    pdfPicker = wx.FilePickerCtrl(self, wx.ID_ANY,message='Please select the PDF to resize.', wildcard='*.pdf', size=(500,20))
    btnSizer.Add(pdfPicker, proportion=1, flag=wx.EXPAND|wx.ALL, border=5)
    btnSizer.AddStretchSpacer(3)

    self.label = wx.StaticText(self, label='Enter Scale (decimal percent):')
    self.field = wx.TextCtrl(self, value=""0.5"", size=(50,20))

    btnSizer.Add(self.label, 0, wx.ALL, 8)
    btnSizer.Add(self.field, 0, wx.ALL, 8)

    # .... more GUI code (buttons, etc.)

def resize_file_main(resize, fileout, self, e=None)
    file1 = ask_file_name('resize1', 'input', '', inputDir)
    fileout = ask_file_name('resize1', 'output')
    input1 = str(file1) + '.pdf'
    dir1 = os.path.join(inputDir, input1)
    backup1 = os.path.join(backupDir, str(file1) + '.pdf')

    resize_file(input1)

    try:
        shutil.move(dir1, backup1)
        print input1, ""has been successfully moved to the backup folder.\n""
    except:
        print_error('\nThe PDF you entered is opened elsewhere.  The file was not backed up.')
        print ""Please move your scanned PDF from /input to /backup or run the backup utility.\n\n      Press enter to continue  ....  ""
        print raw_input('')
    continue

def resize_file(filename, filename2 = None):
    output = PdfFileWriter()

    fIn1 = file(os.path.join(inputDir,filename), 'rb')

    inp1 = PdfFileReader(fIn1)
    p1 = inp1.getPage(0)
    p1.scale(.5,.5)
    output.addPage(p1)

    if filename2 is not None:
        fIn2 = file(os.path.join(inputDir,filename2), 'rb')
        inp2 = PdfFileReader(fIn2)
        p2 = inp2.getPage(0)
        p2.scale(.50,.50)
        output.addPage(p2)

    outputStream = file(os.path.join(validateDir,str(fileout) + '.pdf'),""wb"")
    output.write(outputStream)
    outputStream.close()
    fIn1.close()

Here's my question.  I'd like to get rid of all the text-based user interaction.  How do I take object-based inputs and turn them into passable variables?  I've been able to implement wx.TextCtrl to enter user input and wx.FilePickerCtrl to select the PDF for the input.  Now how do I:

Pass these as variables to my resize_file function? 
Set the output location?
Set the backup location + input PDF name?
Pass the wx.TextCtrl values (for scale) to my resize_file function?

This also may be my problem:

I have a InitUI function doing all the wxPython stuff
The button within InitUi function calls resize_file_main
resize_file_main just handles inputs/outputs and moving the final
files around.  It also calls resize_file.
resize_file is a function that is re-used in several other areas
of the script.  It takes in the various inputs/outputs and actually
resizes the PDF(s).

Is this a bad flow?  I wasn't sure how to combine resize_file_main and resize_file because the input/outputs with the different areas that call resize_file are different.
Thank you for your help!  I know it's convoluted!
Edit:  Thank you!  I believe I have enough information to move forward.  I appreciate the help.
",690
23522728,23522728,1,"I'm trying to run this turtle function:
from turtle import *

def main():


    color('red', 'yellow')
    begin_fill()
    while True:
        forward(200)
        left(170)
        if abs(pos()) < 1:
            break
    end_fill()
    done()

main()

But I keep getting this error: 
 Traceback (most recent call last):
File ""C:\Users\eardery\Desktop\Final Exam Practice\turtlepolygon.py"", line 1, in <module>
 from turtle import *
File ""C:\Users\eardery\Desktop\Final Exam Practice\turtle.py"", line 234
raise Error, ""no color arguments""
           ^
SyntaxError: invalid syntax

I have no idea what this means. 
",127
23522728,23522800,2,"you have a file named turtle.py in the same folder ... you should not name files the same as libraries ... you are importing from your local turtle.py file
rename turtle.py (in this same folder) to myturtle.py and it should be fine
",44
23522781,23523021,2,"Here is a hint.
Given a list of lists:
>>> LoL=[[1,2,3],[4,5,6],[7,8,9]]

You can get the sum of the individual sub lists with a list comprehension:
>>> [sum(li) for li in LoL]
[6, 15, 24]

You can then get the minimum with min:
>>> min(sum(li) for li in LoL)
6

You can get the index of the list this way:
>>> min([(i,sum(li)) for i, li in enumerate(LoL)], key=lambda t: t[1])
(0, 6)

That is 90% of what you need to solve your issue.
",149
23522781,23523061,2,"from itertools import permutations

data = [
    [98, 96, 93, 88, 86],
    [76, 75, 70, 68, 64],
    [64, 64, 66, 60, 59],
    [56, 55, 62, 58, 57],
    [50, 56, 53, 48, 49]
]

def item_sum(cols):
    return sum(row[col] for row,col in zip(data,cols))

best_cols = min(permutations(range(5)), key=item_sum)
total = item_sum(best_cols)

which gives
best_cols  =>  (4, 3, 2, 1, 0)  =>  [86, 68, 66, 55, 50]
total => 321

",145
23522781,23522781,1,"I'm looking to see if I could grab a hand on how to code an issue, preferably within the language Python. What I'd like to do is from 5 separate lists with 5 values in each find what the smallest sum of values would be, where each column/row can only have a value used once or twice. 
For example in the following example:
  ! p  ! q  ! r  ! s  ! t !

A !$98 !$96 !$93 !$88 !$86!

B !$76 !$75 !$70 !$68 !$64!

C !$64 !$64 !$66 !$60 !$59!

D !$56 !$55 !$62 !$58 !$57!

E !$50 !$56 !$53 !$48 !$49!

The desired outcome is to find the smallest total value where only one p, q, r, s or t can be assigned to A, B, C, D or E. i.e. you cannot have 2 values in the p column used twice.
For example, the most simple possibility could be: 98 + 75 + 66 + 58 + 49 = ... but it must be the lowest possible total value.
I'm not sure whether this would be done with lists or arrays or what ever it may be. I really appreciate any help you can provide. 
",276
23526204,23526340,2,"SELECT * FROM attachments WHERE post_id in (SELECT id FROM posts WHERE topic_id = ?);

Something like this?
Edit:
Not what the poster wanted after all. I don't think it is possible to do what you want with your table structure, without using more than one query.
",56
23526204,23526204,1,"So, I have this forum I am building. I'm having trouble writing an elegant select query to get all the posts for a certain topic from the db. Here's the schema:
Table posts have the following structure:
   id int(5) not null auto_increment,
   user_id int(5) not null,
   topic_id int(5) not null,
   post_date datetime not null,

Now, there's also a table called attachments, where post attachments are referenced. I want to be able to select the attachments of a particular post with just one query. Here's the attachments table structure:
   id int(5) not null auto_increment,
   post_id int(5) not null,
   post_name varchar(255) not null default '',
   path varchar(255) not null,

This is not what my tables look like really. I just want a basic idea how to do it. I can't even think of the least of solutions. Now the attachment table has a foreign key that references the post table. But, a post could possibly have many attachments. So how do I select a post and all its attachment in just one query? Maybe subqueries could work?
",227
23526265,23526489,2,"onliner trivial recursive lambda:
sum_mtx = lambda (x, y): x+y if not isinstance(x, list) else map(sum_mtx, zip(x,y))
sum_mtx(([[1, 2], [3, 4]] ,[[5, 6], [7, 8]]))
# [[6, 8], [10, 12]]

",81
23526265,23526265,1,"Let's say I have the lists [[1,2],[3,4]] and [[5,6],[7,8]]
I expect                   [[6, 8], [10, 12]] as the result.
I'm trying to sum up numbers according to their indexes.
def sum_matrix(num1, num2):

",65
23526265,23526331,2,"Another easy job for ndarrays:
>>> from numpy import array
>>> list1, list2 = [[1,2],[3,4]], [[5,6],[7,8]]
>>> (array(list1) + array(list2)).tolist()
[[6, 8], [10, 12]]

",69
23526280,23526280,1,"I'm trying out Sublime Text 2 and have installed Package Control. Now from Package Control in the Command Pallete I'm trying to install PyLint. However, this alert message pops up and am not sure how to proceed: 
Pylinter could not automatically determined the path to lint.py.
Please provide one in the settings file using the pylint_path variable.
NOTE:
If you are using a Virtualenv, the problem might be resolved by launching Sublime Text
from correct Virtualenv.
I am using a virtualenv in my project and am not sure what it means to launch Sublime Test from the virtualenv. Any help is appreciated - thanks!
UPDATE
I think I may need to configure something with build_system variable in the *.sublime_project file to get the virtualenv working. There appears to be a range of somewhat confusing advice in this SO question.
",154
23526280,23889696,2,"If you have installed pylint into your virtualenv, editing the project-file like this should fix it
""settings"":
    {
        ""pylinter"":
        {
             ""python_bin"": ""path/to/your/virtualenv""
        }
    }

See: https://github.com/biermeester/Pylinter#project-settings and 
https://github.com/biermeester/Pylinter/issues/4
",47
23526308,23526308,1,"Decorator pattern takes your function, decorates it with something else and returns you a new decorated function, and this is something I understand. 
But when it comes to Python, I found the implementation of decorators little different. Let's say we have the following code:
def helloWorld(func):
    print(func() + 1)

@helloWorld
def hello():
    return 2

My basic expectation was to call this method by saying hello() however, it somewhat worked with the following exception:
Traceback (most recent call last):
  File ""C:/Python34/test.py"", line 8, in <module>
    hello()
TypeError: 'NoneType' object is not callable

So basically, when it hits hello part of hello(), it prints the correct result and returns None as any function without return statement does in Python. () is trying to invoke a None in this case. So I did this and it worked as I expected:
def helloWorld(func):
    def inside():
        print(func() + 1)
    return inside

@helloWorld
def hello():
    return 2

hello()

I am not sure whether my expectation justifies anything here. But as a person coming from C#, Java, etc. background, I found this little different. I was expecting the same behavior of the second code sample in the first one. What am I missing? Why was this designed in this way? Why is just saying hello is enough to invoke decorator function? What good does this design have? 
Update and Clarification
I was expecting it to work without specifically returning a nested function in the decorator. So in the first example, hello() could have worked as if helloWorld was returning a nested method.
",337
23526308,23526483,2,"When you do
@helloWorld
def hello():
    return 2

Python does
def hello():
    return 2
hello = helloWorld(hello)

So hello will contain whatever helloWorld returns, in your first case you didn't return anything (which is the same as returning None). That's why calling hello() would give the error 'NoneType' object is not callable.
",72
23526309,23543166,2,"Shutil.copy/unicode wasn't the problem here; tyring to copy a non-existent file was.
",15
23526309,23526309,1,"I'm trying to use the shutil module to copy files from one drive to another.  Since this is an ArcGIS script, I store the user's choice for folder source and destination locations as:
src = arcpy.GetParameterAsText(0)
dst = arcpy.GetParameterAsText(1)

Using arcpy.AddMessage(src) to print that out gives me:
C:\Folder1\Folder2\File.extension

Which is what I want! However, when I try to use shutil.copy(src,dst), I get:
u'C:\\Folder1\\Folder2\\File.extension'

IOError: [Errno 2] No such file or directory: u'C:\\Folder1\\Folder2'

What is happening here? Since I'm not spelling out the path I can't change the ""u"" to an ""r"" for raw input... 
",136
23526309,24347824,2,"As a form of file management validation, you should always check to see whether or not the directory exists.  You can do this using the os.path.exists(path) method. If your path exists, you should have no problem. If not, then create it before copying your files over.
See example code below:
if not os.path.exists(dst):
    os.mkdir(dst)

shutil.copy(src, dst)

",78
23526384,23526434,2,"Python bytecode is portable across platforms, but not really across Python versions.
Python 2.7 introduced new syntax, for example, resulting in different, new bytecode instructions that Python 2.6 doesn't support. Also see the warning at the top of the dis module documentation:

CPython implementation detail: Bytecode is an implementation detail of the CPython interpreter! No guarantees are made that bytecode will not be added, removed, or changed between versions of Python. Use of this module should not be considered to work across Python VMs or Python releases.

You can move .pyc bytecode cache files across platforms, regardless of word-size and OS.
",117
23526384,23526384,1,"First question - If we have 2 different versions of python(say 2.6,2.7) on the same platform. Can you execute the bytecode (generated with python 2.6 interpreter) on python 2.7 interpreter?
Second question - If we have exact same version of python say 2.7.2 on unix and windows - can you run the bytecode generated on unix machine with the python on windows machine?
",71
23526390,23526390,1,"I have a program that I'm running that listens to a queue(its not multi-threaded so I want to run several instances of it).  I've tried my best to catch errors but in the event the app crashes due to a error or bad incoming data, I want to be able to respawn the python app(after I log the stacktrace) so it continues to work.
I felt this might be a common problem for people who run python based services so I thought I would ask but I was thinking of writing some code to do a ps -ef and count the instances of the name of the python program(if less than a threshold then I would have the program relaunch it).  
Before I build this, I wanted to know if there was perhaps a better way or a existing tool/module that did this? 
Thanks!
",162
23526390,23526564,2,"You could use a supervisor. A well known one that's written in Python would be supervisord, a more recent one also in Python would be Circus, and then there are Monit or daemontools and probably many more.
",42
23526390,23526956,2,"If you are looking for something more simple, you can use the subprocess module (python default) to start and check your processes...
A basic version would look like this:
# run.py

import subprocess, time

# add your listener processor call here
_PROCESS_ARGS = ['python','/path/to/listener.py']
_PROCESS_TOTAL = 10

process_list = []

# start the processes...
for i in range(_PROCESS_TOTAL):     
    process_list.append(subprocess.Popen(_PROCESS_ARGS))

while True:     
    for i in range(_PROCESS_TOTAL):         
        p = process_list[i]         
            if p.poll() != None: # check if process is running                      
                process_list[i] = subprocess.Popen(_PROCESS_ARGS) # if not, replace with new one
    time.sleep(1) # check only every second...

",141
23526390,23526513,2,"Checkout supervisord.  I use it regularly to launch, monitor all types of things.
Here is how I set it up to launch a wsgi app on my server:
[program:quizzes]
directory = /var/www/quizzes.seasources.net
command = /home/jaime/code/virtualenv/quizzes/bin/uwsgi uwsgi.ini
process_name = quizzes
autostart = true
startsecs = 5
user = www-data
redirect_stderr = true
stdout_logfile = /var/www/quizzes.seasources.net/logs/supervisor-console.log
environment = PYTHON_EGG_CACHE=/tmp/python-eggs

The configuration file format is easy to understand and it even logs stdout/stderr to a file.  Above it's /var/www/quizzes.seasources.net/logs/supervisor-console.log  You can read more about configuration here.
",94
23526410,23527361,2,"I don't have any python interpreter here, but it should be something similar to this:
import re


def url_match(tweet):
    match = re.match(r'RT\s@....+', tweet)
    if match:
        return ""RT""
    else:
        match = re.match(r'@....+', tweet)
        if match:
           return ""mention""
        else
           return ""tweet""

Note: this will work for this classification, but if you want to retrieve usernames i.e. @USERNAME you will have to tweak this a little more.
",102
23526410,23526410,1,"Pulling from a couple of different examples, I've been able to create a simple Python script that parses the JSON output from the Twitter Streaming API, and prints out the screen_name and text for each tweet.  I would like to modify my code to also classify each tweet as one of the following: 
(1) Retweet --> There is an ""RT @anyusername"" somewhere in the tweet text column
(2) Mention --> There is an ""@anyusername"" but no ""RT @anyusername"" in the tweet column
(3) Tweet --> There is no ""RT @anyusername"" nor any ""@anyusername"" in the tweet column
I can do this in Excel with the following formula, but I can figure it out in Python yet. 
=IF(IFERROR(FIND(""RT @"",B2)>0,""False""),""Retweet"",IF(IFERROR(FIND(""@"",B2)>0,""False""),""Mention"",""Tweet""))
Existing Code
import json
import sys
from csv import writer

with open(sys.argv[1]) as in_file, \
    open(sys.argv[2], 'w') as out_file:
    print >> out_file, 'tweet_author, tweet_text, tweet_type'
    csv = writer(out_file)

    for line in in_file:
        try:
            tweet = json.loads(line)
        except:
            pass

        tweet_text = tweet['text']

        row = (
        tweet['user']['screen_name'],
        tweet_text
        )
        values = [(value.encode('utf8') if hasattr(value, 'encode') else value) for value in row]
        csv.writerow(values)

",319
23526486,23526548,2,">>> ""abcd%W""%(123)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ValueError: unsupported format character 'W' (0x57) at index 5

Could something like this work for you instead? 
>>> ""abcd%W"".replace('%W', str(123))
'abcd123'

",78
23526486,23526486,1,"I'm getting this error on creating a string with arguments like ""abcd%s""%(e) but I'm getting e by scraping a web page. Can anyone please tell me what is the best way to avoid this error. 
I found other similar questions but they were using %20 in the url for which they need to replace %20 with %%20 which solved their problem. But my case is different. I tried encoding e but still getting same error.
ValueError: unsupported format character 'W' (0x57)

",104
23527435,23790729,2,"As @Mikko Ohtamaa mentioned, the issue where was that the project root directory contained an __init__.py
",18
23527435,23527435,1,"I am having an issue in my Django project where I am importing the same module twice. This is causing a unit test of mine to fail: this unit test checks that the view found by resolving a URL is the same view imported from within the app being tested. So, my tests file looks something like:
from django.core.urlresolvers import resolve
from django.tests import TestCase
from .views import index

class IndexText(TestCase):
    def test_root_url_resolves_to_index_view(self):
        found = resolve('/someapp/')
        import pdb; pdb.set_trace()           # Using this to debug because the below assertion is failing
        self.assertEqual(found.func, index)

The above assertion is failing because the two functions are not equal. When debugging in pdb, I found that found.func.__module__ is someapp.views while index.__module__ is projectName.someapp.views. 
I was told in #django on Freenode this could be because although I had recently updated to Django 1.6, I was using the old 1.3 project structure, where the project settings.py and urls.py are in the root of the project alongside manage.py. 
I've fixed that by creating a new directory within my project root, with the same name as the directory containing the project root, and placed my urls.py and settings.py in that directory. So, my directory structure looks something like this:
/home
  /joseph
    /myWorkspace
      /projectName
        manage.py
        /projectName
          __init__.py
          urls.py
          settings.py
        /someapp
          __init__.py
          views.py
          tests.py
        /someotherapp
        / ... and so on ...

When I open a shell via manage.py shell, import sys, and print sys.path, the first directory in that list is /home/joseph/myWorkspace/projectName and that seems right to me. The rest of the python path looks pretty normal, pointing to different site packages, etc.  
However, when I run my test from above via manage.py test someapp, if I print the sys.path in pdb, I see that my python path first contains /home/joseph/myWorkspace, and also /home/joseph/myWorkspace/projectName. This does not seem correct to me, and I think this may be why I am having issues with double imports. 
I am not setting PYTHONPATH in my environment variables. As far as I know, I am also not making any adjustments to sys.path within my apps or settings. 
I don't know where to go from here, can anyone lend some insight? 
",412
23527600,23527600,1,"So I am using python pandas have an the following variables:

a dataframe df with a column 'TAG' I created to tag data into
groups based on data from a column 'IDnumber'. 
regex patterns stored in arrays pattern1, pattern2,
pattern2-2, ...etc
an array group which is filled with strings (ie: 'software', 'engineering', 'marketing'...etc).

The code is filling in the column df.TAG with strings from the array group based on the regex patterns  pattern1, pattern2, pattern22, ...etc
So far I have working code but there is redundancy in having multiple for loops that look the same
for i in range(len(pattern1)):
    df.loc[df.IDnumber.str.contains(pattern1[i]) & (df.TAG == ''),'TAG'] = group[1]

for i in range(len(pattern2)):
    df.loc[df.IDnumber.str.contains(pattern2[i]) & (df.TAG == ''),'TAG'] = group[2]

for i in range(len(pattern22)):
    df.loc[df.IDnumber.str.contains(pattern22[i]) & (df.TAG == ''),'TAG'] = group[2]

for i in range(len(pattern33)):
    df.loc[df.IDnumber.str.contains(pattern33[i]) & (df.TAG == ''),'TAG'] = group[3]

for i in range(len(pattern3)):
    df.loc[df.IDnumber.str.contains(pattern3[i]) & (df.TAG == ''),'TAG'] = group[3]

I am also getting a warning.
SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame
  df.loc[df.IDnumber.str.contains(pattern1[i]),'TAG'] = group[1]

But the code works so I would like to know if there is a way to make the code more efficient by reducing the number of for loops and remove the warning without using pd.options.mode.chained_assignment = None to suppress the warnings.
",378
23527600,23527794,2,"Your first for-loop:
for i in range(len(pattern1)):
    df.loc[df.IDnumber.str.contains(pattern1[i]) & (df.TAG == ''),'TAG'] = group[1]

can be replaced with
empty = (df.TAG == '')
mask = df.IDnumber.str.contains('|'.join(pattern1)) & empty
df.loc[mask, 'TAG'] = group[1]

This might be faster, since the entire loop is being replaced with one regex pattern. A similar refactoring can be done for your second and last for-loops.
But your third and fourth for-loops perplex me: for i in range(len(pattern2-2)):. Python names can not contain hyphens. So what does pattern2-2 mean? If pattern2-2 is just another array of strings (albeit with an invalid variable name!?) then your third and fourth loops can be handled the same as shown above.

If all the patterns are simply arrays of strings, then you could refactor all the for-loops with something like
import itertools as IT
patterns = [pattern1, pattern2, pattern3, pattern4, pattern5]
empty = (df.TAG == '')
for pattern, grp in IT.izip(patterns, group):
    mask = df.IDnumber.str.contains('|'.join(pattern)) & empty
    df.loc[mask, 'TAG'] = grp

Note that whenever you have numbered variable names, such as pattern1, pattern2, etc. it is usually a sign that theses variables should be replaced by a single variable which is a list or tuple, such as patterns above. Then instead of referencing pattern1, you'd simply use patterns[0].
",305
23527680,23527680,1,"I'm trying to get an arduino board communicate with a beaglebone ( BB) white running Ubuntu using UART. I have read that the BB uart driver is already interrupt driven. 
I want to store all incoming data into a sort of buffer which I can read when required, similar to the way it's done in microcontrollers. But I'm trying to avoid kernel programming so I won't be able to use the driver's data structures. I'm looking for a complete user space solution. 
I'm planning to use two python processes, one to write all incoming data (to a shared list) and the other to read it as required so that the read is non blocking. 
I have two questions:

Is this the right approach? if yes, please suggest a simple interprocess communication method that will suffice.
What is the right way to implement this?

Note: I'm using the PyBBIO library that reads and writes directly to the /dev/mem special file.
",185
23527680,23529277,2,"You might want to use pyserial, which  uses the kernel interfaces (I don't know what PyBBIO does). It provides automatic input buffering - so you don't need an extra process. If you do want to have more processes use multiprocessing. A simpler alternative is threading, which saves you the communication part. For multiprocessing with network support use Ipython's cluster
",70
23527738,23527738,1,"I have stored a number of 2d arrays in a 3d array and I need to multiply each one with a vector. so I have stored all those vectors in a 2d array. It's like this:
A = np.random.random((L, M, N))
B = np.random.random((L, M))

and I need to multiply each A[l] by B[l] which results in a Nx1 array and the output of the whole operation would be a LxN 2d array. Is there a function that can do this or do I need a loop?
",112
23527738,23528726,2,"An option is np.einsum
import numpy as np
output = np.einsum(""ijk, ij -> ik"", A, B)

This results in a (L, N) sized array containing matrix products of all the A[i].T.dot(B[i])
",53
23531496,23545561,2,"Hi All,
                 With help from a friend I figured out why I was striking out with my earlier bit of code. As @wils484 pointed out, a conditional would have solved the problem (and I tried it even before posting on here, but in vain) but the key to getting it to work was realising that values[1] came out a string and needed to be converted to an integer before it could be used to loop through;
so the bit of code that helped solve the problem is:
    if int(values[1]) == i:
       output.write(values[3]+ "" "")

So that resolved my issue and the code spewed separate files with the pertinent data.
",136
23531496,23531862,2,"It depends on how big your file is, but it isn't too big something like this would probably work, using a dictionary to split the data into their respective runs:
data = {}
with open(""Subject25.txt"", 'r') as input:
    for aline in input:
        values = aline.split()
        if values[3] not in data.keys():
            data[values[3]] = aline + ""\n""
        else:
            data[values[3]] += aline + ""\n""
for key, values in data.iteritems():
    with open('%s.txt'%key, 'w') as output:
        output.write(values)

Your code is failing because it doesn't do anything to verify that the line belongs in the file. It should work if you add something like 
if values[3] == value_for_file:
    output.write('{:10}{:10}\n'.format(values[3]))

The disadvantage of doing that is that you are reading the input file 40x times.
",198
23531496,23531496,1,"I am trying to break a large data file which contains behavioural data from an experiment with around 40 runs on average (varies across subjects), into textfiles corresponding to run-number with the results of that run. 
The data look like this:
Subject, run, ""Sample"", result1, result2, reaction time, feedback
now, run number 1 may have about 10 trials, and run no 2 may have 16 and so on.
I have perused stackoverflow enough to learn how to open multiple output files corresponding to the list of runs, but my code yields unhelpful results (as in all data as opposed to just what I need) when I try to allocate all the values from ""Result1"" corresponding to the trials of ""a"" run to its output file (so, all Result1 values for Run 1 should end up in Run1.txt).
This is my code:
    infile = open(""Subject25.txt"", 'r')
    gamelist = [0, 1, 2, 3, 4, 5, 6, 7, 8,10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 22, 23, 25, 26, 28, 29, 30,31, 34]

for i in gamelist:
   with open(""Subject25.txt"", 'r') as input:
      with open('samples%i.txt' %i, 'w') as output:
         for aline in input:
             values = aline.split()
             output.write('{:10}{:10}\n'.format(values[3]))

I have tried using conditionals to see if I could pick out result1 values but I am not sure I did it correctly. I am learning python on the job and would be grateful for useful advice on this issue. 
",338
23531555,23531555,1,"I'm running into a few issues on my Emacs + Org mode + Python setup. I thought I'd put this out there to see if the community had any suggestions.
Virtualenv:
I'm trying to execute a python script within a SRC block using a virtual environment instead of my system's python implementation. I have a number of libraries in this virtual environment that I don't have on my system's python (e.g. Matplotlib). Now, I set python-shell-virtualenv-path to my virtualenv's root directory. When I run M-x run-python the shell runs from my virtual environment. That is, I can import Matplotlib with no problems. But when I import Matplotlib within a SRC block I get an import error. 

How can I have it so the SRC block uses the python in my virtual
environment and not my system's python? 
Is there any way I can set
    the path to a given virtual environment automatically when I load an
    org file?

HTML5 Export:
I'm trying to export my org-files in 'html5', as opposed to the default 'xhtml-strict'. The manual says to set org-html-html5-fancy to t. I tried searching for org-html-html5-fancy in M-x org-customize but I couldn't find it. I tried adding (setq org-html-html5-fancy t) to my init.el, but nothing happened. I'm not at all proficient in emacs-lisp so my syntax may be wrong. The manual also says I can set html5-fancy in an options line. I'm not really sure how to do this. I tried #+OPTIONS html5-fancy: t but it didn't do anything.

How can I export to 'html5' instead of 'xhtml-strict' in org version
7.9.3f and Emacs version 24.3.1?
Is there any way I can view and customize the back-end that parses
the org file to produce the html?

I appreciate any help you can offer.
",344
23531555,23557258,2,"Reads like a bug, please consider reporting it at emacs-orgmode@gnu.org
As a workaround try setting the virtualenv at the Python-side, i.e. give PYTHONPATH as argument.
Alternatively, mark the source-block as region and execute it the common way, surpassing org
",47
23531807,23531807,1,"Why does my Dragon curve not look like a dragon curve?

Here is the implementation in python with order 10:
def setupForDragonCurve():
    turtle.hideturtle()
    turtle.tracer(1e3, 0)
    turtle.penup()
    turtle.goto(0, -turtle.window_height()/5)
    turtle.pendown()

def generateDragonCurve(n, result='[FX]'):
    for _ in range(n):
        result = result.replace('Y', 'FX-Y')
        result = result.replace('X', 'X+YF')
    return result

def drawDragonCurve(cmds, size):
    stack = []
    for cmd in cmds:
        if cmd=='F':
            turtle.forward(size)
        elif cmd=='-':
            turtle.left(90)
        elif cmd=='+':
            turtle.right(90)
        elif cmd=='X':
            pass
        elif cmd=='Y':
            pass
        elif cmd=='[':
            stack.append((turtle.position(), turtle.heading()))
        elif cmd==']':
            position, heading = stack.pop()
            turtle.penup()
            turtle.setposition(position)
            turtle.setheading(heading)
            turtle.pendown()
        else:
            raise ValueError('Unknown Cmd: {}'.format(ord(cmd)))
    turtle.update()

Here is what it's supposed to look like at order 10:

EDIT: Here is what I get with order 1, with a larger scaled curve:

",241
23531807,23532032,2,"When you run
    result = result.replace('Y', 'FX-Y')
    result = result.replace('X', 'X+YF')

the second line replaces the Xs introduced by the first line. (Also, you're using the wrong replacement rules, but you seem to have noticed that already.)
You need to carry out these replacements in such a way that they don't interact with each other, probably by doing them in a combined step. One way to do this would be to use the more advanced substitution capabilities of the re module. Another would be to write your own replacement routine. You could also use a character other than X in the first replacement, so the second replacement doesn't pick it up, then replace that character with X in a third pass.
",149
23531812,23531812,1,"I'm trying to transpose a dataset of order about 60*75. I'm having a trouble iterating through the matrix to transpose it into 75*60 order. Each column would have different kind of data (numbers, words, mixture, URLs etc). I tried the following code. But it'd just give me the first column transposed. 
f= open('input.txt', ""rb"")    
fw=open(""output.txt"", ""wb"")
l=f.read()
for row in l:
    print ''.join(row)
p= [[row[i] for row in l] for i in range(75)]
print p

Alternaticely I tried 
a = np.array(l)[np.newaxis]
print a.T

None of them gave me the complete transposed matrix. 
I even tried zip(*l) and map(zip(*l)). 
I appreciate your help. 
Thank you.
",167
23531812,23531842,2,"f.read() is a giant string containing all the file's contents. It is not some sort of structured data format; in particular, for row in l iterates over raw characters rather than rows of useful data.
If you want something more useful, the csv module might help, or perhaps something like numpy.loadtxt, depending on the format the file actually contains.
",70
23531825,32658000,2,"It won't work on python3.x afaik.
Sphinx is not very python3 compatible, running __import__(module_name) AND importlib.import_module(module_name) both work in my interpreter, but not in sphinx.
I tried checking out the master branch of sphinx, changed my interpreter to python3.4 and got errors on modules that were removed in the 3.x series. You can see my issue report here:
https://github.com/sphinx-doc/sphinx/issues/2046
",75
23531825,23531825,1,"I am trying to use sphinx to document a project of mine. I have used autodoc strings within all of my modules and files. I used sphinx-apidoc to automatically generate rst files for my code. So far, so good.
The problem is that sphinx is not able to import any of my modules, even though I have added my project to sys.path.
My unit tests pass and can import my modules just fine. I'm kind of at my wit's end; I've tried all sorts of renaming and moving and reloading and reconfiguring without success and it is very frustrating to say the least.
Here is my document structure:
project
├── collectionprocessor.py
├── config.py
├── createdb.py
├── database
│   ├── database.py
│   └── __init__.py
├── docs
│   ├── _build
│   ├── conf.py
│   ├── generated
│   ├── index.rst
│   ├── Makefile
│   ├── modules.rst
│   ├── project.database.rst
│   ├── project.document.rst
│   ├── project.mixins.rst
│   ├── project.parser.rst
│   ├── project.rst
│   ├── project.sequence.rst
│   ├── project.tests.rst
│   ├── _static
│   └── _templates
├── document
│   ├── document.py
│   ├── __init__.py
│   ├── metadata.py
│   ├── parsedparagraph.py
│   ├── sentence.py
│   ├── taggedword.py
│   └── unit.py
├── __init__.py
├── logger.py
├── mixins
│   ├── comparebydict.py
│   ├── __init__.py
│   └── kwargstodict.py
├── models.py
├── parser
│   ├── dependency.py
│   ├── documentparser.py
│   ├── __init__.py
│   └── parseproducts.py
├── README.md
├── runtests.py
├── sequence
│   ├── __init__.py
│   ├── sequenceprocessor.py
│   └── sequence.py
├── stringprocessor.py
├── structureextractor.py
├── tests
│   ├── data
│   ├── __init__.py
│   ├── __pycache__
│   ├── raw_parse.txt
│   ├── testcollectionprocessor.py
│   ├── testdocumentparser.py
│   ├── testextractor.py
│   ├── testlogger.py
│   ├── testsequenceprocessor.py
│   └── teststringprocessor.py

Here is the relevant line in the sphinx config file:
sys.path.insert(0, os.path.abspath(""../""))

Here is the output of sphinx-build:
[docs]─[$]>>> sphinx-build -b html ./ ./generated/
Running Sphinx v1.2.2
loading pickled environment... done
building [html]: targets for 0 source files that are out of date
updating environment: 0 added, 7 changed, 0 removed
reading sources... [100%] project.tests                                                                                                             
/home/plasma/prog/project/docs/project.rst:22: WARNING: autodoc: failed to import module u'project.collectionprocessor'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.collectionprocessor
/home/plasma/prog/project/docs/project.rst:30: WARNING: autodoc: failed to import module u'project.config'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.config
/home/plasma/prog/project/docs/project.rst:38: WARNING: autodoc: failed to import module u'project.createdb'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.createdb
/home/plasma/prog/project/docs/project.rst:46: WARNING: autodoc: failed to import module u'project.logger'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.logger
/home/plasma/prog/project/docs/project.rst:54: WARNING: autodoc: failed to import module u'project.models'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.models
/home/plasma/prog/project/docs/project.rst:62: WARNING: autodoc: failed to import module u'project.runtests'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.runtests
/home/plasma/prog/project/docs/project.rst:70: WARNING: autodoc: failed to import module u'project.stringprocessor'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.stringprocessor
/home/plasma/prog/project/docs/project.rst:78: WARNING: autodoc: failed to import module u'project.structureextractor'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.structureextractor
/home/plasma/prog/project/docs/project.rst:87: WARNING: autodoc: failed to import module u'project'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project
/home/plasma/prog/project/docs/project.database.rst:10: WARNING: autodoc: failed to import module u'project.database.database'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.database.database
/home/plasma/prog/project/docs/project.database.rst:19: WARNING: autodoc: failed to import module u'project.database'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.database
/home/plasma/prog/project/docs/project.document.rst:10: WARNING: autodoc: failed to import module u'project.document.document'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.document.document
/home/plasma/prog/project/docs/project.document.rst:18: WARNING: autodoc: failed to import module u'project.document.metadata'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.document.metadata
/home/plasma/prog/project/docs/project.document.rst:26: WARNING: autodoc: failed to import module u'project.document.parsedparagraph'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.document.parsedparagraph
/home/plasma/prog/project/docs/project.document.rst:34: WARNING: autodoc: failed to import module u'project.document.sentence'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.document.sentence
/home/plasma/prog/project/docs/project.document.rst:42: WARNING: autodoc: failed to import module u'project.document.taggedword'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.document.taggedword
/home/plasma/prog/project/docs/project.document.rst:50: WARNING: autodoc: failed to import module u'project.document.unit'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.document.unit
/home/plasma/prog/project/docs/project.document.rst:59: WARNING: autodoc: failed to import module u'project.document'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.document
/home/plasma/prog/project/docs/project.mixins.rst:10: WARNING: autodoc: failed to import module u'project.mixins.comparebydict'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.mixins.comparebydict
/home/plasma/prog/project/docs/project.mixins.rst:18: WARNING: autodoc: failed to import module u'project.mixins.kwargstodict'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.mixins.kwargstodict
/home/plasma/prog/project/docs/project.mixins.rst:27: WARNING: autodoc: failed to import module u'project.mixins'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.mixins
/home/plasma/prog/project/docs/project.parser.rst:10: WARNING: autodoc: failed to import module u'project.parser.dependency'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.parser.dependency
/home/plasma/prog/project/docs/project.parser.rst:18: WARNING: autodoc: failed to import module u'project.parser.documentparser'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.parser.documentparser
/home/plasma/prog/project/docs/project.parser.rst:26: WARNING: autodoc: failed to import module u'project.parser.parseproducts'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.parser.parseproducts
/home/plasma/prog/project/docs/project.parser.rst:35: WARNING: autodoc: failed to import module u'project.parser'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.parser
/home/plasma/prog/project/docs/project.sequence.rst:10: WARNING: autodoc: failed to import module u'project.sequence.sequence'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.sequence.sequence
/home/plasma/prog/project/docs/project.sequence.rst:18: WARNING: autodoc: failed to import module u'project.sequence.sequenceprocessor'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.sequence.sequenceprocessor
/home/plasma/prog/project/docs/project.sequence.rst:27: WARNING: autodoc: failed to import module u'project.sequence'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.sequence
/home/plasma/prog/project/docs/project.tests.rst:10: WARNING: autodoc: failed to import module u'project.tests.testcollectionprocessor'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.tests.testcollectionprocessor
/home/plasma/prog/project/docs/project.tests.rst:18: WARNING: autodoc: failed to import module u'project.tests.testdocumentparser'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.tests.testdocumentparser
/home/plasma/prog/project/docs/project.tests.rst:26: WARNING: autodoc: failed to import module u'project.tests.testextractor'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.tests.testextractor
/home/plasma/prog/project/docs/project.tests.rst:34: WARNING: autodoc: failed to import module u'project.tests.testlogger'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.tests.testlogger
/home/plasma/prog/project/docs/project.tests.rst:42: WARNING: autodoc: failed to import module u'project.tests.testsequenceprocessor'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.tests.testsequenceprocessor
/home/plasma/prog/project/docs/project.tests.rst:50: WARNING: autodoc: failed to import module u'project.tests.teststringprocessor'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.tests.teststringprocessor
/home/plasma/prog/project/docs/project.tests.rst:59: WARNING: autodoc: failed to import module u'project.tests'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/sphinx/ext/autodoc.py"", line 335, in import_object
    __import__(self.modname)
ImportError: No module named project.tests
looking for now-outdated files... none found
pickling environment... done
checking consistency... /home/plasma/prog/project/docs/modules.rst:: WARNING: document isn't included in any toctree
done
preparing documents... done
writing output... [100%] project.tests                                                                                                              
writing additional files... (1 module code pages) _modules/index genindex search
copying static files... done
copying extra files... done
dumping search index... done
dumping object inventory... done
build succeeded, 36 warnings.

",2098
23531825,23554166,2,"I ended up restructuring my project so that the docs directory is at the same level as my project directory that contains all the files. This works when I use sys.path.insert(0, os.path.abspath(""../"")), and it's probably a bit nicer to look at.
project
    | docs/
    | project/
        | project files, etc.
    | tests/

",67
23531825,32396276,2,"Iam a bit late for the party here is my solution:
You have to go 2 directorys up:
sys.path.insert(0, os.path.abspath('../..'))

",30
23993681,24005429,2,"The logging in your case was triggered by str(e).
Issue was fixed in https://code.google.com/p/appengine-gcs-client/source/detail?r=172
",22
23993681,23993681,1,"I'm using the GCS client library to write files to Google Cloud Storage in my App Engine app in Python.
Before creating a file, I need to make sure it doesn't already exist to avoid overwriting.
To do this I am checking to see if the file exists before trying to create it:
import cloudstorage as gcs

try:
    gcs_file = gcs.open(filename, 'r')
    gcs_file.close()
    return ""File Exists!""
except gcs.NotFoundError as e:
    return ""File Does Not Exist: "" + str(e)

cloudstorage.write() is logging (either directly or indirectly) the fact that it receives a 404 error when trying to read the non-existent file. I would like to suppress this if possible.
Thanks
edit
Here's what is logged:

12:19:32.565 suspended generator _get_segment(storage_api.py:432)
  raised NotFoundError(Expect status [200, 206] from Google Storage. But
  got status 404. Path:
  '/cs-mailing/bde24e63-4d31-41e5-8aff-14b76b239388.html'. Request
  headers: {'Range': 'bytes=0-1048575', 'x-goog-api-version': '2',
  'accept-encoding': 'gzip, *'}. Response headers:
  {'alternate-protocol': '443:quic', 'content-length': '127', 'via':
  'HTTP/1.1 GWA', 'x-google-cache-control': 'remote-fetch', 'expires':
  'Mon, 02 Jun 2014 11:19:32 GMT', 'server': 'HTTP Upload Server Built
  on May 19 2014 09:31:01 (1400517061)', 'cache-control': 'private,
  max-age=0', 'date': 'Mon, 02 Jun 2014 11:19:32 GMT', 'content-type':
  'application/xml; charset=UTF-8'}. Body: ""NoSuchKeyThe specified
  key does not exist."". Extra info: None.

",306
24982969,24982969,1,"i want know witch device connect to witch usb port in ubuntu ...
for example when i connect bluetooth dongle to usb i wnat know that Bluetooth connected tu witch usb port ...
when i run 

tail -f /var/log/messages

in can see usb port number like this :

Jul 27 20:51:58 Smart-Installer kernel: [  711.363300] usb 1-1.2: New USB device found, idVendor=0a12, idProduct=0001
  Jul 27 20:51:58 Smart-Installer kernel: [  711.363331] usb 1-1.2: New USB device strings: Mfr=0, Product=0, SerialNumber=0

i want get usb 1-1.2 programmatically via python
i know lsusb -t get me port but i want also device number and -t get me a few data
",118
24982969,25048696,2,"You can either use PyUSB (python-usb package in Ubuntu and Debian), or walk through /sys/bus/usb/devices yourself, all the info is there in plain text.
",29
24982993,28203006,2,"import os
path = 'myfile.txt'
size = os.path.getsize(path)
with open(path) as f:
    for line in f:
        size -= len(line)
        if not size:
            print('this is the last line')
            print(line)

",47
24982993,24982993,1,"I am reading a file in Python line by line and I need to know which line is the last one while  reading,something like this:
 f = open(""myfile.txt"")
 for line in f:
    if line is lastline:
       #do smth

From the examples I found it involves seeks and complete file readouts to count lines,etc.Can I just detect that the current line is the last one? I tried to go and check for ""\n"" existence ,but in many cases the last lines is not followed by backslash N.
Sorry if my question is redundant as I didn't find the answer on SO
",118
24982993,24983342,2,"Check if line is the last line:
with open(""in.txt"") as f:
    lines = f.readlines()
    last = lines[-1]
    for line in lines:
        if line is last:
            print id(line),id(last)
            # do work on lst line
        else:
            # work on other lines

If you want the second last line use last = lines[-2]
Or simply:
with open(""in.txt"") as f:
    lines = f.readlines()
    last = lines[-1]
    for line in lines[:-1]:
        # work on all but last line
    # work on last

",120
24982993,24983191,2,"You could use the itertools pairwise recipe;
with open('myfile.txt') as infile:
    a,b = itertools.tee(infile)
    next(b, None)
    pairs = zip(a,b)
    lastPair = None
    for lastPair in pairs:
        pass
secondLastLine = lastPair[0]
# do stuff with secondLastLine

",59
24982993,24983104,2,"secondLastLine = None
lastLine = None
with open(""myfile.txt"") as infile:
    secondLastLine, lastLine = infile.readline(), infile.readline()
    for line in infile:
        # do stuff
        secondLastLine = lastLine
        lastLine = line

# do stuff with secondLastLine

",46
24982993,24983057,2,"One thing you could try is to try to get the next line, and catch the exception if it arises, because AFAIK python iterators don't have inbuilt hasNext method.
",33
31239302,31364280,2,"I had the same problem and tracked it down to a combination of CONN_MAX_AGE and CELERYD_MAX_TASKS_PER_CHILD. At that point it became obvious that it must be something to do with Celery not closing connections properly when a worker is replaced and from that I found this bug report: https://github.com/celery/celery/issues/2453
Upgrading to Celery 3.1.18 seems to have solved the issue for me.
",66
31239302,31239302,1,"Using Django 1.7 and Celery on Heroku with Postgres and RabbitMQ.
I recently set the CONN_MAX_AGE setting in Django to 60 or so so I could start pooling database connections. This worked fine until I discovered a problem where if for any reason a database connection was killed, Celery would continue using the bad database connection, consuming tasks but immediately throwing the following error within each task:
OperationalError: SSL SYSCALL error: Bad file descriptor

I would like to keep pooling database connections, but this has happened a few times now and I obviously can't allow Celery to randomly fail. How can I get Django (or Celery) to force a new database connection only when this error is hit?
(Alternatively, another idea I had was to force the Celery worker to run with a modified settings.py that sets CONN_MAX_AGE=0 only for Celery... but that feels very much like the wrong way to do it.)

Please note that this StackOverflow question seems to solve the problem on Rails, but I haven't found an equivalent for Django:
  On Heroku, Cedar, with Unicorn: Getting ActiveRecord::StatementInvalid: PGError: SSL SYSCALL error: EOF detected

",216
31239322,31239322,1,"I'm trying to remove StreamHandler during runtime of my python code execution. 
if (False == consoleOutput):                                                                                                                                                                
    lhStdout = log.handlers[0]  # stdout is the only handler initially                                                                                                                      
    log.removeHandler(lhStdout)  

This is working fine. But I don't like that we assume that stdout is the first handler in handler array. Is there a way to query handlers class to find which type it is? Something like this
 for handler in log.handlers
    if (handler.type == StreamHandler())
        <...>

",94
31239322,31240807,2,"What you're looking for is spelled: if isinstance(handler, StreamHandler): - but I'd really like to know why you want to do such a thing instead of using the sensible solution (ie not configuring a StreamHandler for your logger at all...). 
",52
31239329,31239929,2,"The following Python script will read your file in (assuming it looks like your example) and will create a version removing the common folders:
import os.path, csv

finput = open(""d:\\input.csv"",""r"")
csv_input = csv.reader(finput, delimiter="" "", skipinitialspace=True)
csv_output = csv.writer(open(""d:\\output.csv"", ""wb""), delimiter="" "")

# Create a set of unique folder names

set_folders = set()

for input_row in csv_input:
    set_folders.add(os.path.split(input_row[0])[0])

# Determine the common prefix

base_folder = os.path.split(os.path.commonprefix(set_folders))[0]
nprefix = len(base_folder) + 1

# Go back to the start of the input CSV 

finput.seek(0)

for input_row in csv_input:
    csv_output.writerow([input_row[0][nprefix:]] + input_row[1:])

Using the following as input:
C:/Abc/Def/Test/temp/test/GLNext/FILE0.frag                   0   0   0
C:/Abc/Def/Test/temp/test/GLNext/FILE0.vert                   0   0   0
C:/Abc/Def/Test/temp/test/GLNext/FILE0.link-link-0.frag       16  24  3
C:/Abc/Def/Test/temp/test/GLNext2/FILE0.link-link-0.vert       87  116 69
C:/Abc/Def/Test/temp/test/GLNext5/FILE0.link-link-0.vert.bin   75  95  61
C:/Abc/Def/Test/temp/test/GLNext7/FILE0.link-link-0            0   0
C:/Abc/Def/Test/temp/test/GLNext/FILE0.link-link-6            0   0   0

The output is as follows:
GLNext/FILE0.frag 0 0 0
GLNext/FILE0.vert 0 0 0
GLNext/FILE0.link-link-0.frag 16 24 3
GLNext2/FILE0.link-link-0.vert 87 116 69
GLNext5/FILE0.link-link-0.vert.bin 75 95 61
GLNext7/FILE0.link-link-0 0 0
GLNext/FILE0.link-link-6 0 0 0

With one space between each column, although this could easily be changed.
",266
31239329,31284985,2,"You can use the pandas library for this. Doing so, you can leverage pandas' amazing handling of big CSV files (even in the hundreds of MB).
Code:
import pandas as pd

csv_file = 'test_csv.csv'
df = pd.read_csv(csv_file, header=None)
print df
print ""-------------------------------------------""

path = ""C:/Abc/bcd/Def/Test/temp/test/GLNext/""
df[0] = df[0].replace({path:""""}, regex=True)

print df
# df.to_csv(""truncated.csv"") # Export to new file.

Result:
                                                   0   1    2   3
0    C:/Abc/bcd/Def/Test/temp/test/GLNext/FILE0.frag   0    0   0
1    C:/Abc/bcd/Def/Test/temp/test/GLNext/FILE0.vert   0    0   0
2  C:/Abc/bcd/Def/Test/temp/test/GLNext/FILE0.lin...  16   24   3
3  C:/Abc/bcd/Def/Test/temp/test/GLNext/FILE0.lin...  87  116  69
4  C:/Abc/bcd/Def/Test/temp/test/GLNext/FILE0.lin...  75   95  61
5  C:/Abc/bcd/Def/Test/temp/test/GLNext/FILE0.lin...   0    0 NaN
6  C:/Abc/bcd/Def/Test/temp/test/GLNext/FILE0.lin...   0    0   0
-------------------------------------------
                            0   1    2   3
0                  FILE0.frag   0    0   0
1                  FILE0.vert   0    0   0
2      FILE0.link-link-0.frag  16   24   3
3      FILE0.link-link-0.vert  87  116  69
4  FILE0.link-link-0.vert.bin  75   95  61
5           FILE0.link-link-0   0    0 NaN
6           FILE0.link-link-6   0    0   0

",239
31239329,31283857,2,"So i tried something like this
for dirName, subdirList, fileList in os.walk(Directory):
    for fname in fileList:
        if fname.endswith('.csv'):
            for line in fileinput.input(os.path.join(dirName, fname), inplace = 1):
                location = line.find(r'GLNext')
                if location > 0:
                    location += len('GLNext')
                    print line.replace(line[:location], ""."")
                else:
                    print line

",83
31239329,31239470,2,"^\S+/

You can simply use this regex over each line and replace by empty string.See demo.
https://regex101.com/r/cK4iV0/17
import re
p = re.compile(ur'^\S+/', re.MULTILINE)
test_str = u""C:/Abc/Def/Test/temp/test/GLNext/FILE0.frag                   0   0   0\nC:/Abc/Def/Test/temp/test/GLNext/FILE0.vert                   0   0   0\nC:/Abc/Def/Test/temp/test/GLNext/FILE0.link-link-0.frag       16  24  3\nC:/Abc/Def/Test/temp/test/GLNext/FILE0.link-link-0.vert       87  116 69\nC:/Abc/Def/Test/temp/test/GLNext/FILE0.link-link-0.vert.bin   75  95  61\nC:/Abc/Def/Test/temp/test/GLNext/FILE0.link-link-0            0   0\nC:/Abc/Def/Test/temp/test/GLNext/FILE0.link-link-6            0   0   0 ""
subst = u"" ""

result = re.sub(p, subst, test_str)

",86
31239329,31239445,2,"What about something like,
import csv

with open(""file.csv"", 'rb') as f:
    sl = []
    csvread = csv.reader(f, delimiter=' ')
    for line in csvread:
        sl.append(line.replace(""C:/Abc/Def/Test/temp\.\test\GLNext\"", """"))

To write the list sl out to filenew use,
with open('filenew.csv', 'wb') as f:
    csvwrite = csv.writer(f, delimiter=' ')
    for line in sl:
        csvwrite.writerow(line)

",94
31239329,31239586,2,"You can automatically detect the common prefix without the need to hardcode it. You don't really need regex for this. os.path.commonprefix can be used 
instead:
import csv
import os

with open('data.csv', 'rb') as csvfile:
    reader = csv.reader(csvfile)
    paths = [] #stores all paths
    rows = [] #stores all lines
    for row in reader:
        paths.append(row[0].split(""/"")) #split path by ""/""
        rows.append(row)

    commonprefix = os.path.commonprefix(paths) #finds prefix common to all paths

    for row in rows:
        row[0] = row[0].replace('/'.join(commonprefix)+'/', """") #remove prefix

rows now has a list of lists which you can write to a file
with open('data2.csv', 'wb') as csvfile:
    writer = csv.writer(csvfile)
    for row in rows:
        writer.writerow(row)

",179
31239329,31239329,1,"I have a csv file which contains 65000 lines (Size approximately 28 MB). In each of the lines a certain path in the beginning is given e.g. ""c:\abc\bcd\def\123\456"". Now let's say the path ""c:\abc\bcd\"" is common in all the lines and rest of the content is different. I have to remove the common part (In this case ""c:\abc\bcd\"") from all the lines using a python script. For example the content of the CSV file is as mentioned.
C:/Abc/bcd/Def/Test/temp/test/GLNext/FILE0.frag                   0   0   0
C:/Abc/bcd/Def/Test/temp/test/GLNext/FILE0.vert                   0   0   0
C:/Abc/bcd/Def/Test/temp/test/GLNext/FILE0.link-link-0.frag       16  24  3
C:/Abc/bcd/Def/Test/temp/test/GLNext/FILE0.link-link-0.vert       87  116 69
C:/Abc/bcd/Def/Test/temp/test/GLNext/FILE0.link-link-0.vert.bin   75  95  61
C:/Abc/bcd/Def/Test/temp/test/GLNext/FILE0.link-link-0            0   0
C:/Abc/bcd/Def/Test/temp/test/GLNext/FILE0.link-link-6            0   0   0 

In the above example I need the output as below
FILE0.frag                  0   0   0
FILE0.vert                  0   0   0
FILE0.link-link-0.frag      17  25  2
FILE0.link-link-0.vert      85  111 68
FILE0.link-link-0.vert.bin  77  97  60
FILE0.link-link-0               0   0
FILE0.link                  0   0   0

Can any of you please help me out with this?
",189
31239904,31239904,1,"Can I run a Python folder or directory as a whole to execute all the .py files in it?
Edit: I'm using Windows Powershell.
",28
31239904,31240010,2,"For bash, This was already answered at Run all Python files in a directory
You can run:
for f in *.py; do python ""$f""; done

If you're on Powershell, You can use:
Get-Childitem -Path c:\path\to\scripts -Filter *.py | % {python $_.FullName}

EDIT: Like Duncan said, This is a shorter solution on Powershell:
ls C:\path\to\scripts\*.py | %{ python $_.Fullname}

",81
31239904,31240193,2,"Try this:
import os
path = 'path\\to\\your\\directory\\'
files = os.listdir (path)
for i in files:
    if i.endswith('.py'):
        os.system(""python ""+path+i)

",33
31239904,31241387,2,"In Powershell you can use:
Get-Childitem -Path c:\to\folder\ -Filter *.py | % {& $_.FullName}

",20
31239911,31239911,1,"my small flask project is run normally on python2, but when i upgrade to python3, meet some problems:
first, my files' tree:
.
├── app
│   ├── app1
│   │   ├── app1.py
│   │   ├── __init__.py
│   ├── __init__.py
│   ├── templates
│        ├── base.html
│        └── index.html
├── config.py
├── README.md
├── requirements.txt
├── run.py
├── shell.py
└── test.db

run.py:
from app import app
app.run(debug=True)

app/__init__.py:
from flask import Flask, url_for, render_template
from flask_admin import Admin
from flask_admin.contrib.sqla import ModelView
from flask.ext.sqlalchemy import SQLAlchemy
from flask_login import LoginManager

app = Flask(__name__)
app.config.from_object('config')
db = SQLAlchemy(app)

login_manager = LoginManager()
login_manager.init_app(app)

from app1.app1 import mod as app1

when i use python2 to run:
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat

no problems, but on python3:
Traceback (most recent call last):
  File ""run.py"", line 4, in <module>
    from app import app
  File ""/home/chenhj/flask/multiapp/app/__init__.py"", line 18, in <module>
    from app1.app1 import mod as app1
ImportError: No module named 'app1'

anyone knows why? rookie to python3 and need help :)
",224
31239911,31240028,2,"You are using implicit relative imports (the Python 2 model); you need to use absolute references or explicit relative imports:
from .app1.app1 import mod as app1

where the leading . signals that the rest is relative to the current package, or
from app.app1.app1 import mod as app1

See PEP 328 - Imports: Multi-Line and Absolute/Relative.
You probably will have other issues however. Porting is not that trivial, read up on the issues in the Porting to Python 3 book. This issue is  a common migration problem.
",97
31242920,31244618,2,"Here is another alternative. This is a generic solution to remove any unquoted text:
def only_quoted_text(text):
    output = []
    in_quotes=False

    for letter in a:
        if letter == '""':
            in_quotes = not in_quotes
            output.append(letter)
        elif in_quotes:
            output.append(letter)

    return """".join(output)  


a = ""list of \""java jobs in delhi\"" delhi and \"" python jobs in mumbai \"" mumbai""

print only_quoted_text(a)

The output would be:
""java jobs in delhi"""" python jobs in mumbai ""

It also displays text if the final quote is missing.
",118
31242920,31243111,2,"Use re.sub
>>> a = ""\""java jobs in delhi\"" delhi""
>>> re.sub(r'\bdelhi\b(?=(?:""[^""]*""|[^""])*$)', r'', a)
'""java jobs in delhi"" '
>>> re.sub(r'\bdelhi\b(?=(?:""[^""]*""|[^""])*$)', r'', a).strip()
'""java jobs in delhi""'

OR
>>> re.sub(r'(""[^""]*"")|delhi', lambda m: m.group(1) if m.group(1) else """", a)
'""java jobs in delhi"" '
>>> re.sub(r'(""[^""]*"")|delhi', lambda m: m.group(1) if m.group(1) else """", a).strip()
'""java jobs in delhi""'

",200
31242920,31242920,1,"I have a string like this:
a = ""\""java jobs in delhi\"" delhi""

I want to replace delhi with """". But only delhi which lies outside the double-quotes. So, the output should look like this:
""\""java jobs in delhi\""""

The string is a sample string.The substring not necessarily be ""delhi"".The substring to replace can occur anywhere in the input string. The order and number of quoted and unquoted parts in the string is not fixed
.replace() replaces both the delhi substrings. I can't use rstrip either as it wont necessarily appear at the end of the string. How can I do this?
",128
31242920,31243216,2,"As a general way you can use re.split and a list comprehension :
>>> a = ""\""java jobs in delhi\"" delhi \""another text\"" and this""
>>> sp=re.split(r'(\""[^""]*?\"")',a)
>>> ''.join([i.replace('dehli','') if '""' in i else i for i in sp])
'""java jobs in delhi"" delhi ""another text"" and this'

The re.split() function split your text based on sub-strings that has been surrounded with "" :
['', '""java jobs in delhi""', ' delhi ', '""another text""', ' and this']

Then you can replace the dehli words which doesn't surrounded with 2 double quote!  
",161
31243002,31243002,1,"Given a set of points describing some trajectory in the 2D plane, I would like to provide a smooth representation of this trajectory with local high order interpolation. 
For instance, say we define a circle in 2D with 11 points in the figure below. I would like to add points in between each consecutive pair of points in order or produce a smooth trace. Adding points on every segment is easy enough, but it produces slope discontinuities typical for a ""local linear interpolation"". Of course it is not an interpolation in the classical sense, because

the function can have multiple y values for a given x 
simply adding more points on the trajectory would be fine (no continuous representation is needed).

so I'm not sure what would be the proper vocabulary for this.

The code to produce this figure can be found below. The linear interpolation is performed with the lin_refine_implicit function. I'm looking for a higher order solution to produce a smooth trace and I was wondering if there is a way of achieving it with classical functions in Scipy? I have tried to use various 1D interpolations from scipy.interpolate without much success (again because of multiple y values for a given x).
The end goals is to use this method to provide a smooth GPS trajectory from discrete measurements, so I would think this should have a classical solution somewhere.
import numpy as np
import matplotlib.pyplot as plt

def lin_refine_implicit(x, n):
    """"""
    Given a 2D ndarray (npt, m) of npt coordinates in m dimension, insert 2**(n-1) additional points on each trajectory segment
    Returns an (npt*2**(n-1), m) ndarray
    """"""
    if n > 1:
        m = 0.5*(x[:-1] + x[1:])
        if x.ndim == 2:
            msize = (x.shape[0] + m.shape[0], x.shape[1])
        else:
            raise NotImplementedError

        x_new = np.empty(msize, dtype=x.dtype)
        x_new[0::2] = x
        x_new[1::2] = m
        return lin_refine_implicit(x_new, n-1)
    elif n == 1:
        return x
    else:
        raise ValueError
n = 11
r = np.arange(0, 2*np.pi, 2*np.pi/n)
x = 0.9*np.cos(r)
y = 0.9*np.sin(r)
xy = np.vstack((x, y)).T
xy_highres_lin = lin_refine_implicit(xy, n=3)

plt.plot(xy[:,0], xy[:,1], 'ob', ms=15.0, label='original data')
plt.plot(xy_highres_lin[:,0], xy_highres_lin[:,1], 'dr', ms=10.0, label='linear local interpolation')
plt.legend(loc='best')
plt.plot(x, y, '--k')
plt.xlabel('X')
plt.ylabel('Y')
plt.title('GPS trajectory')
plt.show()

",532
31243002,31244679,2,"I would suggest you try to transform your cartesian coordinates into polar coordinates, that should allow you to use the standard scipy.interpolation without issues as you won't have the ambiguity of the x->y mapping anymore.
",40
31243002,31335255,2,"This is called parametric interpolation. 
scipy.interpolate.splprep provides spline approximations for such curves. This assumes you know the order in which the points are on the curve.
If you don't know which point comes after which on the curve, the problem becomes more difficult. I think in this case, the problem is called manifold learning, and some of the algorithms in scikit-learn may be helpful in that.
",75
31243014,31247817,2,"I see @user3510686 has already answered it. Posing what I tried.
a={}
for i in range(10):
    a[i]=np.random.rand(10)

",33
31243014,31243288,2,"You can use a python dictionary for this purpose
For example
import numpy as np
dic={}
dic['1']=np.zeros(3)
dic['2']=np.ones(5)
print dic['1']
print dic['2']

now dic['1'] and dic['2'] are you arrays
",63
31243014,31243014,1,"Currently, I have this piece of code to create a numpy array
X=np.array([[]]);

if (X.shape[1] == 0):
      X = np.array([vd]);
    else:
      X = np.concatenate((X,np.array([vd])));

I would now like to get multiple numpy arrays X(1) , X(2) etc for different conditions. What is the best way to do this in python. In matlab I can accomplish this using matlab struct.
",102
31243044,31243189,2,"You are not setting the property. You are manipulating a mutable object.
The assignment is not on the property itself, but on a subscription, the [..] part addresses a dictionary key. You could assign the property to a new name and still manipulate that dictionary:
parrot = Parrot()
parrot_voltage = parrot.voltage
parrot_voltage[0] = 100

but you cannot set the property to a new dictionary or a different type of object altogether. This applies to all mutable objects used in a property; lists, sets, instances, etc.
",105
31243044,31243044,1,"@property defined as int
The following code is taken from Python Docs:
class Parrot(object):
    def __init__(self):
        self._voltage = 100000

    @property
    def voltage(self):
        """"""Get the current voltage.""""""
        return self._voltage

When I run:
parrot = Parrot()
print(parrot.voltage)
parrot.voltage = 100
print(parrot.voltage)

I get the following output (as expected, as no setter is defined)
{0: 100000}
Traceback (most recent call last):
   File ""prop.py"", line 13, in <module>
     parrot.voltage = 100
 AttributeError: can't set attribute

@property defined as dict
However, if I define self._voltage = {} the property becomes writeable:
class Parrot(object):
    def __init__(self):
        self._voltage = {}
        self._voltage[0] = 100000

    @property
    def voltage(self):
        """"""Get the current voltage.""""""
        return self._voltage

parrot = Parrot()
print(parrot.voltage)
parrot.voltage[0] = 100
print(parrot.voltage)

The output is then:
{0: 100000}
{0: 100}

Same behavior in Python 2.7.9 and Python 3.4.3. Why is the property writeable, even if no setter is explicitly defined in the code? Here it was proposed to subclass dict to get this behavior. However, it seems that this not required.
",262
31243052,31245636,2,"Yes, it's possible to do it with recursion. You can make combine_n return a list of tuples with all the combinations beginning at index cur_index, and starting with a partial combination of cur_combo, which you build up as you recurse:
def combine_n(elements, r, cur_index=0, cur_combo=()):
    r-=1
    temp_list = []
    for elem_index in range(cur_index, len(elements)-r):
        i = elements[elem_index]
        if r > 0:
            temp_list = temp_list + combine_n(elements, r, elem_index+1, cur_combo+(i,))
        else:
            temp_list.append(cur_combo+(i,))
    return temp_list

elements = list(range(1,6))
print = combine_n(elements, 3)

output:
[(1, 2, 3), (1, 2, 4), (1, 2, 5), (1, 3, 4), (1, 3, 5), (1, 4, 5), (2, 3, 4), (2, 3, 5), (2, 4, 5), (3, 4, 5)]

The for loop only goes up to len(elements)-r, because if you go further than that then there aren't enough remaining elements to fill the remaining places in the tuple. The tuples only get added to the list with append at the last level of recursion, then they get passed back up the call stack by returning the temp_lists and concatenating at each level back to the top.
",296
31243052,31243052,1,"Yesterday, I encountered a problem which requires calculating combinations in an iterable with range 5.
Instead of using itertools.combination, I tried to make a primitive function of my own. It looks like:
def combine_5(elements):
    """"""Find all combinations in elements with range 5.""""""
    temp_list = []
    for i in elements:
        cur_index = elements.index(i)
        for j in elements[cur_index+1 : ]:
            cur_index = elements.index(j)
            for k in elements[cur_index+1 : ]:
                cur_index = elements.index(k)
                for n in elements[cur_index+1 : ]:
                    cur_index = elements.index(n)
                    for m in elements[cur_index+1 : ]:
                        temp_list.append((i,j,k,n,m))

    return temp_list

Then I thought maybe I can abstract it a bit, to make a combine_n function. And below is my initial blueprint:
# Unfinished version of combine_n
def combine_n(elements, r, cur_index=-1):
    """"""Find all combinations in elements with range n""""""
    r -= 1 
    target_list = elements[cur_index+1 : ]
    for i in target_list:
        cur_index = elements.index(i)
        if r > 0:
            combine_n(elements, r, cur_index)
            pass
        else:
            pass

Then I've been stuck there for a whole day, the major problem is that I can't convey a value properly inside the recursive function. I added some code that fixed one problem. But as it works for every recursive loop, new problems arose.  More fixes lead to more bugs, a vicious cycle.
And then I went for help to itertools.combination's source code. And it turns out it didn't use recursion technique.   
Do you think it is possible to abstract this combine_5 function into a combine_n function with recursion technique? Do you have any ideas about its realization?

FAILURE SAMPLE 1:
def combine_n(elements, r, cur_index=-1):
    """"""Find all combinations in elements with range n""""""
    r -= 1 
    target_list = elements[cur_index+1 : ]
    for i in target_list:
        cur_index = elements.index(i)
        if r > 0:
            combine_n(elements, r, cur_index)
            print i
        else:
            print i


This is my recent try after a bunch of overcomplicated experiments.
  The core ideas is: if I can print them right, I can collect them into a container later.
  But the problem is, in a nested for loop, when the lower for-loop hit with an empty list.
  The temp_list.append((i,j,k,n,m)) clause of combine_5 will not work.
  But in FAILURE SAMPLE 1, it still will print the content of the upper for-loop
  like combine_n([0,1], 2) will print 2, 1, 2.
  I need to find a way to convey this empty message to the superior for-loop.
  Which I didn't figure out so far.  

",548
31243172,31450804,2,"The solution to my problem is: 
MaxPlus.Core.EvalMAXScript(WM3_MC_BuildFromNode(for mod in $node.modifiers where isKindOf mod Morpher collect mod)[1] 3 $target)

This solution is found by Swordslayer on the autodesk forum for 3ds Max
",43
31243172,31243172,1,"I used the following code based on the information given in help.autodesk.com for executing maxscript in Python:
import MaxPlus
test = MaxPlus.FPValue()
#The target node has only one morpher and I want to retrieve it using
# .morpher[1]
bool = MaxPlus.Core.EvalMAXScript(""WM3_MC_BuildFromNode $node.morpher[1] 1 $target"", test)
print bool

If I print the boolean, this always print: ""false"". However the following code works (aka the print statement returns true): 
import MaxPlus
test = MaxPlus.FPValue()
#The target node has only one morpher
bool = MaxPlus.Core.EvalMAXScript(""WM3_MC_BuildFromNode $node.morpher 1 $target"", test)
print bool

However I cannot use the latter code since it must be possible in my code that a node has multiple morphers.
Is there a better way using the Python api for maxscript (I didn't find a method) or can anyone give suggestions how the first code can be improved.
Thank you
",182
31243290,36060258,2,"Your option number 2 uses a different data set (xytest) than your version number (1), which uses xtest. Furthermore, your crossvalidation should include the training, not only the prediction.
Apart from that they should be the same, while I advice you to use pipelines.
",55
31243290,31243290,1,"Does running a standard scaler and then a classifier give the same result as using a pipeline?
Hi, I have a classification problem and trying to scale the X variables using scikit learn's StandardScaler(). I see two options of doing this, should they in theory yield the same result? Because I am getting better precision score on my test data set when I use option (1).
(1) 
scalar = StandardScaler()
xtrain_ = scalar.fit_transform(xtrain)
RFC = RandomForestClassifier(n_estimators=100)
RFC.fit(xtrain. ytrain)

xtest_ = scalar.transform(xtest)
score = cross_val_score(RFC, xtest_, ytest,cv=10, scoring ='precision')

(2)
RFCs = Pipeline([(""scale"", StandardScaler()), (""rf"", RandomForestClassifier(n_estimators=100))])
RFCs.fit(xtrain, ytrain)
scores = cross_val_score(RFCs, xytest, ytest, cv=10, scoring='precision')

",176
31243352,31243643,2,"This could be done in 2 steps, generate a new column that creates the expanded str values, then groupby on 'A' and apply list to this new column:
In [62]:
df['expand'] = df.apply(lambda x: ','.join([x['B']] * x['quantity']), axis=1)
df.groupby('A')['expand'].apply(list)

Out[62]:
A
1    [foo, baz,baz, bar,bar, faz]
2                  [foo,foo, bar]
3                   [foo,foo,foo]
Name: expand, dtype: object

EDIT
OK after taking inspiration from @Jianxun Li's answer:
In [130]:
df.groupby('A').apply(lambda x: np.repeat(x['B'].values, x['quantity']).tolist())

Out[130]:
A
1    [foo, baz, baz, bar, bar, faz]
2                   [foo, foo, bar]
3                   [foo, foo, foo]
dtype: object

Also this works:
In [131]:
df.groupby('A').apply(lambda x: list(np.repeat(x['B'].values, x['quantity'])))

Out[131]:
A
1    [foo, baz, baz, bar, bar, faz]
2                   [foo, foo, bar]
3                   [foo, foo, foo]
dtype: object

",287
31243352,31244334,2,"Another way to do it. First reshape the df using pivot_table and then apply np.repeat().tolist().
import pandas as pd
import numpy as np

df

Out[52]: 
   A    B  quantity
0  1  foo         1
1  1  baz         2
2  1  bar         2
3  1  faz         1
4  2  foo         2
5  2  bar         1
6  3  foo         3

df.pivot('A','B','quantity').fillna(0).apply(lambda row: np.repeat(row.index.values, row.values.astype(int)).tolist(), axis=1)

Out[53]: 
A
1    [bar, bar, baz, baz, faz, foo]
2                   [bar, foo, foo]
3                   [foo, foo, foo]
dtype: object

",141
31243352,31243352,1,"My initial DataFrame looks as follows:
   A    B  quantity
0  1  foo         1
1  1  baz         2
2  1  bar         2
3  1  faz         1
4  2  foo         2
5  2  bar         1
6  3  foo         3

I need to group it by 'A' and make a list of 'B' multiplied by 'quantity':
   A                               B
0  1  [foo, baz, baz, bar, bar, faz]
1  2                 [foo, foo, bar]
2  3                 [foo, foo, foo]

Currently I'm using groupby() and then apply():
def itemsToList(tdf, column):

    collist = []
    for row in tdf[column].iteritems():
        collist = collist + tdf['quantity'][row[0]]*[row[1]]

    return pd.Series({column: collist})

gb = df.groupby('A').apply(itemsToList, 'B')

I doubt it is an efficient way, so I'm looking for a good, ""pandaic"" method to achieve this.
",198
31243355,31243355,1,"I have some data file which is array of arrays...For example:
[[142 132 138 ..., 130 128 129]
[137 134 135 ..., 124 117 124]
[127 138 131 ..., 131 131 127]
..., 
[129 131 136 ..., 123 130 117]
[134 141 133 ..., 133 124 121]
[133 138 136 ..., 131 122   0]]

This file has 2.8 MiB and type >i2.. File is fits and I know that its type is int16, but what does >i2 mean in Python?
How I can convert this data file to float?
So I have three data fits files, Dark frame, flat field and image. I need create correction of image. This data files I need convert to float and make some operation (divide and difference of data files values) a then convert result to origin data type int.
I created script which make corrections but I have problem that the result correction image has 6.3 MiB and don't has 2.8 MiB:
import numpy as np
import pyfits
from matplotlib import pyplot as plt
import glob


dark=glob.glob('.../ha/dark/*.fits')
flat=glob.glob('.../ha/flat/*.fits')
img=glob.glob('.../ha/*.fits')

sumd0 = pyfits.open(dark[0])
sumdd=sumd0[0].data
sumdd.astype(float)
for i in range(1,len(dark)):
     sumdi=pyfits.open(dark[i])
     sumdi=sumdi[0].data
     sumdd=sumdd.astype(float)+sumdi.astype(float)
dd=sumdd/len(dark)

sumf0 = pyfits.open(flat[0])
sumff=sumf0[0].data
sumff.astype(float)
for i in range(1,len(flat)):
     sumfi=pyfits.open(flat[i])
     sumfi=sumfi[0].data
     sumff=sumff.astype(float)+sumfi.astype(float)

ff=sumff/len(flat)

df=(ff-dd)

for n in range(len(img)):
    with pyfits.open(img[n],mode='update',memmap=True) as im:
        imgg=im[0].data
        header=im[0].header
        imgg.astype(float)
        imgg=(imgg-dd)/df
        imgg.astype(int)
plt.imshow(imgg,cmap=plt.cm.Greys_r,vmin=0.5,vmax=1.5)
plt.show()

",409
31243355,31243553,2,"From the numpy docs
>>> import numpy as np
>>> big_end_arr = np.ndarray(shape=(2,),dtype='>i2', buffer=big_end_str)
>>> big_end_arr[0]
1
>>> big_end_arr[1]
770


Note the array dtype above of >i2. The > means ‘big-endian’ (< is little-endian) and i2 means ‘signed 2-byte integer’. For example, if our data represented a single unsigned 4-byte little-endian integer, the dtype string would be < u4 .

The  type specifiers  are listed in the structured array docs.
",110
31243376,36051405,2,"You need to pass an instance of MyTextCompleter, not the class itself, to wx.TextCtrl.AutoComplete().  Change this:
        basicText.AutoComplete(MyTextCompleter)

to
        basicText.AutoComplete(MyTextCompleter())

",33
31243376,31247746,2,"I should have warned you more thouroghly: wxPython Phoenix is the future of wxPython (because, in contrast to classic it supports Python 3, too). That said, this does not mean everything is nice and shiny. My personal advice is to keep on going with classic (or in other words: what works now in classic, will most probably also work in Phoenix). In Phoenix, you will stumble into bugs like this more often.
Luckily, in this special case, there has already been something else done:
<wx.TextCtrl>.AutoComplete(…) does accept a list of strings. This already works in 2.9.0/classic. See documentation for wx.TextEntry/AutoComplete.
",127
31243376,31243376,1,"I'm using wxPython (Phoenix).
I wrote a small app with a custom autocompleter, according to these guidelines, but it fails with the following error:
Traceback (most recent call last):
  File ""try2.py"", line 33, in <module>
    frame = TextFrame()
  File ""try2.py"", line 26, in __init__
    basicText.AutoComplete(MyTextCompleter)
TypeError: TextEntry.AutoComplete(): arguments did not match any overloaded call:
  overload 1: argument 1 has unexpected type 'sip.wrappertype'
  overload 2: argument 1 has unexpected type 'sip.wrappertype' 

This is the code:
import wx
class MyTextCompleter(wx.TextCompleterSimple):

    def __init__(self):
        wx.TextCompleterSimple.__init__(self)

    def GetCompletions(self, prefix, res):
        if prefix == ""a"":
            res.append(""This order is"")
            res.append(""very important"")

        elif firstWord == ""b"":
            res.append(""z - It's not in"")
            res.append(""a - lexicographic order"")

        else:
            res.append(""bye"")

class TextFrame(wx.Frame):
    def __init__(self):
        wx.Frame.__init__(self, None, -1, 'Text Entry Example', size=(300, 100))
        panel = wx.Panel(self, -1) 
        basicLabel = wx.StaticText(panel, -1, ""Basic Control:"")
        basicText = wx.TextCtrl(panel, -1, ""I've entered some text!"", size=(175, -1))
        basicText.SetInsertionPoint(0)
        basicText.AutoComplete(MyTextCompleter)

        sizer = wx.FlexGridSizer(cols=2, hgap=6, vgap=6)
        sizer.AddMany([basicLabel, basicText])
        panel.SetSizer(sizer)

app = wx.PySimpleApp()
frame = TextFrame()
frame.Show()
app.MainLoop()

When commenting out the basicText.AutoComplete(MyTextCompleter) it runs successfully (without the autocompletion)
",333
31243388,31243684,2,"You still need to check that the action is a POST, and that the forms are valid, and you must redirect after a successful submission.
def create(request):
    voteTypeForm = VoteTypeForm(request.POST or None)
    voteForm = VoteForm(request.POST or None)
    if request.method == 'POST':
        # check validity separately to avoid short-cutting
        vote_type_valid = voteTypeForm.is_valid()
        vote_form_valid = voteForm.is_valid()
        if vote_type_valid and vote_form_valid:
            instance = voteTypeForm.save(commit=False)
            instance.pub_date = timezone.now()
            instance.save()
            instance2 = voteForm.save(commit=False)
            instance2.save()
            return redirect('<view-you-redirect-to-on-success'>
    context = RequestContext(request,{
            'voteTypeForm': voteTypeForm,
            'voteForm': voteForm,
    })
    return render(request, 'Vote/create.html', context)

",138
31243388,31243722,2,"The easiest way to do it is by making ajax request when you push the submit button.
Considering you have a form 'voteForm', try loading this form using django's inbuilt template as: {{voteForm.as_p}}
This will create your form for, which you have already done.
Now when you press submit button, make an ajax request with your form data in it.
The ajax request will take your data to the form and reverts back with a response which you can use to further do the processing.
A quick example for ajax request would be:
    function youfunctionname()
        $.ajax({
                   type: ""POST"",
                   url: url,
                   data: $(""#yourformname"").serialize(), // serializes the form's elements.
                   success: function(data)
                   {
                       alert(data);
                   }
                 });
  }

",163
31243388,31243388,1,"I am currently trying to learn django. I decided to create a small app. currently I am making a form to create VoteType and Voting candidates on one page. I created a page where u can add as many candidate fields as you want, but when I click the button nothing happenes and even if I don't click the button some data is saved. I was watching this django guide on youtube. This guy is making one simple form. He added method = POST and action = '' to  ...  and in views he used (request.POST or None). I tried to do the similar, but as my form is a bit more complicated I got really confused. 
so this is my views.py code:
def create(request):
    voteTypeForm = VoteTypeForm(request.POST or None)
    voteForm = VoteForm(request.POST or None)
    instance = voteTypeForm.save(commit=False)
    instance.pub_date = timezone.now()
    instance.save()
    instance2 = voteForm.save(commit=False)
    instance2.save()
    #print instance.pub_date
    context = RequestContext(request,{
            'voteTypeForm': voteTypeForm,
            'voteForm': voteForm,
    })
    return render(request, 'Vote/create.html', context)

and this is my create.html django template:
{% load staticfiles %}
    <link rel=""stylesheet"" type=""text/css"" href=""{% static 'Vote/style.css' %}"" />
<fieldset id=""fieldset"">
    <form method = 'POST' action = ''>{%csrf_token %}
        <p>{{ voteTypeForm }}</p>
    </form>
        <div id=""placeholder"">

        </div>
        <p>
            <button type=""button"" name=""Submit"" onclick=""Add();"">+</button>
        </p>
    <input type = 'submit' value=""create""/>
</fieldset>
<script type='text/javascript'>
{#    document.write(code);#}
    var _counter = 0;
    var template = document.createTextNode('')
    function appendStringAsNodes(element, html) {
        var frag = document.createDocumentFragment(),
            tmp = document.createElement('body'), child;
        tmp.innerHTML = html;
        // Append elements in a loop to a DocumentFragment, so that the browser does
        // not re-render the document for each node
        while (child = tmp.firstChild) {
            frag.appendChild(child);
        }
        element.appendChild(frag); // Now, append all elements at once
        frag = tmp = null;
    }
    function Add() {
        var code = '<div id=""template"">' +
                '<p>' +
                    '<fieldset id=""fieldsets"">' +
                        '<legend id=""legends"">Candidate No ['+ String(_counter+1) +']</legend>' +
                       ' <form method = ""POST"" action = """">'+
                              '<input type=""hidden"" name=""csrfmiddlewaretoken"" value=""{{csrf_token }}"" />' +
                            '<p><label for=""id_name"">Name:</label> <input id=""id_name"" maxlength=""50"" name=""name"" type=""text"" /></p>'+
                            '<p><label for=""id_image"">Image:</label> <input id=""id_image"" name=""image"" type=""file"" /></p>'+
                        '</form>' +
                   ' </fieldset>' +
                '</p>' +
            '</div>';
        _counter++;
        appendStringAsNodes(document.getElementById(""placeholder""),code);
        document.getElementById(""someInput"").value = _counter;
    }
</script>

how do I fix this code so that my program only saves instances when I push the create button?
",693
31243476,31243859,2,"Though you could do this, it's generally recommended that you open files using with as that is designed to handle errors by closing the file no matter what happens. You can still pass it to your Labyrinth's init just by passing a reference.
with open(file) as f:
    labyrinth = Labyrinth(f)

Then if any error occurs, the file is still safely closed, and you've also passed your open file to the Labyrinth object anyway.
",89
31243476,31244797,2,"You are at the design phase. 
So, you have to weigh the chances of losing your data due to some error or crash against the importance of your data and the cost of ""protecting"" it.
Using with protects you from some errors. If you consider that python itself may crash (e.g.), then you still have some risk. Saving after each step is evidently safer. How useful it is depends on the volume of saved data (and there are techniques for reducing this as well), the impact of each save on performance, and the chances of such crashes.
Without any further info, and simply guessing, my answer to your specific question:

... is it ok to open the file in the init function of Labyrinth and
  close it at the end of the game...? Or is it better to open and close
  the file every time?

is that I would save after each step.
",177
31243476,31243476,1,"I have a design question. I'm doing an exercise in python (2.7) which is a simple game with a labyrinth. I need to read an write from a specific file every step of the game.
Currently I have 2 classes (Game and Labyrinth). The Labyrinth class is responsible for reading and writing the file.
My question is, is it ok to open the file in the init function of Labyrinth and close it at the end of the game within another function (which can be called from another class)? Or is it better to open and close the file every time?
The reason I don't save the file content into a string with readlines() is because I'm supposed to save to the file each step of the game.
",148
31244238,31244391,2,"If you want ""to get the details"" for output, you can try Python's Data pretty printer:

The pprint module provides a capability to “pretty-print” arbitrary
  Python data structures...

Its output is very much configurable, and you can go down the structure to an arbitrary depth, with depth=....
If you want ""to get the details"" for using it elsewhere, 
I would suggest a few things:

Try not indexing with [0], you may be losing information stored somewhere else in Attributes.
Check available methods with How do I get list of methods in a Python class?, or Finding what methods an object has.
Check type with What's the canonical way to check for type in python?.

With the information obtained on Attributes you should be able to extract any info stored in it.
",158
31244238,31244238,1,"I am using Boto in Python to connect to Amazon MWS. I have successfully connected using their scripts but am having trouble parsing the response as I don't fully understand the documentation, and there are little to no examples on the internet. I am new to Python.
Here is how I get my response from MWS:
mws = MWSConnection(accessKeyId,secretKey,Merchant=merchantId)
response = mws.list_matching_products(MarketplaceId=marketplaceId,Query=""Beanie Babies"")

The first product gives a response of this:
products = response.ListMatchingProductsResult.Products.Product
print(products[0])
>>>Product{}(Identifiers: ^Identifiers^{}(MarketplaceASIN: ^MarketplaceASIN^{}(MarketplaceId: 'ATVPDKIKX0DER', ASIN: 'B000JK67MQ'), SKUIdentifier: None), Offers: None, CompetitivePricing: [], AttributeSets: ^AttributeSets^{}(ItemAttributes: [ItemAttributes{'xml:lang': 'en-US'}(Brand: 'Beanie Babies', Studio: 'Beanie Babies - Teddy Bears', ItemDimensions: 4.00inchesx10.00inchesx6.00inchesx0.31pounds, Languages: None, Binding: 'Toy', Genre: 'cute pets', Color: 'purple', MaterialType: [], Feature: ['Ty Beanie Baby', 'Princess Bear', 'Purple with purple bow with white flower', 'Does have the tag'], ManufacturerMaximumAge: 36{'Units': 'months'}, OperatingSystem: [], Artist: [], Director: [], ProductTypeName: 'TOYS_AND_GAMES', Creator: [], Edition: '1997', Model: '4300', SmallImage: Image{}(Height: 75{'Units': 'pixels'}, Width: 75{'Units': 'pixels'}, URL: 'http://ecx.images-amazon.com/images/I/4193jH4e35L._SL75_.jpg'), GemType: [], PackageDimensions: 0.90inchesx7.40inchesx4.50inchesx0.40pounds, PackageQuantity: '1', ListPrice: None, Actor: [], Platform: [], Manufacturer: 'Beanie Babies - Teddy Bears', PartNumber: '4300', ProductGroup: 'Toy', MediaType: [], IsMemorabilia: 'false', Label: 'Beanie Babies - Teddy Bears', ManufacturerMinimumAge: 36{'Units': 'months'}, IsAutographed: 'false', IsAdultProduct: 'false', Author: [], Format: [], Title: 'Ty Beanie Babies - Princess Bear', Publisher: 'Beanie Babies - Teddy Bears')]), LowestOfferListings: None, Relationships: ^Relationships^{}(VariationParent: []), SalesRankings: ^SalesRankings^{}(SalesRank: [SalesRank{}(Rank: '60161', ProductCategoryId: 'toy_display_on_website'), SalesRank{}(Rank: '1197', ProductCategoryId: '251943011'), SalesRank{}(Rank: '1609', ProductCategoryId: '11350120011')]))

My Issue is trying to get the details from the ItemAttributes:
Attributes = products[0].AttributeSets.ItemAttributes
print(Attributes[0])
>>>ItemAttributes{'xml:lang': 'en-US'}(Brand: 'Beanie Babies', Studio: 'Beanie Babies - Teddy Bears', ItemDimensions: 4.00inchesx10.00inchesx6.00inchesx0.31pounds, Languages: None, Binding: 'Toy', Genre: 'cute pets', Color: 'purple', MaterialType: [], Feature: ['Ty Beanie Baby', 'Princess Bear', 'Purple with purple bow with white flower', 'Does have the tag'], ManufacturerMaximumAge: 36{'Units': 'months'}, OperatingSystem: [], Artist: [], Director: [], ProductTypeName: 'TOYS_AND_GAMES', Creator: [], Edition: '1997', Model: '4300', SmallImage: Image{}(Height: 75{'Units': 'pixels'}, Width: 75{'Units': 'pixels'}, URL: 'http://ecx.images-amazon.com/images/I/4193jH4e35L._SL75_.jpg'), GemType: [], PackageDimensions: 0.90inchesx7.40inchesx4.50inchesx0.40pounds, PackageQuantity: '1', ListPrice: None, Actor: [], Platform: [], Manufacturer: 'Beanie Babies - Teddy Bears', PartNumber: '4300', ProductGroup: 'Toy', MediaType: [], IsMemorabilia: 'false', Label: 'Beanie Babies - Teddy Bears', ManufacturerMinimumAge: 36{'Units': 'months'}, IsAutographed: 'false', IsAdultProduct: 'false', Author: [], Format: [], Title: 'Ty Beanie Babies - Princess Bear', Publisher: 'Beanie Babies - Teddy Bears')

At this point I believe it is a dictionary object.
print(Attributes[0].values()) 
>>>dict_values(['en-US'])

As I am new to this language I can't figure out how to get all the information that is located in the () such as Brand, Studio, etc.
Boto has some built in functions such as Response and ResponseFactory, but I am lost again as I keep hitting a wall trying to get info such as Brand, etc..
Thank you again for any help you can give in this.
",930
31244268,31244440,2,"Have you profiled your code and have an idea where the hotspot is? If it is not computing, it's probably just the disk IO. I doubt you get a performance boost by tricks on the IO logic. In the end it's the sequential disk access that might be the limit. If you do have a RAID system it might makes sense to have multiple threads reading from the disk, but you could do that with python standard threads. Try to ramp up from 1 to a few and measure along the way to find the sweet spot.
The reason why you saw an improvement with gevent downloading images in parallel is that the network IO throughput can be improved a lot with multiple connections. A single network connection can hardly saturate the network bandwidth when the remote server is not directly attached to your network device. Whereas a single disk IO operation can easily saturate the disk throughput.
",171
31244268,31244268,1,"I need to load ~100k files with vectors and aggregate the content in a numpy array.  This process takes ~3mins so I want to speed it up. I tried to use gevent to speed it up, but I could not gain any speedup.
I read that one should use async calls to speed up IO calls and not multiprocessing. I further read that gevent is the recommended library. I wrote an example to download images, where I could see a huge improvement in speed. Here is a simplified version of my code
def chunks(l, n):
    """""" Yield successive n-sized chunks from l.
    """"""
    for i in xrange(0, len(l), n):
        yield l[i:i+n]

file_paths = # list of filenames
numpy_array = numpy.ones([len(file_paths), file_size])
pool = gevent.pool.Pool(poolsize)
for i, list_file_path_tuples in enumerate(chunks(file_paths, CHUNK_SIZE)):
    gevent_results = pool.map(numpy.load, list_file_path_tuples)
    pool.join()
    for i_chunk, result in enumerate(gevent_results):
        index = i * CHUNK_SIZE + i_chunk
        data = result['arr_0']
        numpy_array[index] = data

Using chunks is necessary, because otherwise I would have all the vectors twice in memory.
Is there an issue in my code or do I use the wrong approach?
",255
31247430,31247604,2,"How is could be done in Python. You did not give any code, so I give the basics only, no code as well:

put the root folder in a list
pop one item off the list


list the item
append the directories in this listing to the list
delete the files in the listing.

repeat the second step incl. substeps until the list is empty

Helpful: https://docs.python.org/2/library/ftplib.html
",76
31247430,31247430,1,"I need to write a script which deletes all files on an FTP server without changing the directory structure.
Since there is no find command or similar. 
I tried using lftp but got stuck since the rm -r is not flexible enough.
Anyway it could be bash or python.
I do not have ssh access to that server. FTP only.
",66
31247458,31247535,2,"You have a typo:
{% for product in products %)
and it should be
{% for product in products %}
see difference in bracket } not )
",32
31247458,31247533,2,"You have a syntax error in your template:
{% for product in products %)

should be:
{% for product in products %}

",28
31247458,31247458,1,"I am new to django&python. 
Using python 3.4.2 and django 1.8.
Trying to display a list of products and encountered an error:

""Exception Type:  TemplateSyntaxError 
Exception Value:   Invalid block tag: 'endfor'
Exception Location: myvirtualenv/lib/python3.4/site-packages/django/template/base.py in
  invalid_block_tag, line 395""

Can't figure out what's wrong. Found few related questions on stackoverflow but they did't help.
Give me a hint, please. Thanks in advance.
Views:
from django.shortcuts import render
from .models import Product

def list_items(request):
    products = Product.objects.all()
    return render(request, 'catalog/list_item.html', {'products': products})

list_item.html:
<html>
    <head>
        {% block title %}some title{% endblock %}
    </head>
    <body>
            {% for product in products %)
            {{ product }}
            {% endfor %}
    </body>
</html>

P.S Without   content, {% block title %} renders with no errors.
",182
31247460,31247751,2,"If this is for your own benefit, rather than something you need to show to others, you can use IPython notebooks and the %matplotlib nbagg backend, at least for Seaborn, e.g.:
%matplotlib nbagg
import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(0, 50, 100)
y = x**(0.5)

plt.plot(x, y)

If you don't already have IPython etc. set up, you can quickly test this out by creating a new notebook at try.jupyter.org, pasting the code into a cell, and hitting Shift + Enter to run. Since this is running on a free VM it will be slow, running the notebook locally will mean panning/zooming is much smoother.
",136
31247460,31247460,1,"I've been trying to find a way to make Seaborn and Vincent interactive so that I can, for example, zoom in/out in a specific area of the plot in real time. Is this possible to do? Alternatively, are there other recommended libraries (that are not cloud-based services) that work well for visualizing time series data?
",64
31247460,31253739,2,"I found that using Seaborn with mpld3 worked best for me, thanks.
",14
31247510,31247510,1,"I have red at the help-page of easy_install that I have an ability to do ""install in user site-package"". What does this phase mean, ""user site-package""? How does it affect functionality of the installed software?
",43
31247510,31247842,2,"User site-package refers to packages installed in ~/.local/lib[64]/python-VERSION/site-packages/
These packages are available as any other installed packages, but only to this specific user. It overrides system packages too.
",35
31247578,31247578,1,"I'm using python-social-auth for user login for my organization site. I only allow Google accounts in a white list (the people in my organization) to login to the site. However, I would like to preregister everyone in the database so that I can add their custom fields (leadership positions, etc.). Is it possible to add user accounts before their first login?
",72
31247578,31247744,2,"Try using pipelines in python-social-auth.

Create a custom pipeline by creating pipeline.py file and add your functions here.
A simple example of functions can be found here
",29
31247587,31247797,2,"How about this. 
from rae import Drae

drae = Drae()

if text.startswith('/'):
  if text.startswith('/define'):
    try:
      [command, data] = text.split(' ',1)
    except:
      send('Write: /define <word>')
    meanings = drae.search(data) 
    reply(str(meanings)) 
    setEnabled(chat_id, True)

",71
31247587,31247587,1,"I'm creating a telegram bot that searchs for words in a online dictionary, the problem comes when I need to create the command in Telegram for searching in that dictionary, at the momment I have this:
from rae import Drae
drae = Drae()

  if text.startswith('/'):
     if text =='/define':
        drae.search(u' ')  # The problem is here, I don't know how to implement the command and the word who the client wants to search.
        setEnabled(chat_id, True)


I'm using this Telegram API in Python: https://github.com/yukuku/telebot
And this API for the Dictionary: https://github.com/dialelo/rae-1

",120
31247647,31248670,2,"You need to store each imshow AxesImage in a list and inside update, loop over all of them and update each based on the slider,
import os
from matplotlib import pyplot as plt
from matplotlib.widgets import Slider
import numpy as np

import glob
import h5py
#Define the xy size of the mapped array
xsize=3
ysize=3

lengthh5=9
readlist=[]
for i in range (0,lengthh5):
    npraw=np.random.rand(200,50,50)
    readlist.append (npraw)

fig=plt.figure()
ls = []
for k in range (0,lengthh5):
    ax=fig.add_subplot(xsize,ysize,k)        
    frame = 10
    l = ax.imshow(readlist[k][frame,:,:]) 
    ls.append(l)
    plt.axis('off')

sframe = Slider(fig.add_subplot(50,1,50), 'Frame', 
                0, len(readlist[0])-1, valinit=0)

def update(val):
    frame = np.around(sframe.val)
    for k, l in enumerate(ls):
        l.set_data(readlist[k][frame,:,:])

sframe.on_changed(update)
plt.show()

",201
31247647,31247647,1,"I am quite new to Python, so please excuse if this is a stupid beginner's error. However I am struggling with it for quite some time.
I want to create a figure with n x m subplots, each subplot being np.array of shape [1024,264,264]. As I am looking for differences occuring in the stack along the 0-dimension I want to use a slider to explore all stacks in my figure simultaneously. 
The slider instance works nicely for a figure with one subplot but I can't bring them all to work.
That's the code I am using:
import os
from matplotlib import pyplot as plt
import numpy as np

import glob
import h5py
#Define the xy size of the mapped array
xsize=3
ysize=3

lengthh5=9
readlist=[]
for i in range (0,lengthh5):
    npraw=np.random.rand(200,50,50)
    readlist.append (npraw)

''' Slider visualization'''
from matplotlib.widgets import Slider
fig=plt.figure()
for k in range (0,lengthh5):
    ax=fig.add_subplot(xsize,ysize,k)        
    frame = 10
    l = ax.imshow(readlist[k][frame,:,:]) 
    plt.axis('off')
           sframe = Slider(fig.add_subplot(50,1,50), 'Frame', 0, len(readlist[0])-1, valinit=0)
    def update(val):
        frame = np.around(sframe.val)
        l.set_data(readlist[k][frame,:,:])


sframe.on_changed(update)

plt.show()

For this particular case I stripped it down to a 3x3 array for my figure and just create randmom (smaller) arrays.
The slider is interestinly only operable on the second last subplot. However I have no real idea how to link it to all subplots simulatenously. Perhaps someone has an idea how to do this.
Thanks a lot in advance,
Tilman
",339
31247678,31250371,2,"I cannot speak to the time efficiency of this method, but it might just get what you want done. The basic idea is to create a list to contain the lines of each text file, and then output the list to your new csv file. You save a 'delimiter' variable and then change it by checking each line as you go through the text files.
For example:
I created two text files on my Desktop. They read as follows:
delimiter_test_1.txt

test=delimiter=here 
does-it-work
I'm:Not:Sure

delimiter_test_2.txt

This:File:Uses:Colons
Pretty:Much:The:Whole:Time
does-it-work
If-Written-Correctly-yes

I then ran this script on them:
import csv
import glob
import os

directory = raw_input(""INPUT Folder for Log Dump Files:"")
output = raw_input(""OUTPUT Folder for .csv files:"")

txt_files = os.path.join(directory, '*.txt')

delimiter = ':'
for txt_file in glob.glob(txt_files):
    SavingList = []

    with open(txt_file, 'r') as text:
            for line in text:
                if line == 'test=delimiter=here\n':
                    delimiter = '='
                elif line == 'does-it-work\n':
                    delimiter = '-'
                elif line == ""I'm:Not:Sure"":
                    delimiter = ':'

                SavingList.append(line.split(delimiter))

    with open('%s.csv' %os.path.join(output, txt_file.split('.')[0]), 'wb') as output_file:
            writer = csv.writer(output_file)
            for m in xrange(len(SavingList)):
                writer.writerow(SavingList[m])

And got two csv files with the text split based on the desired delimiter. Depending on how many different lines you have for changing the delimiter you could set up a dictionary of said lines. Then your check becomes:
if line in my_dictionary.keys():
    delimiter = my_dictionary[line]

for example.
",353
31247678,31247678,1,"I have a text file that doesn't have a standard delimiter. I need to be able to check if the current line is equal to a certain phrase and if it is, the code should use a certain delimiter until another phrase is found. delimiters used are  ',' '-',':' and '='. 
Please help me out :) 
This is what my code is at the moment
import csv
import glob
import os

directory = raw_input(""INPUT Folder for Log Dump Files:"")
output = raw_input(""OUTPUT Folder for .csv files:"")

txt_files = os.path.join(directory, '*.txt')

for txt_file in glob.glob(txt_files):
    with open(txt_file, ""rb"") as input_file:
        in_txt = csv.reader(input_file, delimiter=':')
        filename = os.path.splitext(os.path.basename(txt_file))[0] + '.csv'

    with open(os.path.join(output, filename), 'wb') as output_file:
        out_csv = csv.writer(output_file)
        out_csv.writerows(in_txt)

",192
31247763,31247763,1,"I have got the following pandas data frame
          Y         X id WP_NER
0 35.973496 -2.734554  1  WP_01 
1 35.592138 -2.903913  2  WP_02 
2 35.329853 -3.391070  3  WP_03 
3 35.392608 -3.928513  4  WP_04 
4 35.579265 -3.942995  5  WP_05 
5 35.519728 -3.408771  6  WP_06 
6 35.759485 -3.078903 7 WP_07 

I´d like to round Y and X columns using pandas. 
How can I do that ?
",64
31247763,31247824,2,"You can apply round:
In [142]:
df[['Y','X']].apply(pd.Series.round)

Out[142]:
    Y  X
0  36 -3
1  36 -3
2  35 -3
3  35 -4
4  36 -4
5  36 -3
6  36 -3

If you want to apply to a specific number of places:
In [143]:
df[['Y','X']].apply(lambda x: pd.Series.round(x, 3))

Out[143]:
        Y      X
0  35.973 -2.735
1  35.592 -2.904
2  35.330 -3.391
3  35.393 -3.929
4  35.579 -3.943
5  35.520 -3.409
6  35.759 -3.079

EDIT
You assign the above to the columns you want to modify like the following:
In [144]:
df[['Y','X']] = df[['Y','X']].apply(lambda x: pd.Series.round(x, 3))
df

Out[144]:
        Y      X  id WP_NER
0  35.973 -2.735   1  WP_01
1  35.592 -2.904   2  WP_02
2  35.330 -3.391   3  WP_03
3  35.393 -3.929   4  WP_04
4  35.579 -3.943   5  WP_05
5  35.520 -3.409   6  WP_06
6  35.759 -3.079   7  WP_07

",218
31247768,38187541,2,"I am seeing this post very late, but maybe I can help... I am not sure what the function Qobj() is doing, can you please tell me more about it.
Otherwise, there is now a new partial_transpose() function in PICOS (version released today), which hopefully does what you need.
Best,
Guillaume.
",66
31247768,35159905,2,"You need to be able to output a numpy array or sparse matrix to convert to a Qobj. I could not find anything in the picos docs that discusses this option. 
",33
31247768,31247768,1,"I need to write a semidefinite program that minimizes the trace of an operator, say R, subject to the constraint that tr_A(R)^{Tb} >>0 . That means that R represents a 3 qubit quantum system and the trace over the first system gives you an operator that represents the remaining 2 qubit systems. Taking the partial transpose with respect to one of the qubits, you get the partially transposed quantum state of the restricted 2 qubit system. It is this state that I want to make positive semidefinite. 
I am using PICOS (to write the SDP) and qutip (to do the operations). 
P = pic.Problem()

Rho = P.add_variable('Rho',(n,n),'hermitian')

P.add_constraint(pic.trace(Rho)==1)
P.add_constraint(Rho>>0)


RhoQOBJ = Qobj(Rho)

RhoABtr = ptrace(RhoQOBJ, [0,1])
RhoABqbj = partial_transpose(RhoABtr, [0], method='dense')
RhoAB = RhoABqbj.full()    

Problem: I need to make Rho a Qobj, for qutip to be able to understand it, but Rho above is only an instance of the Variable class. Anyone has any idea on how to do this? 
Also I looked here, http://picos.zib.de/tuto.html#variables , it became even more confusing as this function puts the instance in a dictionary and only gives you back a key. 
",265
31247820,31247911,2,"Pass the Event object to the thread target function so that they are shared between main thread and the pool thread:
def poll_files(....., trigger):
    ....
    trigger.set()

# main thread
trigger = threading.Event()
poll = threading.Thread(target=poll_files, args=myargs + (trigger,))
...
trigger.wait()

",61
31247820,31247820,1,"I have a main python script which starts a thread running in the background.
poll = threading.Thread(target=poll_files, args=myargs)
I want my main script to wait until something specific happens in my poll thread. I want to use an Event object. So in my main script, I do:
trigger = threading.Event()
and when I want to wait:
trigger.wait()
My question is, inside my poll thread, how do I set the Event to True? I know I do:
trigger.set()
but do I need to also have trigger = threading.Event() inside my poll thread?
",114
31247829,32295818,2,"Finally i solved this problem, there was a broken link to one of references in windows service. This is the right config for py2exe which solved my problem:
opts = {'py2exe': {
'dll_excludes': ['libzmq.pyd', 'OLEAUT32.dll', 'USER32.dll', 'SHELL32.dll', 'ole32.dll',
                 'MSVCP90.dll', 'ADVAPI32.dll', 'NETAPI32.dll', 'WS2_32.dll', 'GDI32.dll',
                 'VERSION.dll', 'KERNEL32.dll', 'WINSPOOL.DRV', 'mfc90.dll', 'ntdll.dll'],
'includes': ['UserList', 'UserString', 'commands', 'zmq.backend.cython'],
'dist_dir': ""dist""
}}

setup(service=[service], options=opts, zipfile=None,data_files=[(os.path.join(os.getcwd(), 'dist'), (zmq.libzmq.__file__,))])

",144
31247829,31247829,1,"I've implemented a windows service, this service has no problem before compiling with pyinstaller but after that on service start command it gives 1053 error.
Windows service code:
import sys
import win32service
import win32event
import socket
import win32api
import win32serviceutil


class AppServerSvc(win32serviceutil.ServiceFramework):
    _svc_name_ = ""test""
    _svc_display_name_ = ""test""
    _stoped = False

    def __init__(self, *args):
        win32serviceutil.ServiceFramework.__init__(self, *args)
        self.stop_event = win32event.CreateEvent(None, 0, 0, None)

    def SvcStop(self):
        self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)
        win32event.SetEvent(self.hWaitStop)
        self._stoped = True

    def SvcDoRun(self):
        self.ReportServiceStatus(win32service.SERVICE_RUNNING)
        self.main()

    def main(self):
        while True:
            if self._stoped:
                break
            pass

if __name__ == '__main__':
    win32serviceutil.HandleCommandLine(AppServerSvc)

",143
31247829,38726274,2,"To use my win32serviceutil.ServiceFramework class in an exe pyInstaller produced, I needed to add a few .pyd files to the directory containing the exe e.g. win32service.pyd and win32event.pyd.  Assuming you've used the standard install paths for your libraries, these files are found here:

C:\Python27\Lib\site-packages\win32

",52
31247878,31248021,2,"You can get the names from elements with market_listing_item_name class name located in div elements having market_listing_row class:
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from selenium import webdriver

url = ""http://steamcommunity.com/market/search?appid=440""
driver = webdriver.Chrome()
driver.get(url)

# wait for results
WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.CSS_SELECTOR, ""div.market_listing_row"")))

results = [item.text for item in driver.find_elements_by_css_selector(""div.market_listing_row .market_listing_item_name"")]

driver.quit()

# dump results to a file
with open(""output.dat"", ""wb"") as f:
    for item in results:
        f.write(item + ""\n"")

Here is the contents of the output.dat file after running the script:
Mann Co. Supply Crate Key
The Powerhouse Weapons Case
The Concealed Killer Weapons Case
Earbuds
Bill's Hat
Gun Mettle Campaign Pass
Tour of Duty Ticket
Genuine AWPer Hand
Specialized Killstreak Kit
Gun Mettle Key

",177
31247878,31247878,1,"I am trying to record every item on the tf2 market place using selenium. I am trying to record the name of each item in a file on sale. This is the link to the page. I think it is this tag I just dont know how to reference and record the name in a text file with each name on a new line.
<span id=""result_0_name"" class=""market_listing_item_name"" style=""color; #7D6D00;"">

Edit 1:
I have used the solution by alecxe and it works for the first page I'm now trying to run it to select the next button then run again. But to no avail this is what I am trying.
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time

from selenium import webdriver
url=""http://steamcommunity.com/market/search?appid=440#p1_popular_desc""
driver = webdriver.Firefox()
driver.get(url)

x=1
while x==1:
    WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.CSS_SELECTOR, ""div.market_listing_row"")))
    time.sleep(5)
    results = [item.text for item in driver.find_elements_by_css_selector(""div.market_listing_row .market_listing_item_name"")]
    time.sleep(5)
    driver.find_element_by_id('searchResults_btn_next').click()
    with open(""output.dat"", ""a"") as f:
        for item in results:
            f.write(item + ""\n"")

This produces this error
Traceback (most recent call last):
  File ""name.py"", line 14, in <module>
    results = [item.text for item in driver.find_elements_by_css_selector(""div.market_listing_row .market_listing_item_name"")]
  File ""/usr/local/lib/python2.7/dist-packages/selenium/webdriver/remote/webelement.py"", line 61, in text
    return self._execute(Command.GET_ELEMENT_TEXT)['value']
  File ""/usr/local/lib/python2.7/dist-packages/selenium/webdriver/remote/webelement.py"", line 402, in _execute
    return self._parent.execute(command, params)
  File ""/usr/local/lib/python2.7/dist-packages/selenium/webdriver/remote/webdriver.py"", line 175, in execute
    self.error_handler.check_response(response)
  File ""/usr/local/lib/python2.7/dist-packages/selenium/webdriver/remote/errorhandler.py"", line 166, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.StaleElementReferenceException: Message: Element is no longer attached to the DOM
Stacktrace:
    at fxdriver.cache.getElementAt (resource://fxdriver/modules/web-element-cache.js:8956)
    at Utils.getElementAt (file:///tmp/tmpUpLsV7/extensions/fxdriver@googlecode.com/components/command-processor.js:8546)
    at WebElement.getElementText (file:///tmp/tmpUpLsV7/extensions/fxdriver@googlecode.com/components/command-processor.js:11704)
    at DelayedCommand.prototype.executeInternal_/h (file:///tmp/tmpUpLsV7/extensions/fxdriver@googlecode.com/components/command-processor.js:12274)
    at DelayedCommand.prototype.executeInternal_ (file:///tmp/tmpUpLsV7/extensions/fxdriver@googlecode.com/components/command-processor.js:12279)
    at DelayedCommand.prototype.execute/< (file:///tmp/tmpUpLsV7/extensions/fxdriver@googlecode.com/components/command-processor.js:12221)

Any help would be greatly appreciated even if it is links to guides
",439
31248615,31248615,1,"I was trying to use ZMQ - PUSH - PULL to build a distributed task processing system. This was east to do using JMS in Java with a Queue and a listener;Listeners which are free could take the message of the queue and execute it.Once the queue is distributed across nodes, this acts like a load balancer.
With ZMQ (using Python - don't want to use Celery now), I was trying out PUSH and PULL. With the Worker having different processing time. However even when a worker is free, tasks are going in strictly  round robin fashion. That is irrespective of if a worker is free or not, the task goes  in a round robin way.
Is there any way of simulating a distributed queue with ZMQ patterns so that, I can have a pool of workers 'polling' the queue in each node and which ever is free pulls the message from the queue and process it.
",175
31248615,34895786,2,"As pointed out by 0MQ founder Pieter Hintjens in this answer, the PUSH-PULL mechanism is not a load balancer, but rather a simple round robin distributor. That's a typo in the docs that is still there.
That said, for the load balancing pattern you need to add a broker in the middle of your architecture. As pointed out by Jason in the comments, this is well explained in the official guide. There are also examples in Python.

The main idea is to have the workers sending a small ""READY"" message to the broker whenever they are free to receive more jobs. The broker in turns, keep ""pointers"" to free workers in a queue. When he receives a new job request from a client he also propagates the request to the first free worker in the queue, which gets popped out from the queue. As you can see in the picture above, the broker exploits ROUTER sockets in order to avoid a blocking behavior and to get proper load balancing. A small additional detail is that the broker does not poll the clients if there are not free workers in the queue.
This is the simplest way I am aware of for implementing a load balancing pattern with ZeroMQ. It is not exactly like ""polling"" for new jobs in the queue, but I think this is what you need. Also please beware that this is really the simplest way, that is, it is not reliable at all and it does not scale well as is. If you also need reliability, I suggest you to thoroughly read Chapter 4 of the official guide.
As a side note, maybe you should seriously consider Celery for this task. I am really in love with ZeroMQ, however this is exactly the kind of thing that Celery is very good at, and in my opinion it is not so difficult to learn, as someone may think.
",358
31251807,31251807,1,"Recently I have been having trouble opening specific UTF-16 encoded files in Python. I have tried the following:
import codecs
f = codecs.open('filename.data', 'r', 'utf-16-be')
contents = f.read()

but I get the following error:
UnicodeDecodeError: 'utf16' codec can't decode bytes in position 18-19: illegal UTF-16 surrogate

after trying to read the contents of the file. I have tried forcing little-endian as well, but that's no good. The file header is as follows:
0x FE FF EE FF

Which I have read denotes UTF-16 Big Endian. I have been able to read the contents of the file into a raw string by using the following:
f = open('filename.data', 'rb')
raw = f.read()
hex = binascii.hexlify(raw)

Which works for getting me the raw hex, but the thing is - sometimes these files will be little-endian, sometimes they will be big-endian so I essentially just want to normalize the data before I start parsing, which I was hoping codecs would be able to help me out with, but no luck..
Does anyone have an idea of what's going on here? I would provide the file(s) as reference but there is some sensitive data so unfortunately I can't. This file is used by Windows OS.
My end goal, as I mentioned above, is to be able to open/read these files and normalize them so that I can use the same parser for all of them, rather than having to write a few parsers with a bunch of error handling in case the encoding is wacky.
EDIT: As requested, the first 32 bytes of the file:
FE FF EE FF 11 22 00 00 03 00 00 00 01 00 00 00 
92 EC DA 48 1B 00 00 00 63 00 3A 00 5C 00 77 00

",347
31251807,31252161,2,"Looks like you have a header of 24 binary bytes before your utf16-encoded string starts. So you can read the file as binary and decode afterwards:
with open(filename, ""rb"") as data:
    header = data.read(24)
    text = data.read().decode('utf-16-le')

But probably there are other binary parts. Without knowing the exact file format, there cannot be given more help.
",79
31251808,31251808,1,"I'm running series of testcases in multiple files, but I want to run the prereq and cleanup only once through out the run, please let me know is there a way to do it?
",38
31251808,31251843,2,"py.test -> session scoped fixtures and their finalization should help you
You can use conftest.py to code your fixture.
",21
31251871,31252232,2,"One-liner using re.sub function.
>>> s = """"""{12,} {13,} {10,}
{16,} {17, 15,} {22,}
{27,} {28,24,29,} {28,} {28,}""""""
>>> print(re.sub(r'(?<=\d) +(?=\d)', ', ', re.sub(r'\{[^}]*\}', lambda m: ', '.join(sorted(re.findall(r'\d+', m.group(0)), key=lambda x: int(x))), s)))
12, 13, 10
16, 15, 17, 22
27, 24, 28, 29, 28, 28

",152
31251871,31251929,2,"You can use a nested list comprehension: [[f for s in sequences for f in sorted(s)] for sequences in lines]
",28
31251871,31251871,1,"My data looks like:
{12,} {13,} {10,}
{16,} {17, 15,} {22,}
{27,} {28,24,29,} {28,} {28,}

Each line is a sequence. For each line, if multiple numbers occur in {  }, I want to sort them in each bracket(ascending sort ), while keeping the order of the rest. At last, I want to remove the brackets. So I want my output to be like this:
12, 13, 10
16, 15, 17, 22
27, 24, 28, 29, 28, 28

My thought was converting each line into a list, but then I was totally stuck.
",146
31251876,31251876,1,"Hi I'm just learning about how decorators look in my Python flask framework and wanted to confirm if I'm reading this properly since I can get lost in nested functions. Here is a piece of the code I'm working with
def login_required(f):
    @wraps(f)
    def wrap(*args, *kwargs):
        if 'logged_in' in session:
            return f(*args, **kwargs)
        else:
            flash('You need to log in first.')
            return redirect(url_for('logging')
    return wrap

@app.route('/')
@app.route('/index')
@login_required
def index():
    return render_template('index.html',
                            title='test home title')

The thing that confuses me when following this along is the 'f' variable.
So Login_required() is called as a decorator for my index() function. And it looks something like this -->
    login_required(index()). 
Does the 'f' variable become login_required(f()) or login_required(index(f))?
It confuses me in the login_required() function when f is returned with (*args, *kwargs) I'm not exactly sure what's happening and get lost
",226
31251876,31251948,2,"f becomes index.  This:
@login_required
def index():
    return render_template('index.html',
                            title='test home title')

Is the same as this:
def index():
    return render_template('index.html',
                            title='test home title')
index = login_required(index)

That is, the decorator is called with the decorated function as its argument.
",67
31251881,31251881,1,"I have code for radio buttons in tkinter. I am struggling to write the code that invokes the button command. Basically I want the user to be able to choose a time frame and a person. I have three different files that run three different data analyses, so I want the three files to run but only take data from the time frame and for that person. 
from Tkinter import *

class RBDemo:
    def __init__(self, win):
        self.v = IntVar()

        #Put the first group of radio buttons in their own frame.
        f1 = Frame(win, borderwidth=3, relief=RAISED)
        rb1 = Radiobutton(f1, text=""This Week"", variable=self.v, value=1)
        rb2 = Radiobutton(f1, text=""This Month"", variable=self.v, value=2)
        rb3 = Radiobutton(f1, text=""This Year"", variable=self.v, value=3)
        rb1.pack(anchor=W);  rb2.pack(anchor=W);   rb3.pack(anchor=W)
        f1.pack(side=LEFT)

        #Button one will be selected by default
        self.v.set(1)


        #Make a second group of radiobuttons in their own frame.
        #Make first button the default
        self.v2 = StringVar()
        f2 = Frame(win, borderwidth=2, relief=SOLID)

        rb4 = Radiobutton(f2, text=""Bob"", variable=self.v2, value=""Bob"")
        rb5 = Radiobutton(f2, text=""Stacy"", variable=self.v2, value=""Stacy"")
        rb6 = Radiobutton(f2, text=""Both"", variable=self.v2, value=""Both"") 
        rb4.pack(anchor=W);  rb5.pack(anchor=W); rb6.pack(anchor=W)
        f2.pack(side=RIGHT)
        self.v2.set(""Bob"")

        #Make a button that prints what each value is when clicked
        b = Button(win, text=""Let's do this!"", command=self.clicked)
        b.pack(side=BOTTOM, fill=BOTH, expand=1)


    def clicked(self):
        print(""button clicked!"")
        print(""v is:"", self.v.get())
        print(""v2 is:"", self.v2.get() )




mw = Tk()
app = RBDemo(mw)
mw.mainloop()

I tried
    def selected(self):
    if self.my_var.get()==1:
        ""do something""
        elif self.my_var.get()==2:
            ""do something""
            else:
                ""do something""

but this doesn't seem to work, nor is it very pythonic considering I have to run three files using the input from the button. 
",454
31251881,31252593,2,"First, When properly indented, an if - elif block is perfectly ok. So you could just use
if whatevervar.get() == 1:
    dosomethingfancy()
elif whatevervar.get() == 2:
    dosomethingcool()
#and so on

In other languages there is something like a switch - case block: wikipedia There is no such construct in Python, but there is a neat little trick that helps especially when dealing with bigger code blocks:
Options = {
         1: dosomething,
         2: dosomethingelse
}

#execution
Options[myvar.get()]()

Basically, a dictionary is defined, that maps its key values to functions. Mind the parentheses: You don't want to call the function, when the dictionary is defined.
",138
31251915,31252902,2,"HTTP communication also known as RESTful communication is the universal language of the web and is ideal because it can work between operating systems and back end frameworks such as Django and iOS etc.. etc..
You will have quite a bit of research you need to do.  Django is written in Python, so assuming you know some Python, Django should not be too difficult to pickup, but you should definitely start with it's poll app tutorial.  
Django-Rest-Framework is probably the best tool to build a restful web API for your Django project.  Your iOS app will make http calls (GET, POST, PUT) etc... to your Django API.  There is really no better documentation than Django-Rest-Framework's website to learn how to use it.  
Ultimately there is no book that will tell you step by step how to do this.  You have to learn several different areas and put the pieces together.  
",167
31251915,31251915,1,"I am newbie in Django so i'm a bit confused with some of it's features. I have to do a project for my university. It consists of a desktop-web app where employees can do stuff like handle files, record files and upload files on a MySQL database. In addition, there's an iOS native app (written in Objective-C) where users can register themselves, log in, and ask for the data that employees have saved early. 
First of all:  Is it possible to build a server app that communicates with an iOS app (sending-recieve data, notifications) using Django-frameworks (and i suppose also Django-Rest-Framework)? If yes, are there any tutorials/guide/book to better understand how this communication work and how i can really do this?
My second question is related to custumizing users on Django.
For my project, i need two classes of user:

Employee (with registration_id as primary key): this user can only log in on desktop-app
Driver : this user can only log in on ios-app

Can i have multiple users like here:
class Employees(AbstractBaseUser):
registration_id = models.IntegerField(max_length=10,unique=True, null=True, blank=True)
    first_name = models.CharField(max_length=30, null=True, blank=True)
last_name = models.CharField(max_length=60, null=True, blank=True)
    #etc etc etc

class Driver(AbstractBaseUser):
       license_plate =....
       first_name = ....
       #etc etc etc

is that reasonable?
Sorry for my bad English.
I will appeciate any kind of help
Thank you
Cheers
",280
31251933,31251933,1,"sorry to ask a basic question, but it is hard to find on google.
anyway, I have a program that does math from numbers found in various .txt files. one thing I would like to add is a -show argument to show the math step by step (finding bugs in math, finding numbers from certian steps, etc.).
I have it set up in the code like so:
import sys
sys.argv[0]
filename = sys.argv[1]
prop = sys.argv[2]
show = sys.argv[3]
    if show == ""-show"":
        show = 1

(show = 1 does something later on). my problem is when i don't put anything for sys.argv[3] like if i put:
python program.py examplefile.txt exampleline 

then the program doesn't run, I know it is becauce it is expecting an argument and thats why its messing up, but is there a way to tell it that sys.argv[3] is not always used and can be blank?
",190
31251933,31252036,2,"On a side note, it might be good to checkout argparse module in python, from the documentation -

The argparse module makes it easy to write user-friendly command-line interfaces. The program defines what arguments it requires, and argparse will figure out how to parse those out of sys.argv. The argparse module also automatically generates help and usage messages and issues errors when users give the program invalid arguments.

It also has support for optional arguments. Maybe you can take a look at this to get started on that.
",96
31251933,31251934,2,"You should be getting an IndexError on that line when you run it without the third argument. If you're not getting an IndexError, something else is wrong and you should fix it. If you are, all is well - and you simply need to check for the length of sys.argv, taking into account in your code what should happen when that value is 3 or 4:
if len(sys.argv) == 3:
  # Stuff without sys.argv[3]
if len(sys.argv) == 4:
  # Stuff with sys.argv[3]

",103
31251933,31251935,2,"You can check the length of sys.argv, but why bother? Use try/catch:
try: flag = sys.argv[1]
except: flag = False

That way variable flag always has a value, and you can write code the knows that it always has a value. The code has fewer lines than a if/else testing. It's a win all around.
",68
31252022,31267118,2,"""... decides what translation object to install in the current thread context"" (source)
Simple as it is, the language is thread-wide, since one thread serves one request, and request knows about the language.
",41
31252022,31252022,1,"I am just very curious. Django runs the following line:
email_body = get_template(""applicant/email_with_token.txt"").render(Context())

it runs it from form_valid() of a class-based view. New context is created based on a regular python dictionary. No parameters are passed, that come from a view, user, session, etc... Inside of the template there is {% load i18n %} and a bunch of context/variables.
Still, Django recognizes the language of the current session and applies appropriate translations.
So, where from does it know the session language?
",110
31252037,31252122,2,"A <br/> tag is an empty tag, always. There is no text in that tag.
What you have instead is text between two <br/> tags, which may have been confusing. You can drop either tag and it'll still be valid HTML.
You can get text following a tag using the .next_sibling attribute:
soup.select('div.menu br')[0].next_sibling

Demo:
>>> from bs4 import BeautifulSoup
>>> html_doc = """"""
... <HTML and CSS Stuff here>
... <div class=""menu"">
... <span class=""author"">Bob</span> 
... <span class=""smaller"">(06 Jul at 09:21)</span>
... <br/>This message is very important to extract along with the matching author and time of submit<br/>
... </div>
... """"""
>>> soup = BeautifulSoup(html_doc)
>>> soup.select('div.menu br')[0].next_sibling
u'This message is very important to extract along with the matching author and time of submit'

Putting that together with extracting all the data:
for menu in soup.select('div.menu'):
    author = menu.find('span', class_='author').get_text()
    time = menu.find('span', class_='smaller').get_text()
    data = menu.find('br').next_sibling

which produces:
>>> for menu in soup.select('div.menu'):
...     author = menu.find('span', class_='author').get_text()
...     time = menu.find('span', class_='smaller').get_text()
...     data = menu.find('br').next_sibling
...     print 'Author: {}\nTime: {}\nData: {}'.format(author, time, data)
... 
Author: Bob
Time: (06 Jul at 09:21)
Data: This message is very important to extract along with the matching author and time of submit

",361
31252037,31252037,1,"I almost have a grip on BeautifulSoup4 in Python, but I can't seem to pull out the <br/> data for the br tags in HTML data.
Data Structure:
<HTML and CSS Stuff here>
<div class=""menu"">
<span class=""author"">Bob</span> 
<span class=""smaller"">(06 Jul at 09:21)</span>
<br/>This message is very important to extract along with the matching author and time of submit<br/>
</div>

What I'm looking for is:
Author: Bob
Time: (06 Jul at 09:21)
Data: This message is very important to extract along with the matching author and time of submit

The HTML comes in through requests, that all works fine. But I'm just not getting the soup to mix correctly.
Current Code:
from bs4 import BeautifulSoup
import requests
html_doc = """"""
<HTML and CSS Stuff here>
<div class=""menu"">
<span class=""author"">Bob</span> 
<span class=""smaller"">(06 Jul at 09:21)</span>
<br/>This message is very important to extract along with the matching author and time of submit<br/>
</div>
""""""

html_doc = r.text
soup = BeautifulSoup(html_doc, 'html.parser')

x = soup.select('div[class=""menu""]')
for i in x:
    s = soup.select('span[class=""author""]')
    rr = soup.select('span[class=""smaller""]')
    for b in s:
        print b
        print rr

",308
31252107,31252343,2,"Could you try calling plt.draw after plt.vlines? plt.draw is used to interactively redraw the figure after its been modified.
",21
31252107,31254208,2,"If I understood well, you want to use the animation tools of matplotlib. An example (adapted from the doc): 
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation

X_MIN = -6
X_MAX = 6
Y_MIN = -1
Y_MAX = 1
X_VALS = range(X_MIN, X_MAX+1);

def update_line(num, line):
    i = X_VALS[num]
    line.set_data( [i, i], [Y_MIN, Y_MAX])
    return line, 

fig = plt.figure()

x = np.arange(X_MIN, X_MAX, 0.1);
y = np.sin(x)

plt.scatter(x, y)

l , v = plt.plot(-6, -1, 6, 1, linewidth=2, color= 'red')

plt.xlim(X_MIN, X_MAX)
plt.ylim(Y_MIN, Y_MAX)
plt.xlabel('x')
plt.ylabel('y = sin(x)')
plt.title('Line animation')

line_anim = animation.FuncAnimation(fig, update_line, len(X_VALS),\   
                                    fargs=(l, ), interval=100,\
                                    blit=True, repeat=False)

#line_anim.save('line_animation.gif', writer='imagemagick', fps=4);

plt.show()

Resulting gif looks like this:

",217
31252107,31252107,1,"I have a time series plot and I need to draw a moving vertical line to show the point of interest. 
I am using the following toy example to accomplish the same. However, it prints all the lines at the same time while I wanted to show these vertical line plotting one at a time.  
import time
ion() # turn interactive mode on

# initial data
x = arange(-8, 8, 0.1);
y1 = sin(x)
y2 = cos(x)
line1, = plt.plot(x, y1, 'r')
xvals = range(-6, 6, 2);
for i in xvals:
    time.sleep(1)
    # update data
    plt.vlines(i, -1, 1, linestyles = 'solid', color= 'red')
    plt.draw()

",150
31252155,31252155,1,"For the life of me, I cannot find a way to solve this, I don't really know what I'm doing wrong with the inlines.
I'm using django 1.8 as well as django-nested-inline 0.3.4.
Here's the contents of the models.py file:
import datetime
from django.db import models
from django.db.models.fields import CharField, TextField, IntegerField
from django.utils import timezone

# lectura de sensor, no incluye datos, solo fecha y datos apuntan aquí
# también apunta a la pieza y el sensor asignado


class lectura(models.Model):
    lectura_nom = models.CharField(max_length = 20)
    sensor_asign = models.ForeignKey('sensor_asignado')
    pieza = models.ForeignKey('pieza')

    verificacion = models.BooleanField()
    fecha_hora = models.DateTimeField()

    def __str__(self):
        return self.lectura_nom

    def lectura_de_hoy(self):
        return self.fecha_hora >= timezone.now() - datetime.timedelta(days=1)

# pieza asignada al sensor, cambia de acuerdo a hroario de produccion
class pieza(models.Model):
    pieza_nom = models.CharField(max_length = 20)
    paquete =models.ForeignKey('paquete')
    ultima = models.BooleanField()
    no_de_seq = models.IntegerField()
    description= models.TextField()

    def __str__(self):
        return self.pieza_nom

# paquetes incluyen puras piezas únicas al paquete
class paquete(models.Model):
    paquete_nom = models.CharField(max_length = 20)
    no_de_piezas = models.IntegerField()
    no_de_medibles = models.IntegerField()

    def __str__(self):
        return self.paquete_nom


#lugar donde se encuentra el rack, importante para permisos de usuario
class area(models.Model):
    area_nom = models.CharField(max_length = 20)
    description = models.TextField()

    def __str__(self):
        return self.area_nom

# rack del cual fue sacada la pieza    
class linea(models.Model):
    linea_nom = models.CharField(max_length = 20)
    area = models.ForeignKey('area')
    description = models.TextField()

    def __str__(self):
        return self.linea_nom

# sensor del cual se sacó la lectura
class sensor_asignado(models.Model):
    sensor_nom = models.CharField(max_length = 20)
    linea = models.ForeignKey('linea')
    sensor_tipo = models.ForeignKey('tipo_de_sensor')

    def __str__(self):
        return self.sensor_nom

# sensor puede incluir varias configuraciones de envio de datos    
class tipo_de_sensor(models.Model):
    tipo_de_sen_nom = models.CharField(max_length = 20)
    descripcion=models.TextField()

    def __str__(self):
        return self.tipo_de_sen_nom

# diferentes datos para el sensor de tipo especificado anteriormente
# cada dato apunta hacia una lectura    
class salidas_de_sensor(models.Model):
    salida_nom = CharField(max_length = 20)
    descripcion = models.TextField()

    def __str__(self):
        return self.salida_nom

class datos_o(models.Model):
    dato_no = IntegerField(primary_key = True)
    salida_de_sensor = models.ForeignKey('salidas_de_sensor')
    lectura = models.ForeignKey('lectura')

    def __str__(self):
        r_string = str(self.dato_no)
        return r_string

# advertencias, apunta a un tipo y una lectura
class adv(models.Model):
    adv_no = IntegerField(primary_key=True)
    lectura = models.ForeignKey('lectura')
    adv_tipo = models.ForeignKey('adv_tipo')

    def __str__(self):
        adv_string = str(self.adv_no)
        return adv_string

# diferentes tipo de advertencias posibles
class adv_tipo(models.Model):
    adv_tip = CharField(max_length = 20)
    descrip_adv = TextField()

    def __str__(self):
        return self.adv_tip

Here's the contents of my admin.py file:
from django.contrib import admin
from nested_inline.admin import NestedStackedInline, NestedModelAdmin
from mainscreen.models import *

class AreaInline(NestedStackedInline):
    model = area
    extra = 1
    fk_name = area

class LineaInLine(NestedStackedInline):
    model = linea
    extra = 1
    fk_name = linea
    inlines = [AreaInline]

class SensorInLine(NestedModelAdmin):
    model = sensor_asignado
    inlines = [LineaInLine]


    admin.site.register(lectura)
admin.site.register(sensor_asignado, SensorInLine)
admin.site.register(salidas_de_sensor)
admin.site.register(pieza)
admin.site.register(paquete)
admin.site.register(area)
admin.site.register(linea)
admin.site.register(tipo_de_sensor)
admin.site.register(adv)
admin.site.register(adv_tipo)
admin.site.register(datos_o)

And here's the error I'm getting:
Performing system checks...

SystemCheckError: System check identified some issues:

ERRORS:
<class 'mainscreen.admin.LineaInLine'>: (admin.E202) 'mainscreen.linea' has no field named '<class 'mainscreen.models.linea'>'.

System check identified 1 issue (0 silenced).

Anyway, if anyone can identify a solution to this problem, I'll be really thankfull.
Have a good day.
EDIT:
Okay, now I have another problem entirely, and I believe it's more of a design problem.
The way I want the information to be displayed in the admin, is:
when a new Sensor Asignado is created, have the option to choose from a drop downn menu in the following order of hierarchy:
    Area
    Linea (filtered by Area chosen)
    Sensor (Filtered by Linea chosen)

Again, I've been wrecking myself trying to get this right for a while, and any help is much appreciated.
Thank you, have a nice day.
",887
31252155,31252286,2,"Try using a string for fk_name:
fk_name = 'linea'

As an aside, I recommend that you use the Django convention, and name your models SensorAsignado and Linea instead of sensor_asignado and linea. It will make your code easier to understand for other Django users.
",49
31252263,31252263,1,"I'm a beginner for Python and trying to print Fibonacci series recursively. When i try this, the return function doesn't print at all, hence just the argument that i pass gets printed.
Here's the code:
def fibR(n):
 if n == 1 or n ==2:
    return (1)
 return(fibR(n-1) + fibR(n-2))
print(fibR(5))

Can someone help me to get all the numbers in the series?
",93
31252263,31252916,2,"A basic (but not naïve) recursive Fibonacci solution is as follows:
>>> def fib(num, first=0, second=1):
...     if not num: return second
...     return fib(num-1, second, first+second)
...
>>> fib(5)
8

You can save the intermediate values in two ways: create a list and append() to it, or have your function return a tuple.
Method 1:
>>> def fibl(num, first=0, second=1):
...     results.append(second)
...     if not num: return
...     return fibl(num-1, second, first+second)
...
>>> results = []
>>> fibl(5)
>>> results
[1, 1, 2, 3, 5, 8]

Method 2:
>>> def fibr(num, first=0, second=1, *results):
...     if not num: return results+(second,)
...     return fibr(num-1, second, first+second, *(results + (second,)))
...
>>> fibr(5)
(1, 1, 2, 3, 5, 8)

",220
31252263,31252430,2,"The inefficient way to do this is to use a loop:
def fibR(n):
    if n == 1 or n ==2:
        return 1
    return fibR(n-1) + fibR(n-2)

for i in range(1,6):
    print(fibR(i))

However, this is inefficient because it will calculate the lower Fibonacci numbers more than once. You can use an array to store the intermediate Fibonacci numbers and improve performance:
def fibR(n):
    if n > len(fibR.values) - 1:
        for i in range(len(fibR.values), n+1):
            fibR.values.append(fibR(i-2) + fibR(i-1))
    return fibR.values[n]

fibR.values = [0, 1, 1]
for i in range(1,6):
    print(fibR(i))

",155
31252265,31252323,2,"The line:
drive_usage.split()

should be:
drive_usage = drive_usage.split()

As the string method .split() returns a list, whereas you are expecting it to replace the string with a list of its parts which doesn't happen.
",45
31252265,31252265,1,"import os
f=os.popen('df -h')
drive_usage=f.read()
drive_usage.split()
email('test','Critical! Drive usage is very high!')
for x in range(4,len(drive_usage)-1,6):
    s2 = drive_usage[x][0:len(drive_usage[x])]
    drive_usage[x] = s2
    if drive_usage[x]>90:
        #email admin

This code should read the output of df -h on an ubuntu 12.04 server, split the output of the command into a list, and then trim the '%' character off, so that the drive usage can be used as a comparator in an if statement. Then, if it's above 90%, it should email the server admins. 
However, drive_usage[x] = s2 triggers the 'str' object does not support item assignment. To my knowledge, this should not be triggering, as that line should be accessing a string in a list, not a character in a string. Any help or advice would be appreciated!
",200
31252284,31252284,1,"I am learning Django and I'm still in the first tutorial. When I try to run:
python manage.py shell 

I get the following error:
   Traceback (most recent call last):
  File ""manage.py"", line 10, in <module>
    execute_from_command_line(sys.argv)
  File ""C:\Python27\lib\site-packages\django\core\management\__init__.py"", line
338, in execute_from_command_line
    utility.execute()
  File ""C:\Python27\lib\site-packages\django\core\management\__init__.py"", line
312, in execute
    django.setup()
  File ""C:\Python27\lib\site-packages\django\__init__.py"", line 18, in setup
    apps.populate(settings.INSTALLED_APPS)
  File ""C:\Python27\lib\site-packages\django\apps\registry.py"", line 108, in pop
ulate
    app_config.import_models(all_models)
  File ""C:\Python27\lib\site-packages\django\apps\config.py"", line 198, in impor
t_models
    self.models_module = import_module(models_module_name)
  File ""C:\Python27\lib\importlib\__init__.py"", line 37, in import_module
    __import__(name)
  File ""C:\Users\Elashkar\Desktop\django\mysite\polls\models.py"", line 2, in <mo
dule>
    import datatime
ImportError: No module named datatime

",173
31252284,31252433,2,"This looks like a typo in the import statement. datatime should be datetime (a -> e).
",21
31252296,31252370,2,"One solution would be to concatenate tuples.
x = ('a', 'b', 'c') + (('d',) if Y else ()) + (('e',) if Z else ())

One benefit of doing it this way is it's an expression thus usable inside a lambda.
",64
31252296,31252386,2,"Another way is to make a list, then convert the list to a tuple by calling tuple(lst).
",22
31252296,31252422,2,"One way to do it - useful if you're going to be generating the contents of the tuple a lot - is with a generator: 
def gen_X():
    yield 'a'
    yield 'b'
    yield 'c'
    if Y:
        yield 'd'
    if Z:
        yield 'e'

Then make a tuple from the generator results: 
X = tuple(gen_X())

With a slightly modified version you can also supply the arguments which determine what the generator will produce: 
def gen_X(Y = True, Z = True):
    yield 'a'
    yield 'b'
    yield 'c'
    if Y:
        yield 'd'
    if Z:
        yield 'e'

One nice thing about doing it with a generator is that you don't have to keep multiple sets of the tuple in memory - both the contents and the logic are contained in the generator. So you can be simultaneously iterating through lots of different versions of the same data, but it's all contained in only one location and generated on the fly as you need it. 
",184
31252296,31252465,2,"Python does not provide this kind of syntactic sugar.
The easiest way to solve your problem is to use a list. Then you can use my_list.append(), and finally, when you need a tuple, make a tuple out of it: tuple(my_list). (INSTALLED_APPS = tuple(my_list))
",60
31252296,31252611,2,"You can do something like this using a generator expression: 
truth_table = {'a': True, 'b': True, 'c': True, 'd': Y, 'e': Z}
X = tuple(k for k,v in sorted(truth_table.items()) if v)

...or in one line:
X = tuple(k for k,v in sorted({'a': True, 'b': True, 'c': True, 'd': Y, 'e': Z}.items()) if v)

",109
31252296,31252368,2,"You can just concatenate another tuple to the end.
x = ('a','b','c')
if Y:
    x += ('d',)
if Z:
    x += ('e',)
print(x)

",46
31252296,31252296,1,"Say I have a list x = ['a','b','c'] to conditionally add a term to this you could do:
if conditional:
    x.append('d')

But I can't do this for a tuple (x = ('a','b','c'))
For both cases is there a 'clean' way of conditional-alising items in a definition. Pseudo code:
X = (
    'a',
    'b',
    'c',
    'd' if Y,
    'e' if Z,
)

The use case for this is a Django INSTALLED_APPS tuple and urlpatterns list.
",115
31252320,31252320,1,"I have this piece of inelegant code that's supposed to print out something that looks like this to the console:

CSI='\x1B['
reset=CSI+'m'

for i in range(len(recs)):
    print CSI+'36;40m' + str(i+1) + '\t\t', recTitles[i], CSI+'33;40m' + recReleaseYears[i] + reset
    print CSI+'35;40m' + u'\u2588' + 'IMDb Rating:\t' + reset, recRatings[i], '('+recVotes[i]+' votes)'
    print CSI+'34;40m' + u'\u2588' + 'Genre:\t\t' + reset, CSI+'36;1m' + recGenres[i] + reset
    print CSI+'33;40m' + u'\u2588' + 'Cast:\t\t' + reset, recCast[i]
    print CSI+'32;40m' + u'\u2588' + 'Director:\t' + reset, recDirectors[i]
    print CSI+'36;40m' + u'\u2588' + 'Summary:\t' + reset, insertTabs(recPlots[i]) + reset
    print CSI+'31;40m' + u'\u2588' + 'IMDb Link:\t' + reset, CSI+'34;1m' + recIMDbLinks[i] + '\n' + reset

However, most of the time I get this:

I'm not sure where the problem lies with my code because occasionally it works as expected and I get the right output (which drives me crazy).
I'm using Enthought Canopy as my IDE.
",272
31252320,31255642,2,"The problem likely is in the terminal emulator, resetting colors after the tab-characters (which would be a bug).  If you change the tabs so that only spaces are written, it would work around that possibility.
By the way, if your terminal was set to use hard tabs, then you could expect some other unexpected behavior (the tabbed area would be skipped and not colored).  Here is a screenshot (with the script to demonstrate):

",87
31252320,31271453,2,"I still am not sure what the problem might be, but adding a sys.stdout.flush() at the beginning of the for loop seems to have fixed things.
CSI='\x1B['
reset=CSI+'m'
import sys

for i in range(len(recs)):
    sys.stdout.flush()
    print CSI+'36;40m' + str(i+1) + '\t\t', recTitles[i], CSI+'33;40m' + recReleaseYears[i] + reset
    print CSI+'35;40m' + u'\u2588' + 'IMDb Rating:\t' + reset, recRatings[i], '('+recVotes[i]+' votes)'
    print CSI+'34;40m' + u'\u2588' + 'Genre:\t\t' + reset, CSI+'36;1m' + recGenres[i] + reset
    print CSI+'33;40m' + u'\u2588' + 'Cast:\t\t' + reset, recCast[i]
    print CSI+'32;40m' + u'\u2588' + 'Director:\t' + reset, recDirectors[i]
    print CSI+'36;40m' + u'\u2588' + 'Summary:\t' + reset, insertTabs(recPlots[i]) + reset
    print CSI+'31;40m' + u'\u2588' + 'IMDb Link:\t' + reset, CSI+'34;1m' + recIMDbLinks[i] + '\n' + reset

",236
31252353,31260634,2,"You can use the uWSGI alarm subsystem:
http://uwsgi-docs.readthedocs.org/en/latest/AlarmSubsystem.html
the --alarm-backlog function will trigger an alarm whenever the listen queue is full, this alarm can be an ad-hoc script calling aws api to scale
",38
31252353,31252353,1,"I want to add instances to the current setup based on uwsgi listen queue. If uwsgi process have a high backlog, a new machine will spin up and respond to requests.
",34
31252359,31252408,2,"You're using Python 2. In Python 2, input takes your input and tries to evaluate it. You want to use raw_input.
",26
31252359,31252413,2,"I am guessing you are using Python 2.x , in Python 2.x , input actually tries to evaluate the input before returning the result, hence if you put in some name , it will treat that as a variable and try to get its value causing the issue.
Use raw_input(). instead. Example -
participant = raw_input(""Participant name > "")
....
score = raw_input(""Score for "" + participant + ""> "")

",86
31252359,31252359,1,"I have a python script and I am receiving the following error. I'm a new learner to this language, so I created a simple script, called writing.py, to write participant names and scores into a text file named scores.txt. But I keep getting this error:
Traceback (most recent call last):
  File ""writing.py"", line 4, in <module>
    participant = input(""Participant name > "")
  File ""<string>"", line 1, in <module>
NameError: name 'Helen' is not defined

Here is my code:
f = open(""scores.txt"", ""w"")

    while True:
        participant = input(""Participant name > "")

        if participant == ""quit"":
            print(""Quitting..."")
            break

    score = input(""Score for "" + participant + ""> "")
    f.write(participant + "","" + score + ""\n"")

f.close()

",181
31252360,31253142,2,"You can use wand for such basic tasks.  The syntax is very easy to read unlike other ImageMagik libs.  Basically you'd do something like:
from wand.image import Image
from wand.display import display

array = []
with Image(filename='yourfile.png') as img:
    array.append(img.channel_images)        # this is most likely wrong, but it should be something similar

It will be along those lines. Once I leave the office I will try this out.
",84
31252360,31252776,2,"If you have PIL installed then you can create an image with Image.open and get the colors like so:
data = [image.getpixel((x, y)) for x in range(image.width) for y in range(image.height)]

",46
31252360,31252607,2,"You can use the existing pygame module. Import a file into a Surface using pygame.image.load. You can then access the bit array from this using pygame.surfarray.array2d. Please see the Pygame docs for more information.
",38
31252360,31252360,1,"I would like to convert a PNG image to a 2 dimensional array where each array holds a list of the RGB values of that specific pixel. How could one create a program to read-in a *.png file and convert to this type of data structure?
",48
31252473,31253296,2,"The compiler doesn't use the PATH environment variable for finding header files.  Usually it uses INCLUDE.
I think you can pass this into nmake, like so
nmake INCLUDE=pathToPythonHeader -f ap24py27-win64-VC9.mk
",34
31252473,31259320,2,"Why are you trying to compile it from source code? Why aren't you using the precompiled binaries?
There is nothing additional in the latter release that you would really need on Windows, so just grab and use the latest binary version which at this time is 4.4.12.
You also can't do a 'python setup.py install' nor a 'pip install' on Windows.
If you really do for some unknown reason need to build from source code, you do at least need to modify the ap24py27-win64-VC9.mk file and override the locations for where Python and Apache is installed. The default locations are based on a non standard layout that allows me to have many different Python and Apache versions installed for different architectures at the same time.
",137
31252473,31252473,1,"I am having a LOT of trouble to start with these are my specs
windows 8
64 bit
python 2.7
Apachehaus 2.4.12

I cannot for the life of me figure out how to compile mod_wsgi to make the necessary mod_wsgi.so file from the source code.
https://github.com/GrahamDumpleton/mod_wsgi/releases
I'm trying to compile the ap24py27-win64-VC9.mk and after opening Visual C++ 2008 64-bit command prompt
c:\mod_wsgi-4.4.13> nmake -f ap24py27-win64-VC9.mk
c:\mod_wsgi-4.4.13\src\server\wsgi_python.h(24) : fatal error 1083: Cannot open include file: 'Python.h': No such file or directory wsgi_restrict.c
c:\mod_wsgi-4.4.13\src\server\wsgi_python.h(24) : fatal error  1083: Cannot open include file: 'Python.h': No such file or directory wsgi_server.c
c:\python27\mod_wsgi-4.4.13\src\server\wsgi_python.h(24) : fatal error C1083: Cannot open include file: 'Python.h': No such file or directory wsgi_stream.c
c:\mod_wsgi-4.4.13\src\server\wsgi_python.h(24) : fatal error C1083: Cannot open include file: 'Python.h': No such file or directory wsgi_validate.c
c:\mod_wsgi-4.4.13\src\server\wsgi_python.h(24) : fatal error C1083: Cannot open include file: 'Python.h': No such file or directory
Generating Code...
c:\mod_wsgi-4.4.13\src\server\wsgi_memory.c(124) : warning C4711: function 'getPeakRSS' selected for automatic inline expansion
c:\mod_wsgi-4.4.13\src\server\wsgi_memory.c(125) : warning C4711: function 'getCurrentRSS' selected for automatic inline expansion 
NMAKE : fatal error U1077: '""C:\Users\User\AppData\Local\Programs\Common\Microsoft\Visual C++ for Python\9.0\VC\Bin\amd64\cl.EXE""' : return code '0x2'
Stop.

I already set my path to path=%path% Python27/include because that's where the Python.h file is. 
I've also tried
python setup install
Traceback (most recent call last):
    File ""setup.py"", line 139, in <module>
        'missing Apache httpd server packages.' % APXS)
RuntimeError: The 'apxs' command appears not to be installed or is not executable. Please check the list of prerequisites in the documentation for this package and install any missing Apache httpd server packages.

",357
31253148,31343406,2,"I believe the .addSample() method expects one sample at a time. Rather than using .addSample(), try
assert(X.shape[0] == y.shape[0])
DS.setField('input', X)
DS.setField('target', y)

The 'assert()' is recommended because the .setField() method does not verify array dimensions like .addSample() does.
See Pybrain dataset tutorial for more info.
",80
31253148,31253148,1,"from pybrain.structure import FeedForwardNetwork
from pybrain.structure import LinearLayer, SigmoidLayer
from pybrain.structure import FullConnection
from pybrain.datasets import SupervisedDataSet
import numpy as np

X = np.loadtxt('xdatanorm.txt', dtype=float)
y = np.loadtxt('ydatanorm.txt', dtype=float)

n = FeedForwardNetwork()
inLayer = LinearLayer(35)
hiddenLayer = SigmoidLayer(18)
outLayer = LinearLayer(1)

n.addInputModule(inLayer)
n.addModule(hiddenLayer)
n.addOutputModule(outLayer)

in_to_hidden = FullConnection(inLayer, hiddenLayer)
hidden_to_out = FullConnection(hiddenLayer, outLayer)

n.addConnection(in_to_hidden)
n.addConnection(hidden_to_out)
n.sortModules()

DS = SupervisedDataSet(35,1)
DS.addSample(X,y)

Starting using pybrain to get a Neural Network to work on my diffusion energy data. I don't know how to get a dataset working from my X and y values. X is 35 inputs and y is 1 ouput and there are 148 samples. With this code I get the error: ""ValueError: could not broadcast input array from shape (148,35) into shape (35)""
Need to know how to properly prepare a dataset for pybrain.
",201
31253156,31253676,2,"I'd suggest you start with your DataFrame like you have already got:
slide   level    level_title
1       2         Pepsi
3       1         Pepper

Add a couple of columns with nothing in them:
In [24]: df['level1'] = pd.np.nan

In [25]: df['level2'] = pd.np.nan

Then set the values as needed with some conditional wizardry:
In [40]: df.loc[df['level'] == 2, 'level1'] = df.level_title

In [41]: df.loc[df['level'] == 1, 'level2'] = df.level_title

In [42]: df
Out[42]: 
   slide  level level_title level1  level2
0      1      2       Pepsi  Pepsi     NaN
1      3      1      Pepper    NaN  Pepper

(Spot the 'deliberate' mistake that I put the levels and the titles the wrong way round. But you get the idea!)
",162
31253156,31253471,2,"If there are only two levels, then you can use list comprehension , like this -
In [12]: df = pd.DataFrame([[slide, item[0], item[1], ''] if item[0] == '1' else [slide, item[0], '', item[1]] 
for slide, item in d.items()], columns=['Slide', 'Level', 'Level1 Title', 'Level2 Title'])

In [13]: df
Out[13]:
   Slide Level Level1 Title Level2 Title
0      1     2                     Pepsi
1      3     1       Pepper

",118
31253156,31253156,1,"I have this dictionary:
d = {1 : [ '2' , 'Pepsi' ], 3 : [ '1' , 'Pepper' ]}

df = pd.DataFrame([[slide, i[0], i[1]] for slide, item in d.items() for i in
item], columns=['Slide', 'Level', 'Level Title'])

I want to have the following output:
slide   level    'level1 title'   'level2 title'
1       2                          Pepsi
3       1         Pepper

So far, my code outputs this:
slide   level    'level title'
1       2         Pepsi
3       1         Pepper

Basically when the first item in the list is 1, the second item in the list should go to 'level1 title', and when the item is 2, the second item should go to 'level2 title' 
",152
31253163,31257416,2,"Your main issue is probably just reshaping your data so that you have date along one dimension and time along the other.  Once you do that you can use whatever plotting you like best (here I've used matplotlib's mplot3d, but it has some quirks).
What follows takes your data and reshapes it appropriately so you can then plot a surface that I believe is what your are looking for.  The key is using the pivot method, which restructures your data by date and time.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d

fname = 'total_watt.csv'

# Read in the data, but I skipped setting the index and made sure no data
# is lost to a nonexistent header
df = pd.read_csv(fname, parse_dates=[0], header=None, names=['datetime', 'watt'])

# We want to separate the date from the time, so create two new columns
df['date'] = [x.date() for x in df['datetime']]
df['time'] = [x.time() for x in df['datetime']]

# Now we want to reshape the data so we have dates and times making the result 2D
pv = df.pivot(index='time', columns='date', values='watt')

# Not every date has every time, so fill in the subsequent NaNs or there will be holes
# in the surface
pv = pv.fillna(0.0)

# Now, we need to construct some arrays that matplotlib will like for X and Y values
xx, yy = np.mgrid[0:len(pv),0:len(pv.columns)]

# We can now plot the values directly in matplotlib using mplot3d
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

ax.plot_surface(xx, yy, pv.values, cmap='jet', rstride=1, cstride=1)
ax.grid(False)

# Now we have to adjust the ticks and ticklabels - so turn the values into strings
dates = [x.strftime('%Y-%m-%d') for x in pv.columns]
times = [str(x) for x in pv.index]

# Setting a tick every fifth element seemed about right
ax.set_xticks(xx[::5,0])
ax.set_xticklabels(times[::5])
ax.set_yticks(yy[0,::5])
ax.set_yticklabels(dates[::5])

plt.show()

This gives me (using your data) the following graph:

Note that I've assumed when plotting and making the ticks that your dates and times are linear (which they are in this case).  If you have data with uneven samples, you'll have to do some interpolation before plotting.
",506
31253163,31253163,1,"My csv file is, 
https://github.com/camenergydatalab/EnergyDataSimulationChallenge/blob/master/challenge2/data/total_watt.csv
I want to visualize this csv file as clusters.
My ideal result would be the following image.(Higher points (red zone) would be higher energy consumption and lower points (blue zone) would be lower energy consumption.) 
I want to set x-axis as dates (e.g. 2011-04-18), y-axis as time (e.g. 13:22:00), and z-axis as energy consumption (e.g. 925.840613752523).

I successfully visualized the csv data file as values per 30mins with the following program.
from matplotlib import style
from matplotlib import pylab as plt
import numpy as np

style.use('ggplot')

filename='total_watt.csv'
date=[]
number=[]

import csv
with open(filename, 'rb') as csvfile:
    csvreader = csv.reader(csvfile, delimiter=',', quotechar='|')
    for row in csvreader:
        if len(row) ==2 :
            date.append(row[0])
            number.append(row[1])

number=np.array(number)

import datetime
for ii in range(len(date)):
    date[ii]=datetime.datetime.strptime(date[ii], '%Y-%m-%d %H:%M:%S')

plt.plot(date,number)

plt.title('Example')
plt.ylabel('Y axis')
plt.xlabel('X axis')

plt.show()


I also succeeded to visualize the csv data file as values per day with the following program.
from matplotlib import style
from matplotlib import pylab as plt
import numpy as np
import pandas as pd

style.use('ggplot')

filename='total_watt.csv'
date=[]
number=[]

import csv
with open(filename, 'rb') as csvfile:

    df = pd.read_csv('total_watt.csv', parse_dates=[0], index_col=[0])
    df = df.resample('1D', how='sum')





import datetime
for ii in range(len(date)):
    date[ii]=datetime.datetime.strptime(date[ii], '%Y-%m-%d %H:%M:%S')

plt.plot(date,number)

plt.title('Example')
plt.ylabel('Y axis')
plt.xlabel('X axis')

df.plot()
plt.show()


Although I could visualize the csv file as values per 30mins and per days, I do not have any idea to visualize the csv data as clusters in 3D..
How can I program it...?
",447
31253229,31253274,2,"You're creating two, since Toplevel() is the constructor call:
Toplevel()
Toplevel().wm_title(""Directory"")

Instead, create one and save it:
top = Toplevel()
top.wm_title(""Directory"")

",45
31253229,31253229,1,"I have a program that uses Tkinter and I'm trying to assign a command to a button in my root window that opens one additional window. I'm using Toplevel(), but whenever I click the button I've assigned the command to, two windows open, one with my root window's name and one with the name of the additional window I've assigned. 
I've tried using .withdraw and .destroy, to hide or remove this extra root window, but nothing seems to be working. 
Here is my code:
import Tkinter
from Tkinter import *

root = Tk()
root.wm_title(""VACS"")

# # Top label # #

SetParameters = Label(text=""Set Parameters"", width=110, relief=RIDGE)
SetParameters.grid(row=1, column=0, columnspan=7, padx=5, pady=5)

# # Spatial freq settings # #

SpatialFreq = Label(text=""Spatial Frequency"", width=15, relief=RIDGE)
SpatialFreq.grid(row=3, column=0, padx=5, pady=5)

From1 = Label(text=""from"")
From1.grid(row=3, column=1, padx=5, pady=5)

Select1 = Spinbox(from_=0, to=10, width=25)
Select1.grid(row=3, column=2, padx=5, pady=5)

To1 = Label(text=""to"")
To1.grid(row=3, column=3, padx=5, pady=5)

Select2 = Spinbox(from_=0, to=10, width=25)
Select2.grid(row=3, column=4, padx=5, pady=5)

Steps = Label(text=""in steps of"")
Steps.grid(row=3, column=5, padx=5, pady=5)

Select3 = Spinbox(from_=0, to=10, width=25)
Select3.grid(row=3, column=6, padx=5, pady=5)

# # Contrast settings # #

Contrast = Label(text=""Contrast"", width=15, relief=RIDGE)
Contrast.grid(row=5, column=0, padx=5, pady=5)

From2 = Label(text=""from"")
From2.grid(row=5, column=1, padx=5, pady=5)

Select4 = Spinbox(from_=0, to=10, width=25)
Select4.grid(row=5, column=2, padx=5, pady=5)

To2 = Label(text=""to"")
To2.grid(row=5, column=3, padx=5, pady=5)

Select5 = Spinbox(from_=0, to=10, width=25)
Select5.grid(row=5, column=4, padx=5, pady=5)

Steps2 = Label(text=""in steps of"")
Steps2.grid(row=5, column=5, padx=5, pady=5)

Select6 = Spinbox(from_=0, to=10, width=25)
Select6.grid(row=5, column=6, padx=5, pady=5)

# # Test button # #

Test = Button(text=""Begin Test"", width=25, command=Top)
Test.grid(row=6, column=0, columnspan=7, pady=5)

# # Directory input window # #

def Top():
    Toplevel()
    Toplevel().wm_title(""Directory"")

root.mainloop()

If you click ""Begin Test"" in the root window, two extras pop up. I only want the one that says ""Directory."" 
Any ideas?
",536
31256091,31256091,1,"All I'm trying to make is a program that loops through a word or phrase character-by-character like this:
word: ""dog""
d
do
dog
do
d

I wrote this:
word = 'factory'
temp_word = ''
temp_word2 = ''

# Builds up 'factory' letter-by-letter into temp_word
for i in word:
    temp_word += i
    print(temp_word)

# Takes letters off 1 by 1
for i in reversed(word):
    temp_word2 = temp_word.replace(i, """")
    temp_word = temp_word2
    print(temp_word2)

and the output is exactly what I want:
f
fa
fac
fact
facto
factor
factory
factor
facto
fact
fac
fa
f

But, if there is a repeated letter, it'll remove both at once, like this:
h
he
hel
hell
hello
hell
he
he
h

How can I make it just remove the one letter, without removing both? Can't think of a solution
",165
31256091,31256103,2,"You can pass count as 1 to replace to only replace one occurrence.
temp_word2 = temp_word.replace(i, """",1)

You could also just slice the last letter off:
for i in reversed(word):
    temp_word2 = temp_word[:-1]

",49
31256091,31256205,2,"Do you have to use a tempvariable, or are you allowed to just use string slicing?
for i in range(1,len(word)+1):
    print(word[:i])

for i in range(len(word)-1,0,-1): #the third argument of range is step, and -1 causes range to step backwards
    print(word[:i])

",78
31256091,31256220,2,"Here is a solution, it's not the best. Depending of what you want to do with this, I can provide a more specific solution
word = 'hello'
temp_word = ''
temp_word2 = ''

for k in word:
    temp_word += k
    print temp_word
while temp_word:
    temp_word = temp_word[:-1]
    print temp_word 

EDIT
I prefer this version more
word = 'hello'
for k in list(range(1,len(word)+1)) + list(range(len(word)-1,1,-1)):
    print word[:k]

",105
31256157,31256157,1,"I'm using Python 3.4.3 and PyQt 5.4.2 and have some problems, when trying to get data from rowsInserted signal.
I want to get data from all items near the dropped item after drop operation.
Here some of my code (whole model class is too large, hope this part is enough):
class PlModel(QtGui.QStandardItemModel):
    def __init__(self):
        self.rowsInserted.connect(self.printSomeData)

    def supportedDragActions(self):
        return QtCore.Qt.MoveAction

    def supportedDropActions(self):
        return QtCore.Qt.CopyAction | QtCore.Qt.MoveAction

    def canDropMimeData(self, QMimeData, Qt_DropAction, p_int, p_int_1, QModelIndex):
        if not self.itemFromIndex(QModelIndex) and p_int_1 == p_int == -1:
            return False
        elif QModelIndex.isValid():
            if self.item(self.itemFromIndex(QModelIndex).row(), 0) is None:
                return False
            else:
                return QtGui.QStandardItemModel.canDropMimeData(self, QMimeData, Qt_DropAction, p_int, p_int_1, QModelIndex)
        else:
            return QtGui.QStandardItemModel.canDropMimeData(self, QMimeData, Qt_DropAction, p_int, p_int_1, QModelIndex)

    def dropMimeData(self, mimedata, dropaction, row, col, modelindex):
        if not self.itemFromIndex(modelindex):
            if row == col == -1:
                return False    
            else:
                return QtGui.QStandardItemModel.dropMimeData(self, mimedata, dropaction, row, 0, self.index(row, 0))
        elif self.itemFromIndex(modelindex).isDragEnabled():
            if row == col == -1:
                crow = self.item(modelindex.row(), 0).rowCount()
                return QtGui.QStandardItemModel.dropMimeData(self, mimedata, dropaction, crow, 0, self.indexFromItem(self.item(modelindex.row(), 0)))
            else:
                return QtGui.QStandardItemModel.dropMimeData(self, mimedata, dropaction, row, 0, modelindex)
        elif self.itemFromIndex(modelindex).isDropEnabled():
            if col != -1:
                return False
            else:
                return QtGui.QStandardItemModel.dropMimeData(self, mimedata, dropaction, row, 0, modelindex)
        else:
            return QtGui.QStandardItemModel.dropMimeData(self, mimedata, dropaction, row, 0, modelindex)

    def printSomeData(self, ind: QtCore.QModelIndex, first, last):
        row = ind.row()
        item = self.itemFromIndex(ind)
        for i in range(0, item.rowCount()):
            print(self.index(i, 0, ind).data(QtCore.Qt.DisplayRole))

This code prints valid data for all items, excluding dropped item. For dropped item it prints 'None'. After dropping in QTreeView all data is OK. What wrong with it?
Sorry for my bad English.
",448
31256157,31273784,2,"It seems like rowsInserted signal emits after rows were inserted, but before model fills data in inserted rows.
So I call printSomeData function from dropMimeData function (after data was dropped) and it works.
Here corrected code:
class PlModel(QtGui.QStandardItemModel):
    def __init__(self):
        QtGui.QStandardItemModel.__init__(self)
        # no need for connect
        #self.rowsInserted.connect(self.printSomeData)

    def supportedDragActions(self):
        return QtCore.Qt.MoveAction

    def supportedDropActions(self):
        return QtCore.Qt.CopyAction | QtCore.Qt.MoveAction

    def canDropMimeData(self, QMimeData, Qt_DropAction, p_int, p_int_1, QModelIndex):
        if not self.itemFromIndex(QModelIndex) and p_int_1 == p_int == -1:
            return False
        elif QModelIndex.isValid():
            if self.item(self.itemFromIndex(QModelIndex).row(), 0) is None:
                return False
            else:
                return QtGui.QStandardItemModel.canDropMimeData(self, QMimeData, Qt_DropAction, p_int, p_int_1, QModelIndex)
        else:
            return QtGui.QStandardItemModel.canDropMimeData(self, QMimeData, Qt_DropAction, p_int, p_int_1, QModelIndex)

    def dropMimeData(self, mimedata, dropaction, row, col, modelindex):
        if not self.itemFromIndex(modelindex):
            if row == col == -1:
                return False    
            else:
                result = QtGui.QStandardItemModel.dropMimeData(self, mimedata, dropaction, row, 0, self.index(row, 0))
                if result:
                    self.printSomeData(self.index(row, 0))
                return result
        elif self.itemFromIndex(modelindex).isDragEnabled():
            if row == col == -1:
                crow = self.item(modelindex.row(), 0).rowCount()
                result = QtGui.QStandardItemModel.dropMimeData(self, mimedata, dropaction, crow, 0, self.indexFromItem(self.item(modelindex.row(), 0)))
                if result:
                    self.printSomeData(self.indexFromItem(self.item(modelindex.row(), 0)))
                return result
            else:
                result = QtGui.QStandardItemModel.dropMimeData(self, mimedata, dropaction, row, 0, modelindex)
                if result:
                    self.printSomeData(modelindex)
                return result
        elif self.itemFromIndex(modelindex).isDropEnabled():
            if col != -1:
                return False
            else:
                result = QtGui.QStandardItemModel.dropMimeData(self, mimedata, dropaction, row, 0, modelindex)
                if result:
                    self.printSomeData(modelindex)
                return result
        else:
            result = QtGui.QStandardItemModel.dropMimeData(self, mimedata, dropaction, row, 0, modelindex)
                if result:
                    self.printSomeData(modelindex)
                return result

    def printSomeData(self, ind: QtCore.QModelIndex):
        row = ind.row()
        item = self.itemFromIndex(ind)
        for i in range(0, item.rowCount()):
            print(self.index(i, 0, ind).data(QtCore.Qt.DisplayRole))

",463
31256159,31256227,2,"IIUC, you could use itertools.accumulate to generate a forward fill:
>>> from itertools import accumulate
>>> a = [None,1,2,3,None,4,None,None]
>>> list(accumulate(a, lambda x,y: y if y is not None else x))
[None, 1, 2, 3, 3, 4, 4, 4]

",74
31256159,31256231,2,"Here's some code that will do what you want in place, if you don't want it in place then just pass it list(my_list) instead of my_list.
def replaceNoneWithLeftmost(val):
    for i in range(len(val)):
        if val[i] is None:
            for j in range(i-1, -1, -1):
                if val[j] is not None:
                    val[i] = val[j]
                    break
    for i in range(len(val)):
        if val[i] is None:
            for j in range(i+1, len(val)):
                if val[j] is not None:
                    val[i] = val[j]
                    break
    return val

Also, if using python2, use xrange instead of range.
",154
31256159,31256246,2,"a = [None,1,2,3,None,4,None,None]

start = next(ele for ele in a if ele is not None)
for ind, ele in enumerate(a):
    if ele is None:
        a[ind] = start
    else:
        start = ele
print(a)
[1, 1, 2, 3, 3, 4, 4, 4]

You also only need to set start to a value if the first element is None:
if a[0] is None:
   start = next(ele for ele in a if ele is not None)
for ind, ele in enumerate(a):
    if ele is None:
        a[ind] = start
    else:
        start = ele
print(a)

",143
31256159,31256466,2,"a = [None,1,2,3,None,4,None,None]

first = True
for i in range(len(a)):
    if first:
        if a[i] != None:
            a[:i] = [a[i] for _ in range(i)]
            first = False
    if a[i] == None:
        a[i] = a[i-1]

print a

OUT:
[1, 1, 2, 3, 3, 4, 4, 4]

",97
31256159,31256159,1,"Given
a = [None,1,2,3,None,4,None,None]

I'd like
a = [None,1,2,3,3,4,4,4]

Currently I have brute forced it with:
def replaceNoneWithLeftmost(val):
    last = None
    ret = []
    for x in val:
        if x is not None:
            ret.append(x)
            last = x
        else:
           ret.append(last)
    return ret

Finally, I'd like to get to
a = [1,1,2,3,3,4,4,4]

by running this right to left. Currently I have
def replaceNoneWithRightmost(val):
    return replaceNoneWithLeftmost(val[::-1])[::-1]

I'm not fussy about inplace or create a new list, but right now this smells to me. I can't see a way to store a temporary 'last' value and use map/lambda, and nothing else is coming to mind.
",154
31256174,31265221,2,"django-lazysignup, which you are using, allows you to deliver a custom LazyUser class (here). All you need to do is to write a subclass of lazysignup.models.LazyUser with defined is_authenticated method and set it as settings.LAZYSIGNUP_USER_MODEL.
But this is not the end of your troubles. Lots of django apps 
assume that authenticated user has some properties. Mainly, is_staff, is_superuser, permissions, groups. First and foremost, django.contrib.admin needs them to check if it can let the user in and what to show him. Look how django.contrib.auth.models.AnonymousUser mocks them and copy it. Remark: look how AnonymousUser is not subclassing any user class nor db.Model. Supplied user class only needs to quack.
",126
31256174,32236034,2,"Ended up just having to replace all calls to User.is_authenticated().
To prevent django-allauth from redirecting lazy-users from the login page, this ended up looking something like this:
from allauth.account.views import AjaxCapableProcessFormViewMixin

def _ajax_response(request, response, form=None):
    if request.is_ajax():
        if (isinstance(response, HttpResponseRedirect)
            or isinstance(response, HttpResponsePermanentRedirect)):
            redirect_to = response['Location']
        else:
            redirect_to = None
        response = get_adapter().ajax_response(request,
                                           response,
                                           form=form,
                                           redirect_to=redirect_to)
    return response


class RedirectUserWithAccountMixin(object):
    def dispatch(self, request, *args, **kwargs):
        self.request = request
        if user_has_account(request.user):
            redirect_to = self.get_authenticated_redirect_url()
            response = HttpResponseRedirect(redirect_to)
            return _ajax_response(request, response)
        else:
            response = super(RedirectUserWithAccountMixin,
                             self).dispatch(request,
                                            *args,
                                            **kwargs)
        return response

    def get_authenticated_redirect_url(self):
        redirect_field_name = self.redirect_field_name
        return get_login_redirect_url(self.request,
                                      url=self.get_success_url(),
                                      redirect_field_name=redirect_field_name)

class LoginView(RedirectUserWithAccountMixin,
            AjaxCapableProcessFormViewMixin,
            FormView):
...

Where user_has_account() was my own method for checking whether the user was actually signed in.
",211
31256174,31256174,1,"I installed django-lazysignup and am facing the challenge now of User.is_authenticated() returning True, for what are not actually authenticated users, but instead lazy-signup users.  I can update any checks for User.is_authenticated() in my code with my own function.  However, other packages like django-allauth, check this method to decide whether a user is already signed-in, and redirect from attempts to reach the login page.
class RedirectAuthenticatedUserMixin(object):
    def dispatch(self, request, *args, **kwargs):
        self.request = request
        if request.user.is_authenticated():
            redirect_to = self.get_authenticated_redirect_url()
            response = HttpResponseRedirect(redirect_to)
            return _ajax_response(request, response)
...

Are there any recommendations that don't require replacing is_authenticated() in every package that I include?  Seems most of them expect it to function a particular way, and django-lazysignup turns that on its head.  Could I monkey patch the User model with a new is_authenticated() method?  If it's possible, where would I do this so that it's attached to the page request?
",194
31256238,31257573,2,"If your session.execute writes were not successful (they did not meet the required consistency level), then the driver will raise one of the following exceptions:

Unavailable - There were not enough live replicas to satisfy the requested consistency level, so the coordinator node immediately failed the request without forwarding it to any replicas.
Timeout - Replicas did not respond to the coordinator before cassandra timeout.
Write timeout - Replicas did not respond to the coordinator before the write timeout. Configured in cassandra.yaml. There is a similar timeout for reads, read and write timeouts are configured separately in the yaml.
Operation timeout - Operation took longer than the specified client side timeout. Configure in your application code.

You can try tracing your queries and find out what exactly happened for each write. This will show you the coordinators and the replica nodes involved in the operation and how much time the request spent in each.
",169
31256238,31256238,1,"Solved
I was testing update on 3 nodes, and the time on one of those nodes was 1 second behind, so when update a row, the write time is always behind the timestamp, cassandra would not update the rows. I sync all nodes time, and the issue fixed.
Edit:
I double checked the result, all insertions are succeed, partial updates failed. There's no error/exception messages
I have a cassandra cluster(Cassandra 2.0.13) which contains 5 nodes. Using python(2.6.6) cassandra driver(2.6.0c2) for inserting data into database.  my server systems are Centos6.X
The following code is how i connect to cassandra and get session.  I provided at most 2 nodes ip addresses, and select the keyspace.
def get_cassandra_session():
    """"""creates cluster and gets the session base on key space""""""
    # be aware that session cannot be shared between threads/processes
    # or it will raise OperationTimedOut Exception
    if CLUSTER_HOST2:
        cluster = cassandra.cluster.Cluster([CLUSTER_HOST1, CLUSTER_HOST2])
    else:
        # if only one address is available, we have to use older protocol version
        cluster = cassandra.cluster.Cluster([CLUSTER_HOST1], protocol_version=1)

    session = cluster.connect(KEY_SPACE)
    return session 

For each row, I have 17 columns and if the key does not exist in database, I will use session insert key with the rest columns default values, and then update specific column's value.
def insert_initial_row(session, key):
    session.execute(INITIAL_INSERTION_STATEMENT, tuple(INITIAL_COLUMNS_VALUES))


def update_columnX(session, key, column):
    session.execute(""INSERT INTO "" + TABLE + ""("" + KEY + "","" + COLUMN_X + "") VALUES(%s, %s)"", (key, column))

def has_found(session, key):
    """"""checks key is in database or not""""""
    query = ""SELECT "" + ""*"" + "" FROM "" + KEY_SPACE + ""."" + TABLE \
            + "" WHERE "" + KEY + "" = "" + ""'"" + key + ""'""
    # returns a list
    row = session.execute(query)
    return True if row else False

the following is how I invoke them:
for a_key in keys_set:
    """"""keys_set contains 100 no duplicate keys""""""
    if has_found(session, a_key):
        update_columnX(session, a_key, ""column x value"")
    else:
        """"""the key is not in db, initialize it with all default values, then update column x""""""
        insert_initial_row(session,  a_key)
        if has_found(sessin, a_key):
            update_columnX(session,  a_key, ""column x value"")
        else:
            logger.error(""not initialized correctly..."")

I was trying to insert 100 rows and update each row's columnX, but only partial of those 100 rows can be updated, the rest rows columnX are the default values.insert_initial_row has been invoked and initialized default values for all 100 lines, but the update_columnX does not. Event I change the consistency level to Quorum, it doesnt help at all.  ""not initialized correctly..."" never printed out, and I added a print line in update_columnX and the line is printed 100 time, so it is invoked 100 times, but not all of them updated. 
Any idea? Please help.
Thanks
",631
31256252,31257909,2,"np.linalg.solve(A, b) does not compute the inverse of A. Instead it calls one of the gesv LAPACK routines, which first factorizes A using LU decomposition, then solves for x using forward and backward substitution (see here).
np.linalg.inv uses the same method to compute the inverse of A by solving for A-1 in A·A-1 = I where I is the identity*. The factorization step is exactly the same as above, but it takes more floating point operations to solve for A-1 (an n×n matrix) than for x (an n-long vector). Additionally, if you then wanted to obtain x via the identity A-1·b = x then the extra matrix multiplication would incur yet more floating point operations, and therefore slower performance and more numerical error.
There's no need for the intermediate step of computing A-1 - it is  faster and more accurate to obtain x directly.

* The relevant bit of source for inv is here. Unfortunately it's a bit tricky to understand since it's templated C. The important thing to note is that an identity matrix is being passed to the LAPACK solver as parameter B.
",211
31256252,31256252,1,"I do not quite understand why numpy.linalg.solve() gives the more precise answer, whereas numpy.linalg.inv() breaks down somewhat, giving (what I believe are) estimates. 
For a concrete example, I am solving the equation C^{-1} * d  where C denotes a matrix, and d is a vector-array. For the sake of discussion, the dimensions of C are shape (1000,1000) and d is shape (1,1000). 
numpy.linalg.solve(A, b) solves the equation A*x=b for x, i.e. x = A^{-1} * b. Therefore, I could either solve this equation by
(1)
inverse = numpy.linalg.inv(C)
result = inverse * d

or (2)
numpy.linalg.solve(C, d)

Method (2) gives far more precise results. Why is this?  
What exactly is happening such that one ""works better"" than the other? 
",169
31256269,31257236,2,"[Updated answer]
The problem comes in when the 'request' object's url is used by webapp2:
if uri.startswith(('.', '/')):
    request = request or get_request()
    uri = str(urlparse.urljoin(request.url, uri))

In this case you are overriding the RequestHandler's 'self.request' attribute with your own (to call GitHub):
self.request = urllib2.Request(GITHUB_URL_ACCESSTOKEN, self.data)

I surmise that there is no 'url' on this new request object.
I suggest that you use a different variable name, or don't store the Github request on 'self'.
=======================
[Old answer]
Going out on a limb here based on the given info, but is your handler class extending webapp2.RequestHandler? If not the url attribute may not exist in 'self'.
Please include your handler class definition and your (minimal) handler method if this is not the case.
ie:
class SomeHandler(webapp2.RequestHandler):
   ...

",181
31256269,31256269,1,"I'm writing a small web app on AppEngine/Python for accessing the GitHub API. 
I've successfully performed the Oauth2 flow (ie. I can access the logged user info). What I'd like to do now is that as the user goes back to my webpage uri which I specified as the redirect_uri for GitHub, having authorized the app, I make the request for obtaining the access token and then redirect the user to the homepage. 
Now, if I perform the redirect with self.redirect(""/"") at the end of the handler for the redirect_uri, Python trows the error AttributeError: url. 
What am I doing wrong? 
Here's the class definition of the handler for redirect_uri
class RedirectGithub(webapp2.RequestHandler):
    def get(self):
        self.github_code = self.request.get(""code"")
        self.payload = {
        ""client_id"": GITHUB_CLIENT_ID, 
        ""client_secret"": GITHUB_CLIENT_SECRET,  
        ""code"": self.github_code, 
        ""redirect_uri"": GITHUB_REDIRECT_URI, 
    }

    self.data = urllib.urlencode(self.payload)
    self.request = urllib2.Request(GITHUB_URL_ACCESSTOKEN, self.data)
    self.github_response = urllib2.urlopen(self.request)
    github_access_token = urlparse.parse_qs(self.github_response.read())
    self.redirect(""/"")

Here's the full stack trace
    Traceback (most recent call last):
      File ""/base/data/home/runtimes/python27/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 1511, in __call__
        rv = self.handle_exception(request, response, e)
      File ""/base/data/home/runtimes/python27/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 1505, in __call__
        rv = self.router.dispatch(request, response)
      File ""/base/data/home/runtimes/python27/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 1253, in default_dispatcher
        return route.handler_adapter(request, response)
      File ""/base/data/home/runtimes/python27/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 1077, in __call__
        return handler.dispatch()
      File ""/base/data/home/runtimes/python27/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 547, in dispatch
        return self.handle_exception(e, self.app.debug)
      File ""/base/data/home/runtimes/python27/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 545, in dispatch
        return method(*args, **kwargs)
      File ""/base/data/home/apps/MY_APP"", line 125, in get
        self.redirect(""/"")
      File ""/base/data/home/runtimes/python27/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 583, in redirect
        response=self.response)
      File ""/base/data/home/runtimes/python27/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 1740, in redirect
        uri = str(urlparse.urljoin(request.url, uri))
      File ""/base/data/home/runtimes/python27/python27_dist/lib/python2.7/urllib2.py"", line 229, in __getattr__
        raise AttributeError, attr
    AttributeError: url

",397
31256286,31256286,1,"I have a complex document that I am trying to structure most conveniently and efficiently with JSON in Python.  I would like to be able to retrieve one of the items in my document with one line (i.e. not via a for loop)
A demo of the structure looks like this:
{
    ""movies"": {
        ""0"": {
            ""name"": ""charles"",
            ""id"": 0, 
            ""loopable"": true
        },
        ""1"": {
            ""name"": ""ray"",
            ""id"": 1, 
            ""loopable"": true
        }
    }
}

I am trying to be able to easily fetch a movie based on its id field.  To do this, right now, I have made the index the same as the key to the movies object.   So when I json.load the object to find movie 1's name I can just do movie[(id)]['name']
It seems like I should have a list of movies in the json file but it also seems like that would be more complicated. It could look like this:
{
    ""movies"": [
        {
            ""name"": ""charles"",
            ""id"": 0, 
            ""loopable"": true
        },
        {
            ""name"": ""ray"",
            ""id"": 1, 
            ""loopable"": true
        }
    ]
} 

but if that were the case I would have to loop through the entire array like this:
for movie in movies:
    if movie['id'] == (id)
        # Now I can get movie['id']['name']

Is there a more effiecient way of doing this?
",319
31256286,31256350,2,"Let 'movies' be a dict and not a list:
{
    ""movies"": {
        ""12"": {
            ""name"": ""charles"",
            ""id"": 12, 
            ""loopable"": true
        },
        ""39"": {
            ""name"": ""ray"",
            ""id"": 39, 
            ""loopable"": true
        }
    }
} 

and you can access movie by id with yourjson['movies'][str(id)]
",89
31256360,31256360,1,"So, recently, I've been experimenting with the multiprocessing module. I wrote this script to test it:
from multiprocessing import Process
from time import sleep

def a(x):
    sleep(x)
    print (""goo"")

a = Process(target=a(3))
b = Process(target=a(5))
c = Process(target=a(8))
d = Process(target=a(10))

if __name__ == ""__main__"":
    a.start()
    b.start()
    c.start()
    d.start()

However, when I try to run it, it throws this error:
goo
Traceback (most recent call last):
  File ""C:\Users\Andrew Wong\Desktop\Python\test.py"", line 9, in <module>
    b = Process(target=a(5))
TypeError: 'Process' object is not callable

...And I can't tell what's going on.
Does anyone know what happened, and how I can fix it?
",179
31256360,31256426,2,"Pass arguments to the function that is ran by a Process is done differently - looking at the documentation it shows:
from multiprocessing import Process

def f(name):
    print 'hello', name

if __name__ == '__main__':
    p = Process(target=f, args=('bob',)) # that's how you should pass arguments
    p.start()
    p.join()

Or in your case:
from multiprocessing import Process
from time import sleep

def a(x):
    sleep(x)
    print (""goo"")

e = Process(target=a, args=(3,))
b = Process(target=a, args=(5,))
c = Process(target=a, args=(8,))
d = Process(target=a, args=(10,))

if __name__ == ""__main__"":
    e.start()
    b.start()
    c.start()
    d.start()

Addition:
Good catch by Luke (in the comments below) - you're overriding the function a with the variable name a when doing:
a = Process(target=a, args=(3,))

You should use a different name.
",212
31256397,31256397,1,"I have data of the following form:
#@ <abc>
 <http://stackoverflow.com/questions/ask> <question> _:question1 .
#@ <def>
<The> <second> <http://line> .
#@ <ghi>
 _:question1 <http#responseCode> ""200""^^<http://integer> .
#@ <klm>
<The> <second> <http://line1.xml> .
#@ <nop>
 _:question1 <date> ""Mon, 23 Apr 2012 13:49:27 GMT"" .
#@ <jkn>
<G> <http#fifth> ""200""^^<http://integer> .
#@ <k93>
 _:question1 <http#responseCode> ""200""^^<http://integer> .
#@ <k22>
<This> <third> <http://line2.xml> .
#@ <k73>
 <http://site1> <hasAddress> <http://addr1> .
#@ <i27>
<kd8> <fourth> <http://addr2.xml> .

Now whenever two lines are equal, like: _:question1 <http#responseCode> ""200""^^<http://integer> ., then I want to delete the equal lines (lines which match with each other character by character are equal lines) along with (i). the subsequent line (which ends with a fullstop) (ii). line previous to the equal line (which begins with #@).
#@ <abc>
 <http://stackoverflow.com/questions/ask> <question> _:question1 .
#@ <def>
<The> <second> <http://line> .
#@ <nop>
 _:question1 <date> ""Mon, 23 Apr 2012 13:49:27 GMT"" .
#@ <jkn>
<G> <http#fifth> ""200""^^<http://integer> .
#@ <k73>
 <http://site1> <hasAddress> <http://addr1> .
#@ <i27>
<kd8> <fourth> <http://addr2.xml> .

Now one way to do this is to store all these lines in a set in python and whenever two lines are equal (i.e. they match character by character) the previous and subsequent two lines are deleted. However, the size of my dataset is 100GB (and I have RAM of size 64GB), therefore I can not keep this information in set form in main-memory. Is there some way by which I can delete the duplicate lines along with their previous and subsequent two lines in python with limited main-memory space (RAM size 64 GB)
",505
31256397,31256671,2,"Keep a boolean hashtable of hash codes of lines already seen.
For each line:

if line hash()es to something you have already seen, you have a potential match: scan the file to check if it really is a duplicate.
if line hash()es to a new hash, just mark the hash for the first time.

Dedicate as much memory you can to this hashtable, and the false positive rate will be low (i.e. less times you will have to scan for duplicates and found none).
Example:
table_size = 2**16
seen = [False]*table_size
for line in file:
    h = hash(line) % table_size
    if seen[h]:
        dup = False
        with open('yourfile','r') as f:
            for line1 in f:
                if line == line1:
                    dup = True
                    break
            if not dup:
                print(line)
    else:
        seen[h] = True
        print(line)

As it has been pointed out, since you cannot store all the lines in memory you don't have many options, but at least this option doesn't require to scan the file for every single line, because most of the entries in the table will be False, i.e. the algorithm is sub-quadratic if the tabe is not full; it will degenerate to O(n2) once the table is full.
You can make a very memory-efficient implementation of the hash table, that requires only 1 bit for each hash code (e.g. make it an array of bytes, where each byte can store 8 boolean values)

See also Bloom Filters for more advanced techniques.
",310
31256397,31256687,2,"One fairly straightforward way - make a version of your data such that each line includes a field with its line number. Use unix 'sort' to sort that new file, excluding the line number field. The sort utility will merge sort the file even if it exceeds the size of available memory. Now you have a new file in which the duplicates are ordered, along with their original line numbers. Extract the line numbers of the duplicates and then use that as input for linearly processing your original data. 
In more detailed steps. 

Make a new version of your file such that each line is prepended by its line number. So, ""someline"" becomes ""1, someline"" 
sort this file using the unix sort utility  - sort -t"","" -k2,2 file
Scan the new file for consecutive duplicate entries in the second field
the line numbers (first field) of such entries are the line numbers of duplicate lines in your original file - extract these and use them as input to remove duplicates in the original data. Since you know exactly where they are, you need not read in the entire file or create a giant in-memory structure for duplicates

The advantage of this method compared to some of the others suggested - it always works, regardless of the size of the input and the size of your available memory and it does not fail due to hash collisions or other probabilistic artifacts. You are leveraging the merge sort in unix sort where the hard stuff - dealing with larger-than-memory input - 
 has been done for you. 
",290
31256397,31257069,2,"Here's an outline of how I'd do it using UNIX sort/uniq:

Modify the data format so that each record is a single line. You could do this using the methods here.
Sort the data with the sort command. Note the you can specify which fields are important with the --key option, you might need to exclude the #@ <abc> part by selecting all the other fields as keys (I wasn't entirely sure from your description).
Apply the uniq command to the sorted output to get only the unique lines.

This should all work fine on out-of-core data as far as I know.
",119
31256444,31256444,1,"My goal: to extract all of the transcripts in this url, and clean them for my particular use.
I need to recursively extract links which follow a pattern. I am a newbie and am having trouble formulating the full code that will work.
Here are some examples of how the URLs will look:
http://tvmegasite.net/transcripts/oltl/main/1998transcripts.shtml
http://tvmegasite.net/transcripts/oltl/older/2004/oltl-trans-01-20-04.htm
http://tvmegasite.net/transcripts/amc/main/2003transcripts.shtml
http://tvmegasite.net/transcripts/amc/older/2002/amc-trans-01-08-02.shtml

so all begin with http://tvmegasite.net/transcripts, then the show abbreviation, then main or older, etc.
What I've tried so far:
Getting urls from a particular page is easy with BeautifulSoup but I haven't figured out how to do it recursively. I was thinking of just using a scraper like Scrapy to get all the urls starting from tvmegasite.net/transcripts, and then using the re package to search for ones that match the pattern. I'm still not sure how to make this into a full code.
From what I can guess, these are possibly the kinds of regular expressions that can work:
http://tvmegasite.net/transcripts\w+\/main/\d+\w+\.shtml
http://tvmegasite.net/transcripts\w+\/older/\d+/\w+\-\w+\-\d+\-\d+\.shtml

",192
31256444,31261611,2,"If you use Scrapy you do not need regular expressions -- or at least you can limit them to a minimum. For example with the LxmlLinkExtractor you can set up which URLs to follow (allow) and in which XPath-branch (restrict_xpaths).
And you can use your regular expressions (which look fine for me at first glance) in in the allow restriction -- and for this site you do not need a restriction on XPath.
",82
31256450,31270049,2,"For oldest task you should probably use created_at rather than modified_at (the oldest task could have been modified recently, after all).
We don't have any way in the API to specifically get the older task, or order by creation time or anything like that, so for now your workaround is in fact the only way to do it. You may be able to make it a little faster by using ?opt_fields=created_at in the task query to cut down on how much data you're loading.
",95
31256450,31256450,1,"So I have a little bit of a dilemma in trying to obtain the oldest task of a user. I’m able to get the oldest task of each user in my team's workspace, but takes 1 hour and 30 minutes to run.
Here's the current workflow that I have currently:
1) Get all users and store in a users var.
2) Iterate through all users.
3) For each user, grab all tasks and store in a tasks var.
4) Iterate through all tasks, compare task.modified_at attribute to oldest_task var, store in oldest_task if task.modified_at is older.

What's taking so long is the loading of and iterating through each user's tasks. Any ideas on how to make this process faster? Would there potentially be a way to query specifically for the oldest task of a user through the API?
I’m building a tool that will track the age of the oldest task for each user for tracking purposes. I would love to find out if there’s something that I’m missing.
",201
31256469,31256469,1,"I am trying to locate a visible element that will change based on what the user enters on the website. I am successful if use the follow with a static xpath search string:
wait.until(EC.visibility_of_element_located((By.XPATH, ""//a[text()='Default-Test']"")))

Default-Test will change arbitrarily and i have how to get this value but I have not been successful using a variable in the xpath search:
Test method 1 Does not work
dtg_found = ""Default-Test"" 

Test method 2 does not work, this is the actual method for locating the value
dtg_found = driver.find_element_by_name(""result[0].col[1].stringVal"").get_attribute(""value"")

dtg_opt_1 = wait.until(EC.visibility_of_element_located((By.XPATH, ""\""//a[text()='"" + dtg_found + ""']"" + '""'))) 

",164
31256469,31329850,2,"This is the solution to the issue I was encountering.  Tried reworking the string and the related escapes '\' but was not successful. Did however get the following working. I am not clear why this worked.
dtg_opt_1 = driver.find_element_by_xpath(""//*[contains(text(),"" + "" '"" + dtg_found + ""'"" + "")]"")

",71
31256484,31256484,1,"I am looking for a pythonic way to query something like this:
# Models
from django.contrib.auth.models import User


class UserProfile(models.Model):
    user = models.OneToOneField(User, related_name='user_profile_set')
    district = models.ForeignKey(District)
    ...


class District(models.Model):
    name : ...

I'd like to get a list of district (or pks) like what you get when use values_list method, but making the query to User Model. I've tried this:
User.objects.select_related('user_profile').values('user_profile_set__district__pk')
User.objects.all().values('user_profile_set__district__name')
User.objects.all().values_list('user_profile_set__district__name')

But nothing works, I am using django 1.7 (depth parameter is not longer available for select_related method). Thanks in advance.
",136
31256484,31256581,2,"Queries across relationships don't use the _set syntax. Try this:
User.objects.all().values('userprofile__district__name').distinct()

",24
31256484,31257996,2,"You could try to execute the query like this:
UserProfile.objects.values('district__name')

And if you need to query from an especified model instance:
user_profile.objects.filter(user=my_user).values('district__name')

Also if you are using User.objects.all().values('user_profile_set__district__name') and getting this output:
[{'user_profile_set__district__name': None},  
{'user_profile_set__district__name': None}, 
{'user_profile_set__district__name': None} ... ]

It might mean that your save method is not working correctly or at least for the name field, you might want to use the shell and query your District objects to verify if they have a name.
",116
31256583,31256583,1,"After the installation of motionless library, I try to run my code and the following error  message occurs. 
**Traceback (most recent call last):
File ""C:\Users\Kevin\Downloads\tracker.py"", line 4, in <module>

from motionless import CenterMap

File ""<frozen importlib._bootstrap>"", line 2237, in _find_and_load
File ""<frozen importlib._bootstrap>"", line 2222, in _find_and_load_unlocked
File ""<frozen importlib._bootstrap>"", line 2164, in _find_spec
File ""<frozen importlib._bootstrap>"", line 1940, in find_spec
File ""<frozen importlib._bootstrap>"", line 1916, in _get_spec
File ""<frozen importlib._bootstrap>"", line 1897, in _legacy_get_spec
File ""<frozen importlib._bootstrap>"", line 863, in spec_from_loader
File ""<frozen importlib._bootstrap>"", line 904, in spec_from_file_location
File ""C:\Python34\lib\site-packages\motionless-1.1-py3.4.egg\motionless.py"", 

line 55
if label and (len(label) <> 1 or not label in Marker.LABELS):
                          ^

SyntaxError: invalid syntax**

",182
31256583,31256661,2,"Motionless hasn't been updated since 06/08/2010 according to the PyPi Package Index.
I've downloaded it and get the same error immediately just by running:
import motionless

print(motionless.__version__)

It's also not flagged as being Python 3.4 compatible in PyPi; if you are running the latest version of Python this is likely the issue.  Have you tried running it with Python 2.7 instead?
Edit: Looking at the Python 2.7 docs; it states here that != and <> are equvilent, however <> is deprecated.  In the Python 3.4 docs it states here that only != is supported, no mention of <> so I imagine it's been removed.
You could try instead:

Raising an issue on the GitHub Repo; the author may still be updating the library and not know it's incompatible with Python 3.4
Checking out the code yourself from the GitHub Repo and manually fixing the problem (Check out 2to3 for automatically doing this.  It will convert all <> usage to != for you)

",193
31257446,31257446,1,"I'm using idle in OS X, and I'm trying to parse a file with .data extension:
FP = open('<filename>.data','r')
lines=FP.readlines()
print lines

but this prints an empty array:  []
I also tried Sublime but also doesn't work.  What is another method to use in Python to read this?
",70
31257446,31257483,2,"When opening a file just by filename, Python by default will first look in the current working directory.  If you're using IDLE, this may not actually be the same directory in which you're .py file is.  Try running your script from the command line in the same directory as your ""*.data"" file.
",61
31257446,31257464,2,"open it in binary mode and print the repr of its contents instead 
print os.stat(""filename.data"") #ensure that st_size > 0

with open(""filename.data"",""rb"") as f:
    print repr(f.read())

if this gives an empty string than you indeed have an empty file ...
im guessing ls -l tells you that the file is 0 bytes big?
",75
31257446,31257455,2,"instead of .readlines() you can cast a file pointer to a list which automatically gets all the lines. So you can do this:
FP = open('filename.data', 'r')
print(list(FP))

or this:
FP = open('filename.data', 'r')
for line in FP:
    print(line)

",66
31257457,31257521,2,"How about this:
def fix_it(scraps, recycled):
    for i in recycled:
        if i not in scraps:
            return ""give me something useful""
    return recycled

print fix_it('AbCdEfG', 'AhK')
print fix_it('AbCdEfG', 'CdE')

output is:
python test.py
give me something useful
CdE

",60
31257457,31257457,1,"This is a noob question asked by a noob. I need to create a function to check whether or no all the character in the string can be found in 'scraps', if true, just print out the whole string once, else, print the error message below. I am sure the real solution is pretty simple but at the moment I feel like I am solving calculus problem with grade school math. please advise. thanks~
def fix_it(scraps, recycled):
The function should be able to produce:
print fix_it('AbCdEfG', 'AhK') ==> ""Give me something that's not useless next time.""
print fix_it('AbCdEfG', 'CdE') ==> 'CdE'
",135
31257478,31257478,1,"I'm going through Miguel Grinbergs Flask book and at one point he uses the following line:
request.endpoint[:5] != 'auth.'

I know [:5] is a slice operation, but I'm not sure why it is being used here.  What does the list consist of that we only want elements 0-5?
",62
31257478,31257977,2,"
What does the list consist of that we only want elements 0-5?

To be precise, request.endpoint is not a list, it's a string. And it doesn't matter what the rest of it contains, the code is only concerned with it beginning with 'auth.':
('auth.somethingsomething'[:5] == 'auth.') is True

I don't have much experience with Flask so I can't specify what possible auth.* values exist, but it's probably values like auth.username and auth.password.
If you're curious about what value it contains, you can add a debugging breakpoint to the code and inspect it:
# ... previous app code ...
import pdb; pdb.set_trace()
request.endpoint[:5] != 'auth.'

Then run and test the code. When it hits that point, it'll pause execution and give you a pdb shell, which will let you look at the request object and its endpoint attribute.
",179
31257489,31257489,1,"What's going on
I'm collecting data from a few thousand network devices every few minutes in Python 2.7.8 via package netsnmp. I'm also using fastsnmpy so that I can access the (more efficient) Net-SNMP command snmpbulkwalk.
I'm trying to cut down how much memory my script uses. I'm running three instances of the same script which sleeps for two minutes before re-querying all devices for data we want. When I created the original script in bash they would use less than 500MB when active simultaneously. As I've converted this over to Python, however, each instance hogs 4GB each which indicates (to me) that my data structures need to be managed more efficiently. Even when idle they're consuming a total of 4GB.

Code Activity
My script begins with creating a list where I open a file and append the hostname of our target devices as separate values. These usually contain 80 to 1200 names.
expand = []
f = open(self.deviceList, 'r')
for line in f:
    line = line.strip()
    expand.append(line)

From there I set up the SNMP sessions and execute the requests
expandsession = SnmpSession ( timeout = 1000000 ,
    retries = 1,            # I slightly modified the original fastsnmpy
    verbose = debug,        # to reduce verbose messages and limit
    oidlist = var,          # the number of attempts to reach devices
    targets = expand,
    community = 'expand'
)
expandresults = expandsession.multiwalk(mode = 'bulkwalk')

Because of how both SNMP packages behave, the device responses are parsed up into lists and stored into one giant data structure. For example,
for output in expandresults:
    print ouput.hostname, output.iid, output.val
#
host1 1 1
host1 2 2
host1 3 3
host2 1 4
host2 2 5
host2 3 6
# Object 'output' itself cannot be printed directly; the value returned from this is obscure
...

I'm having to iterate through each response, combine related data, then output each device's complete response. This is a bit difficult For example,
host1,1,2,3
host2,4,5,6
host3,7,8,9,10,11,12
host4,13,14
host5,15,16,17,18
...

Each device has a varying number of responses. I can't loop through expecting every device having a uniform arbitrary number of values to combine into a string to write out to a CSV.

How I'm handling the data
I believe it is here where I'm consuming a lot of memory but I cannot resolve how to simplify the process while simultaneously removing visited data.
expandarrays = dict()
for output in expandresults:
    if output.val is not None:
        if output.hostname in expandarrays:
            expandarrays[output.hostname] += ',' + output.val
        else:
            expandarrays[output.hostname] = ',' + output.val

for key in expandarrays:
    self.WriteOut(key,expandarrays[key])

Currently I'm creating a new dictionary, checking that the device response is not null, then appending the response value to a string that will be used to write out to the CSV file.
The problem with this is that I'm essentially cloning the existing dictionary, meaning I'm using twice as much system memory. I'd like to remove values that I've visited in expandresults when I move them to expandarrays so that I'm not using so much RAM. Is there an efficient method of doing this? Is there also a better way of reducing the complexity of my code so that it's easier to follow?

The Culprit
Thanks to those who answered. For those in the future that stumble across this thread due to experiencing similar issues: the fastsnmpy package is the culprit behind the large use of system memory. The multiwalk() function creates a thread for each host but does so all at once rather than putting some kind of upper limit. Since each instance of my script would handle up to 1200 devices that meant 1200 threads were instantiated and queued within just a few seconds. Using the bulkwalk() function was slower but still fast enough to suit my needs. The difference between the two was 4GB vs 250MB (of system memory use).
",754
31257489,34982060,2,"The memory consumption was due to instantiation of several workers in an unbound manner.

I've updated fastsnmpy (latest is version 1.2.1 ) and uploaded it to
  PyPi.  You can do a search from PyPi for 'fastsnmpy', or grab it
  directly from my PyPi page here at FastSNMPy

Just finished updating the docs, and posted them to the project page at fastSNMPy DOCS
What I basically did here is to replace the earlier model of unbound-workers with a process-pool from multiprocessing. This can be passed in as an argument, or defaults to 1.
You now have just 2 methods for simplicity. 
snmpwalk(processes=n) and snmpbulkwalk(processes=n)
You shouldn't see the memory issue anymore. If you do, please ping me on github.
",139
31257489,31257791,2,"You might have an easier time figuring out where the memory is going by using a profiler: 
https://pypi.python.org/pypi/memory_profiler
Additionally, if you're already already tweaking the fastsnmpy classes, you can just change the implementation to do the dictionary based results merging for you instead of letting it construct a gigantic list first. 
How long are you hanging on to the session? The result list will grow indefinitely if you reuse it. 
",79
31257489,31257738,2,"If the device responses are in order and are grouped together by host, then you don't need a dictionary, just three lists:
last_host = None
hosts = []                # the list of hosts
host_responses = []       # the list of responses for each host
responses = []
for output in expandresults:
    if output.val is not None:
        if output.hostname != last_host:    # new host
            if last_host:    # only append host_responses after a new host
                host_responses.append(responses)
            hosts.append(output.hostname)
            responses = [output.val]        # start the new list of responses
            last_host = output.hostname
        else:                               # same host, append the response
            responses.append(output.val)
host_responses.append(responses)

for host, responses in zip(hosts, host_responses):
    self.WriteOut(host, ','.join(responses))

",148
31260814,31260814,1,"How would I go about writing a recursive function that consumes two lists and then produces a list with the elements that are in both lists? If both lists have a number twice in each, the produced list will also have that number in it twice.
This is what I have so far:
def merge(L1, L2, i, j, R):
        if L1[i] == L2[j]:
                R.append(L1[i])
                R.append(L2[j])
                merge(L1, L2, i, j+1, R)
        else:
                merge(L1, L2, i+1, j, R)
def sorted_intersection(lst1, lst2):
        R = []
        return merge(lst1, lst2, lst1[0], lst2[0], R)
Nvm, figured out the code. Thanks for all the help!
",166
31260814,31262754,2,"Recursive sounds like a non-pythonistic way of doing this. What about something as simple as:
a = [1, 2, 3]
b = [2, 3, 4]
c = a + b

produces: c = [1, 2, 3, 2, 3, 4]
",57
31260814,31260954,2,"Please see the documentation of the module collections, which contains helpers for many functional tasks:
from collections import defaultdict, Counter

# Preserves the order of b:
def in_both(a, b):
    # Count the occurrences of elements in a:
    a = Counter(a)
    # Default to 0:
    a = defaultdict(lambda: 0, a)

    # Return true if there is a positive amount of items x in a left:
    def take_from_a(x):
        cur = a[x]
        if cur > 0:
            a[x] = cur - 1
            return True

    # Filter out elements in b that are not often enough in b:
    return filter(take_from_a, b)

in_both(""abbcccdddd"", ""abcdabcde"") == ""abcdbcd""

In here a and b are both iterated once.
",156
31260814,31260886,2,"Why recursive? 
a = [1, 2, 2, 3, 3] 
b = [2, 3, 4] 

result = [[x, b.remove(x)][0] for x in a if x in b] 
result
[2, 3, 3]

",56
31260899,31260966,2,"while (integer1 >= 10 or integer1 <= -10):
    replace1 = eval(input('Integer 1 is invalid, Please enter a valid number: '))

You never change integer1, so if the condition is true, it is always true and it loops forever.
Assign to integer1 rather than replace1. Similarly for the second.
",66
31260899,31260899,1,"integer1 = eval(input('Enter the first integer between -10 and 10: '))

integer2 = eval(input('Enter the second integer between -10 and 10: '))

while (integer1 >= 10 or integer1 <= -10):
    replace1 = eval(input('Integer 1 is invalid, Please enter a valid number: '))

while (integer2 > 10 or integer2 < -10):
    replace2 = input('Integer2 is invalid, Please enter a valid number: ')

",96
31260899,31260959,2,"I think this what you expected:
integer1 = eval(input('Enter the first integer between -10 and 10: '))

integer2 = eval(input('Enter the second integer between -10 and 10: '))

while (integer1 >= 10 or integer1 <= -10):
    integer1 = eval(input('Integer 1 is invalid, Please enter a valid number: '))

while (integer2 > 10 or integer2 < -10):
    integer2 = input('Integer2 is invalid, Please enter a valid number: ')

Since the value of integer1 is not changed in first while loop it will act as a infinite loop
Instead of eval you could use int() since there are some harmful effect on using eval stick with int
Modified:
integer1 = int(input('Enter the first integer between -10 and 10: '))

integer2 = int(input('Enter the second integer between -10 and 10: '))

while (integer1 >= 10 or integer1 <= -10):
    integer1 = int(input('Integer 1 is invalid, Please enter a valid number: '))

while (integer2 > 10 or integer2 < -10):
    integer2 = int(input('Integer2 is invalid, Please enter a valid number: '))

",244
31260915,31260915,1,"import matplotlib.pyplot as plt

from sklearn import datasets

from sklearn import svm

import pandas as pd

clf = svm.SVC()

clf = svm.SVC(gamma=0.001, C=100)

X,y = pd.read_csv(""out.csv"", sep=',', usecols=['Dates'], squeeze=True), pd.read_csv(""out.csv"", sep=',', usecols=['Category'], squeeze=True)

clf.fit(X,y)

I am reading the feature data from the out.csv file and storing in the x variable and i am reading the target data from the out.csv file and storing in the y variable 
after running this program i am getting the error called
ValueError: X and y have incompatible shapes.

X has 1 samples, but y has 10.
can any one provide me the solution for this error
",149
31260915,31261081,2,"Possible Duplicate : 
sklearn error: ""X and y have incompatible shapes.""
The above link answers your question.
",22
31260935,31261004,2,"sys.exc_info returns the tuple of 3 elements, where the third is the traceback.
The returned tuple is like - (type, value, traceback) .
You are doing - str(sys.exc_info()[0:2]) which only selects first two elements.
Try -
str(sys.exc_info())

If you cannot use the traceback module to format the traceback. And if you just want the exception's line number and filename, you can use the following -
sys.exc_info()[2].tb_frame.f_code.co_filename #<---- filename
sys.exc_info()[2].tb_lineno # <------ line number

Please note these can be internal names, and best is to use traceback module.
",131
31260935,31260935,1,"In our production code, we log errors like this:
error = {'tos': str(sys.exc_info()[0:2])}

But it only allows to see this kind of info about the error:
""tos"": ""(<class 'AttributeError'>, AttributeError(\""'NoneType' object has no attribute 'group'\"",))""

Which is not enough - I want to see line number and name of the file with code. However, I could get that info with this code:
import traceback
meta['error'] = {'tos': str(traceback.format_exc())}

But we DO NOT use traceback module in production because it is considered too heavy. So how can I get line number and filename without using traceback?
",147
31260982,31260982,1,"I need to run a program from sudo privileges and I am running .py file inside the program. So when my program runs from sudo then it automatically calls/runs the .py file with sudo. I am running file like ""./myFile.py arg1 arg2""
My requirement is that my program need to be run from sudo but when I runs/calls .py file inside program then it should runs as normal privileges ( without sudo).
If anybody has idea then pleas let me know.
Thanks.
",90
31260982,31261059,2,"Instead of running the python file directly, run it as another user
su -c ""./myFile.py arg1 arg2"" another_user

",21
31260988,31261439,2,"When Python executes a script file, the whole file is parsed first. You can notice that when you introduce a syntax error somewhere: Regardless of where it is, it will prevent any line from executing.
So since Python parses the file first, literals can be loaded effectively into the memory. Since Python knows that these are constant, all variables that represent those constant values can point to the same object in memory. So the object is shared.
This works for ints and floats, but even for strings; even when there is a constant expression that needs to be evaluated first:
a = ""foo""
b = ""foo""
c = ""fo"" + ""o""
print(a is b)
print(a is c)

Now in IDLE, the behavior is very different: As an interactive interpreter, IDLE executes every line separately. So a = 1.1 and b = 1.1 are executed in separated contexts which makes it impossible (or just very hard) to figure out that they both share the same constant literal value and could share the memory. So instead, the interpreter will allocate two different objects, which causes the identity check using is to fail.
For small integers, the situation is a bit different. Because they are often used, CPython stores a set of integers (in the range between -5 and 256) statically and makes that every value of these points to the same int object. That’s why you get a different result for small integers than for any other object. See also the following questions:

""is"" operator behaves unexpectedly with integers
What's with the Integer Cache inside Python?

",315
31260988,31260988,1,"In the python idle:
>>> a=1.1
>>> b=1.1
>>> a is b
False

But when I put the code in a script and run it, I will get a different result:
$cat t.py
a=1.1
b=1.1
print a is b
$python t.py
True

Why did this happen? I know that is compares the id of two objects, so why the ids of two objects are same/unique in python script/idle?
I also found that, if I use a small int, for example 1, instead of 1.1, the result will be the same in both the python script and python idle. Why did small int and small float have different behavior?
I am using CPython 2.7.5.
",134
31261047,31261170,2,"It's not necessary to escape the space character.
os.remove(""/Users/karthik/Desktop/screenshot 2.png"")

",17
31261047,31261079,2,"there is no need to escape the space character.
",10
31261047,31261047,1,"I'm trying to delete a file which contains a space in its name. I'm using a Mac, and escaping the space character. However it stills throws an error.
Here's a screenshot,

How do I fix this?
",45
31261068,31261237,2,"Here every item in item_to_be_found list is a Tag type object so you can get the string inside <loc> tag using .text or .string on them. Though .text and .string have differences both will work same in this case.
for loc in item_to_be_found:
    print item_to_be_found.index(loc) + 1, loc.text

this will give you a result like
1 http://www.htcysnc.com/m/designer-sarees
2 http://www.htcysnc.com/m/anarkali-suits

",72
31261068,31261068,1,"I am crawling a sitemap.xml and my objective is to find all the url's and the incremental count of them.
Below is the structure of the xml
<?xml version=""1.0"" encoding=""UTF-8""?>
<urlset xmlns=""http://www.sitemaps.org/schemas/sitemap/0.9"">
    <url>
        <loc>http://www.htcysnc.com/m/designer-sarees</loc>
        <lastmod>2014-09-01</lastmod>
    <changefreq>hourly</changefreq>
    <priority>0.9</priority>
</url>
<url>
    <loc>http://www.htcysnc.com/m/anarkali-suits</loc>
    <lastmod>2014-09-01</lastmod>
    <changefreq>hourly</changefreq>
    <priority>0.9</priority>
</url>

Below is my code
from BeautifulSoup import BeautifulSoup
import requests
import gzip
from StringIO import StringIO


def crawler():
    count=0
    url=""http://www.htcysnc.com/sitemap/sitemap_product.xml.gz""
    old_xml=requests.get(url)
    new_xml=gzip.GzipFile(fileobj=StringIO(old_xml.content)).read()
    #new_xml=old_xml.text
    final_xml=BeautifulSoup(new_xml)
    item_to_be_found=final_xml.findAll('loc')
    for i in item_to_be_found:
        count=count+1
        print i
        print count
    crawler()

My output is like this
<loc>http://www.htcysnc.com/elegant-yellow-green-suit-seven-east-p63703</loc>
1
<loc>http://www.htcysnc.com/elegant-orange-pink-printed-suit-seven-east-p63705</loc>
2

Need the output as links without loc and /loc. Have tried replace command but that is throwing an error.
",235
31261123,31261123,1,"I have a file that I am currently reading from using
fo = open(""file.txt"", ""r"")

Then by doing
file = open(""newfile.txt"", ""w"")
file.write(fo.read())
file.write(""Hello at the end of the file"")
fo.close()
file.close()

I basically copy the file to a new one, but also add some text at the end of the newly created file. How would I be able to insert that line say, in between two lines separated by an empty line? I.e:
line 1 is right here
                        <---- I want to insert here
line 3 is right here

Can I tokenize different sentences by a delimiter like \n for new line?
",141
31261123,31261196,2,"First you should load the file using the open() method and then apply the .readlines() method, which splits on ""\n"" and returns a list, then you update the list of strings by inserting a new string in between the list, then simply write the contents of the list to the new file using the new_file.write(""\n"".join(updated_list))
NOTE: This method will only work for files which can be loaded in the memory.
with open(""filename.txt"", ""r"") as prev_file, open(""new_filename.txt"", ""w"") as new_file:
    prev_contents = prev_file.readlines()
    #Now prev_contents is a list of strings and you may add the new line to this list at any position
    prev_contents.insert(4, ""\n This is a new line \n "")
    new_file.write(""\n"".join(prev_contents))

",167
31261123,31261375,2,"For Large file
with open (""s.txt"",""r"") as inp,open (""s1.txt"",""w"") as ou:
    for a,d in enumerate(inp.readlines()):
        if a==2:
            ou.write(""hi there\n"")
        ou.write(d)

",56
31261123,31264759,2,"readlines() is not recommended because it reads the whole file into memory. It is also not needed because you can iterate over the file directly.
The following code will insert Hello at line 2 at line 2
with open('file.txt', 'r') as f_in:
    with open('file2.txt','w') as f_out:
        for line_no, line in enumerate(f_in, 1):
            if line_no == 2:
                f_out.write('Hello at line 2\n')
            f_out.write(line)

Note the use of the with open('filename','w') as filevar idiom. This removes the need for an explicit close() because it closes the file automatically at the end of the block, and better, it does this  even if there is an exception.
",148
31261376,31261376,1,"How to read the twitter.avro files in pyspark and extract the values from it? 
rdd=sc.textFile(""twitter.asvc"") is working good
But when I do
rdd1=sc.textFile(""twitter.avro"")
rdd1.collect()

I am getting output below 

['Obj\x01\x02\x16avro.schema\x04{""type"":""record"",""name"":""episodes"",""namespace"":""testing.hive.avro.serde"",""fields"":[{""name"":""title"",""type"":""string"",""doc"":""episode
  title""},{""name"":""air_date"",""type"":""string"",""doc"":""initial
  date""},{""name"":""doctor"",""type"":""int"",""doc"":""main actor playing the
  Doctor in episode""}]}\x00kR\x03LS\x17m|]Z^{0\x10\x04""The Eleventh
  Hour\x183 April 2010\x16""The Doctor\'s Wife\x1614 May 2011\x16&Horror
  of Fang Rock 3 September 1977\x08$An Unearthly Child 23 November
  1963\x02*The Mysterious Planet 6 September 1986\x0c\x08Rose\x1a26
  March 2005\x12.The Power of the Daleks\x1e5 November
  1966\x04\x14Castrolava\x1c4 January 1982', 'kR\x03LS\x17m|]Z^{0']

Is there a python library for reading this format?
",228
31261376,31263508,2,"You should use a FileInputFormat specific for Avro files. 
Unfortunately I am not using python so I can only link you to a solution. You can look into that: https://github.com/apache/spark/blob/master/examples/src/main/python/avro_inputformat.py
The most interesting part is this one:
avro_rdd = sc.newAPIHadoopFile(
    path,
    ""org.apache.avro.mapreduce.AvroKeyInputFormat"",
    ""org.apache.avro.mapred.AvroKey"",
    ""org.apache.hadoop.io.NullWritable"",
    keyConverter=""org.apache.spark.examples.pythonconverters.AvroWrapperToJavaConverter"",
    conf=conf)

",68
31261448,31262473,2,"try this once:
selectEle = Select(driver.find_element_by_xpath(""//select[@class='select_filter_class valid']""));

//In list it may be element instead of webelement in python
List<WebElement> list = select.find_elements_by_tag_name(""option"");

Try this:
for index,element in enumerate(list):
        element= list[index];
        select = Select(selectEle);
        print 'Text  :', element.text
        select.select_by_visible_text(element.text);
        Thread.sleep(2000);

Let me know,is it working or not..

",97
31261448,31261448,1,"<select name=""ctl00$StudentTermClassChooser$ClassChooser"" 
    onchange=""javascript:setTimeout('__doPostBack(\'ctl00$StudentTermClassChooser$ClassChooser\',\'\')', 0)"" 
    id=""ctl00_StudentTermClassChooser_ClassChooser"" class=""select_filter_class valid"">
    <optgroup label=""COM SCI 35L"">
       <option selected=""selected"" value=""187105202"">COM SCI 35L LAB 2</option>
    </optgroup>
    <optgroup label=""COM SCI M51A"">
        <option value=""187154210"">COM SCI M51A LEC 2</option>
        <option value=""187154211"">COM SCI M51A DIS 2A</option>
    </optgroup>
    <optgroup label=""MATH    33B"">
        <option value=""262223210"">MATH 33B LEC 2</option>
        <option value=""262223215"">MATH 33B DIS 2E</option>
    </optgroup>
    <optgroup label=""PHYSICS 4BL"">
        <option value=""318017211"">PHYSICS 4BL LAB 11</option>
    </optgroup>
 </select>

So here is the html for a website I am trying to use selenium on. It is a dropdown menu that lets you choose a class, and when it is selected, the website will go to that class. The url does not change, and all that changes is the spot of the selected=""selected"". How do I use selenium to change which one is selected? I want to open the menu first and somehow click on each class in order so that I can see the info on the classes. But, I don't want to use the name of the classes or their values, so that this process can be repeated for other classes.
",311
31261477,31262013,2,"I think what you are trying to achieve is something like the following:
file1.py
from file2 import my_list

print(my_list)

file2.py
my_list = []

class abc:
    def __init__(self):
        global my_list

        my_list = [12,13,4] #values assigned


my_abc_file2 = abc()

Running file1.py would give:
[12, 13, 4]

As mentioned in the comments, file2.py really only needs to contain the following for it to work though:
file2.py
my_list = [12,13,4]

Tested in Python 2.7
",94
31261477,31261477,1,"I have a hard-coded list that I have to use across many Python files.
I tried doing something like(Say the list is in xyz.py)
global l=list() 
class abc:
    def __init__(self):
        l=[12,13,4] #values assigned

How can I access it in other Python files?
",58
31261528,31936248,2,"What worked for me is as follows:

Install python-psutil: sudo apt-get install python-psutil. If you
have a previous installation of the psutil module from other
method, for example through source or easy_install, remove it first.
Run pyinstaller as you do, without the hidden-import option.

",52
31261528,31313124,2,"pyinstall is hard to configure, the cx_freeze maybe better, both support windows (you can download the exe directly) and linux. Provide the example.py, In windows, suppose you have install python in the default path (C:\\Python27):
$ python c:\\Python27\\Scripts\\cxfreeze example.py -s --target-dir some_path

the cxfreeze is a python script, you should run it with python, then the build files are under some_path (with a lot of xxx.pyd and xxx.dll).
In Linux, just run:
$ cxfreeze example.py -s --target-dir some_path

and also output a lot of files(xxx.so) under some_path.
The defect of cx_freeze is it would not wrap all libraries to target dir, this means you have to test your build under different environments. If any library missing, just copy them to target dir. A exception case is, for example, if your build your python under Centos 6, but when running under Centos 7, the missing of libc.so.6 will throw, you should compile your python both under Centos 7 and Centos 6. 
",197
31261528,31261528,1,"I want to compile my python code to binary by using pyinstaller, but the hidden import block me. For example, the following code import psutil and print the CPU count:
# example.py
import psutil
print psutil.cpu_count()

And I compile the code:
$ pyinstaller -F example.py --hidden-import=psutil

When I run the output under dist:
ImportError: cannot import name _psutil_linux

Then I tried:
$ pyinstaller -F example.py --hidden-import=_psutil_linux

Still the same error. I have read the pyinstall manual, but I still don't know how to use the hidden import. Is there a detailed example for this? Or at least a example to compile and run my example.py?
ENVs:

OS: Ubuntu 14.04
Python: 2.7.6
pyinstaller: 2.1

",136
31261528,31611824,2,"Hi hope you're still looking for an answer. Here is how I solved it:
add a file called hook-psutil.py
from PyInstaller.hooks.hookutils import (collect_data_files, collect_submodules)

datas = [('./venv/lib/python2.7/site-packages/psutil/_psutil_linux.so', 'psutil'),
         ('./venv/lib/python2.7/site-packages/psutil/_psutil_posix.so', 'psutil')]
hiddenimports = collect_submodules('psutil')

And then call pyinstaller --additional-hooks-dir=(the dir contain the above script) script.py
",71
31261543,31288272,2,"I found, the foreign key constraint in the column must be removed
from sqlalchemy import (Column, Integer, create_engine, 
                        ForeignKeyConstraint)
from sqlalchemy.ext.declarative import declarative_base


#engine = create_engine('sqlite:///memory')
engine = create_engine('postgres:///memory')
Base = declarative_base()


class Test(Base):
    __tablename__ = 'test'
    id = Column(Integer, primary_key=True)
    id2 = Column(Integer, primary_key=True)


class Test2(Test):
    __tablename__ = 'test2'
    # No foreign key here
    id = Column(Integer, primary_key=True)
    id2 = Column(Integer, primary_key=True)

    __table_args__ = (
        # this constraints must be the alone to Test model
        ForeignKeyConstraint([id, id2], [Test.id, Test.id2]),
    )


Base.metadata.create_all(engine)

",141
31261543,31261543,1,"With SQLAlchemy, I want inherit a model with more than one primary keys, example:
from sqlalchemy import (Column, Integer, create_engine, ForeignKey,
                        ForeignKeyConstraint, UniqueConstraint)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.ext.declarative import declared_attr


#engine = create_engine('sqlite:///memory')
engine = create_engine('postgres:///memory')
Base = declarative_base()


class Test(Base):
    __tablename__ = 'test'
    id = Column(Integer, primary_key=True)
    id2 = Column(Integer, primary_key=True)


class Test2(Test):
    __tablename__ = 'test2'
    id = Column(Integer, ForeignKey('test.id'), primary_key=True)
    id2 = Column(Integer, ForeignKey('test.id2'), primary_key=True)

    @declared_attr
    def __table_args__(cls):
        return (
            UniqueConstraint(cls.id, cls.id2),
            ForeignKeyConstraint([cls.id, cls.id2], 
                                 [Test.id, Test.id2]),
        )

    @declared_attr
    def __mapper_args__(cls):
        return {
            'inherit_condition': (cls.id == Test.id and cls.id2 == Test.id2),
        }


Base.metadata.create_all(engine)

The inherit work with only one primary key column, and this not really my use case. I have two String primary key column and I use the polymorphic pattern of SQLAlchemy. I don't write all my code, the issue is the same with or without polymorphic configuration.
This code works with sqlite, but not with postgres, the __table_args__ is useless here, What do I miss ?
regards
",264
31261600,31262642,2,"This is by the way a terrible idea, since you already have begun using gitpython, and I have never tried working with that, but I just really want to let you know, that you can do it without cloning it in local, without using gitpython.
Simply run the git command, in a shell, using subprocess..
running bash commands in python

edit: added some demonstration code, of reading stdout and writing stdin.
some of this is stolen from here:
http://eyalarubas.com/python-subproc-nonblock.html
The rest is a small demo..
first two prerequisites
shell.py
import sys
while True:
    s = raw_input(""Enter command: "")
    print ""You entered: {}"".format(s)
    sys.stdout.flush()

nbstreamreader.py:
from threading import Thread
from Queue import Queue, Empty

class NonBlockingStreamReader:

    def __init__(self, stream):
        '''
        stream: the stream to read from.
                Usually a process' stdout or stderr.
        '''

        self._s = stream
        self._q = Queue()

        def _populateQueue(stream, queue):
            '''
            Collect lines from 'stream' and put them in 'quque'.
            '''

            while True:
                line = stream.readline()
                if line:
                    queue.put(line)
                else:
                    raise UnexpectedEndOfStream

        self._t = Thread(target = _populateQueue,
                args = (self._s, self._q))
        self._t.daemon = True
        self._t.start() #start collecting lines from the stream

    def readline(self, timeout = None):
        try:
            return self._q.get(block = timeout is not None,
                    timeout = timeout)
        except Empty:
            return None

class UnexpectedEndOfStream(Exception): pass

then the actual code:
from subprocess import Popen, PIPE
from time import sleep
from nbstreamreader import NonBlockingStreamReader as NBSR

# run the shell as a subprocess:
p = Popen(['python', 'shell.py'],
        stdin = PIPE, stdout = PIPE, stderr = PIPE, shell = False)
# wrap p.stdout with a NonBlockingStreamReader object:
nbsr = NBSR(p.stdout)
# issue command:
p.stdin.write('command\n')
# get the output
i = 0
while True:
    output = nbsr.readline(0.1)
    # 0.1 secs to let the shell output the result
    if not output:
        print ""time out the response took to long...""
        #do nothing, retry reading..
        continue
    if ""Enter command:"" in output:
        p.stdin.write('try it again' + str(i) + '\n')
        i += 1
    print output

",448
31261600,31261600,1,"I am trying to archive a remote git repo using Python code. I did it successfully using Git command line with following command.
> git archive --format=zip --remote=ssh://path/to/my/repo -o archived_file.zip     
HEAD:path/to/directory filename

This command fetches the required file from the repo and stores the zip in my current working directory. Note that there is no cloning of remote repo happening.
Now I have to do it using Python code. I am using GitPython 1.0.1. I guess if it is doable using command line then it should be doable using GitPython library. According to the docs, 
repo.archive(open(join(rw_dir, 'archived_file.zip'), 'wb'))

Above line of code will archive the repo. Here repo is the instance of Repo class. It can be initialized using
repo = Repo('path to the directory which has .git folder')

If I give path to my remote repo(Ex. ssh://path/to/my/repo) in above line it goes to find it in directory where the .py file containing this code is residing(Like, Path\to\python\file\ssh:\path\to\my\repo), which is not what I want. So to sum up I can archive a local repo but not a remote one using GitPython. I may be able to archive remote repo if I am able to create a repo instance pointing to the remote repo. I am very new to Git and Python.
Is there any way to archive a remote repo using Python code without cloning it in local?
",279
31261664,31262622,2,"We can't really diagnose this if you don't tell us what it does return. If you need any sort of login to access it, it won't work because Python doesn't have your browser cookies. Python also won't automatically follow some types of redirects and anything Javascript-dependant is right out of the question. Those would be the first things to check for.
",71
31261664,31261664,1,"I want to capture a file that is downloaded when a certain url is passed in python. The problem is that the downloaded file is NOT returned by the server. The file gets downloaded when I pass the same url in a browser, but not when I do so via urllib2.urlopen(). Is there a way to capture this seemingly triggered side effect in python ? This is what I have so far.
#!/usr/bin/env python

import urllib
import urllib2
import re
import sys
import os

def main(sem_id):
    url = '<url>'
    for i in range(1,71):
        if i < 10:
            rollNo = '<roll_number>0%s'%i
        else:
            rollNo = '<roll_number>%s'%i

        values = { 'id':sem_id, 'regno':rollNo, 'sum':100, 'sessionok':'yes' }
        data = urllib.urlencode(values)
        url = url + '?' + data
        req = urllib2.Request(url)
        req.add_header('User-agent', 'Mozilla/5.0 (Linux i686)')

        response = urllib2.urlopen(req,timeout=100)

        result = response.read()


        with open('%s.pdf'%rollNo,'w') as f:
            f.write(result)


if __name__ == '__main__':
    assert len(sys.argv) == 2
    sem_id = sys.argv[1]
    main(sem_id)

",254
31262702,31262792,2,"y is known only in the scope of the function fpol. You should assign the result to a variable, and only then print its value:
y = fpol(4)
print(y)

Note that y is a different variable here, it has nothing to do with the y inside the function. You could write:
x = fpol(4)
print(x)

",73
31262702,31262804,2,"y is out of scope. It was only in scope for your function call, and since the function fpol has ran and ended, the scope has died with it. We need to assign a variable that's visible to the print command. Let's reuse y for simplicity.
y = fpol(4)
print(y)

The key rule of thumb for python is every time you indent you have started a new scope! You must make sure your variables are in scope to use them.
",96
31262702,31262830,2,"y is a variable with scope that's local to function fpol().
it is not defined outside of that scope.
The code return y does not make y visible outside of the function in which it has been defined. It only returns the value of y to the caller as a function return value.
",60
31262702,31262854,2,"Using your example, the following would show you the value of y
def fpol(x):
    y=x**4
    print(y)      # This will print the value of y
    return(y)

print fpol(4)     # This will print the returned result

But trying to print y after your function call will fail with the not defined error as the variable is defined only inside you function, i.e. it is local to that function. This is often referred to as the scope of a variable.
As soon as the function returns, y ceases to exist.
",107
31262702,31262858,2,"y is not defined outside th function
you can not do print(y)
probably you want
y = fpol(4)
print(y)
is not very good programming style, but you also can make y global variable. then it will be available after function, but pls do not do it.
finally you can do just 
return y
no need ()
",70
31262702,31262886,2,"I suspect you are trying to print(y) outside the function. The variable y is local in scope, that is only defined within fpol(). So you can print it there. You can do:,
def fpol(x):
    y=x**4
    return(y)

y = fpol(4)
print(y)

But not:
def fpol(x):
    y=x**4
    return(y)

print(y)

",82
31262702,31262794,2,"The variable y is only visible from within the function you have declared. To print the result of fpol(4) you can assign the returned value to a new variable:
returnedValue = fpol(4)
print(returnedValue)

",44
31262702,31262702,1,"I've been having some problems with the return statement and I can't seem to figure out what's wrong. It seemed to work fine yesterday, but today no function that contains it seems to work properly. Here's an example of what's going on:
def fpol(x):
    y=x**4
    return(y)

If I then type in
fpol(4)

I'm given the answer 256 (as I would expect). If I then type in
print(y)

or try to use/view y in any way, I'm told
NameError: name 'y' is not defined

I've also tried it with return(y) being replaced by return y . I can also insert print(y) into the original function and that works fine, so I know that during the function, y actually does have a value, it's just not being returned. Any help is much appreciated.
Edit: I've now been able to work past the issue I had with the return function. Thanks to everyone who responded.
",199
31262702,31262790,2,"I guess you need to store the value returned by the function in a variable and then print it:
y = fpol(4)
print y

",28
31266550,31266738,2,"Here's a regular expression that will find which (if any) of your title keywords appear in c_raw:
regex = re.compile(""(?:Dresses)|(?:Wallets)|(?:Pumps)"")
print regex.findall(c_raw)

The format of (?:Dresses) etc., just means match the letters inside the parenthesis in the order they appear together but don't save them as a regex group.
As to what c_raw[0] is doing, basically c_raw is a string. In python, you can treat strings as if they are arrays of characters. So, c_raw[0] is saying, give me the 0th index of the array c_raw -- i.e. get the first character of c_raw
",142
31266550,31266550,1,"I'm new to Python AND Regex and am a little confused. I want to search through a webpage title for three different terms and if the title matches one I want that printed out (I'm using scrapy so it's printing it for each item). I'm unsure how we get it to search through the three terms? Eg if title has ""Dresses|Wallets|Pumps"" print it out.
For now I just got it to print the first term in the title 
c_raw = response.xpath('//title').extract()
c_re = re.search('<title>(.*?) |.*?', c_raw[0])
c = c_re.group(1).lower()

I tried
c_raw = response.xpath('//title').extract()
c_re = re.search('Dresses|Wallets|Pumps', c_raw[0])
c = c_re.group(0)

but some of the c's printed None. I'm also unsure of what c_raw[0] is doing? What does the 0 do?
Any help would be appreciated!
Edit: While the below answer helped - adding brackets around Dresses|Wallets|Pumps also worked
",208
31266550,31266918,2,"try this demo from scrapy shell,
In [1]: text = ""<title>Testing test Pumps abc asdf a</title>""

In [2]: sel = Selector(text=text)

In [3]: sel.xpath('//title').re('Dresses|Wallets|Pumps')
Out[3]: [u'Pumps']

I have made an input html to selector object since you haven't provide any specific start-url
",80
31266658,31266658,1,"I use django_celery with connecting to Amazon Redshift. To migrate database, after ""makemigrations"" I used command ""python manage.py migrate"" and error message show up as shown below. 
The reason is Redshift does not support data type 'serial' but the 'django_migrations' table that contain 'serial' type is automatically created. 
How to stop Django Migrations create this table or avoid using serial on 'django_migrations' table.
D:\code\test_celery_django>python manage.py migrate
Traceback (most recent call last):
  File ""manage.py"", line 10, in <module>
    execute_from_command_line(sys.argv)
  File ""C:\Python27\lib\site-packages\django\core\management\__init__.py"", line 338, in execute_from_command_line
    utility.execute()
  File ""C:\Python27\lib\site-packages\django\core\management\__init__.py"", line 330, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File ""C:\Python27\lib\site-packages\django\core\management\base.py"", line 390, in run_from_argv
    self.execute(*args, **cmd_options)
  File ""C:\Python27\lib\site-packages\django\core\management\base.py"", line 441, in execute
    output = self.handle(*args, **options)
  File ""C:\Python27\lib\site-packages\django\core\management\commands\migrate.py"", line 93, in handle
    executor = MigrationExecutor(connection, self.migration_progress_callback)
  File ""C:\Python27\lib\site-packages\django\db\migrations\executor.py"", line 19, in __init__
    self.loader = MigrationLoader(self.connection)
  File ""C:\Python27\lib\site-packages\django\db\migrations\loader.py"", line 47, in __init__
    self.build_graph()
  File ""C:\Python27\lib\site-packages\django\db\migrations\loader.py"", line 180, in build_graph
    self.applied_migrations = recorder.applied_migrations()
  File ""C:\Python27\lib\site-packages\django\db\migrations\recorder.py"", line 59, in applied_migrations
    self.ensure_schema()
  File ""C:\Python27\lib\site-packages\django\db\migrations\recorder.py"", line 53, in ensure_schema
    editor.create_model(self.Migration)
  File ""C:\Python27\lib\site-packages\django\db\backends\base\schema.py"", line 286, in create_model
    self.execute(sql, params or None)
  File ""C:\Python27\lib\site-packages\django\db\backends\base\schema.py"", line 111, in execute
    cursor.execute(sql, params)
  File ""C:\Python27\lib\site-packages\django\db\backends\utils.py"", line 79, in execute
    return super(CursorDebugWrapper, self).execute(sql, params)
  File ""C:\Python27\lib\site-packages\django\db\backends\utils.py"", line 64, in execute
    return self.cursor.execute(sql, params)
  File ""C:\Python27\lib\site-packages\django\db\utils.py"", line 97, in __exit__
    six.reraise(dj_exc_type, dj_exc_value, traceback)
  File ""C:\Python27\lib\site-packages\django\db\backends\utils.py"", line 62, in execute
    return self.cursor.execute(sql)
django.db.utils.NotSupportedError: Column ""django_migrations.id"" has unsupported type ""serial"".

",413
31266658,43531445,2,"Are you trying to use Redshift as the backend database for your web application? That's a bad idea, Redshift is a data warehouse and as such individual query performance and latency are far from great, not to mention that Redshift doesn't enforce primary keys, which almost surely Django expects. 
My recommendation, use PostgreSQL.
",62
31266687,31285962,2,"if your ticker file is large
(judging by the path name, it may be)
using the csv reader is a waste of time.
it does not suport seek, so the only way you can get to the last line is 
    for row in spamreader:
        pass

since after this ""row"" will contain the last row in the file..
you can see here: Most efficient way to search the last x lines of a file in python
that it's possible to retrieve only the last lines in the file, and then parse it with the csv module afterward..
This will save some computation time..
",113
31266687,31266687,1,"I am running this code below using multiprocessing to run ticker_list through a request and parsing program faster.  The following code works, but it is very slow.  I am not so sure that this is the correct usage of multiprocessing.  If there is a more efficient way to do this then please let me know.
ticker_list = []

with open('/home/a73nk-xce/Documents/Python/SharkFin/SP500_Fin/SP_StockChar/ticker.csv', 'r', newline='') as csvfile:
    spamreader = csv.reader(csvfile)
    for rows in spamreader:
        pass

    for eachTicker in rows:
        ticker_list.append(eachTicker)

def final_function(ticker):
    try:
        GetFinData.CashData(ticker_list)


    except Exception as e:
        pass

if __name__ == '__main__':
    jobs = []
    p = mp.Process(target=final_function, args=(ticker_list,))
    jobs.append(p)
    p.start()
    p.join()      

",150
31266760,31266827,2,"in python pickle refers to a module that provides (a specific) serialization of python objects.
serialization itself is a more general term. python objects can also be serialized into json for example.
https://en.wikipedia.org/wiki/Serialization
",40
31266760,31267072,2,"You are misreading the article. Pickling and serialisation are not synonymous, nor does the text claim them to be.
Paraphrasing slighly, the text says this:

This module implements an algorithm for turning an object into a series of bytes. This process is also called serializing the object.

I removed the module name, pickle, deliberately. The module implements a process, an algorithm, and that process is commonly known as serialisation.
There are other implementations of that process. You could use JSON or XML to serialise data to text. There is also the marshal module. Other languages have other serialization formats; the R language has one, so does Java. Etc.
See the WikiPedia article on the subject:

In computer science, in the context of data storage, serialization is the process of translating data structures or object state into a format that can be stored (for example, in a file or memory buffer, or transmitted across a network connection link) and reconstructed later in the same or another computer environment.

Python picked the name pickle because it modelled the process on how this was handled in Modula-3, where it was also called pickling. See Pickles: Why are they called that?
",229
31266760,31266760,1,"I have came across these two terms so often while reading about python objects. However, there is a confusion between pickling and serialization since at one place I read

The pickle module implements an algorithm for turning an arbitrary
  Python object into a series of bytes. This process is also called
  serializing” the object.

If serializing and pickling is same process, why use different terms for them?
",74
31266768,31267980,2,"I would take a look at itertools. 
https://docs.python.org/3.5/library/itertools.html#itertools.permutations
",13
31266768,31266768,1,"Lets say I have two lists of lists and I wand to build various permutations from it. Look at these two lists:
l1 = [[0, 8], [4], [6, 7], [1], [3], [5], [2]]
l2 = [[4, 8], [0], [6, 7], [3], [1], [5], [2]]

The two lists represent mappings, that I already have. So I know some parts of my permutation but not all of them. As I see it, there are only four permutations possible:
(4, 3, 2, 1, 0, 5, 6, 7, 8)
(8, 3, 2, 1, 0, 5, 7, 6, 4)
(4, 3, 2, 1, 0, 5, 7, 6, 8)
(8, 3, 2, 1, 0, 5, 6, 7, 4)

After thinking about it I have the following idea. I would generate all four possibilities of l1 (being a tuple in the end) make l2 into a tuple, do a dictionary zip, order that dictionary and convert these four dicts into a list of tuples. Sound reasonable?
I have a hard time generating the four tuples. This is what I have so far.
l1 = [[0, 8], [4], [6, 7], [1], [3], [5], [2]]
l2 = [[4, 8], [0], [6, 7], [3], [1], [5], [2]]

out = [()]
def get_permutations(lst):
    for i in lst:
        for perms in permutations(i):
            out[0] = out[0] + perms
    return out

print(get_permutations(l1))

Which prints [(0, 8, 8, 0, 4, 6, 7, 7, 6, 1, 3, 5, 2)]
How do I generate another tuple when I get to an entry in my list where more than one permutation of that list entry is possible?
I also know that print(list(itertools.product(*l1))) does almost what I want but not quite. It yields [(0, 4, 6, 1, 3, 5, 2), (0, 4, 7, 1, 3, 5, 2), (8, 4, 6, 1, 3, 5, 2), (8, 4, 7, 1, 3, 5, 2)]. Maybe there is a way to modify that.
It is not a homework question.
To provide context: I am building a program to test graph isomorphism. The lists are the color classes of the nodes. With these color classes I want to limit all possible permutations of a graph to just a few to make the brute-force quicker.  
",607
31266868,33099661,2,"You can also use php to have an URL like this:
http://xxx.xxx.xxx.xxx/index.php?starttime=1360000000&endtime=1370000000

Then you can define any start and end time to create a dynamic m3u8
Here is an example of index.php
<?php
$starttime = $_REQUEST[""starttime""];
$endtime = $_REQUEST[""endtime""];

$m3u8file = ""#EXTM3U\n#EXT-X-KEY:METHOD=NONE\n#EXT-X-TARGETDURATION:10\n#EXT-X-MEDIA-SEQUENCE:"".substr($starttime,0,9).""\n\n"";
for ($i=substr($starttime,0,9);$i<=substr($endtime,0,9);$i++){
    if (file_exists(""/xxxpath/"".date(""YmdH"",$i*10).""/"".substr($i,0,9)."".ts"")) {
        $m3u8file .= ""#EXTINF:10,\n"";
        $m3u8file .= ""/xxxpath/"".date(""YmdH"",$i*10).""/"".substr($i,0,9)."".ts\n"";
    }
}
$m3u8file .= ""#EXT-X-ENDLIST\n"";

header('Content-Type: application/force-download');
header(""Content-Disposition: attachment; filename=index.m3u8"");
echo $m3u8file;
?>

",218
31266868,31266868,1,"I have created the following config for hls in application 'live':
hls on;
hls_path /hls;
hls_fragment 5s;
hls_playlist_length 12h;
hls_fragment_naming system;
hls_continuous on;
hls_nested on;

pull rtmp://localhost:1935/live/test.stream name=test.stream live=1 static;

Now the index.m3u8 contains timestamp-named chunks and plays fine.
My question is - is it possible to create a timeshifted version of the index.m3u8, which would play the stream shifted back in time ? 
So http://server/live/test.stream/index.m3u8 plays recorded 6 hours earlier.
I heard that i might need a python script/C that select ts.chunks 6 hours. So how would i achieve this.
",110
31266899,31267007,2,"You can create the directory with :
os.mkdir

See the doc : https://docs.python.org/2/library/os.html
",15
31266899,31266899,1,"Suppose I have a function:
def function(a,b,c):
    do some stuff
    array = some calcualated values
    np.savetxt('/home/user/'+str(a)+'<z<'+str(b)+'/mass/M_'+str(c)+'.dat',array)

As you can see I am saving a file called M_+str(c)+.dat in the path /home/user/'+str(a)+'<z<'+str(b)+'/mass/.
The problem I have is that, both str(a)+'<z<'+str(b) and mass folders don't exist. 
I have to create them inside the function and save the file inside these folders. How do I achieve this task?
",132
31266930,31267109,2,"
could we call it from cPython

No, netTcpBinding is interoperable only with WCF clients. 
From here:

The default configuration for the NetTcpBinding is faster than the
  configuration provided by the WSHttpBinding, but it is intended only
  for WCF-to-WCF communication. 

From comments:

does wsHttpBinding work or only basicHttpBinding work?

Short answer is no, basicHttpBinding is the only binding (except for the web bindings) which support interoperability with non-wcf clients. Even then you may have difficulty consuming it from non-windows.
Long answer is that the reason this is the case is because basicHttpBinding supports communication over SOAP 1.1, which is a relatively simple protocol, and most vendors have implemented it in very similar ways. Therefore, these different implementations tend to be interoperable. However, wsHttpBinding is Microsoft's attempt to support the SOAP 1.2 protocol and WS-* web service extensions, which is a much larger and more complex set of standards. So there is a much larger scope for interpretation between the various vendors, leading normally to non-interoperability between implementations. It is theoretically possible, therefore, to call an endpoint exposed over wsHttpBinding from a non-wcf (or even non-windows) client, but you would have to overcome all the niggles.
A much better approach would be to move away from SOAP completely if possible, and just use HTTP/POX or HTTP/REST services. 
",246
31266930,31266930,1,"I've read this question WCF and Python.
But in case, the wcf service use netTcpBinding, could we call it from cPython. If it's possible, please help to give an simple example ?
",39
31266964,31266964,1,"Hello I am new to Python. I have over 5000 .csv.gz files to be loaded on vertica data base. The server disconnects after 10 minutes, thus all 5000 files cannot be copied without re-setting the server connection.
I have two basic problems here:

How can I keep track of copy commands successfully executed?
How can I re-set the connection and restart from last failed command?

The code I am using is:
import pyodbc
conn = pyodbc.connect(""DSN=Vertica_SG;SERVER=54.169.66.95;DATABASE=xyzdwh;PORT=5433;UID=abc123;PWD=abc123"")
cursor = conn.cursor()
cursor.execute(""Copy schema1.table1 from local 'E:\\folder1\\table1.csv.gz' GZIP with Delimiter ',' direct;"")
cursor.execute(""Copy schema1..table2 from local 'E:\\folder1\\table2.csv.gz' GZIP with Delimiter ',' direct;"")

...
[5000 such execute commands]
...

print(""All done"")

",162
31266964,31267634,2,"I suggest you use the STREAM NAME option when you load your data.
Copy schema1.table1 from local 'E:\folder1\table1.csv.gz' STREAM NAME 'E:\folder1\table1.csv.gz' GZIP with Delimiter ',' direct;

Then run a query on the stream loads that were successful and remove them from your list. 
SELECT * FROM v_monitor.load_streams WHERE stream_name = 'My stream name';

Also it's good to see the rejected_row_count column value in the load_streams table as it will tell you how many rows were rejected.
You can use CURRENT_LOAD_SOURCE() as well, with this you will need to add a new column to your table.
But this is used when I do loads from different locations(parallel), and I can identify better which file failed during load.
--create the table 
create table bla(email varchar2(50),source varchar2(200));

--load the table using the CURRENT_LOAD_SOURCE() as a filler for the source column
COPY bla (email, source AS CURRENT_LOAD_SOURCE()) FROM '/home/dbadmin/data*' DELIMITER ',';

select * from bla limit 1;

                        email                     |  source
    ----------------------------------------------+----------
      Steven.Kastrinakis@advantagepharmacy.com.au | data.csv

",240
31266969,31266969,1,"I'm having some troubles and I hope you can help me. I've configured the RPi with the Cam Web Interface, who listens to port 80 in the apache server. 
I also have to use python to get some information from the RPi to the web interface (using Flask) but it cannot use the port 80, so it's using the port 5000. Everything working fine.
I had done a simple scrip with ajax to request data in json to the python module.
But I'm receiving the following error:
169.254.203.139 - - [07/Jul/2015 01:16:31] ""GET /flaskr HTTP/1.1"" 500 -
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1836, in __call__
    return self.wsgi_app(environ, start_response)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1820, in wsgi_app
    response = self.make_response(self.handle_exception(e))
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1403, in handle_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1817, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1477, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1381, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1475, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1461, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/var/www/flaskr/static/templates/hello-flask.py"", line 7, in hello
    return jsonify(myList)
  File ""/usr/local/lib/python2.7/dist-packages/flask/json.py"", line 237, in jsonify
    return current_app.response_class(dumps(dict(*args, **kwargs),
TypeError: cannot convert dictionary update sequence element #0 to a sequence 

Here is my code:
html:
<head>
        <script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min$
</head>

<button onclick=""test()"">
        Test
</button>
<script>
function test(){
   $.getJSON('http://raspberrypi.local:5000/flaskr', {
}, function(data){
alert(data);
alert('JSON posted: ' + JSON.stringify(data));
     // Handles the callback when the data returns
});
}
</script>
<div id=""test"">
</div>

error from the console:
XMLHttpRequest cannot load http://raspberrypi.local:5000/flaskr. No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'http://raspberrypi.local' is therefore not allowed access. The response had HTTP status code 500.

python code:
from flask import Flask, render_template, request, jsonify
app = Flask(__name__)

@app.route(""/flaskr"")
def hello():
        myList=[1,2,3,4,5,6]
        return jsonify(myList)

if __name__==""__main__"":
        app.run(host='raspberrypi.local', debug=True)

",507
31266969,31267472,2,"Take a look at this link, it explains how Access-Control-Allow-Origin works.
You might be hosting it on one url and directing ajax to another, and browsers have a same origin policy preventing this from working.
",39
31267030,31267126,2,"Assuming you want to allow only one hyphenated section then you can do this using an optional group
((\d+-)?\d+)$

Demonstration: https://regex101.com/r/wV6zP7/1
For example, this will match ""0123-456789"" but not ""0123-456-789"".
",46
31267030,31267030,1,"I have an address string like this
addr_str = ""No 123 4th St, 5th Ave NYC\n\tPhone: 9938483902""

Currently, I'm using regex to extract phone number from the end of the string like this:
phone = re.search(r'\d+$', addr_str)
print phone.group()

I just realized that there are some phone numbers like:
040-38488993 
3888-32888222 
01854-29924402

How can I alter this regex to get the numbers before the hyphen? Any help?
Please note that the number of digits before the hyphen vary erratically and I also have numbers without any hyphens which I need as well.
",111
31267030,31267288,2,"In case your string always contains Phone: with the phone number following it at the end, you do not need the regex. Also, note that 1-800-MALL is also a valid phone number.
I suggest this:
addr_str = ""No 123 4th St, 5th Ave NYC\n\tPhone: 1-800-MALL""
idx = addr_str.find(""Phone: "")
if idx > -1:
    print addr_str[idx+7:]
else:
    print addr_str

Or, in case regex is still preferable, another solution:
import re
addr_str = ""No 123 4th St, 5th Ave NYC\n\tPhone: 1-800-MALL""
print re.search(r""Phone:\s*(.*)$"", addr_str).group(1)

",128
31267030,31267257,2,"If you always have a space before the phone number, why not simply:
phone = addr_str[addr_str.rfind(' ') + 1:]

",28
31267030,31267216,2,"You could have your digit pattern to include optional minus sign and expect the group to be repeated 1 or 2 times.
phone = re.search(r'(\d+-?){1,2}$', addr_str)

",41
31267030,31267158,2,"phone = re.search(r'\d[\d-]+\d$', addr_str)

You can simply modify your regex to this.If there is always a possiblity of only   1 - use
phone = re.search(r'\d+-\d+$', addr_str)

",42
31267030,31267122,2,"Just put -, \d inside a char class.
phone = re.search(r'[\d-]+$', addr_str)

If the phonenumber startswith with a optional + then you may try this,
phone = re.search(r'\+?\d+(?-\d+)*$', addr_str)

",56
31267073,31267073,1,"How can I set IncludeExceptionDetailInFaults with Python/SUDS? For one of my requests, I am getting this error message:
  File ""/usr/local/lib/python2.7/dist-packages/suds/client.py"", line 542, in __call__
    return client.invoke(args, kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/suds/client.py"", line 602, in invoke
    result = self.send(soapenv)
  File ""/usr/local/lib/python2.7/dist-packages/suds/client.py"", line 649, in send
    result = self.failed(binding, e)
  File ""/usr/local/lib/python2.7/dist-packages/suds/client.py"", line 702, in failed
    r, p = binding.get_fault(reply)
  File ""/usr/local/lib/python2.7/dist-packages/suds/bindings/binding.py"", line 265, in get_fault
    raise WebFault(p, faultroot)
WebFault: Server raised fault: 'The server was unable to process the request due to an internal error.  For more information about the error, either turn on IncludeExceptionDetailInFaults (either from ServiceBehaviorAttribute or from the <serviceDebug> configuration behavior) on the server in order to send the exception information back to the client, or turn on tracing as per the Microsoft .NET Framework SDK documentation and inspect the server trace logs.'

There is no obvious error in the request, so I would like to obtain additional information, but can't find a way using SUDS API. Is there any way?
",219
31267073,31879325,2,"As I understand it now, the answer is no. The IncludeExceptionDetailInFaults setting seems to be only accessible on the server, there doesn't seem to be a way to change it from the client side.
",39
31267080,31267080,1,"can't copy xlsx and xlsm files with xlrd as it says ""formatting info= True"" not yet implemented and
openpyxl runs out of memory when doing the following:
import xlrd
from xlutils.copy import copy
from openpyxl import load_workbook

if file_format == 'xls':
        input_file=xlrd.open_workbook(input_file_location,formatting_info=True)
        wb = copy(input_file)
        wb.save(destination_file)
if file_format == 'xlsx' or file_format == 'xlsm' :
        input_file  =  load_workbook(input_file_location,data_only=True)
        input_file.save(destination_file)

",86
31267080,31268669,2,"If you just want to copy the file then just do that using shutil. There are still several things that openpyxl doesn't support such as images and charts that will be lost. And, as you're seeing, memory is an issue. Each cell uses about 45 kB of memory.
The openpyxl documentation is pretty clear about the different options used when opening workbooks: data_only only read the results of any formulae and ignore the formulae.
See https://bitbucket.org/openpyxl/openpyxl/issue/171/copy-worksheet-function if you want to copy worksheets.
Otherwise you can use two workbooks, one in read-only mode and the other in write-only mode. But if you want to copy, this is best done in the file system.
If you only want to copy the values from one workbook to another then you can combine read-only and write-only modes to reduce the memory footprint. The following pseudo-code should give you a basis.
wb1 = load_workbook(""file.xlsx"", read_only=True, data_only=True)
wb2 = Workbook(write_only=True)
for ws1 in wb1:
    ws2 = wb2.create_sheet(ws1.title)
    for r1 in ws1:
        ws2.append(r1) # this might need unpacking to use values
wb2.save(""copy.xlsx"")

",217
31267147,31267147,1,"I'm testing a web application using Django-nose to monitor the code coverage. At first it worked perfectly well, but when trying to generate HTML it fails with the error:
Imput error: No module named copy_reg
It happened after a few times (until then in worked). I tried it on a computer with newly installed django, django-nose and coverage and the very same code works fine. Re-installing django and django-nose didn't help.
Any suggestions? Should I re-install any library or something?
Thank you in advance!
",99
31267147,31290685,2,"I fixed this by uninstalling coverage.py with pip and installing it using easy_install.
",14
31267148,31267171,2,"Just pass param for usecols:
In [160]:
t=""""""1;2;1;7.00;
2;32;2;0.76;
3;4;6;6.00;
4;1;5;4.00;""""""
​import pandas as pd
import io
df = pd.read_csv(io.StringIO(t), sep=';', header=None, usecols=range(4))
df

Out[160]:
   0   1  2     3
0  1   2  1  7.00
1  2  32  2  0.76
2  3   4  6  6.00
3  4   1  5  4.00

Here I generate the list [0,1,2,3] to indicate which columns I'm interested in.
",124
31267148,31267148,1,"How do I ignore last whitespace in a line when converting to Pandas DataFrame?
I have a CSV file in the following format:
Column #1   : Type
Column #2   : Total Length
Column #3   : Found
Column #4   : Grand Total

1;2;1;7.00;
2;32;2;0.76;
3;4;6;6.00;
4;1;5;4.00;

I loop through the 'Column #' lines to create my column names first (so 4 columns), then I parse the following lines to create my DataFrame using ';' as the separator.  However some of my files contain a trailing ';' on the end of each line as shown above, so my Pandas DataFrame thinks there is a 5th column containing whitespace, and consequently throws an error to say there aren't enough column names specified
Is there a mechanism in Pandas to remove/ignore the trailing ';', or whitespace when creating a DataFrame?  I am using read_csv to create the DataFrame.
Thanks.
",197
31267174,31267174,1,"I'm currently doing this tutorial here: http://nbviewer.ipython.org/urls/bitbucket.org/hrojas/learn-pandas/raw/master/lessons/01%20-%20Lesson.ipynb
I am currently on the last section where you have to plot a graph. I am running the code in my own idle and not iPython. Here is my code:
q=df['Births'].plot()

MaxValue = df['Births'].max()

MaxName = df['Names'][df['Births'] == df['Births'].max()].values

Text = str(MaxValue) + "" - "" + MaxName

plt.annotate(Text, xy=(1, MaxValue), xytext=(8,0),xycoords=('axes fraction', 'data'), textcoords ='offset points')
print(""The most popular name"")
df[df['Births'] == df['Births'].max()]

print(q)

The output I am getting is:
The most popular name
Axes(0.125,0.1;0.775x0.8)

How do I get it to actually show the graph?
",191
31267174,31268992,2,"Adding plt.show() should do the trick, although if I may make a recomendation..this data would be better represented by a bar chart. Something like the following should do the trick:
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib

df = pd.DataFrame(data = BabyDataSet, columns=['Names', 'Births'])

matplotlib.style.use('ggplot')

ax = df['Births'].plot(kind = 'bar')
ax.set_xticklabels(df.Names)

ax.annotate('Most Popular Name: {}: {} births'.format(df.max().Names, df.max().Births),
        xy=(1, 1), 
        xytext=(1, 800))

ax.set_title(""Number of Births by Name"")
ax.set_ylabel(""No. Births"")

plt.show()


",144
31267174,31267403,2,"If you are not in interactive mode, you might need to add plt.figure() before your code, and plt.show() after.
",26
31268331,31269337,2,"Figured out what was wrong thanks to the user Weeble's comment. When he said 'something is causing your main.py to run twice' I remembered that Bottle has an argument that is called 'reloader'. When set to True, this will make the application load twice, and thus the thread creation is run twice as well.
",62
31268331,31268331,1,"I'm having an issue with threading that I can't solve in any way I've tried. I searched in StackOverflow too, but all I could find was cases that didn't apply to me, or explanations that I didn't understand.
I'm trying to build an app with BottlePy, and one of the features I want requires a function to run in background. For this, I'm trying to make it run in a thread. However, when I start the thread, it runs twice. 
I've read in some places that it would be possible to check if the function was in the main script or in a module using if __name__ == '__main__':, however I'm not able to do this, since __name__ is always returning the name of the module.
Below is an example of what I'm doing right now.
The main script:
# main.py
from MyClass import *
from bottle import *

arg = something

myObject = Myclass(arg1)

app = Bottle()
app.run('''bottle args''')

The class:
# MyClass.py
import threading
import time

class MyClass:
    def check_list(self, theList, arg1):
        a_list = something()
        time.sleep(5)
        self.check_list(a_list, arg1)

    def __init__(self, arg1):
        if __name__ == '__main__':
            self.a_list = arg.returnAList()
            t = threading.Thread(target=self.check_list, args=(a_list, arg1))

So what I intend here is to have check_list running in a thread all the time, doing something and waiting some seconds to run again. All this so I can have the list updated, and be able to read it with the main script.
Can you explain to me what I'm doing wrong, why the thread is running twice, and how can I avoid this?
",342
31268331,31269060,2,"This works fine:
import threading
import time

class MyClass:
    def check_list(self, theList, arg1):
        keep_going=True
        while keep_going:
            print(""check list"")
            #do stuff
            time.sleep(1)

    def __init__(self, arg1):
        self.a_list = [""1"",""2""]
        t = threading.Thread(target=self.check_list, args=(self.a_list, arg1))
        t.start()

myObject = MyClass(""something"")

",82
31268359,31268359,1,"I'm following the implementation of a python script example shown here.
Basically, this does what I need it to do except that we now want to run the script with the domain credentials without prompting the user for a username and password.  Ideally, we'd want to run this during maintenance windows, and we would rather have a domain account's credentials handle the authentication instead of baking it in to the script for anyone to stumble upon.  
Since I'm not really well versed with python, I'm not entirely sure what the capabilities/best practices are in these scenarios.  
",109
31268359,31269439,2,"If all you're looking for is a way to pass in arguments without prompting the user, you could use the library argparse to pass in command line arguments, see https://docs.python.org/3/library/argparse.html
Another source of information would be an .ini file, which is handled by the module ConfigParser or any other file-based source like XML, shelve, json.
All these issues have the weakness, that you have to apply access protection for the file containing the credentials.
",85
31268364,31269128,2,"Your code fails as you have already consumed the iterator on the first call, if you call none_context() in the with block the original code would work:
none_context = contextmanager(lambda: iter([None]))
printing=False

with open(fa, ""r"") if printing else none_context() as writter, \
    open(fb, ""r"") if printing else none_context() as another_writter:
    print 1 if printing else 2

You can see using your original code that if you add a None for each open then the code will work as expected:
none_context = contextmanager(lambda: iter([None,None,None]))()
printing=False

with open(fa, ""r"") if printing else none_context as writter, \
    open(fb, ""r"") if printing else none_context as another_writter,\
    open(fb, ""r"") if printing else none_context as another_writer3:
    print 1 if printing else 2

",185
31268364,31268364,1,"I read in this answer of Is it possible to have an optional with/as statement in python? that you can have a dummy file writer with contextmanager. I want, however, to open multiple dummy file writers in a with statement context.
Say I create two dummy files: touch a and touch b.
Given the first part of the script:
#!/usr/bin/python

from contextlib import contextmanager

# File names
fa=""a""
fb=""b""

# Dummy file handler
none_context = contextmanager(lambda: iter([None]))()

This addition works with a single dummy file writer (it prints 2):
printing=False
with (open(fa) if printing else none_context) as writter:
    print 1 if printing else 2

This also works, because we are indeed reading files (it prints 1):
printing=True
with (open(fa, ""r"") if printing else none_context) as writter, \
    (open(fb, ""r"") if printing else none_context) as another_writter:
    print 1 if printing else 2

However, it doesn't work if we are using two dummy file writers:
printing=False
with (open(fa, ""r"") if printing else none_context) as writter, \
    (open(fb, ""r"") if printing else none_context) as another_writter:
    print 1 if printing else 2

It shows the error:
Traceback (most recent call last):
  File ""dummy_opener.py"", line 23, in <module>
    with (open(fa, ""r"") if printing else none_context) as writter, \
  File ""/usr/lib64/python2.7/contextlib.py"", line 19, in __enter__
    raise RuntimeError(""generator didn't yield"")
RuntimeError: generator didn't yield

Why is this happening? And also: how can I make this multiple with open commands work with a dummy file writer?
",351
31268373,31268373,1,"I need to generate PDF from html tables, and was looking for a library that allows to take html tables with full CSS and make pdf. I'm trying to do this with PDFKit. I installed and I tested some simple examples that are explained in its documentation,it works. Documentation link:PDFKit
I have some tables like this,this is table that i want to convert to pdf: 
 <form action=""{{ url_for('patient_directory.make_pdf')}}"" method=""POST"">
    <button id=""download_s"" type=""submit"" class=""btn btn-success"">Create PDF</button>
    <table class=""patient-view-table"" id=""table_to_pdf"">
      <thead>
        <tr>
          <th>Name</th>
          <th>Surname</th>
          <th>Sex</th>
          <th>Date of birth</th>
          <th>Diagnosis</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td class=""property-value-col"">{{ patient.name }}</td>
          <td class=""property-value-col"">{{ patient.surname }}</td>
          <td class=""property-value-col"">{{ patient.sex }}</td>
          <td class=""property-value-col"">{{ patient.date_of_birth }}</td>    
          <td class=""property-value-col"">{{ patient.diagnosis }}</td>
        </tr>
      </tbody>
    </table>
    </form>

In form action of table above i called this method, I created a route method:
mod_patient_directory.add_url_rule( '/pdf',methods=['GET', 'POST'])
def make_pdf(''):       
        pdfkit.from_string(table_to_pdf, 'example.pdf')

My problem is that when i try to generate pdf i don't know how to pass arguments, for example, how to make know that i want to generate to pdf table with id=""table_to_pdf""?
 Any help?
",357
31268373,31269006,2,"It looks like you are using flask as a framework:
Have a look at some of the example here and here. 
You can pass the form ID in as a URL parameter.
@app.route('/pdf/<formid>/', methods=['GET', 'POST'])
def make_pdf(formid):
    ...

",61
31268453,31268453,1,"I am trying to use a CSV in order to fill a 34 columns SQL database by using Python, even though I can't.
import csv sqlite3
con = sqlite3.connect("":memory:"")
cur = con.cursor()

cur.execute(""CREATE TABLE t (No, Source, Host, Link, Date, Time, time2, Category, AuthorId, AuthorName, AuthorUrl, Auth, Followers, Following, Age, Gender, Language, Country, Province, City, Location, Sentiment, Title, Snippet, Description, Tags, Contents, View, Comments, Rating, Favourites, Duration, Bio, UniqueId);"")}

with open('database.csv', 'rb') as fin:
    dr = csv.reader(fin) 
    dicts = ({'No': line[0], 'Source': line[1], 'Host': line[2], 'Link': line[3], 'Date': line[4], 'Time': line[5], 'time2': line[6], 'Category': line[7], 'AuthorId': line[8], 'AuthorName': line[9], 'AuthorUrl': line[10], 'Auth': line[11], 'Followers': line[12], 'Following': line[13], 'Age': line[14], 'Gender': line[15], 'Language': line[16], 'Country': line[17], 'Province': line[18], 'City': line[19], 'Location': line[20], 'Sentiment': line[21], 'Title': line[22], 'Snippet': line[23], 'Description': line[24], 'Tags': line[25], 'Contents': line[26], 'View': line[27], 'Comments': line[28], 'Rating': line[29], 'Favourites': line[30], 'Duration': line[31], 'Following': line[32], 'UniqueId': line[33]} for line in dr)
    to_db = ((i['No'], i['Source'], i['Host'], i['Link'], i['Date'], i['Time'], i['time2'], i['Category'], i['AuthorId'], i['AuthorName'], i['AuthorUrl'], i['Auth'], i['Followers'], i['Following'], i['Age'], i['Gender'], i['Language'], i['Country'], i['Province'], i['City'], i['Location'], i['Sentiment'], i['Title'], i['Snippet'], i['Description'], i['Tags'], i['Contents'], i['View'], i['Comments'], i['Rating'], i['Favourites'], i['Duration'], i['Bio'], i['UniqueId']) for i in dicts)

cur.executemany(""INSERT INTO t VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"", to_db)
con.commit()

I've been following many indications, although it is my first time pythoning and I don't know how to do that.
Could you please help me with this? Thanks a lot in advanced.
Pd: In case it is not inferable, the csv file is without a header, and I am trying to fill column by column at once.
",785
31268453,31268940,2,"If the CSV elements are positionally correct, can you not do something more straight forward i.e. as an example with the following data
1,2,3
a,b,c

use the following;
import sqlite3
con = sqlite3.connect("":memory:"")
cur = con.cursor()

cur.execute(""CREATE TABLE t (col1,col2,col3);"")

with open('database.csv', 'rb') as fp:
    for line in fp.readlines():
        cur.execute(""INSERT INTO t VALUES (?, ?, ?)"",line.strip().split(','))
con.commit()

for row in cur.execute(""select * from t;""):
    print row

",132
31268453,31269661,2,"This works. I used a few short cuts to save on typing.
import csv
import sqlite3
import itertools

params = ['No', 'Source', 'Host', 'Link', 'Date', 'Time', 'time2', 'Category', 'AuthorId', 'AuthorName', 'AuthorUrl', 'Auth', 'Followers', 'Following', 'Age', 'Gender', 'Language', 'Country', 'Province', 'City', 'Location', 'Sentiment', 'Title', 'Snippet', 'Description', 'Tags', 'Contents', 'View', 'Comments', 'Rating', 'Favourites', 'Duration', 'Bio', 'UniqueId']

create_str = ""CREATE TABLE t (%s);"" % ', '.join('""%s""' % p for p in params)
insert_str = ""INSERT INTO t VALUES (%s)"" % ', '.join(itertools.repeat('?', len(params)))

with open('database.csv') as fin:
    dr = csv.DictReader(fin, fieldnames=params, skipinitialspace=True)
    lst = [tuple(d[p] for p in params) for d in dr]

con = sqlite3.connect("":memory:"")
cur = con.cursor()
cur.execute(create_str)

cur.executemany(insert_str, lst)
con.commit()

for row in cur.execute(""select * from t;""):
    print(row)

Note its bad practice to use string format operations to build up sql query strings. It can lead to sql injection attacks, if used with unknown input data. I am doing so here because the strings are only being built from known values and unknown input (that from the file) is built properly using the standard '?' placeholder with tuple passed to execute method.
Note also you have far too many parameters in one table. It should be more normalised across multiple tables, but I guess you will learn that at some point.
",372
31268494,31268494,1,"I have a use case where I have to send_email to user in my views. Now the user who submitted the form will not receive an HTTP response until the email has been sent . I do not want to make the user wait on the send_mail. So i want to send the mail asynchronously without caring of the email error.  I am using using celery for sending mail async but i have read that it may be a overkill for simpler tasks like this. How can i achieve the above task without using celery
",99
31268494,31272086,2,"I'm assuming you don't want to wait because you are using an external service (outside of your control) for sending email. If that's the case then setup a local SMTP server as a relay. Many services such as Amazon SES, SendGrid, Mandrill/Mailchimp have directions on how to do it. The application will only have to wait on the delivery to localhost (which should be fast and is within your control). The final delivery will be forwarded on asynchronously to the request/response. STMP servers are already built to handle delivery failures with retries which is what you might gain by moving to Celery.
",117
31272409,31272562,2,"Add an optional quantifier ? to the value part so that it is matched zero or one time
>> regex = re.compile('(?P<key>[^=]+)(?:=(?P<val>.+))?')
>>> regex.match('x=y').groupdict()
{'key': 'x', 'val': 'y'}
>>> regex.match('x').groupdict()
{'key': 'x', 'val': None}

Changes made

Moved the = to a non capturing group (?:..)
(?:=(?P<val>.+))? Matched zero or one time. This is ensured by the ?. That is it checks if =value can be matched (capturing only the value part). If not None is captured.

",164
31272409,31272651,2,"Try this:
regex = re.compile('(?P<key>[^=]+)(?:=(?P<val>.*))?')

Edited the regex
Test 1 :  'x=y' , then key='x' and val='y'
Test 2 :  'x=' , then key='x' and val=''
Test 3 :  'x' , then key='x' and val=None
",72
31272409,31272409,1,"I want to match either key or key=val in python such that the resulting groupdict() from the Match object will either have val as None or have a value. 
The following is close:
>>> regex = re.compile('(?P<key>[^=]+)=?(?P<val>.*)')
>>> regex.match('x=y').groupdict()
{'key': 'x', 'val': 'y'}            # yes!
>>> regex.match('x').groupdict()
{'key': 'x', 'val': ''}             # want None, not ''

But I want val to be None in the second case. I tried moving the optional = into the second group:
>>> regex = re.compile('(?P<key>[^=]+)(?P<val>=.+)?')
>>> regex.match('x').groupdict()
{'key': 'x', 'val': None}           # yes!
>>> regex.match('x=y').groupdict()
{'key': 'x', 'val': '=y'}           # don't want the =

That gives me the None, but then attaches the = to val. I also tried using the lookbehind with (?<==) but that didn't work for either expression. Is there a way to achieve this?
",277
31272427,31272427,1,"I have to apologise in advance 'cause this question is quite general and may be not clear enough. The question is: how would you run in parallel a Python function that itself uses a pool of processes for some subtasks and does lots of heavy I/O operations? Is it even a valid task?
I will try to provide some more information. I've got a procedure, say test_reduce(), that I need to run in parallel. I tried several ways to do that (see below), and I seem to lack some knowledge to understand why all of them fail.
This test_reduce() procedure does lots of things. Some of those are more relevant to the question than others (and I list them below):

It uses the multiprocessing module (sic!), namely a pool.Pool instance,
It uses a MongoDB connection,
It relies heavily on numpy and scikit-learn libs,
It uses callbacks and lambdas,
It uses the dill lib to pickle some stuff.

First I tried to use a multiprocessing.dummy.Pool (which seems to be a thread pool). I don't know what is specific about this pool and why it is, eh, ""dummy""; the whole thing worked, and I got my results. The problem is CPU load. For parallelized sections of test_reduce() it was 100% for all cores; for synchronous sections it was around 40-50% most of the time. I can't say there was any increase in overall speed for this type of ""parallel"" execution. 
Then I tried to use a multiprocessing.pool.Pool instance to map this procedure to my data. It failed with the following:
File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 251, in map
    return self.map_async(func, iterable, chunksize).get()
  File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 558, in get
    raise self._value
cPickle.PicklingError: Can't pickle <type 'thread.lock'>: attribute lookup thread.lock failed

I made a guess that cPickle is to blame, and found the pathos lib that uses a far more advanced pickler dill. However it also fails:
File ""/local/lib/python2.7/site-packages/dill/dill.py"", line 199, in load
    obj = pik.load()
  File ""/usr/lib/python2.7/pickle.py"", line 858, in load
    dispatch[key](self)
  File ""/usr/lib/python2.7/pickle.py"", line 1083, in load_newobj
    obj = cls.__new__(cls, *args)
TypeError: object.__new__(generator) is not safe, use generator.__new__()

Now, this error is something I don't understand at all. I've got no output to stdout from my procedure when it works in a pool, so it's hard to guess what's going on. The only thing I know is that test_reduce() runs successfully when no multiprocessing is used.
So, how would you run in parallel something that heavy and complicated?
",527
31272427,31293713,2,"So, thanks to @MikeMcKerns' answer, I found how to get the job done with the pathos lib. I needed to get rid of all pymongo cursors, which (being generators) could not be pickled by dill; doing that solved the problem and I managed to run my code in parallel.
",59
31769514,31769514,1,"I have a xml file ""sample.xml"" as:
<?xml version=""1.0"" encoding=""UTF8"" ?>
< !DOCTYPE nodedescription SYSTEM ""sample.dtd"" >
<node_description>
    <target id=""windows 32bit"">
        <graphics>nvidia_970</graphics>
        <power_plug_type>energenie_eu</power_plug_type>
        <test>unit test</test>
   </target>
   <target id=""windows 64bit"">
       <graphics>nvidia_870</graphics>
       <power_plug_type>energenie_eu</power_plug_type>
       <test>performance test</test>
   </target>
</node_description>

and respective dtd as ""sample.dtd"":
<?xml version=""1.0"" encoding=""UTF-8""?>
<!ELEMENT node_description (target)*>
<!ATTLIST target id CDATA #REQUIRED>
<!ELEMENT target (graphics, power_plug_type, test)>
<!ELEMENT graphics (#PCDATA)*>
<!ELEMENT power_plug_type (#PCDATA)*>
<!ELEMENT test (#PCDATA)*>

I want ""sample.xml"" to get validated against ""sample.dtd"" by making use of python script. How will i achieve this? kindly help.
",213
31769514,31769680,2,"The lxml lib is well suited for this:
With sample.txt and sample.dtd in the current working directory, you can simply run:
from lxml import etree
parser = etree.XMLParser(dtd_validation=True)
tree = etree.parse(""sample.xml"", parser)

Results in:
XMLSyntaxError: root and DTD name do not match 'node_description' and 'nodedescription', line 3, column 18

See here for more detail.  Also, a related question
",78
31769525,31769543,2,"You can do it with another loop, and using a comma after the print:
for e in your_list:
   print e,

Note: The comma will avoid printing a new-line character.
",35
31769525,31769537,2,"You're trying to mutate the list's respresenting string instead of using it's members to build your specific
representation. This is not the way to go.
Use map to create strings and str.join to join the strings with your favorite separator:
lst = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
print(' '.join(map(str, lst)))

An alternative with a generator expression:
lst = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
print(' '.join((str(x) for x in lst)))

",129
31769525,31769525,1,"I have the list :: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
created by a loop. 
I want it to print
0 1 2 3 4 5 6 7 8 9

I've used the .strip('[]') to get rid of the parentheses but I cannot get rid of the commas. 
",74
32491545,32492220,2,"Your php script is executed by the www user.

You could check if the python script interpreter is correctly called, it is usually in one of the directory in the PATH environment variable (like /usr/bin/python), but the www user don't have a PATH environment variable set.
Solution specify the whole path to your python interpreter in your shell_exec call ( also specify the full path to your script when you're at it )
What about the path the nltk library is installed, you could check if the python interpreter would correctly look for it by looking at the sys.path while running python with the www user.
Diagnostic : use the shell_exec call to run a python script to print the sys.path values
Solution : append the library path to the sys.path in your python script before the import nltk

These would be the most obvious solutions considering the information provided in the question.
Update :
As there is 2 version version of python installed (on that haven't got the library installed ), it is recommended to specify the path to the desired interpreter. The first solution help correct the issue.
In unix like system I would recommend using which python command to determine the path of your default python interpreter.
",227
32491545,32492406,2,"Option 1
Setup a simple python httpserver listening on localhost.  This old answer might help but there are plenty of howtos out there. The advantage is that you don't have the overhead of starting the python interpreter each time the ntlk stuff needs to be executed and you don't have to worry about shell script executions, permissions etc. Disadvantage is a little of extra work and a little overhead.
Option 2
Using a task queue. Whatever said and done it's not safe to execute commands from your web facing PHP scripts. If you are already using RabbitMQ or something similar you can use that here. Or else if you are using redis you can use the lpush, rpop methods to make redis behave like a queue. Disadvantage: the result is not immidiately available.
Option 3
Anbother strategy for your php script to enter the data into a table and setup your python script to run as a cron job to check the table once a minute. Disadvantage: the result is not immidiately available.
Option 4
Your current choice but please make sure that you have escaped the data properly by @lafor if this option is chosen @dvhh 's answer ought to work.
",223
32491545,32491545,1,"Im trying to call a python file containing a sentence/word tokenizer from my php file like this:
$output = shell_exec('python tokenizer.py $sentence')

I've tried single exec, full paths to python and tokenizer.py, wrapping $sentence in double quotes. But logically, It should not be the problem because calling print(1) at the beginning of python the python code before actually using any nltk packages makes $output equal to '1'. So I came to conclusion that the problem here is the nltk itself, like the path to the modules is not correct or something...
But, calling python from the shell using the same command as above gives me fully tokenized output! To conclude: looks like when calling python from php magically 'turns off' nltk, while it fully works when executed from the shell.
Here's the part of the python code I am using:
import sys
import nltk
from nltk.tokenize import sent_tokenize

sample_text2 = sys.argv[1]
gust = sent_tokenize(sample_text2)
#print(1) here doesn't work, but everywhere above (before calling sent_tokenize) it does.

The server's running on CentOS (Linux), I am accessing it via SSH.
Obvious question: What am I doing wrong here with PHP? Or generally? Any alternatives?
EDIT
As visible in dvhh's answer and its comments, the situation happened because there were two versions installed on the server (2.6 and 2.7), while the www user had access to 2.6 and through console, the default version was 2.7. The solution was to change the default python to 2.7 for both cases and to put the nltk modules to one of the dependency folders. (Or append the dependency directory using sys.path.append)
",327
38957908,38957908,1,"noob programmer here. I am trying to build a small program in 2.7 which generates a prime number, asks the user to continue or not, and then continues generating primes until the user tells it to stop. Unfortunately my program isn't outputting anything at all, and I can't figure out why.
Here is my code:
First, the part which checks for primes. I know that this part is functioning properly, because the exact same code works properly for my prime factor finder.
def isprime(num): #this checks the numbers to see if they're prime
    prime = True
    for i in range(2,num):
        if num/i == (num*1.0)/i:
            prime = False
            return False
            break
        else:
            prime = True
    if prime == True:
        return True

And second, the part which iterates through all the numbers, prints result, and asks to continue or not. The error must be in here somewhere:
def primegen():
    n = 1
    while True:
        if isprime(n) == True:
            print n
            cont = raw_input(""continue? Enter Y/N"")
            if cont == 'N':
                break
            n+=1

primegen()

",224
38957908,38958001,2,"n should be incremented unconditionally. If it isn't the program gets stuck in an infinite loop the first time it encounters a non-prime.
def primegen():
    n = 1
    while True:
        if isprime(n):
            print n
            cont = raw_input(""continue? Enter Y/N"")
            if cont == 'N':
                break
        n += 1

",66
38957908,38958593,2,"Your code is a bit chaotic, I would rewrite it like this:
def isPrime(num):
    if num % 2 == 0:
        return True
    return False

n=1
while True:
    if isPrime(n):
        print n
        cont = raw_input(""Do you want to continue? (1,0) "")
        if not bool(int(cont)):
            break
    n += 1

",73
38957961,38957961,1,"I have a Player class with a score attribute:
class Player(game_engine.Player):

    def __init__(self, id):
        super().__init__(id)
        self.score = 0

This score increases/decreases as the player succeeds/fails to do objectives. Now I need to tell the player his rank out of the total amount of players with something like
print('Your rank is {0} out of {1}')

First I thought of having a list of all the players, and whenever anything happens to a player:

I check if his score increased or decreased
find him in the list
move him until his score is in the correct place

But this would be extremely slow. There can be hundreds of thousands of players, and a player can reset his own score to 0 which would mean that I'd have to move everyone after him in the stack. Even finding the player would be O(n).
What I'm looking for is a high performance solution. RAM usage isn't quite as important, although common sense should be used. How could I improve the system to be a lot faster?
Updated info: I'm storing a player's data into a MySQL database with SQLAlchemy everytime he leaves the gameserver, and I load it everytime he joins the server. These are handled through 'player_join' and 'player_leave' events:
@Event('player_join')
def load_player(id):
    """"""Load player into the global players dict.""""""
    session = Session()
    query = session.query(Player).filter_by(id=id)
    players[id] = query.one_or_none() or Player(id=id)

@Event('player_leave')
def save_player(id):
    """"""Save player into the database.""""""
    session = Session()
    session.add(players[id])
    session.commit()

Also, the player's score is updated upon 'player_kill' event:
@Event('player_kill')
def update_score(id, target_id):
    """"""Update players' scores upon a kill.""""""
    players[id].score += 2
    players[target_id].score -= 2

",404
38957961,39002736,2,"That kind of information can be pulled using SQLAlchemy's sort_by function. If you perform a Query like:
leaderboard = session.query(Player).order_by(Player.score).all()

You will have the list of Players sorted by their score. Keep in mind that every time you do this you do an I/O with the database which can be rather slow instead of saving the data python variables. 
",74
38957961,39043000,2,"Redis sorted sets help with this exact situation (the documentation uses leader boards as the example usage) http://redis.io/topics/data-types-intro#redis-sorted-sets

The key commands you care about are ZADD (update player rank) and ZRANK (get rank for specific player). Both operations are O(log(N)) complexity.

Redis can be used as a cache of player ranking. When your application starts, populate redis from the SQL data. When updating player scores in mysql also update redis.
If you have multiple server processes/threads and they could trigger player score updates concurrently then you should also account for the mysql/redis update race condition, eg:

only update redis from a DB trigger; or
serialise player score updates; or
let data get temporarily out of sync and do another cache update after a delay; or
let data get temporarily out of sync and do a full cache rebuild at fixed intervals

",168
38957961,39000851,2,"The problem you have is that you want real-time updates against a database, which requires a db query each time. If you instead maintain a list of scores in memory, and update it at a more reasonable frequency (say once an hour, or even once a minute, if your players are really concerned with their rank), then the players will still experience real-time progress vs a score rank, and they can't really tell if there is a short lag in the updates.
With a sorted list of scores in memory, you can instantly get the player's rank (where by instantly, I mean O(lg n) lookup in memory) at the cost of the memory to cache, and of course the time to update the cache when you want to. Compared to a db query of 100k records every time someone wants to glance at their rank, this is a much better option.
Elaborating on the sorted list, you must query the db to get it, but you can keep using it for a while. Maybe you store the last_update, and re-query the db only if this list is ""too old"". So you update quickly by not trying to update all the time, but rather just enough to feel like real-time.
In order to find someone's rank nearly instantaneously, you use the bisect module, which supports binary search in a sorted list. The scores are sorted when you get them.
from bisect import bisect_left

# suppose scores are 1 through 10
scores = range(1, 11)

# get the insertion index for score 7
# subtract it from len(scores) because bisect expects ascending sort
# but you want a descending rank
print len(scores) - bisect_left(scores, 7)

This says that a 7 score is rank 4, which is correct.
",348
38957964,38957964,1,"this is my first post and I would like to thanks the amazing community of Stack Overflow for helping me during all this years! 
However, after extensive research, i couldn't find a solution to my problem. I have the file generated by QtCreator which contain a progressbar.
In my code, I have 2 class, and 1 is a Thread. This Thread must change the value of the progressbar, but I completely fail to do so.
I can't access the variable from my Thread, but i can from the init of Mainwindow. I think the problem is the nature of the ""self"" variable in setprogressBar, but I'm literally stuck finding out what it is.. 
When i tried to execute this code, here is the result :   
File ""C:\test.py"", line 14, in setprogressBar
self.progressBar.setProperty(""value"", pourcentage)
AttributeError: type object 'MainWindow' has no attribute 'progressBar'

File A, generated with QtCreator : 
from PyQt5 import QtCore, QtGui, QtWidgets

class Ui_MainWindow(object):
    def setupUi(self, MainWindow):
        MainWindow.setObjectName(""MainWindow"")
        self.progressBar = QtWidgets.QProgressBar(self.widget)
        self.progressBar.setObjectName(""progressBar"")

File B, my code :
from PyQt5 import QtWidgets
from UImainwindow import Ui_MainWindow
from threading import Thread
import sys

class MainWindow(QtWidgets.QMainWindow, Ui_MainWindow):
 # access variables inside of the UI's file
    def __init__(self):
        super(MainWindow, self).__init__()
        self.setupUi(self) # gets defined in the UI file
        self.progressBar.setProperty(""value"", 24) #This is working

    def setprogressBar(self, pourcentage):
        self.progressBar.setProperty(""value"", pourcentage) #This is not

class B(Thread):
    def __init__(self):
        Thread.__init__(self)

    def run(self):          
        MainWindow.setprogressBar(MainWindow ,48)

APP = QtWidgets.QApplication(sys.argv)

Bi = B()
Bi.start()

MAINWIN = MainWindow()
MAINWIN.show()

APP.exec_()

Thx a lot for the help guys !
",376
38957964,38958044,2,"MainWindow is a class, not an object. What you should do instead is something like:
class B(Thread):
    def __init__(self, target):
        self.__target = target

    def run(self):
        self.__target.setprogressBar(48)

MAINWIN = MainWindow()

bi = B(MAINWIN)
bi.start()

MAINWIN.show()

",62
38958048,38958092,2,"Something like this:
r'\w*[0-9]\w*'

That should match any contiguous run of word characters containing at least one digit.
",24
38958048,38958048,1,"It's easy to remove all digits from the following string:
>>> string = ""asprx12303 hello my 321 name is Tom 2323dsad843, 657a b879 843aa943 aa... 2ci 2ci""
>>> modified = re.sub(r'\d+', '', string)
>>> print(modified)
'asprx hello my  name is Tom dsad, a b aa aa... ci ci'

But I want to remove every standalone digit (e.g. 321 by itself) and every combination of letters and numbers in the string (e.g. 843aa943 and asprx12303).
This is what I have so far:
>>> modified2 = re.sub(r'\w+\d+', '', string)
>>> print(modified2)
' hello my  name is Tom , a   aa... 2ci 2ci'

So, these two patterns work pretty well, but I'm left with 2ci at the end. How can I make an all-encompassing regex for this issue? My solution is ok so far, but is not quite what I need.
",190
38958114,38958451,2,"You may want to use facts.d and place your python script there to be available as a fact.
Or write a simple action plugin that returns json object to eliminate the need in stdout->from_json conversion.
",39
38958114,38958114,1,"i am very new to ansible and would like to test a few things.
I have a couple of Amazon EC2 instances and would like to install different software components on them. I don't want to have the (plaintext) credentials of the technical users inside of ansible scripts or config files. I know that it is possible to encrypt those files, but I want to try keepass for a central password management tool. So my installation scripts should read the credentials from a .kdbx (Keepass 2) database file before starting the actual installation.
Till now i wrote a basic python script for reading the .kdbx file. The script outputs a json object via:
print json.dumps(inventory, sort_keys=False)

The ouput looks like the following: 
{""cdc"": 
    {""cdc_test_server"": 
        {""cdc_test_user"": 
            {""username"": ""cdc_test_user"", 
             ""password"": ""password""}
        }
    }
}

Now I want to achieve, that the python script is executed by ansible and the key value pairs of the output are included/registered as ansible variables. So far my playbook looks as follows:
- hosts: 127.0.0.1
  connection: local
  tasks:
  - name: ""Test Playboook Functionality""
    command: python /usr/local/test.py
    register: pass

  - debug: var=pass.stdout

  - name: ""Include json user output""
    set_fact: passwords=""{{pass.stdout | from_json}}""

  - debug: "" {{passwords.cdc.cdc_test_server.cdc_test_user.password}} ""

The first debug generates the correct json output, but i am not able to include the variables in ansible, so that I can use them via jinja2 notation. set_fact doesn't throw an exception, but the last debug just returns a ""Hello world"" - message? So my question is: How do I properly include the json key value pairs as ansible variables via task?
",343
38958212,39005037,2,"As already said in the comments:
import re

rx = re.compile(r'\b\d\.[-.xdev\d]+\b')

data = """"""
Views 2.x-dev
Viewfield 6.x-2.x, xxxx-xx-xx
Version 6.x-2.3
ctools 7.x-1.x-dev
XML sitemap 6.x-1, 2009-08-24
6.x-1.6
""""""

versions = rx.findall(data)
print(versions)
# ['2.x-dev', '6.x-2.x', '6.x-2.3', '7.x-1.x-dev', '6.x-1', '6.x-1.6']


See a demo on regex101.com.
",81
38958212,38964290,2,"I hope you find this solution satisfactory:
import re

data = """"""
Views 2.x-dev
Viewfield 6.x-2.x, xxxx-xx-xx
Version 6.x-2.3
ctools 7.x-1.x-dev
XML sitemap 6.x-1, 2009-08-24
6.x-1.6
""""""
## Solution 1: 
## 1) Finds all results that matches with the pattern 
## '<number>.x-<other_chars>' in a not greedy way 
## (that is why the '?')
## 2) Treat the found match and remove any ','

results = []
for result in re.findall( r""[0-9]\.x{1}\-.*x?"", data ):
    results.append(result.split("","")[0])

print (results)

## Solution 2: (and my favourite)
## Create a list with those words that matches the pattern 
## <number>.x at their beginning
## The words are the result of splitting the 'data' with the
## split function from the regex module with a
## non-capturing version of regular parentheses.

result = [ x  for x in re.split(r'(?:,|\s)\s*', data) \ 
           if re.match(r'^[0-9].x',x) ]

print (result)

",235
38958212,38958212,1,"import re
data = """"""
Views 2.x-dev
Viewfield 6.x-2.x, xxxx-xx-xx
Version 6.x-2.3
ctools 7.x-1.x-dev
XML sitemap 6.x-1, 2009-08-24
6.x-1.6
""""""
print data


How to get 2.x-dev, 6.x-2.x, 6.x-2.3, 7.x-1.x-dev, 6.x-1 6.x-1.6.
Thanks
",43
38958233,38958233,1,"I was following a tutorial called ""Black Hat Python"" and got a ""the requested address is not valid in its context"" error. I'm Python IDE version: 2.7.12
This is my code:
import socket
import threading

bind_ip = ""184.168.237.1""
bind_port = 21

server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

server.bind((bind_ip,bind_port))

server.listen(5)

print ""[*] Listening on %s:%d"" % (bind_ip,bind_port)

def handle_client(client_socket):

    request = client_socket.rev(1024)

    print ""[*] Recieved: %s"" % request

    client_socket.close()

while True:

    client,addr = server.accept()

    print ""[*] Accepted connection from: %s:%d"" % (addr[0],addr[1])

    client_handler = threading.Thread(target=handle_client,args=(client,))
    client_handler.start()

and this is my error:
Traceback (most recent call last):
  File ""C:/Python34/learning hacking.py"", line 9, in <module>
    server.bind((bind_ip,bind_port))
  File ""C:\Python27\lib\socket.py"", line 228, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 10049] The requested address is not valid in its context
>>> 

",246
38958233,38958939,2,"You are trying to bind to an IP address that is not actually assigned to your network interface:
bind_ip = ""184.168.237.1""

See the Windows Sockets Error Codes documentation:

WSAEADDRNOTAVAIL 10049
Cannot assign requested address.
The requested address is not valid in its context. This normally results from an attempt to bind to an address that is not valid for the local computer.

That may be an IP address that your router is listening to before using NAT (network address translation) to talk to your computer, but that doesn't mean your computer sees that IP address at all.
Either bind to 0.0.0.0, which will use all available IP addresses (both localhost and any public addresses configured):
bind_ip = ""0.0.0.0""

or use any address that your computer is configured for; run ipconfig /all in a console to see your network configuration.
You probably also don't want to use ports < 1024; those are reserved for processes running as root only. You'll have to pick a higher number than that if you want to run an unprivileged process (and in the majority of tutorials programs, that is exactly what you want):
port = 5021  # arbitrary port number higher than 1023

I believe the specific tutorial you are following uses BIND_IP = '0.0.0.0' and BIND_PORT = 9090.
",245
38960631,38960694,2,"You need to convert the list to numpy array in order to use vectorized operation such as == and &:
import numpy as np
np.where((np.array(x) == ""A"") & (np.array(y) == ""2""))
# (array([1, 5]),)

Shorter version (if you are sure that x and y are numpy arrays):
>>> np.where(np.logical_and(x == 'A', y == '2'))
(array([1, 5]),)

",106
38960631,38960631,1,"I have two numpy lists:
x = ['A', 'A', 'C', 'A', 'V', 'A', 'B', 'A', 'A', 'A']
y = ['1', '2', '1', '1', '3', '2', '1', '1', '1', '1']

How can I find indexes when simulataneously x equals 'A' and y equals '2'?
I expect to get indexes [1, 5].
I tried to use:
np.where(x == 'A' and y == '2') but it didn't help me. 
",124
38960631,38960855,2,"If you want to work with lists:
idx1 = [i for i, x in enumerate(x) if x == 'A']
idx2 = [i for i, x in enumerate(y) if x == '2']
list(set(idx1).intersection(idx2))

",57
38960631,38960750,2,"pure python solution:
>>> [i for i,j in enumerate(zip(x,y)) if j==('A','2')]
[1, 5]

",38
38960708,38960708,1,"Trying to deploy my project on the server, and i'm stuck in migrations becouse there is some error:
  File ""/app/.heroku/python/lib/python2.7/site-packages/django/db/backends/base/schema.py"", line 382, in add_field
definition, params = self.column_sql(model, field, include_default=True)
  File ""/app/.heroku/python/lib/python2.7/site-packages/django/db/backends/base/schema.py"", line 145, in column_sql
default_value = self.effective_default(field)
  File ""/app/.heroku/python/lib/python2.7/site-packages/django/db/backends/base/schema.py"", line 210, in effective_default
default = field.get_db_prep_save(default, self.connection)
  File ""/app/.heroku/python/lib/python2.7/site-packages/django/db/models/fields/related.py"", line 915, in get_db_prep_save
return self.target_field.get_db_prep_save(value, connection=connection)
File ""/app/.heroku/python/lib/python2.7/site-packages/django/db/models/fields/__init__.py"", line 728, in get_db_prep_save
prepared=False)
File ""/app/.heroku/python/lib/python2.7/site-packages/django/db/models/fields/__init__.py"", line 968, in get_db_prep_value
value = self.get_prep_value(value)
File ""/app/.heroku/python/lib/python2.7/site-packages/django/db/models/fields/__init__.py"", line 976, in get_prep_value
return int(value)
TypeError: int() argument must be a string or a number, not 'Profile'

And here is my models.py file:
class Post(models.Model):
author = models.ForeignKey('auth.User')
title = models.CharField(max_length=75)
image = models.ImageField(null=True, blank=True, width_field=""width_field"", height_field=""height_field"", upload_to='images') #must be installed Pillow for ImageField
height_field = models.IntegerField(default=0)
width_field = models.IntegerField(default=0)
content = models.TextField()
updated = models.DateTimeField(default=datetime.now())
timestamp = models.DateTimeField(default=datetime.now())


def __str__(self):
    return self.title

@receiver(pre_delete, sender=Post)
def post_delete(sender, instance, **kwargs):
# Pass false so FileField doesn't save the model.
if instance.image:
    instance.image.delete(False)




class Profile(models.Model):
onlyletters = RegexValidator(r'^[a-zA-Z]*$', 'Only letters are allowed.')
author = models.ForeignKey('auth.User')
name = models.CharField(max_length=20, null = True, validators=[onlyletters])
surname = models.CharField(max_length=25, null = True, validators=[onlyletters])
city = models.CharField(max_length=30, blank=True, validators=[onlyletters])
birth_date = models.DateField(blank=True, null=True, help_text=""Can not be more than 100 years - (Format yyyy-mm-dd)"")

topic = models.CharField(max_length = 50, null=True)


def __str__(self):
    return self.topic

class Favourite(models.Model):
name = models.CharField(max_length=50)
members = models.ManyToManyField(Profile, through='Membership')

def __str__(self):              # __unicode__ on Python 2
    return self.name


class Membership(models.Model):
author = models.CharField(max_length=50)
profile = models.ForeignKey(Profile, default=Profile)
favourite = models.ForeignKey(Favourite)
created = models.DateTimeField(default=timezone.now)



class Comment(models.Model):
post = models.ForeignKey('blogapp.Post', related_name='comments')
author = models.ForeignKey('auth.User')
text = models.TextField()
created_date = models.DateTimeField(default=timezone.now)

def __str__(self):
    return self.text

Just trying to make some fresh app on heroku, making new migrations and deleting olders but it's still didn't work. Can anybody know what's wrong with it? Thanks for the answer.
",546
38960708,38961132,2,"Here's the culprit:
profile = models.ForeignKey(Profile, default=Profile)
#                                    ^^^^^^^^^^^^^^^

You can't set a Model class as a Foreignkey default. If you're thinking of setting an hardcoded default then you should use an int and be sure the selected value exists as a key in your Profile model.
",57
38960714,38960864,2,"Here's one way to use itertools for this problem.
import itertools

def makenum(digits):
    return int(''.join(map(str, digits)))

for p in itertools.permutations(range(10)):
    a = makenum(p[:5])
    b = makenum(p[5:])
    if a == 9 * b:
        print(a, b)

",75
38960714,38961307,2,"This is a minor variation on @David's answer.
If we look at itertools.permutations([1,2,3,4])
>>> for p in itertools.permutations([1,2,3,4]):
...     print(p)
...
(1, 2, 3, 4)
(1, 2, 4, 3)
(1, 3, 2, 4)
(1, 3, 4, 2)
(1, 4, 2, 3)
(1, 4, 3, 2)
(2, 1, 3, 4)
(2, 1, 4, 3)
(2, 3, 1, 4)
(2, 3, 4, 1)
(2, 4, 1, 3)
(2, 4, 3, 1)
(3, 1, 2, 4)
(3, 1, 4, 2)
(3, 2, 1, 4)
(3, 2, 4, 1)
(3, 4, 1, 2)
(3, 4, 2, 1)
(4, 1, 2, 3)
(4, 1, 3, 2)
(4, 2, 1, 3)
(4, 2, 3, 1)
(4, 3, 1, 2)
(4, 3, 2, 1)

You'll notice that for a tuple (a,b,c,d), (c,d,a,b) also appears.  If a number a == 9*b, then b != a*9.  We'll use this to our advantage.
Also, note that if a = 9*b, a must be bigger than b, unless we're using negative numbers or non-integers.
You'll see that as we look at the results, splitting the tuples in half and turning them into numbers gives, initially, a small number followed by a larger number.  This is a side effect of passing permutations a sorted list.  Again, we can use this to our advantage.
import itertools

def makenum(digits):
    return int(''.join(map(str, digits)))

for p in itertools.permutations(range(10)):
    first_half = makenum(p[:5])    
    second_half = makenum(p[5:])
    if second_half < first_half: # first half is smaller for first half of permutations, second half of permutations has been covered, backwards.
        break
    if second_half == 9 * first_half:
        print(first_half, second_half)

If you're taking user input, you should be able to get the same result simply by sorting your input:
for p in itertools.permutations(sorted(digits)):
    # ...

",513
38960714,38960714,1,"Recently, I read a math problem inspiring me to write a program. It asked to arrange the digits 0-9 once each so that xx xxx / xx xxx = 9. I wrote a python program to find the solutions and had a bit of trouble making sure the digits were different. I found a way using nested whiles and ifs, but I'm not quite happy with it.
b,c,x,y,z = 0,0,0,0,0  #I shortened the code from b,c,d,e,v,w,x,y,z
for a in range (10):
    while b < 10:
        if b != a:
            while c < 10:
                if c != b and c != a:
                    while x < 10:
                        if x != c and x != b and x != a:
                            while y < 10:
                                if y != x and y != c and y != b and y != a:
                                    while z < 10:
                                        if z != y and if z != z and y != c and z != b and z != a:
                                            if (a*100 + b*10 + c)/(x*100 + y*10 + z) == 9:
                                                print ()
                                                print (str (a*100 + b*10 + c) + ""/"" + str (x*100 + y*10 + z)
                                        z += 1
                                    z = 0
                                y += 1
                            y,z = 0,0
                        x += 1
                    x,y,z = 0,0,0
                c += 1
            c,x,y,z = 0,0,0,0
        b += 1
    b,c,x,y,z = 0,0,0,0,0

As you can see, the code is very long and repetitive, even the shortened form. Running it on my laptop takes almost a minute (and my laptop is new). I have searched for answers, but I only found ways to generate random numbers. I tried using itertools.permutations as well, but that only shows the permutations, not creating a number. 
Generating all ten digits takes too long, and I want to know if there is a faster, simpler way, with an explanation, using python 3..
Thanks
",417
38960714,38961548,2,"Adapting Wayne Werner's solution you can do this to add the digit uniqueness constraint (assuming Python 3):
[(9*num, num) 
 for num in range(10000, 100000 // 9) 
 if len(set(str(num) + str(num * 9))) == 10]

This runs in 1.5 ms on my machine.
Note, that you can only check numbers between 10000 and 100000 / 9 = 11111. 
And if you want to allow preceding zeros, you can do it like this:
[(9*num, num) 
 for num in range(0, 100000 // 9) 
 if len(set((""%05d"" % num) + (""%05d"" % (num * 9)))) == 10]

And this one takes 15 ms.
",156
38960714,38961088,2,"Take advantage of algebra:
a / b = 9 == a = 9 * b

Knowing that, you only have to bother generating the values:
[(9*num, num) for num in range(10000, 100000)]

If you need to filter things out by some criteria, you can easily write a filter function:
def unique_numbers(num):
    num = str(num)
    return len(num) == len(set(num))

[(9*num, num) for num in range(10000, 100000) if unique_numbers(num) and unique_numbers(9*num)]

If you wanted to shorten things a bit, you could re-write your function so that it returns the valid pair, or None otherwise.
def good_nums_or_none(num):
    a = num * 9
    b = num
    str_a = str(a)
    str_b = str(b)
    if len(a) == len(set(a)) and len(b) == len(set(b)):
         return a, b
    else:
         return None

[nums for nums in (good_nums_or_none(num) for num in range(10000, 100000)) if nums is not None]

Or, just create a generator and iterate over that:
 def target_numbers(factor=9, min=10000, max=100000):
     cur = min
     while cur < max:
         a = factor*cur
         b = cur
         str_a = str(a)
         str_b = str(b)
         if len(a) == len(set(a)) and len(b) == len(set(b)):
             yield a, b

 [num for num in target_numbers()]

If you want to allow zero padded numbers in b then you can use this filter:
def target_numbers(factor=9, min=1000, max=100000):
    cur = min                                      
    while cur < max:                               
        b = cur                                    
        a = factor*cur                             
        text = str(a) + str(b).zfill(5)            
        if len(text) == len(set(text)):            
            yield a, b                             
        cur += 1    

",391
38960714,38961197,2,"Runs in 0.7 secs. Faster than most solutions mentioned, though bit clumsy.
def sol(a,b,zero):
  for i in range(a,b):
     fl = 0
     marked = 10*[0]
     marked[0] = zero
     tmp = i
     while tmp > 0:
       marked[tmp%10] = marked[tmp%10] + 1
       tmp = tmp/10
     numerator = i*9

     while numerator > 0:
       marked[numerator%10] = marked[numerator%10] + 1
       numerator = numerator/10

     for j in range(10):
        if marked[j] != 1:
           fl = 1

     if fl == 0:
        print ""found a solution "",i*9,""/"",i


sol(1000,10000,1)
sol(10000,100000,0)

The solution printed is:
 found a solution  57429 / 6381
 found a solution  58239 / 6471
 found a solution  75249 / 8361
 found a solution  95742 / 10638
 found a solution  95823 / 10647
 found a solution  97524 / 10836

",190
38961251,38961806,2,"I believe that the cause of this problem is coalesce(), which despite the fact that it avoids a full shuffle (like repartition would do), it has to shrink the data in the requested number of partitions.
Here, you are requesting all the data to fit into one partition, thus one task (and only one task) has to work with all the data, which may cause its container to suffer from memory limitations.
So, either ask for more partitions than 1, or avoid coalesce() in this case.

Otherwise, you could try the solutions provided in the links below, for increasing your memory configurations:

Spark java.lang.OutOfMemoryError: Java heap space
Spark runs out of memory when grouping by key

",138
38961251,39106641,2,"The problem for me was indeed coalesce(). 
What I did was exporting the file not using coalesce() but parquet instead using df.write.parquet(""testP""). Then read back the file and export that with coalesce(1).
Hopefully it works for you as well.
",55
38961251,38961251,1,"I'm invoking Pyspark with Spark 2.0 in local mode with the following command:
pyspark --executor-memory 4g --driver-memory 4g

The input dataframe is being read from a tsv file and has 580 K x 28 columns. I'm doing a few operation on the dataframe and then i am trying to export it to a tsv file and i am getting this error.
df.coalesce(1).write.save(""sample.tsv"",format = ""csv"",header = 'true', delimiter = '\t')

Any pointers how to get rid of this error. I can easily display the df or count the rows.
The output dataframe is 3100 rows with 23 columns
Error:
Job aborted due to stage failure: Task 0 in stage 70.0 failed 1 times, most recent failure: Lost task 0.0 in stage 70.0 (TID 1073, localhost): org.apache.spark.SparkException: Task failed while writing rows
    at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261)
    at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)
    at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
    at org.apache.spark.scheduler.Task.run(Task.scala:85)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.OutOfMemoryError: Unable to acquire 100 bytes of memory, got 0
    at org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:129)
    at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:374)
    at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:396)
    at org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:94)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
    at org.apache.spark.sql.execution.WindowExec$$anonfun$15$$anon$1.fetchNextRow(WindowExec.scala:300)
    at org.apache.spark.sql.execution.WindowExec$$anonfun$15$$anon$1.<init>(WindowExec.scala:309)
    at org.apache.spark.sql.execution.WindowExec$$anonfun$15.apply(WindowExec.scala:289)
    at org.apache.spark.sql.execution.WindowExec$$anonfun$15.apply(WindowExec.scala:288)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:766)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:766)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
    at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
    at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
    at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
    at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:96)
    at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:95)
    at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
    at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
    at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253)
    at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252)
    at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252)
    at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1325)
    at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258)
    ... 8 more

Driver stacktrace:

",591
38961251,41681999,2,"In my case the driver was smaller than the workers. Issue was resolved by making the driver larger.  
",20
38961252,38961252,1,"I am working with a big set of data (240, 131000) and bigger. I am currently using the code below to plot this.
fig,ax = pyplot.subplots()
spectrum = ax.pcolor(waterfallplot, cmap='viridis')
pyplot.colorbar()
pyplot.show()

However, it's taking a very long time (30 min+) and the plot still hasn't shown up yet. A quick breakpoint check says the code gets to the spectrum= line, but doesn't go past. Looking at the memory on my computer, it hasn't even gotten close to the limit. 
Does anyone have a better way of doing this?
",120
38961252,38975543,2,"pcolorfast works best for large arrays and updates quickly.
",10
38961284,38961284,1,"I'm trying to divvy up the task of looking up historical stock price data for a list of symbols by using Pool from the multiprocessing library.  
This works great until I try to use the data I get back.  I have my hist_price function defined and it outputs to a list-of-dicts pcl.  I can print(pcl) and it has been flawless, but if I try to print(pcl) after the if __name__=='__main__': block, it blows up saying pcl is undefined.  I've tried declaring global pcl in a couple places but it doesn't make a difference.
from multiprocessing import Pool

syms = ['List', 'of', 'symbols']

def hist_price(sym):
    #... lots of code looking up data, calculations, building dicts...
    stlh = {""Sym"": sym, ""10D Max"": pcmax, ""10D Min"": pcmin} #simplified
    return stlh

#global pcl
if __name__ == '__main__':
    pool = Pool(4)
    #global pcl
    pcl = pool.map(hist_price, syms)
    print(pcl) #this works
    pool.close() 
    pool.join()

print(pcl) #says pcl is undefined

#...rest of my code, dependent on pcl...

I've also tried removing the if __name__=='__main__': block but it gives me a RunTimeError telling me specifically to put it back.  Is there some other way to call variables to use outside of the if block?  
",274
38961284,38961389,2,"I think there are two parts to your issue. The first is ""what's wrong with pcl in the current code?"", and the second is ""why do I need the if __name__ == ""__main__"" guard block at all?"".
Lets address them in order. The problem with the pcl variable is that it is only defined in the if block, so if the module gets loaded without being run as a script (which is what sets __name__ == ""__main__""), it will not be defined when the later code runs.
To fix this, you can change how your code is structured. The simplest fix would be to guard the other bits of the code that use pcl within an if __name__ == ""__main__"" block too (e.g. indent them all under the current block, perhaps). An alternative fix would be to put the code that uses pcl into functions (which can be declared outside the guard block), then call the functions from within an if __name__ == ""__main__"" block. That would look something like this:
def do_stuff_with_pcl(pcl):
    print(pcl)

if __name__ == ""__main__"":
    # multiprocessing code, etc
    pcl = ...
    do_stuff_with_pcl(pcl)

As for why the issue came up in the first place, the ultimate cause is using the multiprocessing module on Windows. You can read about the issue in the documentation.
When multiprocessing creates a new process for its Pool, it needs to initialize that process with a copy of the current module's state. Because Windows doesn't have fork (which copies the parent process's memory into a child process automatically), Python needs to set everything up from scratch. In each child process, it loads the module from its file, and if you the module's top-level code tries to create a new Pool, you'd have a recursive situation where each of the child process would start spawning a whole new set of child processes of its own.
The multiprocessing code has some guards against that, I think (so you won't fork bomb yourself out of simple carelessness), but you still need to do some of the work yourself too, by using if __name__ == ""__main__"" to guard any code that shouldn't be run in the child processes.
",439
38961293,38961293,1,"I'm trying to connect to website with python requests, but not with my real IP. So, I found some proxy on the internet and wrote this code:
import requests

proksi = {
    'http': 'http://5.45.64.97:3128'
}

x = requests.get('http://www.whatsmybrowser.org/', proxies = proksi)
print(x.text)

When I get output, proxy simple don't work. Site returns my real IP Address. What I did wrong? Thanks.
",86
38961293,38961568,2,"The answer is quite simple. Although it is a proxy service, it doesn't guarantee 100% anonymity. When you send the HTTP GET request via the proxy server, the request sent by your program to the proxy server is:
GET http://www.whatsmybrowser.org/ HTTP/1.1
Host: www.whatsmybrowser.org
Connection: keep-alive
Accept-Encoding: gzip, deflate
Accept: */*
User-Agent: python-requests/2.10.0

Now, when the proxy server sends this request to the actual destination, it sends:
GET http://www.whatsmybrowser.org/ HTTP/1.1
Host: www.whatsmybrowser.org
Accept-Encoding: gzip, deflate
Accept: */*
User-Agent: python-requests/2.10.0
Via: 1.1 naxserver (squid/3.1.8)
X-Forwarded-For: 122.126.64.43
Cache-Control: max-age=18000
Connection: keep-alive

As you can see, it throws your IP (in my case, 122.126.64.43) in the HTTP header: X-Forwarded-For and hence the website knows that the request was sent on behalf of 122.126.64.43
Read more about this header at: https://tools.ietf.org/html/rfc7239
If you want to host your own squid proxy server and want to disable setting X-Forwarded-For header, read: http://www.squid-cache.org/Doc/config/forwarded_for/
",188
38961327,38961327,1,"We are creating a python program that executes specific macros within Polyworks based on user input into the program. Right now the code is:
roto.command.CommandExecute('MACRO EXEC(""C:\\RotoWorks\\Macros\\CentrifugalCompressor"")')

However this assumes that our program is always installed in C:\RotoWorks. Ideally, our app is portable. I'm sure theres a way to retrieve the filepath that Rotoworks is stored in, then just concatenate the rest of the filepath to the end. How do I do this?
",94
38961327,38961378,2,"You can retrieve the path from the __file__ attribute of the file. Use os.path.abspath on that attribute to retrieve the absolute path of the file and then os.path.dirname to retrieve the containing directory:
import os

file_directory = os.path.dirname(os.path.abspath(__file__))
path = os.path.join(file_directory, other_path) # join directory to an inner path
roto.command.CommandExecute('MACRO EXEC({})'.format(path))

Use os.path.dirname recursively to move out as many directories as you want.
",87
38961360,38961360,1,"I am working on a project that requires me to read a spreadsheet provided by the user and I need to build a system to check that the contents of the spreadsheet are valid. Specifically I want to validate that each column contains a specific datatype.
I know that this could be done by iterating over every cell in the spreadsheet, but I was hoping there is a simpler way to do it.
",77
38961360,39088757,2,"I ended up just manually looking at each cell. I have to read them all into my data structures before I can process anything anyways so it actually made sense to check then.
",35
38961360,39077066,2,"In openpyxl you'll have to go cell by cell.
You could use Excel's builtin Data Validation or Conditional Formatting, which openpyxl supports, for this. Let Excel do the work and talk to it using xlwings.
",42
38963631,38963631,1,"I'm using Sphinx version 1.4.5.
My project structure is the following:
+ src > main.py
+ docs (generated with sphinx-quickstart)
Even after adding the path to the src folder in docs/conf.py:
sys.path.insert(0, os.path.abspath('../src'))

And generating the rst file for src/main.py (i.e. docs/src.rst and docs/modules.rst) with:
$ sphinx-apidoc -fo docs src

When I try to build the html webpages with:
$ make clean
$ make html

It couldn't find both the src module and src/main.py:
WARNING: autodoc: failed to import module u'src.main'; the following exception was raised
",111
38963631,38963810,2,"Try doing this for your path insertion instead:
sys.path.insert(0, os.path.abspath('../'))

Also consider a better name for your directory than src.
",30
38963631,38963769,2,"Your current working directory should be the directory of your makefile, which should be docs.
",17
38963653,38964103,2,"This has nothing to do with the parser, you'll see the same behavior just from mktime() alone, since datetime.timetuple() doesn't have any time zone offset information, and mktime() is the inverse of localtime. You can correct this by converting it to localtime before calling timetuple():
from time import mktime
from datetime import datetime
from dateutil import tz

dt_base = datetime(2012, 11, 9, 9, 4, 2)

dt_est = dt_base.replace(tzinfo=tz.tzoffset('EST', -5 * 3600))
dt_pst = dt_base.replace(tzinfo=tz.tzoffset('PST', -8 * 3600))

def print_mktime(dt):
    print(mktime(dt.timetuple()))

# Run in UTC
print_mktime(dt_est)   # 1352469842.0
print_mktime(dt_pst)   # 1352469842.0

# Convert to local time zone first first
print_mktime(dt_est.astimezone(tz.tzlocal())) # 1352469842.0
print_mktime(dt_pst.astimezone(tz.tzlocal())) # 1352480642.0

Note that there is a chart on the documentation for time() (python 2.x docs) that tells you how to convert between these representations:
From                        To                           Use
---------------------------------------------------------------------------
seconds since the epoch   | struct_time in UTC        |  gmtime()
seconds since the epoch   | struct_time in local time |  localtime()
struct_time in UTC        | seconds since the epoch   |  calendar.timegm()
struct_time in local time | seconds since the epoch   |  mktime()

My personal preference would be to convert the parsed date to UTC, in which case calendar.timegm() would be the appropriate function:
from calendar import timegm
def print_timegm(dt):
    print(timegm(dt.timetuple()))

print_timegm(dt_est.astimezone(tz.tzutc())) # 1352469842.0
print_timegm(dt_pst.astimezone(tz.tzutc())) # 1352480642.0

",362
38963653,38963653,1,"I have looked at many possible ways to parse python times. Using parse seems link the only method that should work. While trying to use datetime.strptime causes an error because %z does not work with python 2.7. But using parse.parse incorrectly recognizes the time zone.
I parse both Fri Nov 9 09:04:02 2012 -0500 and Fri Nov 9 09:04:02 2012 -0800 and get the exact same timestamp in unix time. 1352480642

My version of python 2.7.10 
My version of dateutil 1.5

Here is my code that runs the test.
#!/usr/bin/python
import time
from dateutil import parser

def get_timestamp(time_string):
    timing = parser.parse(time_string)
    return time.mktime(timing.timetuple())

test_time1 = ""Fri Nov 9 09:04:02 2012 -0500""
test_time2 = ""Fri Nov 9 09:04:02 2012 -0800""
print get_timestamp(test_time1)
print get_timestamp(test_time2)

Output
1352480642.0
1352480642.0

Expected output
1352469842.0
1352480642.0

",161
38963698,38970460,2,"The refactored ""Version 2"" code in the question suffers from a concurrency / timing problem.
sse_request() is called for each of the web-clients  (in the test case 3 instances). We thus have 3 instances looping in event_stream().
These calls happen ""more or less"" in parallel: which actually means in random sequence.
However the list change_objects is shared, so the first web-client that spots a change will update the ""old"" copy in the shared WalkReporter instance to the latest state, and may do so before the other clients spot the change. i.e. the first successful web-client effectively hides the change from the other web-clients.
This is easily fixed, by giving each web-client its own copy of change_objects. 
i.e. change_objects is moved into sse_request() as shown below.
@app.route('/server_events')
def sse_request():
    change_objects = [
        WalkReporter(name=""walk1"", reportee=walks[0]),
        ... more objects to be tracked...
    ]
    return Response(
            event_stream(change_objects),
            mimetype='text/event-stream')

With this minor change, each instance of sse_request() can spot the changes, and thus all the web-clients receive the sse-events as expected.
",226
38963698,38963698,1,"I have a Flask web-server which generates server-sent-events (sse) which should be received by all connected web-clients.
In ""Version 1"" below this works. All web-clients receive the events, and update accordingly.
In ""Version 2"" below, which is a refactoring of Version 1, this no longer works as expected:
Instead I get:

mostly only one of the web-clients gets the event, or 
rarely
multiple web-clients get the event, or 
rarely none of the
web-clients get the event

As far as I can make out, the server is always generating the events, and normally at least one client is receiving.
My initial test hosted the web-server on a Raspberry Pi 3, with the web-clients on the Pi, on Windows and OSX using a variety of browsers.
To eliminate any possible network issues I repeated the same test with the web-server and 3 instances of Chrome all hosted on the same OSX laptop.
This gave the same results: Version 1 ""OK"", Version 2 ""NOT OK"".
The client that successfully receives seemingly varies randomly from event to event: so far I can't discern a pattern.
Both Version 1 and Version 2 have a structure change_objects containing ""things that should be tracked for changes""

In Version 1 change_objects is a dict of dicts.
In Version 2 I refactored change_objects to be a list of instances of the class Reporter, or sub classes of Reporter.

The changes to the ""things"" are triggered based on web-services received elsewhere in the code.
Version 1 (OK: sse events received by all web-clients)
def check_walk(walk_new, walk_old):
    if walk_new != walk_old:
        print(""walk change"", walk_old, walk_new)
        return True, walk_new
    else:
        return False, walk_old

def walk_event(walk):
    silliness = walk['silliness']
    data = '{{""type"": ""walk_change"", ""silliness"": {}}}'.format(silliness)
    return ""data: {}\n\n"".format(data)

change_objects = {
    ""walk1"": {
        ""object"": walks[0],
        ""checker"": check_walk,
        ""event"": walk_event,
    },
    ... more things to be tracked...
}

def event_stream(change_objects):
    copies = {}
    for key, value in change_objects.items():
        copies[key] = {""obj_old"": deepcopy(value[""object""])}  # ensure a true copy, not a reference!

    while True:
        gevent.sleep(0.5)
        for key, value in change_objects.items():
            obj_new = deepcopy(value[""object""]) # use same version in check and yield functions
            obj_changed, copies[key][""obj_old""] = value[""checker""](obj_new, copies[key][""obj_old""])
            if (obj_changed):
                yield value[""event""](obj_new)

@app.route('/server_events')
def sse_request():
    return Response(
            event_stream(change_objects),
            mimetype='text/event-stream')

Version 2 (NOT OK: sse events NOT always received by all web-clients)
class Reporter:

    def __init__(self, reportee, name):
        self._setup(reportee, name)

    def _setup(self, reportee, name):
        self.old = self.truecopy(reportee)
        self.new = reportee
        self.name = ""{}_change"".format(name)

    def truecopy(self, orig):
        return deepcopy(orig)

    def changed(self):
        if self.new != self.old:
            self.old = self.truecopy(self.new)
            return True
        else:
            return False

    def sse_event(self):
        data = self.new.copy()
        data['type'] = self.name
        data = json.dumps(data)
        return ""data: {}\n\n"".format(data)

class WalkReporter(Reporter):

    # as we are only interested in changes to attribute ""silliness"" (not other attributes) --> override superclass sse_event
    def sse_event(self): 
        silliness = self.new['silliness']
        data = '{{""type"": ""walk_change"", ""silliness"": {}}}'.format(silliness)
        return ""data: {}\n\n"".format(data)

change_objects = [
    WalkReporter(name=""walk1"", reportee=walks[0]),
    ... more objects to be tracked...
] 

def event_stream(change_objects):
    while True:
        gevent.sleep(0.5)
        for obj in change_objects:
            if obj.changed():
                yield obj.sse_event()

@app.route('/server_events')
def sse_request():
    return Response(
            event_stream(change_objects),
            mimetype='text/event-stream')

Full disclosure: This question is a follow on to the question: Refactor a (multi)generator python function
which focussed on refactoring the event_stream() function when tracking changes to multiple ""things"".
However the problem here is clearly outside the scope of the original question, hence a new one.
",921
38963711,38963782,2,"Okay, i've used the API docs and found the problem.
The parameter you need to use to order the data is: ""order=asc|desc"", and not ""sort_order"" as previously thought.
Please use this function:
def download_contract_from_quandl(contract, dl_dir):
""""""
Download an individual futures contract from Quandl and then
store it to disk in the 'dl_dir' directory. An auth_token is
required, which is obtained from the Quandl upon sign-up.
""""""
# Construct the API call from the contract and auth_token
api_call = ""https://www.quandl.com/api/v3/datasets/""
api_call += ""CME/%s.csv"" % contract
# If you wish to add an auth token for more downloads, simply
# comment the following line and replace MY_AUTH_TOKEN with
# your auth token in the line below
params = ""?auth_token=YOUR_TOKEN""
params += ""&order=asc""
full_url = ""%s%s"" % (api_call, params)

# Download the data from Quandl
data = requests.get(full_url).text
# Store the data to disk
fc = open('%s/%s.csv' % (dl_dir, contract), 'w')
fc.write(data)

Note:
The way you are using the api, by simple http request, altough works, is the not the ideal way to use their API.
There is a python package called Quandl, you can install like so:
pip3 install quandl

On your system.
Also then you would have a single (and not multiple using auth_token=YOUR_TOKEN in each request) auth call like so:
quandl.ApiConfig.api_key = 'YOUR_TOEKN'

And then each api call will be simple and elegent using their package instead or creating an http request manually, like so:
data = quandl.get(""CME/ESH2010.csv"", order=""asc"")

I will advise using the second method of using the API, but both will work perfectly.
Cheers, Or.
",349
38963711,38963711,1,"I have the following code:
#!/usr/bin/python
# -*- coding: utf-8 -*-

# quandl_data.py

from __future__ import print_function

import matplotlib
import matplotlib.pyplot as plt
import pandas as pd
import requests


def construct_futures_symbols(
        symbol, start_year=2010, end_year=2016
):
    """"""
    Constructs a list of futures contract codes
    for a particular symbol and timeframe.
    """"""
    futures = []
    # March, June, September and
    # December delivery codes
    months = 'HMUZ'
    for y in range(start_year, end_year+1):
        for m in months:
            futures.append(""%s%s%s"" % (symbol, m, y))
    return futures


def download_contract_from_quandl(contract, dl_dir):
    """"""
    Download an individual futures contract from Quandl and then
    store it to disk in the 'dl_dir' directory. An auth_token is
    required, which is obtained from the Quandl upon sign-up.
    """"""
    # Construct the API call from the contract and auth_token
    api_call = ""https://www.quandl.com/api/v3/datasets/""
    api_call += ""CME/%s.csv"" % contract
    # If you wish to add an auth token for more downloads, simply
    # comment the following line and replace MY_AUTH_TOKEN with
    # your auth token in the line below
    params = ""?sort_order=asc""
    params = ""?auth_token=myTokenHere&sort_order=asc""
    full_url = ""%s%s"" % (api_call, params)

    # Download the data from Quandl
    data = requests.get(full_url).text
    # Store the data to disk
    fc = open('%s/%s.csv' % (dl_dir, contract), 'w')
    fc.write(data)
    fc.close()


def download_historical_contracts(
        symbol, dl_dir, start_year=2010, end_year=2016
    ):
    """"""
    Downloads all futures contracts for a specified symbol
    between a start_year and an end_year.
    """"""
    contracts = construct_futures_symbols(
        symbol, start_year, end_year
    )
    for c in contracts:
        print(""Downloading contract: %s"" % c)
        download_contract_from_quandl(c, dl_dir)

if __name__ == ""__main__"":
    symbol = 'ES'

    # Make sure you've created this
    # relative directory beforehand
    dl_dir = 'quandl/futures/ES'

    # Create the start and end years
    start_year = 2010
    end_year = 2016

    # Download the contracts into the directory
    download_historical_contracts(
        symbol, dl_dir, start_year, end_year
    )

    # Open up a single contract via read_csv
    # and plot the settle price
    es = pd.io.parsers.read_csv(
        ""%s/ESH2010.csv"" % dl_dir, index_col=""Date""
    )
    es[""Settle""].plot()
    plt.show()

The code runs without error, however it is plotting in the wrong direction. Seems to be plotting from new to old dates. I would like to plot the oldest data first.

How do I achieve this? I thought changing the params = ""?sort_order=asc"" to params = ""?sort_order=desc"", which only changes the .csv file order not the plot.
Any ideas?
",527
38963734,38963734,1,"How to change the marker sizes in pandas.scatter_matrix() using python 3.5.2 and pandas 0.18.0? 
",17
38963734,38964654,2,"use the s parameter.
from pandas.tools.plotting import scatter_matrix

df = pd.DataFrame(np.random.rand(10, 2))
scatter_matrix(df, alpha=0.5, figsize=(8, 8), diagonal='kde', s=1000)


",38
38963751,38963870,2,"This is not about django but about html in general. This is your template:
<div class=""form-group"">
    <div class=""checkbox"">
        <label><input type=""checkbox"" name=""{{ form.primal.name }}"" value=""True"" id=""primal1"">Primal</label>
    </div>
</div>

Your checkbox, when unchecked, will not fly because it will not make a {{ form.primal.name }}=True in the url or post body.
To solve your problem, you should ensure a way to add {{ form.primal.name }}=False to the url. The standard solution involves a fixed additional field (a hidden one) like this:
<div class=""form-group"">
    <div class=""checkbox"">
        <input type=""hidden"" name=""{{ form.primal.name }}"" value=""False"" />
        <label><input type=""checkbox"" name=""{{ form.primal.name }}"" value=""True"" id=""primal1"">Primal</label>
    </div>
</div>

Which will generate a query string part like {{ form.primal.name }}=False if checkbox is unchecked, or {{ form.primal.name }}=False&{{ form.primal.name }}=True if checkbox is checked. In this case, only the latter occurrence counts, so you will have ""True"" when checked and ""False"" when unchecked.
",266
38963751,38963751,1,"I am developing a django application. In my forms. I have a checkbox in my form. My forms submits fine when the box is checked, but when the box is unchecked, the form fails to submit. The field I am using is Boolean.
Here is my code:
#models.py

class Ingredient(models.Model):
    user = models.ForeignKey('auth.User')
    recipe_id = models.ForeignKey(Recipe, on_delete=models.CASCADE)
    title = models.CharField(max_length=500)
    instructions = models.CharField(max_length=500)
    rules = models.TextField(max_length=500,blank=True)
    primal = models.CharField(default='False',max_length=500)
    def __str__(self):
        return self.title

#views.py

def create_ingredient(request):
    form = IngredientForm(current_user=request.user)
    if request.method == 'POST':
           form = IngredientForm(request.POST, current_user=request.user)
           if form.is_valid():
               current_user = request.user
               data = form.cleaned_data
               ingredient_data=Ingredient.objects.create(user=current_user, recipe_id=data['recipe_id'], title=data['title'], primal=data['primal'], instructions=data['instructions'], rules=data['rules'])
               ingredient_data.save()
               ingredient = Ingredient.objects.get(pk = ingredient_data.pk)
               return redirect('ingredient_detail', pk=ingredient.pk)
           else:
               messages.error(request, ""Error"")
    return render(request, 'create_ingredient.html', {'form': form })


#in my template
....
<div class=""form-group"">
<div class=""checkbox"">
      <label><input type=""checkbox"" name=""{{ form.primal.name }}"" value=""True"" id=""primal1"">Primal</label>
</div>
</div>
....

Does anyone have a solution?
",296
38963816,38963816,1,"I have tried to build exceptions for this problem like asked in the problem below. Unfortunately I can't make it work. I would greatly appreciate any input whatsoever. Thank you in advance.
Compute 2^x where x is the user input. x should be greater than or equal to 5 and less than or equal to 25. If the user input is not an integer then raise an exception. Create custom exceptions and raise if x is less than 5 and greater than 25. Then add the digits of 2x. For example if user inputs 6, then find 26 = 64, so the sum of the digits is 6 + 4 = 10.
import sys

i = int(raw_input(""Please provide a value for x (between 5 and 25): "" ))

try:

x = int(i)

except ValueError as v:
    print 'You did not enter a valid integer',v

except NotAValidValue as n:
    if x < 5 or x > 25:
        print 'Your entry is not valid. Please provide a number between 5 and 25',n
    sys.exit(0)

    exp = 2 ** x

print(exp)

Again, Thank you so much for giving this your time. 
",230
38963816,38963918,2,"Here's a working example, it's written to be executed on python 2.x:
import sys

try:
    x = int(raw_input(""Please provide a value for x (between 5 and 25): ""))

    if x < 5 or x > 25:
        print('Your entry is not valid {0}.' +
              'Please provide a number between 5 and 25'.format(x))
    else:
        exp = 2 ** x
        print(exp)
except ValueError as v:
    print('You did not enter a valid integer {0}'.format(v))

One advice though, try carefully to read & understand all the code and start
tweaking it here and there to make it yours. You won't learn too much using other's code 'as it is', next time try to be more specific asking which specific parts of your code don't understand :)
Have fun learning python!
",174
38963816,38964486,2,"The way you define custom exceptions in python is as shown below. You need to define each custom exception as a subclass of the Exception class. You can then catch your own custom exceptions with a catch-except block.
import sys

class TooSmallExc(Exception):
    def __init__(self):
        Exception.__init__(self,""The number is less than 5"") 

class TooLargeExc(Exception):
    def __init__(self):
        Exception.__init__(self,""The number is greater than 25"") 

print 'How are you?'
i = raw_input(""Please provide a value for x (between 5 and 25): "" )

try:
    x = int(i)
    if x<5:
    raise TooSmallExc
    if x>25:
    raise TooLargeExc
except ValueError:
    print 'I just caught a ValueError exception, which is a Python built-in exception'
except TooSmallExc:
    print 'I just caught a custom exception that I made for integers less than 5'
except TooLargeExc:
    print 'I just caught a custom exception that I made for integers greater than 25'

",195
38963822,38963822,1,"I've an html page, I'd like to display the search bar only when the passed in table object is NOT empty. But my check is not working properly. Here's the code:
<!-- We'll display the search bar only when the user has access to at least one item, otherwise, hide it. -->
{% if item_info %}
Number of entries: {{ item_info|length }}, nothing? {{item_info}}
<section>
    <form method=""post"" action=""."">
      {% csrf_token %}
      <input type=""text"" class=""search-query span80"" id=""search"" name=""search"" placeholder=""Enter ItemNo to search"">
      <button type=""submit"" class=""btn"">Search</button> 
    </form>
</section>
{% else %}
No item_info.
{% endif%}

Here's what I see on the browser:
 
item_info is blank, I think it should go to else branch, however, it entered if branch, any help is greatly appreciated!  
Edit after elethan's answer:
I've printed it out to debug, here's the screenshot:

So, looks like this item_info is really empty, I didn't see any item_info object gets printed out.
Also, to help debug, here's my view code:
def item_info(request):    
    iteminfo= ItemInfo.objects.all().filter(Q(some_query)
    table = ItemInfoTable(iteminfo)
    RequestConfig(request).configure(table)
    return render(request, 'item_info.html', {'item_info':table,})

And here's my table definition:
import django_tables2 as tables  
class ItemInfoTable(tables.Table):
    itmno = tables.Column(verbose_name=""Item #"")
    class Meta:
        model = ItemInfo
        empty_text = ""There is no item record."" 

And here's the ItemInfo table it refers to:
class ItemInfo(models.Model):
    itmno = models.CharField(primary_key=True, max_length=11L, db_column='ItmNo', blank=True)
    class Meta:
        db_table = 'item_info'

",381
38963822,38963940,2,"If item_info is a RawQuerySet, try {% if item_info.all %} instead of {% if item_info %}. RawQuerySet does not define a __bool__() method, so the instances are always considered True. See the warnings in this section of the docs, repeated below, just in case this link dies in the future:

While a RawQuerySet instance can be iterated over like a normal
  QuerySet, RawQuerySet doesn’t implement all methods you can use with
  QuerySet. For example, bool() and len() are not defined in
  RawQuerySet, and thus all RawQuerySet instances are considered True.
  The reason these methods are not implemented in RawQuerySet is that
  implementing them without internal caching would be a performance
  drawback and adding such caching would be backward incompatible.

",144
38963838,38963838,1,"In my python script I need to execute a command over SSH that also takes a heredoc as an argument. The command calls an interactive script that can be also called as follows:
dbscontrol << EOI
HELP
QUIT
EOI

I also found this Q&A that describes how to do it using subprocess but I really like pexpect.pxssh convenience.
Code example would be greatly appreciated 
",70
38963838,39381252,2,"I don't have pexpect handy to test my answer to your question, but I have a suggestion that should work and, if not, may at least get you closer.  
Consider this command:
$ ssh oak 'ftp << EOF
lpwd
quit
EOF'
Local directory: /home/jklowden
$ 

What is happening?  The entire quoted string is passed as a single argument to ssh, where it is ""executed"" on the remote.  While ssh isn't explicit about what that means, exactly, we know what execv(2) does: if execve(2) fails to execute its passed arguments, the execv function will invoke /bin/sh with the same arguments (in this case, our quoted string).  The shell then evaluates the quoted string as separate arguments, detects the HereDoc redirection, and executes per usual.  
Using that information, and taking a quick look at the pexpect.pxssh documentation, it looks like you want:
s = pxssh.pxssh()
...
s.sendline('ftp << EOF\nlpwd\nquit\nEOF')

If that doesn't work, something is munging your data.  Five minutes with strace(1) will tell you what happened to it, and you can start pointing fingers.  ;-)
HTH.  
",227
38963841,38963841,1,"I am trying to play around with Seaborn on Spyder (installed as part of Anaconda). 
import seaborn as sb

returns:
ImportError: No module named seaborn
This despite the Anaconda website listing seaborn as one of the default packages and the seaborn site saying that Anaconda is the easiest way to get the package. 
What am I doing wrong? 
",65
38963841,42792862,2,"For those using GUI (navigator), select Environments > root. On the right is a list of packages - installed, not installed, etc. 
Select ""not installed"", search for seaborn. If it appears, then click the row, and select Apply button at bottom of that page.
If it doesn't appear, then something else is wrong with your install.
Hope that helps.
",77
38963841,42807727,2,"Since version 4.3.0 dated 2017-01-31, Anaconda comes with seaborn installed by default. Try upgrading your Anaconda installation.
",20
38963841,45656342,2,"Just do conda install seaborn. If its installed it will updated it.
",14
38963841,40393774,2,"Usually Seaborn is imported as 'sns' so if you use that, that will be much easier for you initially (because all the code examples use that), and for others later when you share your code.
Have fun with Seaborn, it is an amazing package. 
",52
38963857,45093065,2,"My 2 cents: if you want to build&test your lambda function in the environment as similar to actual lambda as possible but still under your control, I would suggest using LambdaCI's Docker images. They are based on dumps of original lambda filesystem. Also they have build-specific variants (tags build-python2.7 and build-python3.6 are most interesting for us).
These images are not very small - more than 500mb - but they allow you to avoid any headache when building.
Important benefit over Amazon Linux is that all package versions etc are the same as on the real lambda.
Here is how I did building myself:
cd PROJECT_DIR
docker run --rm -it -v ""$PWD"":/var/task lambci/lambda:build-python2.7 bash
### now in docker
mkdir deps
pip install -t deps -r requirements.txt
# now all dependencies for our package are installed to deps/ directory,
# without any garbage like wheel or setuptools - unlike when using virtualenv
zip -r archive.zip MYCODE.py MYMODULE MYMODULE2.py
cd deps
# it's important to use . here, not * - or else some dot-starting directories will be omitted
zip -r ../archive.zip .
exit
### now locally
# just upload archive to lambda, with or without s3

For automating it with GitLab CI, just instruct it to use that same docker image
and put these commands in deploy script section:
deploy:
    stage: deploy
    image: lambci/lambda:build-python2.7
    script:
        - mkdir deps
        - pip install -t deps -r requirements.txt
        - zip -r archive.zip MYCODE.py MYMODULE MYMODULE2.py
        - cd deps && zip -r ../archive.zip . && cd ..
        - aws s3 cp archive.zip ${bucket}/${key}
        - aws lambda update-function-code --function-name ${func} --s3-bucket ${bucket} --s3-key ${key}
    variables:
        bucket: ...
        key: ...
        func: ...

",337
38963857,38966482,2,"The zip commands in that tutorial are missing a parameter. I ran into this exact problem today with pysftp, which is built on paramiko. libffi-72499c49.so.6.0.4 is in a hidden dot directory inside lib64/python2.7/site-packages/.libs_cffi_backend. Depending on how you zipped up the dependencies in your virtualenv, you may have inadvertantly excluded this directory.

First, make sure libffi-devel and openssl-devel are installed on your Amazon Linux instance, otherwise the cryptography module may not be compiling correctly.
sudo yum install libffi-devel openssl-devel


If those packages were not installed before, delete and rebuild your virtualenv.

Make sure that when you are zipping up your site-packages that you use '.' instead of '*', otherwise you will not be including files and directories that are hidden because their names begin with a period.
cd path/to/my/helloworld-env/lib/python2.7/site-packages
zip -r9 path/to/zip/worker_function.zip .
cd path/to/my/helloworld-env/lib64/python2.7/site-packages
zip -r9 path/to/zip/worker_function.zip .


",154
38963857,38963857,1,"So I'm trying to create an aws lambda function, to log in to an instance and do some stuff. And the script works fine outside of lambda, but when I package it using the same instructions as this https://aws.amazon.com/blogs/compute/scheduling-ssh-jobs-using-aws-lambda/ it doesn't work. It throws this error.
libffi-72499c49.so.6.0.4: cannot open shared object file: No such file or directory: ImportError
Traceback (most recent call last):
  File ""/var/task/lambda_function.py"", line 12, in lambda_handler
    key = paramiko.RSAKey.from_private_key(key)
  File ""/var/task/paramiko/pkey.py"", line 217, in from_private_key
    key = cls(file_obj=file_obj, password=password)
  File ""/var/task/paramiko/rsakey.py"", line 42, in __init__
    self._from_private_key(file_obj, password)
  File ""/var/task/paramiko/rsakey.py"", line 168, in _from_private_key
    self._decode_key(data)
  File ""/var/task/paramiko/rsakey.py"", line 173, in _decode_key
    data, password=None, backend=default_backend()
  File ""/var/task/cryptography/hazmat/backends/__init__.py"", line 35, in default_backend
    _default_backend = MultiBackend(_available_backends())
  File ""/var/task/cryptography/hazmat/backends/__init__.py"", line 22, in _available_backends
    ""cryptography.backends""
  File ""/var/task/pkg_resources/__init__.py"", line 2236, in resolve
    module = __import__(self.module_name, fromlist=['__name__'], level=0)
  File ""/var/task/cryptography/hazmat/backends/openssl/__init__.py"", line 7, in <module>
    from cryptography.hazmat.backends.openssl.backend import backend
  File ""/var/task/cryptography/hazmat/backends/openssl/backend.py"", line 15, in <module>
    from cryptography import utils, x509
  File ""/var/task/cryptography/x509/__init__.py"", line 7, in <module>
    from cryptography.x509.base import (
  File ""/var/task/cryptography/x509/base.py"", line 15, in <module>
    from cryptography.x509.extensions import Extension, ExtensionType
  File ""/var/task/cryptography/x509/extensions.py"", line 19, in <module>
    from cryptography.hazmat.primitives import constant_time, serialization
  File ""/var/task/cryptography/hazmat/primitives/constant_time.py"", line 9, in <module>
    from cryptography.hazmat.bindings._constant_time import lib
ImportError: libffi-72499c49.so.6.0.4: cannot open shared object file: No such file or directory

",333
38963882,38964596,2,"Recursive groupby and apply
def append_tot(df):
    if hasattr(df, 'name') and df.name is not None:
        xs = df.xs(df.name)
    else:
        xs = df
    gb = xs.groupby(level=0)
    n = xs.index.nlevels
    name = tuple('Total' if i == 0 else '' for i in range(n))
    tot = gb.sum().sum().rename(name).to_frame().T
    if n > 1:
        sm = gb.apply(append_tot3)
    else:
        sm = gb.sum()
    return pd.concat([sm, tot])

fields = ['project__name', 'person__username',
          'activity__name', 'issue__subject']
append_tot(df.set_index(fields))


",129
38963882,38963882,1,"I'm trying to achieve a table with subtotals as shown here, but either that code doesn't work with the latest pandas version (0.18.1) or the example is wrong for multiple columns instead of one. My code here results in the following table
                                                                   2014    2015    2016
project__name person__username activity__name    issue__subject                        
Influenster   employee1        Development                        161.0   122.0   104.0
                                                 Fix bug           22.0     0.0     0.0
                                                 Refactor view      0.0     7.0     0.0
                               Quality assurance                  172.0   158.0   161.0
              employee2        Development                        119.0   137.0   155.0
                               Quality assurance                  193.0   186.0   205.0
              employee3        Development       Refactor view      0.0     0.0     1.0
Profit tools  employee1        Development                        177.0   136.0   216.0
                               Quality assurance                  162.0   122.0   182.0
              employee2        Development                        154.0   168.0   124.0
                               Quality assurance                  130.0   183.0   192.0
                                                 Fix bug           22.0     0.0     0.0
All                                                              1312.0  1219.0  1340.0

and my desired output would be something like:
                                                                   2014    2015    2016
project__name person__username activity__name    issue__subject                        
Influenster   employee1        Development                        161.0   122.0   104.0
                                                 Fix bug           22.0     0.0     0.0
                                                 Refactor view      0.0     7.0     0.0
                                                 Total              xxx     xxx     xxx
                               Quality assurance                  172.0   158.0   161.0
                                                 Total              xxx     xxx     xxx
                               Total                                xxx     xxx     xxx
              employee2        Development                        119.0   137.0   155.0
                                                 Total              xxx     xxx     xxx
                               Quality assurance                  193.0   186.0   205.0
                                                 Total              xxx     xxx     xxx
                               Total                                xxx     xxx     xxx
              employee3        Development       Refactor view      0.0     0.0     1.0
                                                 Total              xxx     xxx     xxx
                               Total                                xxx     xxx     xxx
              Total                                                 xxx     xxx     xxx
Profit tools  employee1        Development                        177.0   136.0   216.0
                                                 Total              xxx     xxx     xxx
                               Quality assurance                  162.0   122.0   182.0
                                                 Total              xxx     xxx     xxx
                               Total                                xxx     xxx     xxx
              employee2        Development                        154.0   168.0   124.0
                                                 Total              xxx     xxx     xxx
                               Quality assurance                  130.0   183.0   192.0
                                                 Fix bug           22.0     0.0     0.0
                                                 Total              xxx     xxx     xxx
                               Total                                xxx     xxx     xxx
              Total                                                 xxx     xxx     xxx
All                                                              1312.0  1219.0  1340.0

Any help on how to achieve this is appreciated.
",283
38963882,38965198,2,"Consider running three level pivot_tables with stack and concatenate them for a final groupby object. As mentioned, the docs does work if you see the use of .stack() on the corresponding pivot_table columns value:
# ISSUE_SUBJECT PIVOT
pt1 = pd.pivot_table(data=df, values=['2014', '2015', '2016'], 
                     columns=['issue__subject'], aggfunc=np.sum, 
                     index=['project__name', 'person__username', 'activity__name'], 
                     margins=True, margins_name = 'Total')    
pt1 = pt1.stack().reset_index()

# ACTIVITY_NAME PIVOT
pt2 = pd.pivot_table(data=df, values=['2014', '2015', '2016'], 
                     columns=['activity__name'], aggfunc=np.sum, 
                     index=['project__name', 'person__username'], 
                     margins=True, margins_name = 'Total' )    
pt2 = pt2.stack().reset_index()

# PERSON_USERNAME PIVOT
pt3 = pd.pivot_table(data=df, values=['2014', '2015', '2016'],        
                     columns=['person__username'], 
                     aggfunc=np.sum, index=['project__name'],
                     margins=True, margins_name = 'Total')    
pt3 = pt3.stack().reset_index()

# CONCATENATE ALL THREE
gdf = pd.concat([pt1, 
                 pt2[(pt2['project__name']=='Total') | 
                     (pt2['activity__name']=='Total')],
                 pt3[(pt3['project__name']=='Total') |
                     (pt3['person__username']=='Total')]]).reset_index(drop=True)

# REPLACE NaNS IN COLUMN
gdf = gdf.apply(lambda x: np.where(pd.isnull(x), '', x), axis=1)

# FINAL GROUPBY (A COUNT USED TO RENDER GROUPBY)
gdf = gdf.groupby(['project__name', 'person__username',
                   'activity__name', 'issue__subject',
                   '2014', '2015', '2016']).agg(len)

Output
project__name  person__username  activity__name     issue__subject  2014    2015    2016  
Influenster    Total                                                667.0   610.0   626.0     1
               employee1         Development                        161.0   122.0   104.0     1
                                                    Fix bug         22.0    0.0     0.0       1
                                                    Refactor view   0.0     7.0     0.0       1
                                                    Total           183.0   129.0   104.0     1
                                 Quality assurance                  172.0   158.0   161.0     1
                                                    Total           172.0   158.0   161.0     1
                                 Total                              355.0   287.0   265.0     1
               employee2         Development                        119.0   137.0   155.0     1
                                                    Total           119.0   137.0   155.0     1
                                 Quality assurance                  193.0   186.0   205.0     1
                                                    Total           193.0   186.0   205.0     1
                                 Total                              312.0   323.0   360.0     1
               employee3         Development        Refactor view   0.0     0.0     1.0       1
                                                    Total           0.0     0.0     1.0       1
                                 Total                              0.0     0.0     1.0       1
Profit tools   Total                                                645.0   609.0   714.0     1
               employee1         Development                        177.0   136.0   216.0     1
                                                    Total           177.0   136.0   216.0     1
                                 Quality assurance                  162.0   122.0   182.0     1
                                                    Total           162.0   122.0   182.0     1
                                 Total                              339.0   258.0   398.0     1
               employee2         Development                        154.0   168.0   124.0     1
                                                    Total           154.0   168.0   124.0     1
                                 Quality assurance                  130.0   183.0   192.0     1
                                                    Fix bug         22.0    0.0     0.0       1
                                                    Total           152.0   183.0   192.0     1
                                 Total                              306.0   351.0   316.0     1
Total                                                               1268.0  1212.0  1339.0    1
                                                    Fix bug         44.0    0.0     0.0       1
                                                    Refactor view   0.0     7.0     1.0       1
                                                    Total           1312.0  1219.0  1340.0    1
                                 Development                        633.0   570.0   600.0     1
                                 Quality assurance                  679.0   649.0   740.0     1
                                 Total                              1312.0  1219.0  1340.0    1
               Total                                                1312.0  1219.0  1340.0    1
               employee1                                            694.0   545.0   663.0     1
               employee2                                            618.0   674.0   676.0     1
               employee3                                            0.0     0.0     1.0       1

",551
38964528,38964528,1,"Strange error, and I don't understand where is the mistake. The traceback shows nothing relevant:
  File ""/home/popovvasile/work/intiativa_new/local/lib/python2.7/site-packages/django/core/checks/urls.py"", line 27, in check_resolver
    warnings.extend(check_pattern_startswith_slash(pattern))
  File ""/home/popovvasile/work/intiativa_new/local/lib/python2.7/site-packages/django/core/checks/urls.py"", line 63, in check_pattern_startswith_slash
    regex_pattern = pattern.regex.pattern

It seems like something is wrong in the urls.py file:
urlpatterns = [
    # Examples:
    url(r'^$', 'newsletter.views.home', name='home'),
    url(r'^contact/$', 'newsletter.views.contact', name='contact'),
    url(r'^about/$', About.as_view(), name='about'),
    # url(r'^blog/', include('blog.urls')),

    url(r'^noway/', include(admin.site.urls)),



    url(r'^petitions/', include('newsletter.petitions_urls', namespace=""petitions"")),
    url(r'^laws/', include('newsletter.laws_urls', namespace=""laws"")),


    url(r'^accounts/register/$', RegistrationView.register, {'backend': 'registration.backends.default.DefaultBackend','form_class': UserRegForm}, name='registration_register'),(r'^accounts/', include('registration.urls')),
    url(r'^news/', include('newsletter.news_urls', namespace=""news"")),
    url(r'^petition-thanks/$', PetitionThanksView.as_view(), name='thanks_petitions'),
    url(r'^addpetitions/$', create_new_petition, name='add_petitions'),
    url(r'^comments/', include('fluent_comments.urls')),

    # url(r'^comments/posted/$', 'newsletter.views.comment_posted' )
]

",266
38964528,38964560,2,"There's a trailing tuple lurking somewhere between those lines; the url(r'^accounts/register/$'...) line:
(r'^accounts/', include('registration.urls'))

You intend to have that as a url pattern not a tuple:
url(r'^accounts/', include('registration.urls')),

",56
38964578,38971474,2,"Yes. Why didn't you simply try? ;)
Fixtures are put in conftest.py files to be able to use them in multiple test files.
",28
38964578,38964578,1,"I am new to Python and I have a doubt in pytest
test_client.py 
# Simple Example tests

import pytest

def test_one():
   assert False == False

def test_two():
   assert True == True

def cleanup():
   # do some cleanup stuff

conftest.py
import pytest
import test_client

@pytest.fixture(scope=""session"", autouse=True)
def do_clean_up(request):
    request.addfinalizer(test_client.cleanup)

Is it possible to move the fixture defined in conftest.py to the test_client.py thereby eliminating having conftest.py
",89
38967478,38967478,1,"I am trying to plot the values of several arrays in separate plots of a figure using imshow.
When I plot one image only, and use the plt.imshow() command with the correct extents, the figure comes out perfectly.
However, when I try to create multiple plots of this image in the same figure, using plt.subplot(), each of these plots ends up with incorrect x-axis settings, and there are white margins. I tried correcting the x-axis range with the set_xlim() command, but it has no effect (which I also don't understand). 
The minimal working sample is below - any help would be appreciated!
from matplotlib import pyplot as plt
import numpy as n

image = n.array([[ 1.,  2.,  2.,  5.],
   [ 1.,  0.,  0.,  3.],
   [ 1.,  2.,  0.,  2.],
   [ 4.,  2.,  3.,  2.]])
xextent, yextent= n.shape(image)

fig, ax = plt.subplots(2,sharex=True, sharey=True)
im0 = ax[0].imshow(image, extent=(0,xextent,yextent,0),interpolation='nearest');
ax[0].set_xlim([0,4])
im1 = ax[1].imshow(image, extent=(0,xextent,yextent,0),interpolation='nearest');
ax[1].set_xlim([0,4])

plt.show()


",271
38967478,38967970,2,"I believe the reason for the whitespace is the size of the window. You can either change the window size (you'd have to figure out the numbers) or you can adjust the subplot. I found this out by playing with the ""configure subplots"" button in the image popup.
plt.subplots_adjust(right=0.4)

With this line the plot will have no whitespace, but still some empty space (which you can fix by adjusting the window size).
",87
38967478,39013209,2,"So the options are:

Remove the sharex/sharey keywords - seems to clash with imshow in the subplot environment. (Suggested by xnx)
Use plt.subplots_adjust with appropriate settings, in combination with plt.gcf().tight_layout() (Suggested by mwormser)
Use pcolormesh instead of imshow in the subplot environment.

",55
38967533,38967533,1,"Just looking for a simple api return, where I can input a ticker symbol and receive the full company name:
ticker('MSFT')
will return
""Microsoft""
",32
38967533,38968465,2,"You need to first find a website / API which allows you to lookup stock symbols and provide information. Then you can query that API for information. 
I came up with a quick and dirty solution here: 
import requests


def get_symbol(symbol):
    symbol_list = requests.get(""http://chstocksearch.herokuapp.com/api/{}"".format(symbol)).json()

    for x in symbol_list:
        if x['symbol'] == symbol:
            return x['company']


company = get_symbol(""MSFT"")

print(company)

This website only provides company name. I didn't put any error checks. And you need the requests module for it to work. Please install it using pip install requests. 
Update: Here's the code sample using Yahoo! Finance API: 
import requests


def get_symbol(symbol):
    url = ""http://d.yimg.com/autoc.finance.yahoo.com/autoc?query={}&region=1&lang=en"".format(symbol)

    result = requests.get(url).json()

    for x in result['ResultSet']['Result']:
        if x['symbol'] == symbol:
            return x['name']


company = get_symbol(""MSFT"")

print(company)

",222
38967533,40244249,2,"Here's another Yahoo API call. @masnun's call will return all results that contain the search param, for example trying AMD (Advanced Micro Devices):
http://d.yimg.com/autoc.finance.yahoo.com/autoc?query=amd&region=1&lang=en
gives you AMD (Advanced Micro Devices, Inc.), AMDA (Amedica Corporation), DOX (Amdocs Limited), etc.
If you know the ticker, you can try either of these Yahoo APIs:z
http://finance.yahoo.com/d/quotes.csv?s=amd&f=nb4t8 (well documented, this particular call asks for n=name; b4=book value; t8=1yr target price).
https://query2.finance.yahoo.com/v7/finance/options/amd (not very well documented but new...see more info here about this API: https://stackoverflow.com/a/40243903/933972)
Forgot to include the Google API, which seems ok for stock quotes, but not reliable for full data on option chains:
'https://www.google.com/finance?q=nyse:amd&output=json'
",164
38967581,38968045,2,"Just add [0].text, hope this help !
horoscope = soup.findAll(""div"", {""class"": ""block-horoscope-text f16 l20""}, text=True)[0].text
print(horoscope)

",41
38967581,38967581,1,"I have this code:
import requests
from bs4 import BeautifulSoup

url = ""https://www.horoscope.com/us/horoscopes/general/horoscope-general-daily-today.aspx?sign=1""
page = requests.get(url)


soup = BeautifulSoup(page.text, ""html.parser"")

horoscope = soup.findAll(""div"", {""class"": ""block-horoscope-text f16 l20""}, text=True)

But the returned result includes the  tag as well. 
<div class=""block-horoscope-text f16 l20"">
            It could be scary for you to do anything risky for fear of conflict or failure, Aries. Perhaps you've tried to become invisible in different situations so you can avoid being noticed. These defense mechanisms may serve you for a while, but acting out of fear or guilt won't get you where you need to go. To achieve what you want, you must act with confidence, love, and faith.
        </div>

How do I remove them? Thanks for your help.
",171
38967599,38967621,2,"SOLUTION 1
Just try to concatenate your queryset using | 
final_q = q1 | q2

In your example
final_q = summary | awards_used

UPDATED:
| does not works using calculated attributes, so, we can select our queryset first and then mapping our extra attributes 
summary = Award.objects.filter(awardreceived__date__lte=query_date) 
awards_used = Award.objects.filter(awardused__date__lte=query_date)
final_q = summary | awards_used

final_q = final_q.annotate(used=Sum('awardused__date__lte__units')).annotate(awarded=Sum('awardissuedactivity__units_awarded'))

SOLUTION 2 
Using chain built-in function 
from itertools import chain
final_list = list(chain(summary, awards_used))

There is an issue with this approach, you won't get a queryset, you will get a list containing instances. 
",126
38967599,38967599,1,"Suppose I have the following models
class Award(models.Model):
    user = models.ForeignKey(User)


class AwardReceived(models.Model):
    award = models.ForeignKey(award)
    date = models.DateField()
    units = models.IntegerField()


class AwardUsed(models.Model):
    award = models.ForeignKey(award)
    date = models.DateField()
    units = models.IntegerField()  

Now, suppose I want to get the number of awards for all users and the number of awards used for all users (ie, a queryset containing both). I prefer to do it one query for each calculation - when I combined it in my code I had some unexpected results. Also for some of my queries it won't be possible to do it one query, since the query will get too complex - I'm calculating 8 fields. This is how I solved it so far:
def get_summary(query_date)
    summary = (Award.objects.filter(awardreceived__date__lte=query_date))
                    .annotate(awarded=Sum('awardissuedactivity__units_awarded')))

    awards_used = (Award.objects.filter(awardused__date__lte=query_date)
                        .annotate(used=Sum('awardused__date__lte__units')))

    award_used_dict = {}
    for award in awards_used:
        award_used_dict[award] = award.used

    for award in summary:
        award.used = award_used_dict.get(award, 0)

    return summary

I'm sure there must be a way to solve this without the dictionary approach? For instance, something like this: awards_used.get(award=award), but this causes a db lookup every loop.
Or some other fancy way to join the querysets?
Note this is a simplified example and I know for this example the DB structure can be improved, I'm just trying to illustrate my question.
",301
38967666,38984224,2,"If I make all fields varchar(255) then it reads missing fiels as ''. sqlalchemy cannot force a '' from the csv file into another datatype.
Best is to use varchar to purely reads the csv file and then afterwards convert it to the proper formats
",51
38967666,38967666,1,"i try to load data from a csv file into a mysql table using odo in python.
the csv file contains blank cells. The odo command files when it encounters blank cells.
how can  I use the odo command to load the data and insert a null value by default for missing data.
I'm trying to import a simple CSV file that I downloaded from Quandl into a MySQL table with the odo python package
t = odo(csvpathName)

The rsow look like this in the CSV. The second line has a value missing.
A   7/25/2016   46.49   46.52   45.92   46.14   1719772 0   1   46.49   46.52   45.92   46.14   1719772
B   7/25/2016   46.49   46.52   45.92           1719772 0   1   46.49   46.52   45.92   46.14   1719772

The MySQL table is defined as follows:
Ticker varchar(255) NOT NULL,
Date varchar(255) NOT NULL,
Open numeric(15,2) NULL,
High numeric(15,2) NULL,
Low numeric(15,2) NULL,
Close numeric(15,2) NULL,
Volume bigint NULL,
ExDividend numeric(15,2), 
SplitRatio int NULL,
OpenAdj numeric(15,2) NULL,
HighAdj numeric(15,2) NULL,
LowAdj numeric(15,2) NULL,
CloseAdj numeric(15,2) NULL,
VolumeAdj bigint NULL,
PRIMARY KEY(Ticker,Date)

It throws an exception 1366 with the following info:
sqlalchemy.exc.InternalError: (pymysql.err.InternalError) (1366, ""Incorrect decimal value: '' for column 'High' at row 185"") [SQL: 'LOAD DATA  INFILE %(path)s\n            INTO TABLE QUANDL_DATA_WIKI\n            CHARACTER SET %(encoding)s\n            FIELDS\n                TERMINATED BY %(delimiter)s\n                ENCLOSED BY %(quotechar)s\n                ESCAPED BY %(escapechar)s\n            LINES TERMINATED BY %(lineterminator)s\n            IGNORE %(skiprows)s LINES\n            '] [parameters: {'quotechar': '""', 'encoding': 'utf8', 'path': 'C:\ProgramData\MySQL\MySQL Server 5.6\Uploads\WIKI_20160725.partial.csv', 'lineterminator': '\n', 'escapechar': '\', 'skiprows': 0, 'delimiter': ','}]
Does anyone know how to configure ODO so I can upload missing values as NULL values with the simple command?
",401
38967678,38967678,1,"I am writing a piece of code to recursively processing *.py files. The code block is as the following:
class FileProcessor(object):

    def convert(self,file_path):
        if os.path.isdir(file_path):
            """""" If the path is a directory,then process it recursively 
                untill a file is met""""""
            dir_list=os.listdir(file_path)
            print(""Now Processing Directory:"",file_path)

            i=1

            for temp_dir in dir_list:
                print(i,"":"",temp_dir)
                i=i+1
                self.convert(temp_dir)
        else:
            """""" if the path is not a directory""""""
            """""" TODO something meaningful """"""

if __name__ == '__main__':
    tempObj=FileProcessor()
    tempObj.convert(sys.argv[1])

When I run the script with a directory path as argument, it only runs the first layer of the directory, the line:
self.convert(temp_dir)

seems never get called. I'm using Python 3.5.
",179
38967678,38968143,2,"As temp_dir has the filename only without parent path, you should change
self.convert(temp_dir)

to
self.convert(os.path.join(file_path, temp_dir))

",27
38967678,38967808,2,"The recursion is happening fine, but temp_dir is not a directory so it passes control to your stub else block. You can see this if you put print(file_path) outside your if block.
temp_dir is the name of the next directory, not its absolute path. ""C:/users/adsmith/tmp/folder"" becomes just ""folder"". Use os.path.abspath to get that
self.convert(os.path.abspath(temp_dir))

Although the canonical way to do this (as mentioned in my comment on the question) is to use os.walk.
class FileProcessor(object):
    def convert(self, file_path):
        for root, dirs, files in os.walk(file_path):
            # if file_path is C:/users/adsmith, then:
            #   root == C:/users/adsmith
            #   dirs is an iterator of each directory in C:/users/adsmith
            #   files is an iterator of each file in C:/users/adsmith
            # this walks on its own, so your next iteration will be
            # the next deeper directory in `dirs`

            for i, d in enumerate(dirs):
                # this is also preferred to setting a counter var and incrementing
                print(i, "":"", d)
                # no need to recurse here since os.walk does that itself
            for fname in files:
                # do something with the files? I guess?

",241
38967706,38967706,1,"I'm trying to write a python program that finds and updates a document in mongodb:
db.collection.find_one_and_update({""Machine"": ""24"", ""Available"": True},
                                  {""$set"": {""Overview.Available"": False}},
                                  projection= {""_id"": 0, ""Machine"": 1, ""Available"": 1},
                                  return_document= ReturnDocument.AFTER)

But I'm getting the following error message (apparently I'm not using return_document correctly but all pymongo documentation says I am)

NameError: global name 'ReturnDocument' is not defined

",107
38967706,38967956,2,"You need to import the ReturnDocument class first. Add this to the top of your script: 
from pymongo.collection import ReturnDocument

Detailed docs: http://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.ReturnDocument 
",30
38967729,38967729,1,"I was wondering if there's a library that I can use to create bar codes in Django using just HTML + CSS rather than producing the bar codes as images (like in reportlab & pybarcode). I'm reluctant to use images because I'm creating many bar codes on the same page and I feel images could be a little slow. 
P.S This technique has been used by  dinesh/barcode as a laravel library in php 
",80
38967729,38970441,2,"I have decided to use jquery-barcode which is completely works at client end 
",13
38967833,38967833,1,"I’m trying to use Python’s requests library to automatically get my grades from a university website. The URL is https://acorn.utoronto.ca/sws/transcript/academic/main.do?main.dispatch, but there are several redirects. I have the following simple code but it doesn't seem to be doing what I want.
import requests

payload = {""user"" : ""username"", ""pass"" : ""password""}
r = requests.post(""https://acorn.utoronto.ca/sws/transcript/academic/main.do?main.dispatch"", data= payload)
print(r.text)

The output is as follows:
C:\Users\johnp\AppData\Local\Programs\Python\Python35-32\python.exe 

C:/Users/johnp/Desktop/git_stuff/16AugRequests/acorn_requests.py <html> <head> </head> <body onLoad=""document.relay.submit()""> <form method=post action=""https://weblogin.utoronto.ca/"" name=relay> <input type=hidden name=pubcookie_g_req value=""b25lPWlkcC51dG9yYXV0aC51dG9yb250by5jYSZ0d289Q0lNRl9TaGliYm9sZXRoX1BpbG90JnRocmVlPTEmZm91cj1hNWEmZml2ZT1HRVQmc2l4PWlkcC51dG9yYXV0aC51dG9yb250by5jYSZzZXZlbj1MMmxrY0M5QmRYUm9iaTlTWlcxdmRHVlZjMlZ5Um05eVkyVkJkWFJvJmVpZ2h0PSZob3N0bmFtZT1pZHAudXRvcmF1dGgudXRvcm9udG8uY2EmbmluZT0xJmZpbGU9JnJlZmVyZXI9KG51bGwpJnNlc3NfcmU9NSZwcmVfc2Vzc190b2s9LTczODQ3MDk2OCZmbGFnPTA="">

   You do not have Javascript turned on,   please click the button to continue.     
Am I going about this the right way? I feel like I should be trying to pass a cookie instead, but how would I get the cookie?
Thanks in advance. 
Edit: this is the stuff I get from Firefox:
Network tab
Does this mean I need to fill out the entire form as parameters in the request?
",223
38967833,38971901,2,"You can try logging in then getting whatever page you want, there is more more data to be posted which you can get with bs4:
import requests
from bs4 import BeautifulSoup
url = ""https://weblogin.utoronto.ca/""
with requests.Session() as s:
    s.headers.update({""User-Agent"":""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36""})
    soup = BeautifulSoup(s.get(url).content)
    data = {inp[""name""]: inp[""value""] for inp in soup.select(""#query input[value]"")}
    data[""user""] = ""username""
    data[""pass""] = ""password""
    post = s.post(url, data=data)
    print post
    print(post.content)
    protect = s.get(""protected_page"")

If we run the code and just print the data dict, you can see bs4 populates the required fields:
In [14]: with requests.Session() as s:
   ....:         s.headers.update({""User-Agent"":""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36""})
   ....:         soup = BeautifulSoup(s.get(url).content,""html.parser"")
   ....:         data = {inp[""name""]:inp[""value""] for inp in soup.select(""#query input[value]"")}
   ....:         data[""user""] = ""username""
   ....:         data[""pass""] = ""password""
   ....:         print(data)
   ....:     
{'seven': '/index.cgi', 'sess_re': '0', 'pre_sess_tok': '0', 'pass': 'password', 'four': 'a5', 'user': 'username', 'reply': '1', 'two': 'pinit', 'hostname': '', 'three': '1', 'pinit': '1', 'relay_url': '', 'nine': 'PInit', 'create_ts': '1471341718', 'referer': '', 'six': 'weblogin.utoronto.ca', 'first_kiss': '1471341718-777129', 'flag': '', 'five': '', 'post_stuff': '', 'creds_from_greq': '1', 'fr': '', 'eight': '', 'one': 'weblogin.utoronto.ca', 'file': ''}

",449
38967979,38968056,2,"You need to serialize the data to a common format that is accessible from both C# and Python. For example - XML or JSON. I would recommend using JSON. 
Then you have several options: 

Use sockets to transfer the data. 
Use http to transfer the data. 
Write to a file from C# and read that file from Python

Sockets would probably the faster. Using http might be easier. With files, you will need to have some sort of scheduling or notification system to let your Python program know when you have written to the file.  
",107
38967979,38967979,1,"How can i send this list to list in Python script? This is so big for sending as arguments. Thank you. 
 List<String> cefList= new List<String>();
               for(int i=0; i<1000; i++){
                cefList.Add(""CEF:0|ArcSight|ArcSight|6.0.3.6664.0|agent:030|Agent [test] type [testalertng] started|Low| 
    eventId=1 mrt=1396328238973 categorySignificance=/Normal categoryBehavior=/Execute/Start 
    categoryDeviceGroup=/Application catdt=Security Mangement categoryOutcome=/Success 
    categoryObject=/Host/Application/Service art=1396328241038 cat=/Agent/Started 
    deviceSeverity=Warning rt=1396328238937 fileType=Agent 
    cs2=<Resource ID\=""3DxKlG0UBABCAA0cXXAZIwA\=\=""/> c6a4=fe80:0:0:0:495d:cc3c:db1a:de71 
    cs2Label=Configuration Resource c6a4Label=Agent 
    IPv6 Address ahost=SKEELES10 agt=888.99.100.1 agentZoneURI=/All Zones/ArcSight 
    System/Private Address Space 
    Zones/RFC1918: 888.99.0.0-888.200.255.255 av=6.0.3.6664.0 atz=Australia/Sydney 
    aid=3DxKlG0UBABCAA0cXXAZIwA\=\= at=testalertng dvchost=SKEELES10 dvc=888.99.100.1 
    deviceZoneURI=/All Zones/ArcSight System/Private Address Space Zones/RFC1918: 
    888.99.0.0-888.200.255.255 dtz=Australia/Sydney _cefVer=0.1"");
                }

",126
38967979,38968841,2,"Since your C# program runs the python script, I guess the easiest solution would be to redirect the standard input of the python process:
     Process pyProc = Process.Start(
        new ProcessStartInfo(""python.exe"", @""/path/to/the/script.py"")
        {
           RedirectStandardInput = true,
           UseShellExecute = false
        }
     );
     for (int ii = 0; ii < 100; ++ii)
     {
        pyProc.StandardInput.WriteLine(string.Format(""this is message # {0}"", ii));
     }

At the python script side, you just need to use built-in function raw_input like below (please note the function has been renamed to raw_input in 3.x):
while True:
    data = raw_input()
    print(data)

",130
38971898,38971898,1,"I have used SimpleDocTemplate for making a table and now I want to draw a rect on this same page, but I don't know how to do.
I have tried this :
draw = Drawing(100, 1)
draw.add(Rect(0, 100, 500, 100))

But it don't works...
",62
38971898,39016545,2,"The reason you code is not working is mostly likely because you are creating a Drawing of just 1 pixel high and 100 pixels wide. Which could never fit a Rect of 500 by 100 pixels.
So your code should be something like this:
draw = Drawing(500, 200)
draw.add(Rect(0, 100, 500, 100))

",68
38971926,38971926,1,"What I'm trying to do is apply a dynamic background color to black text.
The color is calculated as a result of a hash of the text.
The problem is, all too often the color comes out too dark to be able to read the text.
How can I lighten the color to keep it in a decent visual range (not too dark, not too light)?
The color can't be brighter than beige or darker than teal.
(Keep in mind that Blue at 255 is darker than Green at 255, because the human eye is most sensitive to Green and the least sensitive to Blue)
",119
38971926,38985310,2,"You can pick the color in the HSL space
def color(Hmin=0.0, Hmax=360.0,
          Smin=0.0, Smax=1.0,
          Lmin=0.0, Lmax=1.0):
    H = (Hmin + random.random()*(Hmax - Hmin)) % 360.0
    S = Smin + random.random()*(Smax - Smin)
    L = Lmin + random.random()*(Lmax - Lmin)

    # Compute full-brightness, full-saturation color wheel point
    if 0.0 <= H < 60.0:
        R, G, B = (1.0, H/60.0, 0.0)             # R -> Y
    elif 60.0 <= H < 120.0:
        R, G, B = (1-(H-60.0)/60.0, 1.0, 0.0)    # Y -> G
    elif 120.0 <= H < 180.0:
        R, G, B = (0.0, 1.0, (H-120.0)/60.0)     # G -> C
    elif 180.0 <= H < 240.0:
        R, G, B = (0.0, 1.0-(H-180.0)/60.0, 1.0) # C -> B
    elif 240.0 <= H < 300.0:
        R, G, B = ((H-240.0)/60.0, 0.0, 1.0)     # B -> M
    else:
        R, G, B = (1.0, 0.0, 1.0-(H-300.0)/60.0) # M -> R

    # Compute amount of gray
    k = (1.0 - S) * L

    # Return final RGB
    return (k + R*(L-k), k + G*(L-k), k + B*(L-k))

",285
38971926,38983335,2,"QColor supports the HSL representation. You want to limit the range of lightness:
QColor limitLightness(const QColor & color) {
  auto hsl = src.toHsl();
  auto h = hsl.hslHueF();
  auto s = hsl.hslSaturationF();
  auto l = hsl.lightnessF();
  qreal const lMin = 0.25;
  qreal const lMax = 0.75;
  return QColor::fromHslF(h, s, qBound(lMin, lMax, l));
}

",84
38972011,38972011,1,"I have the following code: (simplified)
def main_func():
    anotherDic = {}
    dic = {(1,2):44, (4,6):33, (1,1):4, (2,3):4}
    ks = dic.keys()
    for i in ks:
        func_A(anotherDic, i[0], i[1], dic[i], 5) 

The main dictionary (dic) is quite big, and the for loops goes for 500 million iterations. I want to use multiprocessing to parallelize the loop on a multi-core machine. I have read several SO questions and multiprocessing lib documentation, and this very helpful video and still cannot figure it out. 
I want the program to fork into several threads when it reaches this loop, run in parallel and then after all processes have finished it should continue the program on single process from the line after the loop section. 
func_A received the dictionary value and key from dic, calculates some simple operations, and updates the anotherDic data. This is an independent process, as long as all the same i[0] keys are handles by same process. So, I cannot use pool map function which automatically divides the data between cores. I am going to sort the keys by the first element of key tuple, and then divide them manually between the threads. 
How can i pass/share the very big dictionary (dic) between the processes? Different process will read and write to different keys (i.e. keys that each process deals with are different from the rest of processes)
If I cannot find answer to this, I will just use smaller temporary dic for each process, and in the end just join the dics.
Then question is, how I can force process to fork and go muliprocessor just for the loop section, and after the loop all the processes join before continuing with rest of the code on a single thread?
",366
38972011,38972273,2,"A general answer involves using a Manager object. Adapted from the docs:
from multiprocessing import Process, Manager

def f(d):
    d[1] += '1'
    d['2'] += 2

if __name__ == '__main__':
    manager = Manager()

    d = manager.dict()
    d[1] = '1'
    d['2'] = 2

    p1 = Process(target=f, args=(d,))
    p2 = Process(target=f, args=(d,))
    p1.start()
    p2.start()
    p1.join()
    p2.join()

    print d

Output:
$ python mul.py 
{1: '111', '2': 6}

Original answer: Python multiprocessing: How do I share a dict among multiple processes?
",138
38972052,44072944,2,"The
conda update --all

is actually the way to go. But it is possible that there are pending conflicts. Conda usually warns of those.
It is still possible to upgrade the packages by hand but I expect a warning (for breaking a certain dependency) to be shown.
That's why you 'cannot' upgrate them all.
Considering your update: I think you can upgrade them each separately, but doing so will not only include an upgrade but also a downgrade of another package as well.
So you still cannot upgrade them all by doing the upgrades separately; the dependencies are just not satisfiable.
",119
38972052,38972052,1,"I tried the conda search --outdated, there are lots of outdated packages, for example the scipy is 0.17.1 but the latest is 0.18.0. However, when I do the conda update --all. It will not update any packages.
update 1
conda update --all --alt-hint

Fetching package metadata .......
Solving package specifications: ..........

# All requested packages already installed.
# packages in environment at /home/dracula/opt/anaconda2:
#

update 2
I can update those packages separately. I can do conda update scipy. But why I cannot update all of them in one go?
",111
38972209,38972209,1,"I use conda created an environment called testEnv and activated it, after that I use the command jupyter notebook to call the jupyter editor. It works, but the problem is that, I can only create file in the root environment. How can I create file in testEnv environment?
Here are the steps what I have done:
$ conda create -n testEnv python=3.5 # create environmet
$ source activate testEnv # activate the environmet

(testEnv)$ jupyter notebook # start the jupyter notebook

Here are the result, which shows I can only create file with in ""root"" but not in ""testEnv"" (There is only Root, but no testEnv):

In the Tab Conda, I can see the testEnv, but how can I switch to it?

",144
38972209,38972519,2,"The answer is that you probably shouldn't do this. Python virtualenvs and Conda environments are intended to determine the resources available to the Python system, which are completely independent of your working directory.
You can use the same environment to work on multiple projects, as long as they have the same dependencies. The minute you start tweaking the environment you begin messing with something that is normally automatically maintained.
So perhaps the real question you should ask yourself is ""why do I think it's a good idea to store my notebooks inside the environment used to execute them.""
",109
38972209,38982381,2,"You have two options. You can install the Jupyter Notebook into each environment, and run the Notebook from that environment:
conda create -n testEnv python=3.5 notebook
source activate testEnv
jupyter notebook

or you need to install the IPython kernel from testEnv into the environment from which you want to run Jupyter Notebook. Instructions are here: http://ipython.readthedocs.io/en/stable/install/kernel_install.html#kernels-for-different-environments To summarize:
conda create -n testEnv python=3.5
source activate testEnv
python -m ipykernel install --user --name testEnv --display-name ""Python (testEnv)""
source deactivate
jupyter notebook

",97
38972245,38972647,2,"It's a shame when you find something ready to go but very time consuming to make it work on windows. My advice would be avoid the hazzle of installing that non-ready-to-go-on-windows library and just looking for another alternative, there are few ones dealing with spherical harmonics. What about this one? pyspharm
Also, posting an issue in the library's github issues could speed it up things.
",73
38972245,38972245,1,"We need a tool to work with 3D-Harmonics and we've come across https://github.com/SHTOOLS/SHTOOLS - which fits all of our needs, but could not be installed properly on our windows computers (as it's intended for linux\osx).
When we tried to run pip install . in the directory SHTOOLS-3.3 (we use anaconda for managing packages and it includes pip), we at first got an error saying that we need a Fortran compiler (gfortran) - which we fixed by installing gcc with conda install -c r gcc. Afterwards, we got an error saying we need to install visual C++ compiler - which we downloaded as suggested from https://www.microsoft.com/en-gb/download/details.aspx?id=44266.
Alas, running the command again, this time from the visual C++ 2008 command prompt, we still get a fatal error and are still stuck with installing the library.
Some of the errors we get: 
could not find library 'fftw3' in directories ['build\\temp.win-amd64-2.7']
could not find library 'm' in directories ['build\\temp.win-amd64-2.7']
could not find library 'lapack' in directories ['build\\temp.win-amd64-2.7']
could not find library 'blas' in directories ['build\\temp.win-amd64-2.7']

Followed by
LINK: fatal error LNK1181: cannot open input file 'fftw3.lib'

and 
Failed building wheel for pyshtools

The full output of the installation attempt can be found here and here.
We've tried to download the lib files of the FFTW3, LAPACK and BLAS libraries but couldn't build them properly.
We would appreciate any help (suggesting a similar library that is compatible with windows \ helping with the install of SHTOOlS).
",293
38972380,39922584,2,"The best way to achieve this seems to be to create a new generator class expanding the one provided by Keras that parses the data augmenting only the images and yielding all the outputs.
",35
38972380,41872896,2,"The example below might be self-explanatory!
The 'dummy' model takes 1 input (image) and it outputs 2 values. The model computes the MSE for each output.
x = Convolution2D(8, 5, 5, subsample=(1, 1))(image_input)
x = Activation('relu')(x)
x = Flatten()(x)
x = Dense(50, W_regularizer=l2(0.0001))(x)
x = Activation('relu')(x)

output1 = Dense(1, activation='linear', name='output1')(x)
output2 = Dense(1, activation='linear', name='output2')(x)

model = Model(input=image_input, output=[output1, output2])
model.compile(optimizer='adam', loss={'output1': 'mean_squared_error', 'output2': 'mean_squared_error'})

The function below generates batches to feed the model during training. It takes the training data x and the label y where y=[y1, y2]
batch_generator(x, y, batch_size):
        ....transform images
        ....generate batch batch of size: batch_size 
        yield(X_batch, {'output1': y1, 'output2': y2} ))

Finally, we call the fit_generator()
    model.fit_generator(batch_generator(X_train, y_train, batch_size))

",242
38972380,44907847,2,"If you have separated both mask and binary value you can try something like this:
generator = ImageDataGenerator(rotation_range=5.,
                                width_shift_range=0.1, 
                                height_shift_range=0.1, 
                                horizontal_flip=True,  
                                vertical_flip=True)

def generate_data_generator(generator, X, Y1, Y2):
    genX = generator.flow(X, seed=7)
    genY1 = generator.flow(Y1, seed=7)
    while True:
            Xi = genX.next()
            Yi1 = genY1.next()
            Yi2 = function(Y2)
            yield Xi, [Yi1, Yi2]

So, you use the same generator for both input and mask with the same seed to define the same operation. You may change the binary value or not depending on your needs (Y2). Then, you call the fit_generator():
model.fit_generator(generate_data_generator(generator, X, Y1, Y2),
                epochs=epochs)

",147
38972380,38972380,1,"In a Keras model with the Functional API I need to call fit_generator to train on augmented images data using an ImageDataGenerator.
The problem is my model has two outputs: the mask I'm trying to predict and a binary value
I obviously only want to augment the input and the mask output and not the binary value.
How can I achieve this?
",67
38972419,38972588,2,"UPDATE:
After trying to reproduce the error on linux it's showing a similar behavior, working fine with the first file but with the second is returning Errno32.
Traceback:
Traceback (most recent call last):
  File ""Glob.py"", line 207, in <module>
    runGlobPlot()
  File ""Glob.py"", line 179, in runGlobPlot
    smooth = SavitzkyGolay('smoothFrame',0, sum_vector)
  File ""Glob.py"", line 105, in SavitzkyGolay
    stdin.write(`data`+'\n')
IOError: [Errno 32] Broken pipe

Update:
Some calls of the SG_bin return that the -n parameter is the wrong  type.
Wrong type of parameter for flag -n. Has to be unsigned,unsigned

This parameter comes from the window variable that is passed to the SavitzkyGolay function.
Surrounding the stdin.write with a trycatch block reveals that it breaks a hadnfull of times.
try:
   for data in datalist:
        stdin.write(repr(data)+'\n')
except:
    print ""It broke""

",181
38972419,38972419,1,"The script, originally taken and modified from (http://globplot.embl.de/):
#!/usr/bin/env python
# Copyright (C) 2003 Rune Linding - EMBL
# GlobPlot TM
# GlobPlot is licensed under the Academic Free license

from string import *
from sys import argv
from Bio import File
from Bio import SeqIO
import fpformat
import sys
import tempfile
import os
from os import system,popen3
import math

# Russell/Linding
RL =     {'N':0.229885057471264,'P':0.552316012226663,'Q':-0.187676577424997,'A':-0.261538461538462,'R':-0.176592654077609, \
  'S':0.142883029808825,'C':-0.0151515151515152,'T':0.00887797506611258,'D':0.227629796839729,'E':-0.204684629516228, \
  'V':-0.386174834235195,'F':-0.225572305974316,'W':-0.243375458622095,'G':0.433225711769886,'H':-0.00121743364986608, \
  'Y':-0.20750516775322,'I':-0.422234699606962,'K':-0.100092289621613,'L':-0.337933495925287,'M':-0.225903614457831}

def Sum(seq,par_dict):
sum = 0
results = []
raws = []
sums = []
p = 1
for residue in seq:
    try:
        parameter = par_dict[residue]
    except:
        parameter = 0
    if p == 1:
        sum = parameter
    else:
        sum = sum + parameter#*math.log10(p)
    ssum = float(fpformat.fix(sum,10))
    sums.append(ssum)
    p +=1
return sums

def getSlices(dydx_data, DOM_join_frame, DOM_peak_frame, DIS_join_frame,     DIS_peak_frame):
DOMslices = []
DISslices = []
in_DOMslice = 0
in_DISslice = 0
beginDOMslice = 0
endDOMslice = 0
beginDISslice = 0
endDISslice = 0
for i in range( len(dydx_data) ):
#close dom slice
    if in_DOMslice and dydx_data[i] > 0:
        DOMslices.append([beginDOMslice, endDOMslice])
        in_DOMslice = 0
#close dis slice
    elif in_DISslice and dydx_data[i] < 0:
        DISslices.append([beginDISslice, endDISslice])
        in_DISslice = 0
    # elseif inSlice expandslice
    elif in_DOMslice:
        endDOMslice += 1
    elif in_DISslice:
        endDISslice += 1
# if not in slice and dydx !== 0 start slice
    if dydx_data[i] > 0 and not in_DISslice:
        beginDISslice = i
        endDISslice = i
        in_DISslice = 1
    elif dydx_data[i] < 0 and not in_DOMslice:
        beginDOMslice = i
        endDOMslice = i
        in_DOMslice = 1
#last slice
if in_DOMslice:
    DOMslices.append([beginDOMslice, endDOMslice])
if in_DISslice:
    DISslices.append([beginDISslice,endDISslice])
k = 0
l = 0
while k < len(DOMslices):
    if k+1 < len(DOMslices) and DOMslices[k+1][0]-DOMslices[k][1] < DOM_join_frame:
        DOMslices[k] = [ DOMslices[k][0], DOMslices[k+1][1] ]
        del DOMslices[k+1]
    elif DOMslices[k][1]-DOMslices[k][0]+1 < DOM_peak_frame:
        del DOMslices[k]
    else:
        k += 1
while l < len(DISslices):
    if l+1 < len(DISslices) and DISslices[l+1][0]-DISslices[l][1] < DIS_join_frame:
        DISslices[l] = [ DISslices[l][0], DISslices[l+1][1] ]
        del DISslices[l+1]
    elif DISslices[l][1]-DISslices[l][0]+1 < DIS_peak_frame:
        del DISslices[l]
    else:
        l += 1
return DOMslices, DISslices


def SavitzkyGolay(window,derivative,datalist):
SG_bin = 'sav_gol'
stdin, stdout, stderr = popen3(SG_bin + '-D' + str(derivative) + ' -n' + str(window)+','+str(window))
for data in datalist:
    stdin.write(`data`+'\n')
try:
    stdin.close()
except:
    print stderr.readlines()
results = stdout.readlines()
stdout.close()
SG_results = []
for result in results:
    SG_results.append(float(fpformat.fix(result,6)))
return SG_results


def reportSlicesTXT(slices, sequence, maskFlag):
if maskFlag == 'DOM':
    coordstr = '|GlobDoms:'
elif maskFlag == 'DIS':
    coordstr = '|Disorder:'
else:
    raise SystemExit
if slices == []:
    #by default the sequence is in uppercase which is our search space
    s = sequence
else:
    # insert seq before first slide
    if slices[0][0] > 0:
        s = sequence[0:slices[0][0]]
    else:
        s = ''
    for i in range(len(slices)):
        #skip first slice
        if i > 0:
            coordstr = coordstr + ', '
        coordstr = coordstr + str(slices[i][0]+1) + '-' + str(slices[i][1]+1)
        #insert the actual slice
        if maskFlag == 'DOM':
            s = s + lower(sequence[slices[i][0]:(slices[i][1]+1)])
            if i < len(slices)-1:
                s = s + upper(sequence[(slices[i][1]+1):(slices[i+1][0])])
            #last slice
            elif slices[i][1] < len(sequence)-1:
                s = s + lower(sequence[(slices[i][1]+1):(len(sequence))])
        elif maskFlag == 'DIS':
            s = s + upper(sequence[slices[i][0]:(slices[i][1]+1)])
            #insert untouched seq between disorder segments, 2-run labelling
            if i < len(slices)-1:
                s = s + sequence[(slices[i][1]+1):(slices[i+1][0])]
            #last slice
            elif slices[i][1] < len(sequence)-1:
                s = s + sequence[(slices[i][1]+1):(len(sequence))]
return s,coordstr


def runGlobPlot():
try:
    smoothFrame = int(sys.argv[1])
    DOM_joinFrame = int(sys.argv[2])
    DOM_peakFrame = int(sys.argv[3])
    DIS_joinFrame = int(sys.argv[4])
    DIS_peakFrame = int(sys.argv[5])
    file = str(sys.argv[6])
    db = open(file,'r')
except:
    print 'Usage:'
    print '         ./GlobPipe.py SmoothFrame DOMjoinFrame DOMpeakFrame DISjoinFrame DISpeakFrame FASTAfile'
    print '         Optimised for ELM: ./GlobPlot.py 10 8 75 8 8 sequence_file'
    print '         Webserver settings: ./GlobPlot.py 10 15 74 4 5 sequence_file'
    raise SystemExit
for cur_record in SeqIO.parse(db, ""fasta""):
    #uppercase is searchspace
    seq = upper(str(cur_record.seq))
    # sum function
    sum_vector = Sum(seq,RL)
    # Run Savitzky-Golay
    smooth = SavitzkyGolay('smoothFrame',0, sum_vector)
    dydx_vector = SavitzkyGolay('smoothFrame',1, sum_vector)
    #test
    sumHEAD = sum_vector[:smoothFrame]
    sumTAIL = sum_vector[len(sum_vector)-smoothFrame:]
    newHEAD = []
    newTAIL = []
    for i in range(len(sumHEAD)):
        try:
            dHEAD = (sumHEAD[i+1]-sumHEAD[i])/2
        except:
            dHEAD = (sumHEAD[i]-sumHEAD[i-1])/2
        try:
            dTAIL = (sumTAIL[i+1]-sumTAIL[i])/2
        except:
            dTAIL = (sumTAIL[i]-sumTAIL[i-1])/2
        newHEAD.append(dHEAD)
        newTAIL.append(dTAIL)
    dydx_vector[:smoothFrame] = newHEAD
    dydx_vector[len(dydx_vector)-smoothFrame:] = newTAIL
    globdoms, globdis = getSlices(dydx_vector, DOM_joinFrame, DOM_peakFrame, DIS_joinFrame, DIS_peakFrame)
    s_domMask, coordstrDOM = reportSlicesTXT(globdoms, seq, 'DOM')
    s_final, coordstrDIS = reportSlicesTXT(globdis, s_domMask, 'DIS')
    sys.stdout.write('>'+cur_record.id+coordstrDOM+coordstrDIS+'\n')
    print s_final
    print '\n'
return

runGlobPlot()

My input and output files are here: link
This script takes a input (input1.fa)  and gives following output output1.txt 
But when I try to run this script with similar type but larger input file (input2.fa)  .. It shows following error:
Traceback (most recent call last):
  File ""final_script_globpipe.py"", line 207, in <module>
    runGlobPlot()
  File ""final_script_globpipe.py"", line 179, in runGlobPlot
    smooth = SavitzkyGolay('smoothFrame',0, sum_vector)
  File ""final_script_globpipe.py"", line 105, in SavitzkyGolay
    stdin.write(`data`+'\n')
IOError: [Errno 22] Invalid argument

I have no idea where the problem is. Any type of suggestion is appriciated.
I am using python 2.7 in windows 7 machine. I have also attached the Savitzky Golay module  which is needed to run the script.
Thanks
",1626
38972452,38986144,2,"The quickstart.py example sets the scope to:
https://www.googleapis.com/auth/spreadsheets.readonly

To update the spreadsheet you need to set the scope to:
https://www.googleapis.com/auth/spreadsheets

You can do this by first deleting the existing authentication file in ~/.credentials (that is the location on a raspberry.). It will likely be called ""sheets.googleapis.com-python-quickstart.json.
After you removed it you will need to re-authenticate, which should happen automatically when you re-run the script.
",77
38972452,38972452,1,"I'm following the tutorial from this official link : https://developers.google.com/sheets/quickstart/python
I did execute 'quickstart.py' to authenticated.
After that, I ran 'quickstart.py' again and saw the data from 'https://docs.google.com/spreadsheets/d/1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms/edit#gid=0' as this tutorial gets.
I did change spreadsheet ID to my own id and make it to get the data from my spreadsheet by the method :service.spreadsheets().values().get().execute()
But my goal is to add data to my spreadsheet, so I used  the method 'update' as below:
rangeName = 'A2:D'
body['range'] = rangeName
body['majorDimension'] = 'ROWS'
body['values'] = ['test','test','test','test']
result = service.spreadsheets().values().update(
    spreadsheetId=spreadsheetId, range=rangeName, body=body).execute()
print('result:'+str(result))

Then I got an error : 

googleapiclient.errors.HttpError: https://sheets.googleapis.com/v4/spreadsheets/MY_SPREADSHEET_ID/values/A2%3AD?alt=json  returned ""Request had insufficient authentication scopes."">

I don't know why this erorr occurs when trying to update my sheet and why this error doesn't occur when trying to get data from my sheet.(If it is cause by authentication, the method 'get' should cause it too!)
Thank you.
",243
38972493,38995575,2,"I found a way to copy/insert data from list to flex table (in vertica) using python:
For list
# for python list
tempList = list()    
tempList.append('{ ""_id"" : ""01011"", ""city"" : ""CHESTER-APL21"", ""loc"" : [ -72.988761, 42.279421 ], ""pop"" : 1688, ""state"" : ""MA"" }')
tempList.append('{ ""_id"" : ""01011"", ""city"" : ""CHESTER-APL21"", ""loc"" : [ -72.988761, 42.279421 ], ""pop"" : 1688, ""state"" : ""MA"" }')
cur.copy( ""COPY STG.unstruc_data FROM STDIN parser fjsonparser() "", ''.join(tempList))
connection.commit()

For JSON
# for json file
with open(""D:/SampleCSVFile_2kb/tweets.json"", ""rb"") as fs:
    my_file = fs.read().decode('utf-8')
    cur.copy( ""COPY STG.unstruc_data FROM STDIN parser fjsonparser()"", my_file)
    connection.commit()

For CSV
# for csv file
with open(""D:/SampleCSVFile_2kb/SampleCSVFile_2kb.csv"", ""rb"") as fs:
    my_file = fs.read().decode('utf-8','ignore')
    cur.copy( ""COPY STG.unstruc_data FROM STDIN PARSER FDELIMITEDPARSER (delimiter=',', header='false') "", my_file) # buffer_size=65536
    connection.commit()

",255
38972493,38989666,2,"Vertica-Python supports INSERT INTO. 
Unless you need frequent and very small inserts, writing your data to a file and using COPY would most likely give better performance. If you do it through python, does that still not meet your idea of 'programmatically' ?

https://github.com/uber/vertica-python
https://pypi.python.org/pypi/vertica-python/

",54
38972493,38972493,1,"I am planning to save my unstructured data in flex tables in vertica. I am receiving lists of data (type of data in list may vary in every call) from client, i want to save this in vertica flex table using python 3.
How this can be done?
I found stuff on google, but there data is being loaded in flex table directly using csv or json file, not programmatically. I want to save it programmatically using python.
Thanks in advance for help -:)
",96
38972503,43039300,2,"
Does anyone know the historic reasons for the default behaviour?

Guido van Rossum commented that he liked update() better and thinks a + operator wouldn't read clearly in code.
FWIW, he did approve PEP 448 which gives your another way to do it using star-unpacking generalizations:
>>> a = {'a': 1, 'b': 2}
>>> b = {'c': 3, 'b': 4}
>>> {**a, **b}
{'a': 1, 'b': 4, 'c': 3}

There are several reasons why + might not be a good idea.  Usually, we expect addition to be commutative, but dict addition would fail whenever there were overlapping keys with distinct values.   The ""normal"" use case is to update only dict in-place, but the usual semantics of + would copy the contents of both inputs to create a new dict (which is somewhat wasteful).
In addition, Python has collections.ChainMap which replaces the expense of copying with a new expense of potentially having multiple lookups.
",205
38972503,38972503,1,"I've been musing as to why the Python language's standard dict class doesn't support addition/subtraction operators such as '+' or '+=', e.g.
>>> foo = {'a': 1, 'b': 2}
>>> bar = {'c': 3, 'd': 4}

>>> foo + bar
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: unsupported operand type(s) for +: 'dict' and 'dict'

My wishful thinking would be for the following result instead:
>>> foo + bar 
{'a': 1, 'b': 2, 'c': 3, 'd': 4}

Equally why doesn't __radd__(self, other) result in the same as self.__update__(other)?
>>> foo += bar
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: unsupported operand type(s) for +=: 'dict' and 'dict'    

Does anyone know the historic reasons for the default behaviour? 
(I concede that it could be ambiguous which value should be used in scenarios where foo and bar have the same key but different key values)
",245
38973433,38973433,1,"I have a table of moves that decide whether or not the player wins based on their selection against the AI. Think Rock, Paper, Scissors with a lot more moves.
I'll be eventually coding it in Python, but before I begin, I want to know if there is a better way of doing this rather than LOTS and LOTS of IF statements?
The table looks like this:

I'm thinking that the moves will need to be assigned numbers, or something like that? I don't know where to start...
",102
38973433,38973575,2,"You could use a dict? Something like this:  
#dict of winning outcomes, the first layer represents the AI moves, and the inner 
#layer represent the player move and the outcome
ai = {
    'punch' : {
        'punch' : 'tie',
        'kick' : 'wins',
    },
    'stab' : {
        'punch' : 'loses',
        'kick' : 'loses'
    }
}

ai_move = 'punch'
player_move = 'kick'
print ai[ai_move][player_move] #output: wins


ai_move = 'stab'
player_move = 'punch'
print ai[ai_move][player_move] #output: loses

I didn't map out all the moves, but you get the gist
",123
38973433,38973624,2,"Yes, store the decisions as key/value pairs in a dictionary with all the possible combinations as the key and decision as result. Basically make a lookup table for the possible moves. 
This speeds up decision making at the expense of having to store all possible combinations.
def tie():
    print(""It's a tie!"")

def lose():
    print(""You lose"")

def win():
    print(""You win"")
moves = { 
     # player_ai action
    'punch_punch': tie,
    'punch_kick': lose,
    'punch_stab': lose,
    <..>
}
player_move = <move>
ai_move = <move>
key = ""_"".join([player_move, ai_move])
if key in moves:
    # Execute appropriate function
    return moves[key]()
raise Exception(""Invalid move"")

",161
38973433,38973828,2,"You could use a python dictionary to map moves to numbers:
move = {'Punch': 0, 'Kick': 1}

And then use a matrix to determine the outcome. Numpy can be used for that
import numpy
move = {'Punch': 0, 'Kick': 1}

outcome = numpy.matrix([['Tie','Loses'],['Wins','Tie']])


# Punch vs Punch => Tie
print outcome[move['Punch'], move['Punch']]

# Punch vs Kick => Punch loses
print outcome[move['Punch'], move['Kick']]

",123
38973433,38973896,2,"I'd use a 2-dimensional list for this. Each attack is decoded to an index 0 to 5 and win tie and loss are decoded as 1, 0 and -1.
So the list will look something like this (not based on your example, I just put some random numbers):
table = [[1,0,-1,0,1,-1],[1,1,0,1,0,-1],...,etc.]

And you will retrieve it like this:
table[0][1]

",93
38973433,38973897,2,"You can create a map of attacks similar to your table above like this
map = [
    [0,-1,-1,1,1,-1],
    [1,0,-1,-1,1,-1],
    [1,1,0,-1,-1,1],
    [-1,1,1,0,-1,1],
    [-1,-1,1,1,0,-1],
    [1,1,-1,-1,1,0]
]

Here, 0 is a draw, 1 is a win and -1 is a loss.
Now create an array of attacks where the places of the attacks corresponds with the map above.
attacks = [""Punch"", ""Kick"", ""Stab"", ""Throw"", ""Fling"", ""Uppercut""]

Now you can easily find out if one attack beats another
map[attacks.index(""Stab"")][attacks.index(""Punch"")]

>>> 1

Stab wins over punch
",165
38973433,38973682,2,"You could try a dictionary of dictionaries (nested dictionary). Keep values and keys in text form, rather than map to numbers, to improve readability.
outcome = {}
outcome['punch'] = {}
outcome['punch']['punch'] = 'Tie'
outcome['punch']['kick'] = 'Lose'
...
outcome['kick'] = {}
outcome['kick']['punch'] = 'Win'
outcome['kick']['kick'] = 'Tie'
...
i_do = 'punch'
he_does = 'fling'
...
if outcome[i_do][he_does] == 'Win':
    print(""Woohoo!"")

",122
38973452,38974250,2,"The call to test.keywords.create(...) doesn't call the keyword, it merely creates one to be called later. If you want the results to be assigned to a variable, use the assign attribute when calling create. This argument takes a list of variable names. 
For example, given this line in plain text format:
${greps}=    grep file    log.txt    url:    encoding_errors=ignore

... you would create it like this using the API:
test.keywords.create('grep file', 
                     args=['log.txt', 'url:',  'encoding_errors=ignore'],
                     assign=['${greps}'])

",113
38973452,38973452,1,"I hope you can help me, I am quite stuck with this issue :(
I am trying to create all the tests using the robot api with python, I followed the example in the documentation, but I need to capture the output from a keyword and I dont find how can I do it
I tried as usual in rf-ride syntax:
 test.keywords.create('${greps}=  grep file', args=['log.txt', 'url:',  'encoding_errors=ignore'])

It says: No keyword with name '${grep}=          grep file' found.
I tried: 
output = test.keywords.create('grep file', args=['log.txt', 'url:',  'encoding_errors=ignore'])

but the variable output is having just the keyword name, not the output from kw
I dont know where to look for more info, all the examples are creating kw which dont return any value...
",171
38976893,38976893,1,"I'm trying to write a cross-platform tool that runs specific commands, expects certain output for verification, and sends certain output (like username/password) for authentication.
On Unix, I have been successful in programming a Python tool that uses the pexpect library (via pip install pexpect). This code works perfectly and is exactly what I am trying to do. I've provided a small excerpt of my code for proof-of-concept below:
self.process = pexpect.spawn('/usr/bin/ctf', env={'HOME':expanduser('~')}, timeout=5)
self.process.expect(self.PROMPT)
self.process.sendline('connect to %s' % server)
sw = self.process.expect(['ERROR', 'Username:', 'Connected to (.*) as (.*)'])
if sw == 0:
    pass
elif sw == 1:
    asked_for_pw = self.process.expect([pexpect.TIMEOUT, 'Password:'])
    if not asked_for_pw:
        self.process.sendline(user)
        self.process.expect('Password:')
    self.process.sendline(passwd)
    success = self.process.expect(['Password:', self.PROMPT])
    if not success:
        self.process.close()
        raise CTFError('Invalid password')
elif sw == 2:
    self.server = self.process.match.groups()[0]
    self.user = self.process.match.groups()[1].strip()
else:
    info('Could not match any strings, trying to get server and user')
    self.server = self.process.match.groups()[0]
    self.user = self.process.match.groups()[1].strip()
info('Connected to %s as %s' % (self.server, self.user))

I tried running the same source on Windows (changing /usr/bin/ctf to c:/ctf.exe) and I receive an error message:
Traceback (most recent call last):
  File "".git/hooks/commit-msg"", line 49, in <module> with pyctf.CTFClient() as c:
  File ""C:\git-hooktest\.git\hooks\pyctf.py"", line 49, in __init__
    self.process = pexpect.spawn('c:/ctf.exe', env={'HOME':expanduser('~')}, timeout=5)
  AttributeError: 'module' object has no attribute 'spawn'

According to the pexpect documentation:

pexpect.spawn and pexpect.run() are not available on Windows, as they rely on Unix pseudoterminals (ptys). Cross platform code must not use these.

That led me on my search for a Windows equivalent. I have tried the popular winpexpect project here and even a more recent (forked) version here, but neither of these projects seem to work. I use the method:
self.process = winpexpect.winspawn('c:/ctf.exe', env={'HOME':expanduser('~')}, timeout=5)

only to sit and watch the Command Prompt do nothing (it seems as though it's trapped inside the winspawn method). I was wondering what other means I could go about programming a Python script to interact with the command line to achieve the same effect as I have been able to in Unix? If a suitable working Windows-version pexpect script does not exist, what other means could I use to go about this?
",563
38976893,46219607,2,"Instead of using pexpect.spawn you can use pexpect.popen_spawn.PopenSpawn for windows.
child = pexpect.popen_spawn.PopenSpawn('cmd', timeout=1)
child.send('ipconfig')
child.expect('Wireless', timeout=None)

",32
38977005,38977005,1,"Consider the following code, which generates a (basic) GUI:
import Tkinter as tk

class Game:
    def __init__(self, root):
        self.root = root
        button = tk.Button(root, text=""I am a button"")
        button.pack()

root = tk.Tk()
root.title(""This is a game window"")     # I would like to move this code to the Game class
game = Game(root)
root.mainloop()

The resulting GUI looks like this:

I'd like to achieve the same effect but move the setting of the window title to the class definition. (I've tried self.root.title = ""This is a game window"" in the __init__ but this seemed to have no effect). Is this possible?
",141
38977005,38977181,2,"Sure. You need to call the .title method. Doing 
root.title = ""This is a game window"" 

doesn't set the title, it overwrites the method with the string.
import Tkinter as tk

class Game:
    def __init__(self, root):
        self.root = root
        root.title(""This is a game window"")

        button = tk.Button(root, text=""I am a button"")
        button.pack()

root = tk.Tk()
game = Game(root)
root.mainloop()

You could also do self.root.title(""This is a game window"") but it's more typing, and using self.root is slightly less efficient than using the  root parameter that was passed to the __init__ method, since self.root requires an attribute lookup but root is a simple local variable.
",147
38977008,38977069,2,"Your if excludes the elif condition:
if position <= 18:

This matches if position == 18 too. Python ignores all following elif conditions when a if or elif branch has matched.
If you want to run additional code for the == 18 case, use a new if statement:
if position <= 18:
     # will run for all values of `position up to and including 18

if position == 18:
     # will *also* be run for the `position == 18` case

Alternatively, fix your conditions to not overlap:
if position < 18:
     # ...
elif position == 18:
     # runs only when position is exactly equal to 18

or
if position <= 18:
     # ...
else:
     # runs when the first condition no longer matches, so position > 18

",146
38977008,38977059,2,"if position <= 18:
    ...
elif position == 18:
    ...

Both the if and elif branches are catching position if position == 18 (note the <= in the if statement), so the elif branch will never be executed.
",46
38977008,38977079,2,"Change
if position <= 8:

by 
if position < 8:

And leave your elif the same.
By having the condition <=8 your code always enter the if (When position is <=8), but when is 9 (>8) it will enter the elif. So if you want that the code enters the elif statement when position = 8, the if can not be true when position = 8.
",81
38977008,38977008,1,"The aim is to open the page get url nr 18, swap the url with url in position 18 and rerun 7 times but my code is stuck after getting position 18, why is the elif not running in line 24 ? (no traceback given, program just sitting there)
import urllib
from BeautifulSoup import *

#not the real url
url= ('http://abc.googl.com')
count=7
position = 0

#n will be used to check that the code i running
n=0

while count >= 0:
    html = urllib.urlopen(url).read()
    soup = BeautifulSoup(html)

    tags = soup('a')
    for tag in tags:
        x= tag.get('href', None)
        position=position+1
        if position <= 18:
            n=n+1
            print 'calculating', n
            print x
        elif position == 18:
            url=(x)
            print 'new url', x
            count=count-1
            print 'new count', count
            position=0


if count == 0:
    print ""done""
    print x

",179
38977041,38984902,2,"I've found the string.count() method to be very fast since it's implemented in C. Anything that avoids for loops will generally be faster, even if you iterate the string multiple times. This is probably the fastest solution:
>>> s = 'aaa*bbb#ccc*ddd'
>>> a, b = s.count('*'), s.count('#')
>>> if a == b: a, b = -s.find('*'), -s.find('#')
... 
>>> s.split('*' if a > b else '#')
['aaa', 'bbb#ccc', 'ddd']

",124
38977041,38977361,2,"
it is required to split either by a delimiter that occurs first in the string or by a delimiter that is most frequent in the string.

So you can first find all the delimiters and preserve them in a proper container with their frequency, then find the most common and first one, then split your string based on them.
Now for finding the delimiters, you need to separate them from the plain text based on a particular feature, for example if they are none word characters, and for preserving them we can use a dictionary in order to preserve the count of similar delimiters (in this case collections.Counter() will do the job).
Demo:
>>> s = ""aaa#bbb*ccc#ddd*rkfh^ndjfh*dfehb*erjg-rh@fkej*rjh""
>>> delimiters = re.findall(r'\W', s)
>>> first = delimiters[0]
'#'
>>> Counter(delimiters)
Counter({'*': 5, '#': 2, '@': 1, '-': 1, '^': 1})
>>> 
>>> frequent = Counter(delimiters).most_common(1)[0][0]
'*'
>>> re.split(r'\{}|\{}'.format(first, frequent), s)
['aaa', 'bbb', 'ccc', 'ddd', 'rkfh^ndjfh', 'dfehb', 'erjg-rh@fkej', 'rjh']

Note that if you are dealing with delimiters that are more than one characters you can use re.escape() in order to escape the special regex characters (like *).
",305
38977041,38977041,1,"First of all the question is tagged with Python and regex but it is not really tied to those - an answer can be high level.
At the moment I'm splitting a string with multiple delimiters with the following pattern. There are actually more delimiting patterns and they are more complex but let's keep it simple and limit them to 2 characters - # and *:
parts = re.split('#|*', string)
Which such approach a string aaa#bbb*ccc#ddd is split to 4 substrings aaa, bbb, ccc, ddd. But it is required to split either by a delimiter that occurs first in the string or by a delimiter that is most frequent in the string. aaa#bbb*ccc#ddd should be split to aaa, bbb*ccc, ddd and aaa*bbb#ccc*ddd should be split to aaa, bbb#ccc, ddd.
I know a straightforward way to achieve that - to find what delimiter occurs first or is the most frequent in a string and then split with that single delimiter. But the method has to be efficient and I'm wondering if it is possible to achieve that by a single regex expression. The question is mostly for splitting with the first occurance of the set of delimiters - for most frequent delimiter case almost for sure it will be required to calculate occurrence count in advance.
Update:
The question does not ask to split by first occurrence or most frequent delimiter simultaneously - any of this methods individually will be sufficient. I do understand that splitting by most frequent delimiter is not possible with regex without preliminary determination of the delimiter but I think there's a chance that splitting by first occurrence is possible with regex and lookahead without preparation made in advance.
",324
38977169,38977169,1,"I'm trying to click on a particular button on a webpage with Selenium in Python. How do I select and click on the button ""Replace"", with the following HTML?
<div class=""ui-dialog ui-widget ui-widget-content ui-corner-all ui-front ui-dialog-buttons ui-draggable ui-resizable no-close"" tabindex=""-1"" role=""dialog"" style=""position: absolute; height: auto; width: 300px; top: 411.009px; left: 692.001px; display: block;"" aria-describedby=""dialog"" aria-labelledby=""ui-id-5"">
    <div class=""ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix"">
        <span id=""ui-id-5"" class=""ui-dialog-title"">Search and replace</span>
        <button class=""ui-button ui-widget ui-state-default ui-corner-all ui-button-icon-only ui-dialog-titlebar-close"" role=""button"" aria-disabled=""false"" title=""close"">
            <span class=""ui-button-icon-primary ui-icon ui-icon-closethick""></span>
            <span class=""ui-button-text"">close</span>
        </button>
    </div>
    <div id=""dialog"" class=""title ui-dialog-content ui-widget-content"" style=""width: auto; min-height: 62px; max-height: none; height: auto;"">
        <div class=""findReplaceText"">Search for:</div>
        <input type=""text"" id=""FindTerm"" class=""text ui-widget-content ui-corner-all input-large"">
        <br>
        <div class=""findReplaceText"">Replace with:</div> 
        <input type=""text"" id=""ReplaceTerm"" class=""text ui-widget-content ui-corner-all input-large""></div>
    <div class=""ui-resizable-handle ui-resizable-n"" style=""z-index: 90;"">
    </div>
    <div class=""ui-resizable-handle ui-resizable-e"" style=""z-index: 90;""></div>
    <div class=""ui-resizable-handle ui-resizable-s"" style=""z-index: 90;""></div>
    <div class=""ui-resizable-handle ui-resizable-w"" style=""z-index: 90;""></div>
    <div class=""ui-resizable-handle ui-resizable-se ui-icon ui-icon-gripsmall-diagonal-se"" style=""z-index: 90;""></div>
    <div class=""ui-resizable-handle ui-resizable-sw"" style=""z-index: 90;""></div>
    <div class=""ui-resizable-handle ui-resizable-ne"" style=""z-index: 90;""></div>
    <div class=""ui-resizable-handle ui-resizable-nw"" style=""z-index: 90;""></div>
    <div class=""ui-dialog-buttonpane ui-widget-content ui-helper-clearfix"">
        <div class=""ui-dialog-buttonset"">
            <button type=""button"" class=""ui-button ui-widget ui-state-default ui-corner-all ui-button-text-only"" role=""button"" aria-disabled=""false"">
                <span class=""ui-button-text"">Replace</span>
            </button>
            <button type=""button"" class=""ui-button ui-widget ui-state-default ui-corner-all ui-button-text-only"" role=""button"" aria-disabled=""false"">
                <span class=""ui-button-text"">Cancel</span>
            </button>
        </div>
    </div>
</div>

There is another instance of a ""Replace"" button within the HTML which should be ignored:
<button type=""button"" class=""titleButton replaceButton"" id=""riskFieldTemplateReplace"">Replace</button>

Update: 
Unsure if this is helpful. How the buttons HTML updates when hovering / clicking.
When hovering over the button:
<div class=""ui-dialog-buttonset"">
    <button type=""button"" class=""ui-button ui-widget ui-state-default ui-corner-all ui-button-text-only ui-state-hover"" role=""button"" aria-disabled=""false"">
        <span class=""ui-button-text"">Replace</span>
    </button>
</div>

When clicking the button:
<div class=""ui-dialog-buttonset"">
    <button type=""button"" class=""ui-button ui-widget ui-state-default ui-corner-all ui-button-text-only ui-state-hover ui-state-focus ui-state-active"" role=""button"" aria-disabled=""false"">
        <span class=""ui-button-text"">Replace</span>
    </button>
</div>

When I run: 
driver.find_element_by_xpath("".//button[//span[text() = 'Replace' and @class = 'ui-button']]"").click()

I'm presented with the following: 
Traceback (most recent call last):
  File ""<pyshell#205>"", line 1, in <module>
    driver.find_element_by_xpath("".//button[//span[text() = 'Replace' and @class = 'ui-button']]"").click()
  File ""C:\Python34\lib\site-packages\selenium\webdriver\remote\webdriver.py"", line 294, in find_element_by_xpath
    return self.find_element(by=By.XPATH, value=xpath)
  File ""C:\Python34\lib\site-packages\selenium\webdriver\remote\webdriver.py"", line 748, in find_element
    {'using': by, 'value': value})['value']
  File ""C:\Python34\lib\site-packages\selenium\webdriver\remote\webdriver.py"", line 237, in execute
    self.error_handler.check_response(response)
  File ""C:\Python34\lib\site-packages\selenium\webdriver\remote\errorhandler.py"", line 194, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element: {""method"":""xpath"",""selector"":"".//button[//span[text() = 'Replace' and @class = 'ui-button']]""}
  (Session info: chrome=52.0.2743.116)
  (Driver info: chromedriver=2.22.397933 (1cab651507b88dec79b2b2a22d1943c01833cc1b),platform=Windows NT 6.1.7601 SP1 x86_64)

",922
38977169,38977374,2,"def click_link(self, link_name: str):
    if link_name in self.driver.page_source:
        elem = self.driver.find_element_by_link_text(link_name)
        elem.click()

",24
38977169,38977366,2,"Try xpath:
//span[.='Replace']

",8
38977169,38977261,2,"You should try using xpath as below :-
driver.find_element_by_xpath("".//button[contains(.,'Replace')]"").click()

Or if there is multiple button with same text Replace try as below :-
driver.find_element_by_xpath("".//button[child::span[text() = 'Replace' and @class = 'ui-button-text']]"").click()

Or
driver.find_element_by_xpath("".//span[text() = 'Replace' and @class = 'ui-button-text']/parent::button"").click()

Edited : If you are unable to click on element due to overlay of other element, you can try to click using execute_script as below :-
 replace = driver.find_element_by_xpath("".//span[text() = 'Replace' and @class = 'ui-button-text']/parent::button"");

driver.execute_script(""arguments[0].click()"", replace); 

",166
38977184,38977184,1,"I have a data.frame looking similar to this (except much longer and with way more colornames):
ff = pd.DataFrame({'OldCol':['darkbrown','lightbeige','lightbrown / beige','beige','brown','beige / cognac'], 'NewCol':['nan','nan','nan','nan','nan','nan']})

I want the data.frame to look like this:
ffnew = pd.DataFrame({'OldCol':['darkbrown','lightbeige','lightbrown / beige','beige','brown','beige / cognac'], 'NewCol':['brown','beige','beige / brown','sand','brown','sand / brown']})

I tried the following:
ff.loc[ff['OldCol'].str.contains(r'brown|cognac',na=False) & ff['NewCol'].str.contains(r'nan'), 'NewCol'] = 'brown'
ff.loc[ff['OldCol'].str.contains(r'brown|cognac',na=False) & ~ff['NewCol'].str.contains(r'nan|brown'), 'NewCol'] = ff['NewCol']+'/ brown'

ff.loc[ff['OldCol'].str.contains(r'beige|sand',na=False) & ff['NewCol'].str.contains(r'nan'), 'NewCol'] = 'beige'
ff.loc[ff['OldCol'].str.contains(r'beige|sand',na=False) & ~ff['NewCol'].str.contains(r'nan|beige'), 'NewCol'] = ff['NewCol'] +'/ beige'

In my longer data.frame I typically get the error: 

ValueError: cannot reindex from a duplicate axis

Can anyone help?
Thanks very much in advance!
",314
38977184,38977377,2,"There is problem with duplicates in index. You can replace all values of index by reset_index to Regular Index (0,1,2..len(df)-1). Old values are removed by parameter drop=True:
ff.reset_index(drop=True, inplace=True)

Test:
ff = pd.DataFrame({'OldCol':['darkbrown','lightbeige','lightbrown / beige','beige','brown','beige / cognac'], 'NewCol':['nan','nan','nan','nan','nan','nan']})
ffnew = pd.DataFrame({'OldCol':['darkbrown','lightbeige','lightbrown / beige','beige','brown','beige / cognac'], 'NewCol':['brown','beige','beige / brown','sand','brown','sand / brown']})
ff.index = [0,0,2,3,4,5]
#ValueError: cannot reindex from a duplicate axis
ff.reset_index(drop=True, inplace=True)

",181
38977285,38977553,2,"Instead of the spaces, I would highly recommend the following:
for i in range(n):
    NameList.append(input(""What is the name of worker "" + i +""?""))

If you absolutely need to accept spaces, 
use DeepSpace's answer.
",53
38977285,38977345,2,"If a worker name can't include a space, you can check that and change the behavior accordingly.
This will even work if the user combine the 2 options (e.g. entering 'a' and then 'b c'), although it is not perfect (it is possible to get more than n names, for example if n == 3 and inputting 'a b' and 'c d')
n = int(input(""enter the no of workers \n""))
print(""enter the names of workers"")
NameList = []
while len(NameList) < n:
    worker_name = input()
    if ' ' in worker_name:
        if NameList:
            NameList.extend(worker_name.split())
        else:
            NameList = worker_name.split()
    else:
        NameList.append(worker_name)
print(NameList)

",151
38977285,38977285,1,"I am not sure about input format that user will enter. There are two possibities:
code:
n=int(input(""enter the no of workers \n""))
print(""enter the names of workers"")
NameList=[]
for i in range(n):
    NameList.append(input())
print(NameList)

possibility 1:
enter the no of workers 
4
enter the names of workers
name1
name2
name3
name4
['name1', 'name2', 'name3', 'name4']
working great...!!
possibility 2:
input will be : 
enter the no of workers 
4
enter the names of workers
name1 name2 name3 name4
in this case my code will fail. I'll need to write different code( i know how to write that ;) ) to accept this format of input.
So is there anyway that one code will work for both input formats. By treating spaces as Enter. Thanks
",175
38977344,39014240,2,"Based on the answer of Simon Visser on question : 
Cannot install numpy from wheel format
the solution is to replace ""cp26mu"" in the name of the file by none. 
",34
38977344,38977344,1,"I am trying to install numpy from a wheel package (that I have generated in my virtualenv) on a Redhat 6.5 with python version =2.6.6:
pip install numpy-1.11.1-cp26-cp26mu-linux_x86_64.whl

I am getting the following error: 
numpy-1.11.1-cp26-cp26mu-linux_x86_64.whl is not a supported wheel on this platform.

Any way to fix that? thanks :) 
",57
38977360,38977360,1,"I have a class called run_c which is being used to initialize and execute the run of a kinematics simulation. I assign default values to the attributes of run_c, such as x, before run_c.__init__() is executed. All __init__() is doing is extracting user-input values, aggregated into a dictionary, and assigning them to corresponding attributes in run_c if they exists. For example...
import vars.Defaults as dft
class run_c:

   ...
   dt     = dft.dt
   x      = dft.x0
   states = [ [], [], [], [] ]
   ...

   def __init__(self, input):
      for key in input.keys():
         if hasattr(self, key): setattr(self, key, input[key])
      ...
      self.execute()

run_c.states is a list of lists that is being used to record the values of run_c attributes as they change with timesteps. Later, within run_c.execute(), I am storing x values in states[1], incrementing the timestep dt, and updating x using velocity and timestep. It's pretty simple stuff, right?...
Here's where the problem begins, though. The first time that the instance of run_c is created, initialized, and executed, it runs perfectly. However, I am operating this simulation by creating, initializing, and executing multiple runs based off of list read from a JSON file. As such, in the driver module...
from Run import run_c
def main():
   ...
   for runEntry in runList:
      currRun = run_c(runEntry)
      ...
   ...

What happens is that all the values that were stored in run_c.states do not get wiped after each iteration of the loop. I thought that new instances of run_c are being created every time I run the loop so as to execute __init__() fresh with new information. Why are the data values, such as old values for x, being retained after the end of each loop?
Update: When I add in a line of code to reset the values of states back to empty lists within __init__(), the problem goes away. But this shouldn't be a necessary step in what I want to do...should it?
",411
38977360,38977584,2,"In addition to the existing answer: you can initialize these variables in the __new__ method.
class A:
    val = 1

    def __new__(cls, v):
        t = super().__new__(cls)
        t.val = v
        return t

    def __init__(self, v):
        val = v

a, b = A(1), A(2)

print(a.val, b.val)

If you remove the __new__ method, the output'd be 1 1 and otherwise 1 2. 
",93
38977360,38977521,2,"dt, x, and states are defined as class variables, not instance variables. Every instance of that class that you make will share them. If you need them to be initialized each time you generate a new instance, that's exactly what __init__() is for:
class run_c:

   def __init__(self, input):
      ...
      self.dt     = dft.dt
      self.x      = dft.x0
      self.states = [ [], [], [], [] ]
      ...

      for key in input.keys():
         if hasattr(self, key): setattr(self, key, input[key])
      ...
      self.execute()

",117
38977390,38977390,1,"i run my python file on web browser but i have some error can u help me solve them
Internal Server Error

The server encountered an internal error or misconfiguration and was unable to complete your request.
Please contact the server administrator at webmaster@localhost to inform them of the time this error occurred, and the actions you performed just before this error.
More information about this error may be available in the server error log.
Apache/2.4.18 (Ubuntu) Server at localhost Port 80
",90
38977390,38977559,2,"you can find your apache logs in this directory /var/log/apache/. Error 500 usually means there is a server error. If you can't find the error in these logs, then try to use verbose logging.
",39
38977401,38977401,1,"I have an array of team names from NCAA, along with statistics associated with them. The school names are often shortened or left out entirely, but there is usually a common element in all variations of the name (like Alabama Crimson Tide vs Crimson Tide). These names are all contained in an array in no particular order. I would like to be able to take all variations of a team name by fuzzy matching them and rename all variants to one name. I'm working in python 2.7 and I have a numpy array with all of the data. Any help would be appreciated, as I have never used fuzzy matching before.
I have considered fuzzy matching through a for-loop, which would (despite being unbelievably slow) compare each element in the column of the array to every other element, but I'm not really sure how to build it.
Currently, my array looks like this:
{Names , info1, info2, info 3} 
The array is a few thousand rows long, so I'm trying to make the program as efficient as possible.
",205
38977401,38986394,2,"The Levenshtein edit distance is the most common way to perform fuzzy matching of strings.  It is available in the python-Levenshtein package.  Another popular distance is Jaro Winkler's distance, also available in the same package.
Assuming a simple array numpy array:
import numpy as np
import Levenshtein as lv

ar = np.array([
      'string'
    , 'stum'
    , 'Such'
    , 'Say'
    , 'nay'
    , 'powder'
    , 'hiden'
    , 'parrot'
    , 'ming'
    ])

We define helpers to give us indexes of Levenshtein and Jaro distances, between a string we have and all strings in the array.
def levenshtein(dist, string):
    return map(lambda x: x<dist, map(lambda x: lv.distance(string, x), ar))

def jaro(dist, string):
    return map(lambda x: x<dist, map(lambda x: lv.jaro_winkler(string, x), ar))

Now, note that Levenshtein distance is an integer value counted in number of characters, whilst Jaro's distance is a floating point value that normally varies between 0 and 1.  Let's test this using np.where:
print ar[np.where(levenshtein(3, 'str'))]
print ar[np.where(levenshtein(5, 'str'))]
print ar[np.where(jaro(0.00000001, 'str'))]
print ar[np.where(jaro(0.9, 'str'))]

And we get:
['stum']
['string' 'stum' 'Such' 'Say' 'nay' 'ming']
['Such' 'Say' 'nay' 'powder' 'hiden' 'ming']
['string' 'stum' 'Such' 'Say' 'nay' 'powder' 'hiden' 'parrot' 'ming']

",322
38977457,44454088,2,"Shobhit Bhatnagar, try this one - pip install -e git+https://github.com/dvska/gdata-python3#egg=gdata
",15
38977457,38977457,1,"I'm installing google api python client on python 3 using pip from the GitHub repository.
But getting the following error:

Microsoft Visual C++ 10.0 is required. Get it with ""Microsoft Windows SDK 7.1"": www.microsoft.com/download/details.aspx?id=8279

I already installed microsoft windows sdk 7.1 and net framework 4.0.
But it's not working.
Please check the snapshot for more details.
",69
38977457,38977627,2,"gdata is deprecated and does not have support for Python 3.x in the first place. Use any of these APIs.
",22
38977472,38977472,1,"I have the following code:
import threading
import time

class TestWorker (threading.Thread):
    def __init__(self, threadID, name):
        threading.Thread.__init__(self)
        self.threadID = threadID
        self.name = name

    def run(self):
        print ""Starting "" + self.name
        time.sleep(20)
        print ""Exiting "" + self.name
        # how do I let the calling thread know it's done?

class TestMain:
    def __init__(self):
        pass

    def do_work(self):
        thread = TestWorker(1, ""Thread-1"")
        thread.start()

    def do_something_else(self):
        print ""Something else""

    def on_work_done(self):
        print ""work done""

How can I let the main thread know that the TestWorker has finished (call on_work_done()), without blocking calls to do_something_else() as thread.join() would?
",155
38977472,38978075,2,"import threading
dt = {}
threading.Thread(target=dt.update, kwargs=dict(out=123)).start()

while 'out' not in dt:
    print('out' in dt)
print(dt)

",36
38977472,38980087,2,"import queue
import threading

class SThread(threading.Thread, queue.Queue):
    def __init__(self, queue_out: object):
        threading.Thread.__init__(self)
        queue.Queue.__init__(self)
        self.queue_out = queue_out
        self.setDaemon(True)
        self.start()

    def run(self):
        print('Thread start')
        while True:
            cmd = self.get()
            if cmd is None:
                break  # exit thread
            self.queue_out.put(cmd['target'](*cmd.get('args', ())), **cmd.get('kwargs', {}))
            self.task_done()
        print('Thread stop')

def testFn(a):
    print('+ %s' % a)
    return a

if __name__ == '__main__':
    print('main 1')
    # init
    queue_out = queue.Queue()
    thread = SThread(queue_out)

    # in
    for a in range(5): thread.put(dict(target=testFn, args=(a,)))

    thread.put(None)
    print('main 2')
    # out
    while True:
        try:
            print('- %s' % queue_out.get(timeout=3))
        except queue.Empty:
            break
    print('main 3')

OUT:
main 1
Thread start
main 2
+ 0
+ 1
+ 2
+ 3
+ 4
Thread stop
- 0
- 1
- 2
- 3
- 4
main 3

",241
38977472,38978438,2,"You can give your thread instance an optional callback function to call when it's finished.
Note I added a Lock to prevent concurrent printing (which does block).
import threading
import time

print_lock = threading.Lock()  # prevent threads from printing at same time

class TestWorker(threading.Thread):
    def __init__(self, threadID, name, callback=None):
        threading.Thread.__init__(self)
        self.threadID = threadID
        self.name = name
        self.callback = callback if callback else lambda: None

    def run(self):
        with print_lock:
            print(""Starting "" + self.name)
        time.sleep(3)
        with print_lock:
            print(""Exiting "" + self.name)
        self.callback()

class TestMain:
    def __init__(self):
        self.work_done = False

    def do_work(self):
        thread = TestWorker(1, ""Thread-1"", self.on_work_done)
        thread.start()

    def do_something_else(self):
        with print_lock:
            print(""Something else"")

    def on_work_done(self):
        with print_lock:
            print(""work done"")
        self.work_done = True

main = TestMain()
main.do_work()
while not main.work_done:
    main.do_something_else()
    time.sleep(.5)  # do other stuff...

print('Done')

Output:
Starting Thread-1
Something else
Something else
Something else
Something else
Something else
Something else
Exiting Thread-1
Something else
work done
Done

",241
38977525,38984338,2,"Some details of what you must do may depend on what you want to do with IDLE's Shell once you have it running.  I would like to know more about that.  But let us start simple and make the minimum changes to pyshell.main needed to make it run with other code.  
Note that in 3.6, which I use below, PyShell.py is renamed pyshell.py. Also note that everything here amounts to using IDLE's private internals and is 'use at your own risk'.
I presume you want to run Shell in the same process (and thread) as your tkinter code.  Change the signature to
def main(tkroot=None):

Change root creation (find # setup root) to
if not tkroot:
    root = Tk(className=""Idle"")
    root.withdraw()
else:
    root = tkroot

In current 3.6, there are a couple more lines to be indented under if not tkroot:
    if use_subprocess and not testing:
        NoDefaultRoot()

Guard mainloop and destroy (at the end) with
if not tkroot:
    while flist.inversedict:  # keep IDLE running while files are open.
        root.mainloop()
    root.destroy()
# else leave mainloop and destroy to caller of main

The above adds 'dependency injection' of a root window to the function.  I might add it in 3.6 to make testing (an example of 'other code') easier.
The follow tkinter program now runs, displaying the both the root window and an IDLE shell.
from tkinter import *
from idlelib import pyshell

root = Tk()
Label(root, text='Root id is '+str(id(root))).pack()
root.update()
def later():
    pyshell.main(tkroot=root)
    Label(root, text='Use_subprocess = '+str(pyshell.use_subprocess)).pack()

root.after(0, later)
root.mainloop()

You should be able to call pyshell.main whenever you want.
",349
38977525,38977525,1,"I need to embed an interative python interpreter into my tkinter program. Could anyone help me out as to how to integrate it?
I have already looked at the main() function, but it's way to complex for my needs, but I can't seem to reduce it without breaking it.
",58
38978073,44909825,2,"Using tf.metrics did the trick for me : 
#define the method
x = tf.placeholder(tf.int32, )
y = tf.placeholder(tf.int32, )
acc, acc_op = tf.metrics.accuracy(labels=x, predictions=y)
rec, rec_op = tf.metrics.recall(labels=x, predictions=y)
pre, pre_op = tf.metrics.precision(labels=x, predictions=y)

#predict the class using your classifier
scorednn = list(DNNClassifier.predict_classes(input_fn=lambda: input_fn(testing_set)))
scoreArr = np.array(scorednn).astype(int)

#run the session to compare the label with the prediction
sess=tf.Session()
sess.run(tf.global_variables_initializer())
sess.run(tf.local_variables_initializer())
v = sess.run(acc_op, feed_dict={x: testing_set[""target""],y: scoreArr}) #accuracy
r = sess.run(rec_op, feed_dict={x: testing_set[""target""],y: scoreArr}) #recall
p = sess.run(pre_op, feed_dict={x: testing_set[""target""],y: scoreArr}) #precision

print(""accuracy %f"", v)
print(""recall %f"", r)
print(""precision %f"", p)

Result : 
accuracy %f 0.686667
recall %f 0.978723
precision %f 0.824373

Note : for Accuracy I would use :
accuracy_score = DNNClassifier.evaluate(input_fn=lambda:input_fn(testing_set),steps=1)[""accuracy""]

As it is simpler and already compute in the evaluate.
Also call variables_initializer if you don't want cumulative result.
",277
38978073,38978073,1,"I was wondering if there was a simple solution to get recall and precision value for the classes of my classifier? 
To put some context, I implemented a 20 classes CNN classifier using Tensorflow with the help of  Denny Britz code : https://github.com/dennybritz/cnn-text-classification-tf .
As you can see at the end of text_cnn.py he implements a simple function to compute the global accuracy : 
# Accuracy
        with tf.name_scope(""accuracy""):
            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))
            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, ""float""), name=""accuracy"")

Any ideas on how i could do something similar to get the recall and precision value for the differents categories? 
Maybe my question will sound dumb but I'm a bit lost with this to be honest. Thanks for the help.
",153
38978084,38978161,2,"If I understand correctly, I think this is what you're looking for:
params = {}

if x:
    params['full_name'] = 'something'
if y:
    params['role'] = 'something else'

an_agent.update(r_agent_id, **params)

UPDATE
There are other options, assuming you control the code for Agents. E.g., you could redefine the method like this:
def update(self, agent_id, full_name=None, role=None, status=None):
    if full_name is None: full_name = 'none'
    if role is None: role = 'none'
    if status is None: status = 'none'
    ...

and then always pass all arguments:
full_name = None
role = None
status = None

if x:
    full_name = 'something'
if y:
    role = 'something else'

an_agent.update(r_agent_id, full_name, role, status)

or perhaps keep the definition of update the same and just initialize your parameters to the string 'none'.
",168
38978084,38978084,1,"Ok I am not even sure the proper terminology to use to describe what I am trying to do. Anyway, I want to know if it is possible to programmatically or dynamically build a function call in python.
Let me explain.
I have a function inside a class that is defined with optional parameters like so:
    class Agents(object):

        def update(self, agent_id, full_name = ""none"", role = ""none"", status = ""none""):
            # do some stuff

So when I when I go to use that function, I may be updating just one, two or all 3 of the optional parameters. Sometimes it may be full_name and role but not status... or status and role but not name, or just status, or well you get the idea.
So I could handle this with a big block of if elif statements to account for all the permutations but that strikes me as really clumsy.
Currently I am calling the function like so:
    an_agent = Agents()
    an_agent.update(agent_id = r_agent_id)

Is there anyway to construct that function call programmatically, like appending the arguments to the call before making it. So that I can account for multiple scenarios like so:
    an_agent = Agents()
    an_agent.update(agent_id = r_agent_id, full_name = r_full_name)

or 
    an_agent = Agents()
    an_agent.update(agent_id = r_agent_id, full_name = r_full_name, status = r_status)

Anyway, what is the best way to approach this?
",279
38978160,38989657,2,"Martijn answer was right with this
$ curl --data-urlencode @conf.json -G http://localhost:8080/resources/

But you do not need to use urllib to get the args with flask. I would instead on my endpoint just use the following.
@resources.route('/resources')
def test():
    args = request.args
    return args.get('parameter')

Also I would look at using the flask_testing extension that way you can setup reproducible test cases that exist within the context of the running application.
from flask_testing import TestCase
from app import app

class TestQueryString(TestCase):

    def create_app(self): # Must be implemented
        return app

    def test_json_parse_args(self):
        data = {
            'parameter': 'value'
        }

        r = self.client.get('/resources', query_string=data)
        self.assert200(r)
        self.assertEqual(r.data, 'value')

https://pythonhosted.org/Flask-Testing/
",152
38978160,38978160,1,"I am implementing a REST API in Python using Flask.
I have to get parameters to perform a query and return resources. To be aligned with REST principles, I am going to use a GET request for this operation.
Given that there can be a lot of parameters, I want to send them through a conf.json file, for instance:
{""parameter"": ""xxx""}

I perform the request through curl:

$ curl -H ""Content-Type: application/json"" --data @conf.json -G http://localhost:8080/resources/

The request is redirected to the route with these operations:
@resources.route('/resources/', methods=['GET'])
def discover():
if request.get_json():
    json_data=request.get_json()
    return jsonify(json_data)

what I get back is:
<head> 
<title>Error response</title> 
</head> 
<body> 
<h1>Error response</h1> 
<p>Error code 400. 
<p>Message: Bad request syntax ('GET /resources/?{""parameter"": ""xxx""} HTTP/1.1'). 
<p>Error code explanation: 400 = Bad request syntax or unsupported method. </body>

Somebody knows how to get the json data and properly handle it in the request?
",237
38978160,38978254,2,"request.get_json() looks for JSON data in the request body (e.g. what a POST request would include). You put the JSON data in the URL query string of a GET request instead.
Your curl command sends your JSON un-escaped, and produces an invalid URL, so the server rightly rejects that:
http://localhost:8080/resources/?{""parameter"": ""xxx""}

You can't have spaces in a URL, for example. You'd have to use --data-urlencode instead for this to be escaped properly:
$ curl --data-urlencode @conf.json -G http://localhost:8080/resources/

Note that the Content-Type header is not needed here; you don't have any request body to record the content of.
The adjusted curl command now sends a properly encoded URL:
http://localhost:8080/resources/?%7B%22parameter%22%3A%20%22xxx%22%7D%0A%0A

Access that data with request.query_string. You will also have to decode the URL encoding again before passing this to json.loads():
from urllib import unquote

json_raw_data = unquote(request.query_string)
json_data = json.loads(json_raw_data)

Take into account that many webservers put limits on how long a URL they'll accept. If you are planning on sending more than 4k characters in a URL this way, you really need to reconsider and use POST requests instead. That's 4k with the JSON data URL encoded, which adds a considerable overhead.
",267
38978163,38984401,2,"Seems -2 returns 254 on echo $? in bash.
The Bash is interpreting the -2 as 254 probably due to how it handles negative numbers which I believe is by returning 8-bit unsigned integers.  
Soln: Use a postive integer number > 0 but < 256.  Bash will see a return code > 0 as an error.
Does the lock below need to be global? I believe so, in order that it stays in scope until the program finishes.
Ans: If there are no other conflicts with locking the file than I would say that the global lock is fine.
",110
38978163,38978163,1,"I am setting up cron job for single instance.
Does the lock below need to be global? I believe so, in order that it stays in scope until the program finishes. Or at least outside of try/except block. Also, return values should be positive from Python? Seems -2 returns 254 on echo $? in bash. 
import time, fcntl, sys

LOCK_FILE = '/tmp/test_flock.lock'
lock = None    
def do_wait():
    print ('waiting N sec')
    time.sleep(3)

def main(argv=None):
    try:
        global lock        
        lock = open(LOCK_FILE,'w')
        fcntl.flock(lock, fcntl.LOCK_EX | fcntl.LOCK_NB)
        print (""got flock"")
    except IOError as err:
        print (""Could not obtain lock file"")
        return -2

    if argv is None:
        argv = sys.argv

    try:
        print ('entering main')
        print ('waiting')
#        raise ValueError(""Error raised"")
        do_wait()
        print ('done')
    except Exception as err:
        print (""Exception in main"")
        return -1

if __name__ == '__main__':
    sys.exit(main())

",208
38978174,38998694,2,"Thanks for your help. The final solution is kind of stupid. I started spyder via the anaconda GUI. If I do so the above code does not work.
If I run this directly via the console or start spyder via the console everything is fine. It seems that the bash_profile is not loaded when spyder is loaded but requires the console to do so
",69
38978174,38978267,2,"With shell=True the cmd needs to be a string.
subprocess.check_call(command, shell=True)
where command is of type str
",22
38978174,38978174,1,"I am using am using Python 2.7 on MacOS and want to use a bash command within a python script.
command = ""someProgram --option1 value 1 --option2 value 2""

I had to include the path of this program in my bash_profile in order to run it. I tested so far:
os.system(command)

and
subprocess.check_call(command.split("" ""),shell=True)

Neither worked. The latter threw error 127 and the first one only returned 32512. A google search told me that this occurs when the command is not known. 
If I now start this command within the terminal everything works perfectly fine.
Do I have to include something such that python can find this command? Why is this behavior?
",136
38978182,39120531,2,"Since I wasn't able to implement an Observer to watch widgets like the ttk.Combobox, I've decided to create a workaround. Here are the steps I took, in order to achieve a MVC architecture from Bryan Oakleys example (link is in the question), which refreshes its model class via the controller class, whenever a user takes an action in the view (GUI). 
Step 1: Add a model class
First, in order to use a MVC architecture, we have to seperate the code into model, view and control. In this example, model is class Model:, control is class PageControl(tk.Tk): and view are the pages class StartPage(tk.Frame), PageOne(tk.Frame) and PageTwo(tk.Frame).
Step 2: Set up your model class
Now we have to decide on which variables we want to have in the model class. In this example, we have directories and keys (status of the comboboxes), which we want to save in dictionaries. After setting them up empty, all we have to do is add setters and getters for each variable, so we can refresh data in model and also retrieve some, if we want. Additionally, we could implement delet methods for each variable, if we wanted to.
Step 3: Add push and pull methods to the control class
Now that there is a model class, we can refrence it via e. g. self.model = Model() in PageControl(tk.Tk) (control). Now we have the basic tools to set data in Model via e. g. self.model.set_keys(self.shared_keys) and also get data from Model. Since we want our control class to do that, we need some methods, that can achieve this. So we add the push and pull methods to the PageControl (e. g. def push_key(self)), which in turn can be refrenced from view (StartPage, PageOne, PageTwo) via controller.
Step 4: Add your widgets to the view class
Now we have to decide on which widgets shall be on which page and what you want them to do. In this example, there are buttons for navigation, which for the sake of the task can be ignored, two comboboxes and a button, which opens a file dialog. 
Here, we want the comboboxes to refresh their status whenever it is changed and send the new status via controller to the model. Whereas the Open button of PageOne shall open a file dialog, where the user then selects files he/she wants to open. The directories we got from this interaction then shall be send via controller to model.
Step 5: Get all your functionality into the controller class
Since there is a controller variable, we can use it to refrence methods, which are in the controller class. This way, we can outsource all our methods from the pages into the controller and reference them via self.controller.function_of_controller_class. But we have to be aware, that methods, which are bound to commands via lambda: can't return any values, but they are also not called on programme startup. So keep that in mind.
Step 6: Set up your bindings and wrappers
Here we have to set up the .bind() for our comboboxes. Since the controller allready is set up to store data and the comboboxes have a textvariable, we can use this to gather information about the status of the comboboxes via combobox.bind(<<ComboboxSelect>>). All we have to do is to set up a wrapper which is called, whenever combobox.bind(<<ComboboxSelect>>) is throwing an event.
Closing statement
Now we have it, a programme based on Bryan Oakleys example of ""How to get variable data from a class"", which utilises a model, which is updated via controller whenever the user takes a corresponding action in the view. Unfortunately it doesn't utilise a Observer class, as first intended, but I'll keep working on it and update this, when I've found a satisfying solution.
",761
38978182,38978182,1,"I'm currently working on a GUI which is based on the thread How to get variable data from a class. Since there will be a lot of data to handle, I would like to use a Model-Class, which get's its updates via Observer.
Right now, changes in the ttk.Combobox on Page One are registered via <<ComboboxSelect>>, pulled into the variable self.shared_data of the Controller and passed to the Model. This way, no Oberserver/Observable logic is used. Instead, the data in Model is changed, whenever the user takes a corresponding action in the GUI.
I, however, would love not to have to use bindings like <<ComboboxSelect>> to change the corresponding data in the Model, but an Observer/Observable logic, which detects, that i.e. the entry ""Inputformat"" in the dictionary self.shared_data within the Controller was changed, which in turn refreshes the data in the Model, i.e. self.model_data, where the actual state of the ttk.Combobox is saved.
In short, I want to achieve the following, by using an Observer: 
User selects i.e. ""Entry 01"" in the ttk.Combobox --> self.shared_data[""Inputformat""] in the Controller is now filled with ""Entry 01"" --> an Observer/Observable logic detects this --> the corresponding variable in the Model is beeing changed.
For you to have something to work with, here is the code.
# -*- coding: utf-8 -*-

import csv
import Tkinter as tk   # python2
import ttk
import tkFileDialog


# Register a new csv dialect for global use.
# Its delimiter shall be the semicolon:
csv.register_dialect('excel-semicolon', delimiter = ';')

font = ('Calibri', 12)

''' 
############################################################################### 
#                                 Model                                       # 
###############################################################################
'''

class Model:
    def __init__(self, *args, **kwargs):
        # There shall be a variable, which is updated every time the entry
        # of the combobox is changed
        self.model_keys = {}
        self.model_directories = {}

    def set_keys(self, keys_model):
        self.model_keys = keys_model
        keys = []
        keyentries = []
        for key in self.model_keys:
            keys.append(key)
        for entry in self.model_keys:
            keyentries.append(self.model_keys[entry].get())

        print ""model_keys: {0}"".format(keys) 
        print ""model_keyentries: {0}"".format(keyentries)

    def get_keys(self):
        keys_model = self.model_keys
        return(keys_model)

    def set_directories(self, model_directories):
        self.model_directories = model_directories
        print ""Directories: {0}"".format(self.model_directories)

    def get_directories(self):
        model_directories = self.model_directories
        return(model_directories)


''' 
############################################################################### 
#                               Controller                                    # 
###############################################################################
'''

# controller handles the following: shown pages (View), calculations 
# (to be implemented), datasets (Model), communication
class PageControl(tk.Tk):

    ''' Initialisations '''
    def __init__(self, *args, **kwargs):
        tk.Tk.__init__(self, *args, **kwargs) # init
        tk.Tk.wm_title(self, ""MCR-ALS-Converter"") # title

        # initiate Model
        self.model = Model()

        # file dialog options
        self.file_opt = self.file_dialog_options()

        # stores checkboxstatus, comboboxselections etc.
        self.shared_keys = self.keys()

        # creates the frames, which are stacked all over each other
        container = self.create_frame()
        self.stack_frames(container)

        #creates the menubar for all frames
        self.create_menubar(container)

        # raises the chosen frame over the others
        self.frame = self.show_frame(""StartPage"")      


    ''' Methods to show View'''
    # frame, which is the container for all pages
    def create_frame(self):        
        # the container is where we'll stack a bunch of frames
        # on top of each other, then the one we want visible
        # will be raised above the others
        container = ttk.Frame(self)
        container.pack(side=""top"", fill=""both"", expand=True)
        container.grid_rowconfigure(0, weight=1)
        container.grid_columnconfigure(0, weight=1)
        return(container)

    def stack_frames(self, container):
        self.frames = {}
        for F in (StartPage, PageOne, PageTwo):
            page_name = F.__name__
            frame = F(parent = container, controller = self)
            self.frames[page_name] = frame
            # put all of the pages in the same location;
            # the one on the top of the stacking order
            # will be the one that is visible.
            frame.grid(row=0, column=0, sticky=""nsew"")

    # overarching menubar, seen by all pages
    def create_menubar(self, container):       
        # the menubar is going to be seen by all pages       
        menubar = tk.Menu(container)
        menubar.add_command(label = ""Quit"", command = lambda: app.destroy())
        tk.Tk.config(self, menu = menubar)

    # function of the controller, to show the desired frame
    def show_frame(self, page_name):
        #Show the frame for the given page name
        frame = self.frames[page_name]
        frame.tkraise()
        return(frame)


    ''' Push and Pull of Data from and to Model ''' 
    # calls the method, which pushes the keys in Model (setter)
    def push_keys(self):
        self.model.set_keys(self.shared_keys)

    # calls the method, which pulls the key data from Model (getter)    
    def pull_keys(self):
        pulled_keys = self.model.get_keys()
        return(pulled_keys)

    # calls the method, which pushes the directory data in Model (setter) 
    def push_directories(self, directories):
        self.model.set_directories(directories)

    # calls the method, which pulls the directory data from Model (getter)
    def pull_directories(self):
        directories = self.model.get_directories()
        return(directories)


    ''' Keys '''
    # dictionary with all the variables regarding widgetstatus like checkbox checked    
    def keys(self):
        keys = {}
        keys[""Inputformat""] = tk.StringVar()
        keys[""Outputformat""] = tk.StringVar() 
        return(keys)


    ''' Options '''  
    # function, which defines the options for file input and output     
    def file_dialog_options(self):
        #Options for saving and loading of files:
        options = {}
        options['defaultextension'] = '.csv'
        options['filetypes'] = [('Comma-Seperated Values', '.csv'), 
                                ('ASCII-File','.asc'), 
                                ('Normal Text File','.txt')]
        options['initialdir'] = 'C//'
        options['initialfile'] = ''
        options['parent'] = self
        options['title'] = 'MCR-ALS Data Preprocessing'
        return(options)


    ''' Methods (bindings) for PageOne '''
    def open_button(self):
        self.get_directories()


    ''' Methods (functions) for PageOne '''
    # UI, where the user can selected data, that shall be opened
    def get_directories(self):
        # open files
        file_input = tkFileDialog.askopenfilenames(** self.file_opt)
        file_input = sorted(list(file_input))
        # create dictionary 
        file_input_dict = {}
        file_input_dict[""Input_Directories""] = file_input
        self.push_directories(file_input_dict) 


''' 
############################################################################### 
#                                   View                                      # 
###############################################################################
'''


class StartPage(ttk.Frame):

    ''' Initialisations '''
    def __init__(self, parent, controller):
        ttk.Frame.__init__(self, parent)
        self.controller = controller

        self.labels()
        self.buttons()


    ''' Widgets '''        
    def labels(self):
        label = tk.Label(self, text = ""This is the start page"", font = font)
        label.pack(side = ""top"", fill = ""x"", pady = 10)

    def buttons(self):
        button1 = ttk.Button(self, text = ""Go to Page One"",
                            command = lambda: self.controller.show_frame(""PageOne""))
        button2 = ttk.Button(self, text = ""Go to Page Two"",
                            command = lambda: self.controller.show_frame(""PageTwo""))
        button_close = ttk.Button(self, text = ""Close"",
                                command = lambda: app.destroy())                    
        button1.pack(side = ""top"", fill = ""x"", pady = 10)
        button2.pack(side = ""top"", fill = ""x"", pady = 10)
        button_close.pack(side = ""top"", fill = ""x"", pady = 10)


class PageOne(ttk.Frame):

    ''' Initialisations '''
    def __init__(self, parent, controller):
        ttk.Frame.__init__(self, parent)
        self.controller = controller

        self.labels()
        self.buttons()
        self.combobox()

    ''' Widgets '''
    def labels(self):
        label = tk.Label(self, text = ""On this page, you can read data"", font = font)
        label.pack(side = ""top"", fill = ""x"", pady = 10)

    def buttons(self): 
        button_open = ttk.Button(self, text = ""Open"", 
                                 command = lambda: self.controller.open_button())
        button_forward = ttk.Button(self, text = ""Next Page >>"",
                                command = lambda: self.controller.show_frame(""PageTwo""))
        button_back = ttk.Button(self, text = ""<< Go back"",
                                command = lambda: self.controller.show_frame(""StartPage""))
        button_home = ttk.Button(self, text = ""Home"",
                                command = lambda: self.controller.show_frame(""StartPage""))
        button_close = ttk.Button(self, text = ""Close"",
                                command = lambda: app.destroy())
        button_open.pack(side = ""top"", fill = ""x"", pady = 10)
        button_forward.pack(side = ""top"", fill = ""x"", pady = 10)
        button_back.pack(side = ""top"", fill = ""x"", pady = 10)
        button_home.pack(side = ""top"", fill = ""x"", pady = 10)
        button_close.pack(side = ""top"", fill = ""x"", pady = 10)

    def combobox(self):                                  
        entries = ("""", ""Inputformat_01"", ""Inputformat_02"", ""Inputformat_03"") 
        combobox = ttk.Combobox(self, state = 'readonly', values = entries,
                                     textvariable = self.controller.shared_keys[""Inputformat""])
        combobox.current(0)
        combobox.bind('<<ComboboxSelected>>', self.updater)
        combobox.pack(side = ""top"", fill = ""x"", pady = 10)


    ''' Bindings '''
    # wrapper, which notifies the controller, that it can update keys in Model
    def updater(self, event):
        self.controller.push_keys()



class PageTwo(ttk.Frame):

    ''' Initialisations '''
    def __init__(self, parent, controller):
        ttk.Frame.__init__(self, parent)
        self.controller = controller

        self.labels()
        self.buttons()
        self.combobox()


    ''' Widgets '''        
    def labels(self):
        label = tk.Label(self, text = ""This is page 2"", font = font)
        label.pack(side = ""top"", fill = ""x"", pady = 10)

    def buttons(self):
        button_back = ttk.Button(self, text = ""<< Go back"",
                                command = lambda: self.controller.show_frame(""PageOne""))
        button_home = ttk.Button(self, text = ""Home"",
                                command = lambda: self.controller.show_frame(""StartPage""))
        button_close = ttk.Button(self, text = ""Close"",
                                command = lambda: app.destroy())                        
        button_back.pack(side = ""top"", fill = ""x"", pady = 10)
        button_home.pack(side = ""top"", fill = ""x"", pady = 10)
        button_close.pack(side = ""top"", fill = ""x"", pady = 10)

    def combobox(self):
        entries = (""Outputformat_01"", ""Outputformat_02"") 
        combobox = ttk.Combobox(self, state = 'readonly', values = entries,
                                     textvariable = self.controller.shared_keys[""Outputformat""])
        combobox.bind('<<ComboboxSelected>>', self.updater)
        combobox.pack(side = ""top"", fill = ""x"", pady = 10)


    ''' Bindings '''
    # wrapper, which notifies the controller, that it can update keys in Model
    def updater(self, event):
        self.controller.push_keys()



if __name__ == ""__main__"":
    app = PageControl()
    app.mainloop()

",2729
38981503,38981503,1,"I have created a user registration form in Django but every time I test it, the user is not being saved to the database because when the password is entered it raises ""this field can not be null. I do not understand why this is happening since the form was working before, and it follows the same procedures as other registration forms. I have been trying to figure out the cause of the issue for days and I am still not  understanding how the field can be null and still wont work. However in the terminal I am not getting any error messages, and the login form is working because users are created via the admin. Thanks for the help and  here is the code I am using.
the user form
class UserForm(forms.ModelForm):
    email = forms.EmailField(label=""Email Address"")
    password = forms.CharField(widget=forms.PasswordInput)

    class Meta:
        model = User
        fields = ['first_name', 'last_name', 'username', 'email', 'password']

    def clean_email(self):
        email = self.cleaned_data.get('email')
        email_qs = User.objects.filter(email=email)
        if email_qs.exists():
            raise forms.ValidationError(""This email already exists."")
        return email

    def clean_username(self):
        username = self.cleaned_data.get('username')
        username_qs = User.objects.filter(username=username)
        if username_qs.exists():
            raise forms.ValidationError(""This username already exists."")
        return username

    def clean_password(self):
        password = self.cleaned_data.get('password')
        if len(password) <= 8:
            raise forms.ValidationError(""Password must be longer than 8 characters"")
        else:
            print(""password saved"")

My view registration function
def register_view(request):
    form = UserForm(request.POST or None)
    if form.is_valid():
        user = form.save(commit=False)
        username = form.cleaned_data['username']
        password = form.cleaned_data['password']
        user.set_password(password)
        user.save()
        new_user = authenticate(username=username, password=password)
        if user is not None:
            if user.is_active:
                login(request, new_user)
                return redirect('/appname/account/')
    return render(request, ""appname/index.html"", {""form"": form})

My html
% block signup %}
  <div clss=""container-fluid"">
    <div class=""row"">
      <div class=""col-sm-6"" style=""background-color:yellow;"">
        <h1>appname</h1>
        <p>x and x</p>
      </div>
      <div>
        <div class=""col-sm-6"">
          <h3>Sign Up</h1>
            <form method=""POST"" action=""{% url 'appname:index' %}"">
              {% csrf_token %}
              {{ form }}
              <input type=""submit"" class=""btn"" value=""Sign Up""/>
            </form>

            <h5>Have an Account? <a href=""{% url 'appname:login' %}"">Login</a></h5>
        </div>
      </div>
   </div>
</div>
{% endblock %}

",567
38981503,38981919,2,"If you clean a django form field (using def clean_myfield) you need to return that field
def clean_password(self):
    password = self.cleaned_data.get('password')
    if len(password) <= 8:
        raise forms.ValidationError(""Password must be longer than 8 characters"")
    else:
        print(""password saved"")
    return password  #  <--  add this line

",70
38981700,38981700,1,"I need to override createsuperuser.py's handle method in Django Command class.
I created myapp\management\commands\createsuperuser.py:
import getpass
import sys

import django.contrib.auth.management.commands.createsuperuser as makesuperuser

from django.contrib.auth.management import get_default_username
from django.contrib.auth.password_validation import validate_password
from django.core import exceptions
from django.core.management.base import CommandError
from django.utils.encoding import force_str
from django.utils.text import capfirst

class Command(makesuperuser.Command):
    def handle(self, *args, **options):
        # the rest of code is copied from Django source and is almost
        # standart except few changes related to how info of
        # REQUIRED_FIELDS is shown

When I do in terminal ./manage.py createsuperuser I do not see any changes. If I change the name of my file to lets say mycmd.py and do ./manage.py mycmd everything starts to work as I expect.
How to get changes I need using ./manage.py createsuperuser?
",140
38981700,38982236,2,"Put your application name on top in the INSTALLED_APPS list. 
",11
38981765,38981765,1,"EDIT
This is my code:
import pyperclip
text_file = open(""dewey.txt"", ""rt"")                     #Open the list containing the values
text = text_file.read().split('\n')
text_file.close()


num = []
with open ('dewey num.txt', 'rt') as in_file:           #Open the list containing the keys
    for line in in_file:
        num.append(line[:3])
intenum = []
for i in num:
    intenum.append(int(i))                              #Make the keys as integers


dict1= dict(zip(intenum,text))                          #Merge the 2 lists in a dictionary
print ('This software will give you the subjects for each stack.')                                  #Instructions for the user
print ('I need to know the range first, please only use the first three numbers of the classmark.') #Instructions for the user

x=input('Please key in the starting number for this stack: ')                                       #Input for start
y=input('Please key in the last number for this stack: ')                                           #Input for stop

start = int(x)
stop = int(y)

values = [dict1[k] for k in range(start, stop + 1) if k in dict1]                                   #Call the values in range
print('The subject(s) for this section: ')
for i in values:                                                                                    #Print the values in range
    print (i)
output = '\n'.join (values)
pyperclip.copy(output)

It's working as it should within python, so I compiled it to .exe using Pyinstaller.
I managed to have an exe file now but I'm struggling with the import of pyperclip.
the exe works except for that last command. I tried -p dir and also --hidden-import pyperclip.
I ran it with -d and the debug I get is:
PyInstaller Bootloader 3.x
LOADER: executable is C:\Users\aless\AppData\Local\Programs\Python\Python35\dist\deweycheck\deweycheck.exe
LOADER: homepath is C:\Users\aless\AppData\Local\Programs\Python\Python35\dist\deweycheck
LOADER: _MEIPASS2 is NULL
LOADER: archivename is C:\Users\aless\AppData\Local\Programs\Python\Python35\dist\deweycheck\deweycheck.exe
LOADER: No need to extract files to run; setting extractionpath to homepath
LOADER: SetDllDirectory(C:\Users\aless\AppData\Local\Programs\Python\Python35\dist\deweycheck)
LOADER: Already in the child - running user's code.
LOADER: Python library: C:\Users\aless\AppData\Local\Programs\Python\Python35\dist\deweycheck\python35.dll
LOADER: Loaded functions from Python library.
LOADER: Manipulating environment (sys.path, sys.prefix)
LOADER: Pre-init sys.path is C:\Users\aless\AppData\Local\Programs\Python\Python35\dist\deweycheck\base_library.zip;C:\Users\aless\AppData\Local\Programs\Python\Python35\dist\deweycheck
LOADER: sys.prefix is C:\Users\aless\AppData\Local\Programs\Python\Python35\dist\deweycheck
LOADER: Setting runtime options
LOADER: Initializing python
LOADER: Overriding Python's sys.path
LOADER: Post-init sys.path is C:\Users\aless\AppData\Local\Programs\Python\Python35\dist\deweycheck\base_library.zip;C:\Users\aless\AppData\Local\Programs\Python\Python35\dist\deweycheck
LOADER: Setting sys.argv
LOADER: setting sys._MEIPASS
LOADER: importing modules from CArchive
LOADER: extracted struct
LOADER: callfunction returned...
LOADER: extracted pyimod01_os_path
LOADER: callfunction returned...
LOADER: extracted pyimod02_archive
LOADER: callfunction returned...
LOADER: extracted pyimod03_importers
LOADER: callfunction returned...
LOADER: Installing PYZ archive with Python modules.
LOADER: PYZ archive: out00-PYZ.pyz
LOADER: Running pyiboot01_bootstrap.py
LOADER: Running dewey.py

I'm not sure how to proceed right now :\ any input?
",567
38981765,39013958,2,"In the end I managed to import all the dependencies by using the command 'pyi-makespec' which then required an additional step to make the actual exe file. The documentation for pyinstaller is really extensive and accurate. Thanks @Repiklis for your inputs
",45
38981814,38981814,1,"I'm new to using Classes in Python, and could use some guidance on what resources to consult/how to use a class in a loop.
Sample data:
df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))
df2 = pd.DataFrame(np.random.randint(0, 1, size=(100, 1)), columns=list('E'))
df['E']= df2

here's the code outside of a class:
styles = [1, 3, 7]

def train_model(X, y):
    clf = LogisticRegression(random_state=0, C=1, penalty='l1')
    clf.fit(X, y)

for value in styles:
    X = df[['A', 
            'B',
            'C']][df['D']==value]
    y = df['E'][df['D']==value]
    train_model(X, y)   

I need to translate this into a class, like so:
class BaseTrainer(object):
"""""" Abstract class to define run order """"""

    def run(self):
        self.import_training_data()
        for value in [1, 3, 7]:
            self.extract_variables(value)
            self.train_model()
            # I think there's a better way to do this
            if value = 1:
                pickle_model(self.model, self.model_file)
            if value = 3:
                pickle_model(self.model, self.model_file2)
            if value = 7:
                pickle_model(self.model, self.model_file3)


class ModelTrainer(BaseTrainer):
"""""" Class to train model for predicting Traits of Customers """"""

     def __init__(self):
        self.model_file = '/wayfair/mnt/crunch_buckets/central/data_science/customer_style/train_modern.pkl'
        self.model_file2 = '/wayfair/mnt/crunch_buckets/central/data_science/customer_style/train_traditional.pkl'
        self.model_file3 = '/wayfair/mnt/crunch_buckets/central/data_science/customer_style/train_rustic.pkl'

def import_training_data(self):
    _execute_vertica_query('get_training_data')

    self.df = _read_data('training_data.csv')
    self.df.columns = [['CuID', 'StyID', 'StyName', 
    'Filter', 'PropItemsViewed', 'PropItemsOrdered', 'DaysSinceView']]

def extract_variables(self, value):
    # Take subset of columns for training purposes (remove CuID, Segment)
    self.X = self.df[['PropItemsViewed', 'PropItemsOrdered', 
    'DaysSinceView']][df['StyID']==value]

    y = self.df[['Filter']][df['StyID']==value]
    self.y = y.flatten()

def train_model(self):
    self.model = LogisticRegression(C=1, penalty='l1')

    self.model.fit(self.X, self.y)

I think there must be a better way to structure it or run through the three different values in the styles list.  But I don't even know what to search for to improve this.  Any suggestions, pointers, etc. would be appreciated!  
",492
38981814,38982181,2,"An elegant way to do it is to iterate through both lists at the same time using zip
def run(self):
    self.import_training_data()
    for value,model_file in zip([1, 3, 7],[self.model_file, self.model_file2, self.model_file3]):
        self.extract_variables(value)
        self.train_model()

        pickle_model(self.model, model_file)

As for the design it could be improved
For instance, define your model files as a list directly:
self.model_list = map(lambda x : os.path.join('/wayfair/mnt/crunch_buckets/central/data_science/customer_style',x),['train_modern.pkl','train_traditional','train_rustic.pkl'])

Which gives:
def run(self):
    self.import_training_data()
    for value,model_file in zip([1, 3, 7],self.model_file_list):
        self.extract_variables(value)
        self.train_model()

",147
38981814,38981970,2,"You could just enumerate the files like so
files = [self.model_file, self.model_file2, self.model_file3]
values = [1 ,5 ,7]
for n in range(len(value)):  
    pickle_model(self.model, files[n]) 

Does this answer the question?
",50
38981847,38981959,2,"The easy way to ensure that dependent modules are installed is through pip -r.
Essentially, make a requirements.txt along with your script for users to install the correct modules and versions. 
Inside the text file should look like this:
Flask==0.11.1

You can use:
pip freeze

To find which modules you have installed
EDIT: This is implied your script is small and not packaged inside a .dmg or .exe file. 
",75
38981847,38982133,2,"numpy, scipy takes a long time to install in a virtualenv.  For this reason I would not recommend virtualenv like others have.  You could try pyinstaller to create an OS specific executable.  Haven't tried it with numpy or scipy myself though.
http://www.pyinstaller.org
",50
38981847,38981847,1,"I apologise in advance, as I am rather new to Python programming, but I was curious as to how this system works. My question is: if I write a Python script and make it distributable, but my program imports other external libraries such as numpy or scipy (which is what I am working on currently) how does it all work together? It is my understanding that the user would still have to install the libraries separately or I need to write a separate makefile that runs a script to install it while my distributable is being installed. Am  I right in this opinion? Advice would be greatly appreciated. Also a explanation how it works internally given your answer. Thanks a lot! Appreciate your time! 
",138
38981885,38982172,2,"You can use agg function after the groupby:
df.groupby(['name', 'age'])['phonenumber'].agg({'phonecount': pd.Series.nunique, 'phonenumber': lambda x: ','.join(x)})

#               phonenumber  phonecount
# name  age     
#    A   10   Phone1,Phone2           2
#    B   21 PhoneB1,PhoneB2           2
#    C   23          PhoneC           1

Or a shorter version according to @root and @Jon Clements:
df.groupby(['name', 'age'])['phonenumber'].agg({'phonecount': 'nunique', 'phonenumber': ','.join})

",114
38981885,38981885,1,"I have some entries in dataframe like :
name, age, phonenumber
 A,10, Phone1
 A,10,Phone2
 B,21,PhoneB1
 B,21,PhoneB2
 C,23,PhoneC

Here is what I am trying to achieve as result of pivot table:
 name, age, phonenumbers, phonenocount
 A,10, ""Phone1,Phone2"" , 2
 B,21,  ""PhoneB1,PhoneB2"", 2
 C,23, ""PhoneC"" , 1

I was trying something like :
pd.pivot_table(phonedf, index=['name','age','phonenumbers'], values=['phonenumbers'], aggfunc=np.size)

however I want the phone numbers to be concatenated as part of aggfunc. 
Any Suggestions ?
",121
38981912,38983726,2,"Here's a compact vectorized approach without those error checks -
def unique_map_pixels_vectorized(imgs):
    N,H,W = len(imgs), imgs.shape[2], imgs.shape[3]
    img2D = imgs.transpose(0, 2, 3, 1).reshape(-1,3)
    ID = np.ravel_multi_index(img2D.T,img2D.max(0)+1)
    _, firstidx, tags = np.unique(ID,return_index=True,return_inverse=True)
    return tags.reshape(N,H,W), img2D[firstidx]

Runtime test and verification -
In [24]: # Setup inputs (3x smaller than original ones)
    ...: N,H,W = 200,24,32
    ...: imgs = np.random.randint(0,10,(N,3,H,W))
    ...: 

In [25]: %timeit unique_map_pixels(imgs)
1 loop, best of 3: 2.21 s per loop

In [26]: %timeit unique_map_pixels_vectorized(imgs)
10 loops, best of 3: 37 ms per loop ## 60x speedup!

In [27]: map1,unq1 = unique_map_pixels(imgs)
    ...: map2,unq2 = unique_map_pixels_vectorized(imgs)
    ...: 

In [28]: np.allclose(map1,map2)
Out[28]: True

In [29]: np.allclose(np.array(map(list,unq1)),unq2)
Out[29]: True

",256
38981912,38981912,1,"Lets say one has 600 annotated semantic segmentation mask images, which contain 10 different colors, each representing one entity. These images are in a numpy array of shape (600, 3, 72, 96), where n = 600, 3 = RGB channels, 72 = height, 96 = width. 
How to map each RGB-pixel in the numpy array to a color-index-value? For example, a color list would be [(128, 128, 0), (240, 128, 0), ...n], and all (240, 128, 0) pixels in the numpy array would be converted to index value in unique mapping (= 1).
How to do this efficiently and with less code? Here's one solution I came up with, but it's quite slow.
# Input imgs.shape = (N, 3, H, W), where (N = count, W = width, H = height)
def unique_map_pixels(imgs):
  original_shape = imgs.shape

  # imgs.shape = (N, H, W, 3)
  imgs = imgs.transpose(0, 2, 3, 1)

  # tupleview.shape = (N, H, W, 1); contains tuples [(R, G, B), (R, G, B)]
  tupleview = imgs.reshape(-1, 3).view(imgs.dtype.descr * imgs.shape[3])

  # get unique pixel values in images, [(R, G, B), ...]
  uniques = list(np.unique(tupleview))

  # map uniques into hashed list ({""RXBXG"": 0, ""RXBXG"": 1}, ...)
  uniqmap = {}
  idx = 0
  for x in uniques:
    uniqmap[""%sX%sX%s"" % (x[0], x[1], x[2])] = idx
    idx = idx + 1
    if idx >= np.iinfo(np.uint16).max:
      raise Exception(""Can handle only %s distinct colors"" % np.iinfo(np.uint16).max)

  # imgs1d.shape = (N), contains RGB tuples
  imgs1d = tupleview.reshape(np.prod(tupleview.shape))

  # imgsmapped.shape = (N), contains uniques-index values
  imgsmapped = np.empty((len(imgs1d))).astype(np.uint16)

  # map each pixel into unique-pixel-ID
  idx = 0
  for x in imgs1d:
    str = (""%sX%sX%s"" % (x[0], x[1] ,x[2]))
    imgsmapped[idx] = uniqmap[str]
    idx = idx + 1

  imgsmapped.shape = (original_shape[0], original_shape[2], original_shape[3]) # (N, H, W)
  return (imgsmapped, uniques)

Testing it:
import numpy as np
n = 30
pixelvalues = (np.random.rand(10)*255).astype(np.uint8)
images = np.random.choice(pixelvalues, (n, 3, 72, 96))

(mapped, pixelmap) = unique_map_pixels(images)
assert len(pixelmap) == mapped.max()+1
assert mapped.shape == (len(images), images.shape[2], images.shape[3])
assert pixelmap[mapped[int(n*0.5)][60][81]][0] == images[int(n*0.5)][0][60][81]
print(""Done: %s"" % list(mapped.shape))

",653
38981915,38981915,1,"I've installed a virtualenv with pyenv using Python v2.7.12. Inside this virtualenv, I installed matplotlib v1.5.1 via:
pip install matplotlib

with no issues. The problem is that a simple
import matplotlib.pyplot as plt
plt.scatter([], [])
plt.show()

script fails to produce a plot window. The backend that I see in the virtualenv using:
import matplotlib
print matplotlib.rcParams['backend']

is agg, which is apparently the root cause of the issue. If I check the backend in my system-wide installation, I get Qt4Agg (and the above script when run shows a plot window just fine).
There are already several similar questions in SO, and I've tried the solutions given in all of them.

Matplotlib plt.show() isn't showing graph
Tried to create the virtualenv with the --system-site-packages option. No go.
How to ensure matplotlib in a Python 3 virtualenv uses the TkAgg backend?
Installed sudo apt install tk-dev, then re-installed using pip --no-cache-dir install -U --force-reinstall matplotlib. The backend still shows as agg.
Matplotlib doesn't display graph in virtualenv
Followed install instructions given in this answer, did nothing (the other answer involves using easy_install, which I will not do)
matplotlib plot window won't appear
The solution given here is to ""install a GUI library (one of Tkinter, GTK, QT4, PySide, Wx)"". I don't know how to do this. Furthermore, if I use:
import matplotlib.rcsetup as rcsetup
print(rcsetup.all_backends)

I get:
[u'GTK', u'GTKAgg', u'GTKCairo', u'MacOSX', u'Qt4Agg', u'Qt5Agg', u'TkAgg', u'WX', u'WXAgg', u'CocoaAgg', u'GTK3Cairo', u'GTK3Agg', u'WebAgg', u'nbAgg', u'agg', u'cairo', u'emf', u'gdk', u'pdf', u'pgf', u'ps', u'svg', u'template']

meaning that all those backends are available in my system (?).
matplotlib does not show my drawings although I call pyplot.show()
My matplotlibrc file shows the line:
backend      : Qt4Agg

I don't know how to make the virtualenv aware of this?

Some of the solutions involve creating links to the system version of matplotlib (here and here), which I don't want to do. I want to use the version of matplotlib installed in the virtualenv. 
If I try to set the backend with:
import matplotlib
matplotlib.use('GTKAgg')

I get ImportError: Gtk* backend requires pygtk to be installed (same with GTK). But if I do sudo apt-get install python-gtk2 python-gtk2-dev, I see that they are both installed.
Using:
import matplotlib
matplotlib.use('Qt4Agg')

(or Qt5Agg) results in ImportError: Matplotlib qt-based backends require an external PyQt4, PyQt5, or PySide package to be installed, but it was not found. Not sure if I should install some package?
Using:
import matplotlib
matplotlib.use('TkAgg')

results in ImportError: No module named _tkinter, but sudo apt-get install python-tk says that it is installed.
Using:
import matplotlib
matplotlib.use('GTKCairo')

results in ImportError: No module named gtk. So I try sudo apt-get install libgtk-3-dev but it says that it already installed.
How can I make the virtualenv use the same backend that my system is using?
",624
38981915,38988308,2,"You can consider changing your backend to TkAgg in the Python 2 virtualenv by running the following:
sudo apt install python-tk  # install Python 2 bindings for Tk
pip --no-cache-dir install -U --force-reinstall matplotlib  # reinstall matplotlib   

To confirm the backend is indeed TkAgg, run
python -c 'import matplotlib as mpl; print(mpl.get_backend())'

and you should see TkAgg.
",69
38981977,38981977,1,"code to make test data:
 import pandas as pd
 df = pd.DataFrame({'A': pd.date_range(start='1-1-2016',periods=5, freq='M')})
 df['B'] = df.A.dt.month
 print(df)

data looks like
   B          A
0  1     2016-01-31
1  2     2016-02-29
2  3     2016-03-31
3  4     2016-04-30
4  5     2016-05-31

how to shift column A backwards by number of months as value from column B
to the effect of 
 df['A'] - pd.DateOffset(months=value_from_column_B)

",90
38981977,38982372,2,"You can try:
df['C'] = df[['A', 'B']].apply(lambda x: x['A'] - pd.DateOffset(months=x['B']), axis=1)

",42
38981977,38983098,2,"Here is a vectorized way to compose arrays of dates (NumPy datetime64s) out of
date components (such as years, months, days):
import numpy as np
import pandas as pd

def compose_date(years, months=1, days=1, weeks=None, hours=None, minutes=None,
                 seconds=None, milliseconds=None, microseconds=None, nanoseconds=None):
    years = np.asarray(years) - 1970
    months = np.asarray(months) - 1
    days = np.asarray(days) - 1
    types = ('<M8[Y]', '<m8[M]', '<m8[D]', '<m8[W]', '<m8[h]',
             '<m8[m]', '<m8[s]', '<m8[ms]', '<m8[us]', '<m8[ns]')
    vals = (years, months, days, weeks, hours, minutes, seconds,
            milliseconds, microseconds, nanoseconds)
    return sum(np.asarray(v, dtype=t) for t, v in zip(types, vals)
               if v is not None)

df = pd.DataFrame({'A': pd.date_range(start='1-1-2016',periods=5, freq='M')})
df['B'] = df['A'].dt.month
df['C'] = compose_date(years=df['A'].dt.year, 
                       months=df['A'].dt.month-df['B'], 
                       days=df['A'].dt.day)
print(df)
#            A  B          C
# 0 2016-01-31  1 2015-12-31
# 1 2016-02-29  2 2015-12-29
# 2 2016-03-31  3 2015-12-31
# 3 2016-04-30  4 2015-12-30
# 4 2016-05-31  5 2015-12-31


In [135]: df = pd.DataFrame({'A': pd.date_range(start='1-1-2016', periods=10**3, freq='M')})

In [136]: df['B'] = df['A'].dt.month

In [137]: %timeit compose_date(years=df['A'].dt.year, months=df['A'].dt.month-df['B'], days=df['A'].dt.day)
10 loops, best of 3: 41.2 ms per loop

In [138]: %timeit df[['A', 'B']].apply(lambda x: x['A'] - pd.DateOffset(months=x['B']), axis=1)
10 loops, best of 3: 169 ms per loop

",451
38982002,38982002,1,"I have am writing a program that accepts arguements in the following form.
SYNOPSIS
pdf_form.py <input PDF file | - | PROMPT> [ <operation> <operation arguments> ]
    [ output <output filename | - | PROMPT> ] [ flatten ]
Where:
    <operation> may be empty, or: [generate_fdf | fill_form |dump_data | dump_data_fields ]
OPTIONS
--help, -h
    show summary of options.

<input PDF files | - | PROMPT>
    An input PDF file. Use - to pass a single PDF into pdftk via stdin.

[<operation> <operation arguments>]
    Available operations are:
    generate_fdf,fill_form, dump_data,dump_data_fields
    Some operations takes additional arguments, described below.

    generate_fdf
        Reads a single, input PDF file and generates an FDF file suitable for fill_form out of it
        to the given output filename or (if no output is given) to stdout. Does not create a new PDF.

    fill_form <FDF data filename | - | PROMPT>
        Fills the input PDF's form fields with the data from an FDF file, or stdin.
        Enter the data filename after fill_form, or use - to pass the data via stdin, like so:

        ./pdf_form.py form.pdf fill_form data.fdf output form.filled.pdf

        After filling a form, the form fields remain interactive unless flatten is used.
        flatten merges the form fields with the PDF pages. You can also use flatten alone, as shown:

        ./pdf_form.py form.pdf fill_form data.fdf output out.pdf flatten

        or:

        ./pdf_form.py form.filled.pdf output out.pdf flatten

    dump_data
        Reads a single, input PDF file and reports various statistics,  to the given output filename or (if no output is given).

    dump_data_fields
         Reads a single, input PDF file and reports form field statistics to the given output filename

[flatten]
    Use this option to merge an input PDF's interactive form fields and their data with the PDF's pages.


I am currently parsing these options using the following (messy) code
def parse(arg):
    """""" Parses commandline arguments. """"""
    #checking that request is valid
    if len(arg) == 1:
        raise ValueError(info())
    if arg[1] in ('-h', '--help'):
        print(info(verbose=True))
        return
    if len(arg) == 2:
        raise ValueError(info())

    path = arg[1]
    if path == 'PROMPT':
        path = input('Please enter a filename for an input document:')

    func = arg[2]

    if func == 'fill_form':
        fdf = arg[3]
        if len(arg) > 5 and arg[4] == 'output':
            out = arg[5]
        else:
            out = '-'
    else:
        fdf = None
        if len(arg) > 4 and arg[3] == 'output':
            out = arg[4]
        else:
            out = '-'

    if out == 'PROMPT':
        out = input('Output file: ')

    flatten = 'flatten' in arg

    return path, func, fdf, out, flatten

Is there a better way to do this with either argparse or docopt?
",583
38982002,38984485,2,"One argument could be
parser.add_argument('-o','--output', default='-')

and later
if args.output in ['PROMPT']:
    ... input...

others:
parser.add_argument('--flatten', action='store_true')
parser.add_argument('--fill_form', dest='ftd')
parser.add_argument('--path')

if args.path in ['PROMPT']:
     args.path = input...

",71
38982763,38982763,1,"I wrote this very simple code, it doesn't work and I can't understand why. I answer 'good' but if part doesn't work!
def howdy ():
    p = input(""how's it going?"")
    if p is ""good"":
        print(""Cheers"")
howdy()

",62
38982763,38982810,2,"have you tried
if p == ""good"":

the double equal means the same as.
Also, if you are looking for a helpful free beginner course, try codecademy. They helped me loads learn python
",40
38982763,38982857,2,"You have to use:

If p == 'good':

The == compares the value of the two. The Is keyword checks for identity by comparing  the memory address. 
Hope this explains it enough. 
Hannes
",39
38982776,38983968,2,"from random import *

m = [[0,0,0],
    [0,0,0]]
A = 20
B = 25

x = 1     #or other number, not relevant
rows = len(m)
cols = len(m[0])

def runner(list1, a1, b1, x1):
    list1_backup = list(list1)
    rows = len(list1)
    cols = len(list1[0])

    for r in range(rows): 
        while sum(list1[r]) <= a1:  
            c = randint(0, cols-1)
            list1[r][c] += x1

    for c in range(cols): 
        cant = sum([list1[r][c] for r in range(rows)])
        while cant >= b1:
            r = randint(0, rows-1)
            if list1[r][c] >= x1:  #I don't want negatives
                list1[r][c] -= x1
    good_a_int = 0
    for r in range(rows):
        test1 = sum(list1[r]) > a1
        good_a_int += 0 if test1 else 1

    if good_a_int == 0:
        return list1
    else:
        return runner(list1=list1_backup, a1=a1, b1=b1, x1=x1)

m2 = runner(m, A, B, x)
for row in m:
    print ','.join(map(lambda x: ""{:>3}"".format(x), row))

",277
38982776,38982776,1,"I have a list of lists m which I need to modify
I need that the sum of each row to be greater than A and the sum of each column to be lesser than B 
I have something like this
x = 5     #or other number, not relevant
rows = len(m)
cols = len(m[0])

for r in range(rows): 
    while sum(m[r]) < A:  
        c = randint(0, cols-1)
        m[r][c] += x 

for c in range(cols): 
    cant = sum([m[r][c] for r in range(rows)])
    while cant > B:
        r = randint(0, rows-1)
        if m[r][c] >= x:  #I don't want negatives
            m[r][c] -= x

My problem is: I need to satisfy both conditions and, this way, after the second for I won't be sure if the first condition is still met.
Any suggestions on how to satisfy both conditions and, of course, with the best execution? I could definitely consider the use of numpy
Edit (an example)
#input
m = [[0,0,0],
    [0,0,0]]
A = 20
B = 25

# one desired output (since it chooses random positions)
m = [[10,0,15],
    [15,0,5]]

I may need to add 
This is for the generation of the random initial population of a genetic algorithm, the restrictions are to make them a possible solution, and I would need to run this like 80 times to get different possible solutions
",320
38982776,38983999,2,"Something like this should to the trick:
import numpy
from scipy.optimize import linprog

A = 10
B = 20
m = 2
n = m * m

# the coefficients of a linear function to minimize.
# setting this to all ones minimizes the sum of all variable
# values in the matrix, which solves the problem, but see below.
c = numpy.ones(n)

# the constraint matrix.
# This is matrix-multiplied with the current solution candidate
# to form the left hand side of a set of normalized 
# linear inequality constraint equations, i.e.
#
# x_0 * A_ub[0][0] + x_1 * A_ub[0][1] <= b_0
# x_1 * A_ub[1][0] + x_1 * A_ub[1][1] <= b_1
# ...
A_ub = numpy.zeros((2 * m, n))

# row sums. Since the <= inequality is a fixed component,
# we just multiply everthing by (-1), i.e. we demand that
# the negative sums are smaller than the negative limit -A.
#
# Assign row ranges all at once, because numpy can do this.
for r in xrange(0, m):
    A_ub[r][r * m:(r + 1) * m] = -1

# We want that the sum of the x  in each (flattened)
# column is smaller than B
#
# The manual stepping for the column sums in row-major encoding
# is a little bit annoying here.
for r in xrange(0, m):
    for j in xrange(0, m):
        A_ub[r + m][r + m * j] = 1

# the actual upper limits for the normalized inequalities.
b_ub = [-A] * m + [B] * m

# hand the linear program to scipy
solution = linprog(c, A_ub=A_ub, b_ub=b_ub)

# bring the solution into the desired matrix form
print numpy.reshape(solution.x, (m, m))

Caveats

I use <=, not < as indicated in your question, because that's what numpy supports.
This minimizes the total sum of all values in the target vector.
For your use case, you probably want to minimize the distance
to the original sample, which the linear program cannot handle, since neither the squared error nor the absolute difference can be expressed using a linear combination (which is what c stands for). For that, you will probably need to go to full minimize(). 

Still, this should get you rough idea.
",494
38982776,38983417,2,"A NumPy solution:
import numpy as np

val = B / len(m) # column sums <= B
assert val * len(m[0]) >= A # row sums >= A

# create array shaped like m, filled with val
arr = np.empty_like(m)
arr[:] = val

I chose to ignore the original content of m - it's all zero in your example anyway.
",82
38982784,38982984,2,"Edit: Also, your server isn't accepting/listing for connections
You should make the server multithreaded so that it can send and receive at the same time. Here's how it might look:
import socket
import os
from threading import Thread
import thread

def main():
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    s.bind((host,port))
    s.listen(10)
    serverThreads = []

    while True:
        print ""Server is listening for connections...""
        client, address = s.accept()
        serverThreads.append(Thread(target=runserver, args=(client,address)).start())
    s.close()

def runserver(client, address):
    clients = set()
    lockClients = threading.Lock()
    print (""Connection from: "" + address)
    with lockClients:
        clients.add(client)
    try:    
        while True:
            data = client.recv(1024)
            if data:
                print data.decode()
                with lockClients:
                    for c in clients:
                        c.sendall(data)
            else:
                break
    finally:
        with lockClients:
            clients.remove(client)
            client.close()

When you send a string over TCP you need to encode it to bytes. So your client file should look like this instead:
def run(self):
    self._client.send(('Welcome to the chatroom!').encode())
    self._name = self._client.recv(BUFSIZE).decode()
    self._client.send(str(self._record),CODE)
    while True:
        message = self._client.recv(BUFSIZE).decode()
        if not message:
            print('Client disconnected')
            self._client.close()
            break
        else:
            message=self._name +'' + \
                    ctime()+'\n'+message
            self._record.add(message)
            self._client.send(self._record.encode())

I much prefer the .encode() and .decode() syntax as it makes the code a little more readable IMO.
",335
38982784,38982784,1,"I am writing a multi-chat which consists of the Client handler, the server and chat record. The Client should allow multiple chats. At the moment it doesn't allow for even one chat, I get an error message after the name has been entered. 
This is the Client handler file
from socket import*
from codecs import decode
from chatrecord import ChatRecord
from threading import Thread
from time import ctime

class ClientHandler (Thread):

def __init__(self, client, record):
     Thread.__init__(self)
     self._client = client
     self._record = record

def run(self):
    self._client.send(str('Welcome to the chatroom!'))
    self._name = decode(self._client.recv(BUFSIZE),CODE)
    self._client.send(str(self._record),CODE)
    while True:
        message = decode(self._client.recv(BUFSIZE),CODE)
        if not message:
            print('Client disconnected')
            self._client.close()
            break
        else:
            message=self._name +'' + \
                    ctime()+'\n'+message
            self._record.add(message)
            self._client.send(str(self._record),CODE)

HOST ='localhost'
PORT = 5000
ADDRESS = (HOST,PORT)
BUFSIZE = 1024
CODE = 'ascii'
record = ChatRecord()
server = socket(AF_INET,SOCK_STREAM)
server.bind(ADDRESS)
server.listen(5)

while True:
    print ('Waiting for connection...')
    client,address = server.accept()
    print ('...connected from:',address)
    handler = ClientHandler(client,record)
    handler.start() 

This is the server file
from socket import *
from codecs import decode

HOST ='localhost'
PORT = 5000
BUFSIZE = 1024
ADDRESS = (HOST,PORT)
CODE = 'ascii'

server = socket(AF_INET,SOCK_STREAM)
server.connect(ADDRESS)
print (decode(server.recv(BUFSIZE), CODE))
name = raw_input('Enter your name:')
server.send(name)

while True:
    record = server.recv(BUFSIZE)
    if not record:
        print ('Server disconnected')
        break
    print (record)
    message = raw_input('>')
    if not message:
        print ('Server disconnected')
        break
    server.send(message, CODE)
server.close()

This is the Chartrecord
class ChatRecord(object):
def __init__(self):
    self.data=[]
def add(self,s):
    self.data.append(s)
def __str__(self):
    if len(self.data)==0:
        return 'No messages yet!'
    else:
        return'\n'.join(self.data)

This is the error message I get when running the chat record. I can enter a name then after that I get the error message below
Waiting for connection...
('...connected from:', ('127.0.0.1', 51774))
Waiting for connection...
Exception in thread Thread-1:
Traceback (most recent call last):
  File     ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/threadin g.py"", line 532, in __bootstrap_inner
self.run()
  File ""/Users/basetsanamanele/Documents/workspace/HAAAAAAAFF/ClientHandler"", line 17, in run
self._client.send(str(self._record),CODE)
TypeError: an integer is required

Please assist
",550
38982807,38984367,2,"Presumably you already have a Flask app object and routes set up, but if you create the app like this:
import flask

app = flask.Flask(__name__)

then set up your @app.route()s, and then when you want to start the app:
import gevent

app_server = gevent.wsgi.WSGIServer((host, port), app)
app_server.serve_forever()

Then you can just run your application directly rather than having to tell gunicorn or uWSGI or anything else to run it for you.
I had a case where I wanted the utility of flask to build a web application (a REST API service) and found the inability to compose flask with other non-flask, non-web-service elements a problem. I eventually found gevent.wsgi.WSGIServer and it was just what I needed. After the call to app_server.serve_forever(), you can call app_server.stop() when your application wants to exit.
In my deployment, my application is listening on localhost: using flask and gevent, and then I have nginx reverse-proxying HTTPS requests on another port and forwarding them to my flask service on localhost.
",200
38982807,38982989,2,"When you ""run Flask"" you are actually running Werkzeug's development WSGI server, and passing your Flask app as the WSGI callable.
The development server is not intended for use in production.  It is not designed to be particularly efficient, stable, or secure.
Replace the Werkzeug dev server with a production-ready WSGI server such as Gunicorn or uWSGI when moving to production, no matter where the app will be available.

The answer is similar for ""should I use a web server"".  WSGI servers happen to have HTTP servers but they will not be as good as a dedicated production HTTP server (Nginx, Apache, etc.).

Flask documents how to deploy in various ways. Many hosting providers also have documentation about deploying Python or Flask.
",143
38982807,38982807,1,"Setting up Flask with uWSGI and Nginx is quite difficult, and even with buildout scripts it takes quite some time, and has to be recorded to instructions to be reproduced later.
If I don't plan a big load on server (it's hidden from public), does it make sense to run it without uWSGI? (Flask can listen to a port. Can Nginx just forward requests?)
Does it make sense to not use even Nginx, just running bare flask app on a port?
",96
38986113,38986113,1,"I try to make Async ping process using subprocess.Popen , I try to understand how i implement it in this case 
aList = []
async def sn(frm, to):
    i = 0
    for i in list(range(frm, to)):
        aList.append(i)

    cmd = ""ping -n 1 "" + '10.0.0.'
    coroutines = [subprocess.Popen(cmd + str(i), stdout=subprocess.PIPE) for i in aList]
    results = await asyncio.gather(*coroutines)
    print(results)

loop = asyncio.get_event_loop()
loop.run_until_complete(sn(frm, to))
loop.close()

",112
38986113,39173449,2,"class rscan(object):

    state = {'online': [], 'offline': []} # Dictionary with list
    ips = [] # Should be filled by function after taking range

    # Amount of pings at the time
    thread_count = 8

    # Lock object to prevent race conditions
    lock = threading.Lock()

    # Using Windows ping command
    def ping(self, ip):
        answer = subprocess.call(['ping','-n','1',ip],stdout = open('1.txt','w'))
        return answer == 0 and ip


    def pop_queue(self):
        ip = None

        self.lock.acquire() # lock !!!
        if self.ips:
            ip = self.ips.pop()

        self.lock.release()

        return ip


    def noqueue(self):
        while True:
            ip = self.pop_queue()

            if not ip:
                return None

            result = 'online' if self.ping(ip) else 'offline'
            self.state[result].append(ip) ### check again


    def start(self):
        threads = []

        for i in range(self.thread_count):

            t = threading.Thread(target=self.noqueue)
            t.start()
            threads.append(t)

        # Wait for all threads

        [ t.join() for t in threads ]

        return self.state

    def rng(self, frm, to, ip3):
        self.frm = frm
        self.to = to
        self.ip3 = ip3
        for i in range(frm, to):
            ip = ip3 + str(i)
            self.ips.append(ip)


if __name__== '__main__':
    scant = rscan()
    scant.thread_count = 8

edited a bit class i have found also used threads instead of Async & await
Credit: http://blog.boa.nu/2012/10/python-threading-example-creating-pingerpy.html
",304
38986117,38986117,1,"I was using Jupyter in Windows and just switched to Ubuntu. I found the colour of the code is very weird in the firefox browser. E.g. it highlights the variables in every other line.
I tried to solve this problem by installing a custom theme and the effect should be like

Instead, it still highlights every other variable on my side, like

This just makes my eyes very tired when try to debug the code.
I also tried disabling all the add-ons in Firefox which didn't help. Is there any setting that I can change to restore to the default colour display? 
",112
38986117,39112179,2,"I sometimes get this if I'm copying/pasting from a source that has a different indentation size than that of the jupyter notebook. In your screenshot it looks like a small indent size so this seems like the likely culprit. Try highlighting the full block of indented code and hitting ctrl+[ then ctrl+] (this unindents the selected lines of code, then reindents them using the jupyter indent size). 
If this doesn't work, you might try checking to see if there are any custom indentation settings specified in either "".jupyter/nbconfig/notebook.json"" or "".jupyter/custom/custom.js"" (... or whatever the Windows equivalents are).
In "".jupyter/nbconfig/notebook.json"", I have the indentUnit set to 4 spaces (and have also enabled linewrapping).
{
  ""CodeCell"": {
    ""cm_config"": {
      ""indentUnit"": 4,
      ""lineWrapping"":true
    }
  }
}

Most editors allow you to set your indent size (Atom, sublime text, etc.) so you can avoid this issue in the future by making sure you have the same indent size everywhere you're swapping code to/from (assuming this is what's causing the red highlighting). Python's default is 4 so def recommend sticking with that. 
",227
38986202,38986284,2,"As I understand it, you are matching on the first field and the file is sorted.  In that case, try:
$ awk 'NR>1{printf ""%s%s"",($1==last?"" "":""\n""),$0}; NR==1{printf ""%s"",$0} {last=$1} END{print""""}' file
ALI P 18:00:40.583 0.0 ALI S 18:00:58.188 1.4
BRD Pg 18:00:48.918 0.4 BRD Sg 18:01:09.437 -1.8
GAN Pn 18:00:58.207 -0.0 GAN Sn 18:01:27.791 0.1
GLB P 18:00:27.265 -0.4 GLB S 18:00:34.187 0.1
GOB S 18:01:13.638 -0.6
IML Pg 18:00:52.264 -0.6

How it works

NR==1{printf ""%s"",$0}
For the first line, we print it with no trailing newline.
NR>1{printf ""%s%s"",($1==last?"" "":""\n""),$0}
For lines after the first, we print a space if the first fields match or a newline if they don't, followed by the line.
The tricky-looking part here is the ternary statement $1==last?"" "":""\n"".  This just tests to see if the first field is equal to the last first field.  If it is, it returns the string after the ?.  If it isn't, it returns the string after the :.
last=$1
We update the variable last to the most recent first field.
END{print""""}
After we have finished reading the file and to make sure that we have a complete final line, we print a newline.

",305
38986202,38986768,2,"another awk
$ awk '{a[$1]=a[$1]?a[$1] FS $0:$0} 
    END{for(k in a) print a[k] | ""sort"" }' file | column -t

ALI  P   18:00:40.583  0.0   ALI  S   18:00:58.188  1.4
BRD  Pg  18:00:48.918  0.4   BRD  Sg  18:01:09.437  -1.8
GAN  Pn  18:00:58.207  -0.0  GAN  Sn  18:01:27.791  0.1
GLB  P   18:00:27.265  -0.4  GLB  S   18:00:34.187  0.1
GOB  S   18:01:13.638  -0.6
IML  Pg  18:00:52.264  -0.6

accumulate records with the same key, print at the end and sort (by the key), column for prettying.  Doesn't require the keys to be contiguous or sorted.
",126
38986202,38993617,2,"This could be approached in Python as follows:
from itertools import groupby

data = """"""ALI P 18:00:40.583 0.0
ALI S 18:00:58.188 1.4
BRD Pg 18:00:48.918 0.4
BRD Sg 18:01:09.437 -1.8
GAN Pn 18:00:58.207 -0.0
GAN Sn 18:01:27.791 0.1
GLB P 18:00:27.265 -0.4
GLB S 18:00:34.187 0.1
GOB S 18:01:13.638 -0.6
IML Pg 18:00:52.264 -0.6""""""    

print '\n'.join(' '.join(g) for k,g in groupby(data.splitlines(), key=lambda x: x.split()[0]))

This would display:
ALI P 18:00:40.583 0.0 ALI S 18:00:58.188 1.4
BRD Pg 18:00:48.918 0.4 BRD Sg 18:01:09.437 -1.8
GAN Pn 18:00:58.207 -0.0 GAN Sn 18:01:27.791 0.1
GLB P 18:00:27.265 -0.4 GLB S 18:00:34.187 0.1
GOB S 18:01:13.638 -0.6
IML Pg 18:00:52.264 -0.6    

",135
38986202,38986202,1,"I am working with a data set:
ALI P 18:00:40.583 0.0

ALI S 18:00:58.188 1.4

BRD Pg 18:00:48.918 0.4

BRD Sg 18:01:09.437 -1.8

GAN Pn 18:00:58.207 -0.0

GAN Sn 18:01:27.791 0.1

GLB P 18:00:27.265 -0.4

GLB S 18:00:34.187 0.1

GOB S 18:01:13.638 -0.6

IML Pg 18:00:52.264 -0.6

Using AWK and I need lines that match, to be printed onto the same line.
i.e. 
ALI P 18:00:40.583 0.0 ALI S 18:00:58.188 1.4

BRD Pg 18:00:48.918 0.4 BRD Sg 18:01:09.437 -1.8

I've been trying all sorts of different ideas but cannot locate code to do this.
I have been trying to use AWK, as instructed by my superior. Would be interested to see if it would be easier in Python?
(note white space between lines to preserve structure)
",138
38986205,38986230,2,"You can use re.split() using a positive lookbehind to AM or PM having an optional - and a space character as a delimiter:
>>> import re
>>>
>>> s = ""8:30 AM- 10:00 PM Subject: Math""
>>> re.split(r""(?<=AM|PM)-?\s"", s)
['8:30 AM', '10:00 PM', 'Subject: Math']

",81
38986205,38986205,1,"An example of the python string is '8:30 AM- 10:00 PM Subject: Math'. In general, the string contains a start time, end time, and the subject I want to separate this string into 3 components: the start time, end time, and subject. For example 8:30 AM, 10:00 PM, and Subject: Math.
How can I do this using regex in python?
",75
38986217,38986217,1,"Using Python I am trying to install a library called yappi through easy_install. However I am getting this error below on Windows 7 Command Shell:

I explored alternative installations. I tried previously 'pip install yappi' but this didn't work due to a separate error (can't build wheel) which is a separate question. 
",61
38986217,39050639,2,"Try downloading the appropriate wheel from here.
Then use pip install [package].
",16
38986225,38986225,1,"I have a create form in which I'm submitting the field ['created_at'] as a hidden datetimefield. I'm importing from django.utils import timezone and have this working on another model. Does anyone have any insight into why the value is blank? Thanks for any help!
form:
class CreateForm(ModelForm):
    class Meta:
        model = Animal
        fields = ('name', 'course', 'core', 'animal_group', 'image_on', 'image_off', 'created_at',)
        labels = { ""image_on"": ""Animal Image"", ""image_off"": ""Incompleted Animal Image"", }
        widgets = {
            'name': forms.Textarea(attrs={'cols': 80, 'rows': 1}),
        }

    def __init__(self, *args, **kwargs):
         super(CreateForm, self).__init__(*args, **kwargs)
         self.fields['created_at'] = forms.DateTimeField(initial = timezone.now())
         self.fields['created_at'].widget = forms.HiddenInput()

    def clean(self):
        cleaned_data = self.cleaned_data
        return cleaned_data

However when the templates renders the hidden input is there, but it doesn't have a value. 

Here is the template code snippet:
<form action=""/support/add_animal/"" enctype=""multipart/form-data"" method=""POST"">
      {% csrf_token %}
      {{ create_form | crispy }}
      <br>
      <input type=""submit"" class=""btn btn-success"" value=""Save"">
  </form>

",270
38986225,39009987,2,"Rather than continue banging my head on the wall with this, I opted to set the created_at time in the model rather than worry about it elsewhere. 
Set the attribute to have (default=timezone.now) and that cured my woes.
",43
38986235,44858602,2,"They've updated the spacy version to spacy-alpha V2.0.0.
You can check it here for the new documentation Here
",20
38986235,38986235,1,"So lately I've been playing around with a WikiDump.
I preprocessed it and trained it on Word2Vec + Gensim
Does anyone know if there is only one script within Spacy that would generate
tokenization, sentence recognition, part of speech tagging, lemmatization, dependency parsing, and named entity recognition all at once
I have not been able to find clear documentation
Thank you 
",68
38986235,39058506,2,"Spacy gives you all of that with just using en_nlp = spacy.load('en'); doc=en_nlp(sentence). The documentation gives you details about how to access each of the elements.
An example is given below:
In [1]: import spacy
   ...: en_nlp = spacy.load('en')

In [2]: en_doc = en_nlp(u'Hello, world. Here are two sentences.')

Sentences can be obtained by using doc.sents:
In [4]: list(en_doc.sents)
Out[4]: [Hello, world., Here are two sentences.]

Noun chunks are given by doc.noun_chunks:
In [6]: list(en_doc.noun_chunks)
Out[6]: [two sentences]

Named entity is given by doc.ents:
In [11]: [(ent, ent.label_) for ent in en_doc.ents]
Out[11]: [(two, u'CARDINAL')]

Tokenization: You can iterate over the doc to get tokens. token.orth_ gives str of the token.
In [12]: [tok.orth_ for tok in en_doc]
Out[12]: [u'Hello', u',', u'world', u'.', u'Here', u'are', u'two', u'sentences', u'.']

POS is given by token.tag_:
In [13]: [tok.tag_ for tok in en_doc]
Out[13]: [u'UH', u',', u'NN', u'.', u'RB', u'VBP', u'CD', u'NNS', u'.']

Lemmatization:
In [15]: [tok.lemma_ for tok in en_doc]
Out[15]: [u'hello', u',', u'world', u'.', u'here', u'be', u'two', u'sentence', u'.']

Dependency parsing. You can traverse the parse tree by using token.dep_ token.rights or token.lefts. You can write a function to print dependencies:
In [19]: for token in en_doc:
    ...:     print(token.orth_, token.dep_, token.head.orth_, [t.orth_ for t in token.lefts], [t.orth_ for t in token.rights])
    ...:     
(u'Hello', u'ROOT', u'Hello', [], [u',', u'world', u'.'])
(u',', u'punct', u'Hello', [], [])
(u'world', u'npadvmod', u'Hello', [], [])
...

For more details please consult the spacy documentation.
",487
38986244,38986244,1,"In Python, what are the running time and space complexities if a list is converted to a set?
Example:
data = [1,2,3,4,5,5,5,5,6]

# this turns list to set and overwrites the list
data = set(data)

print data 
# output will be (1,2,3,4,5,6)

",52
38986244,38986278,2,"You have to iterate through the entire list, which is O(n) time, and then insert each into a set, which is O(1) time. So the overall time complexity is O(n), where n is the length of the list.
No other space other than the set being created or the list being used is needed.
",70
38986244,38986289,2,"Converting a list to a set requires that every item in the list be visited once, O(n). Inserting an element into a set is O(1), so the overall time complexity would be O(n).
Space required for the new set is less than or equal to the length of the list, so that is also O(n).
Here's a good reference for Python data structures.
",83
38986261,38986261,1,"I understand that in order to find a nested document that I am to use the dot notation, but I need what follows the dot be in variable form. I am doing the following:
collection.update_one(
                    {'_id': md5_hash}, 
                    {'$addToSet' : {
                     'src_id': src_id,
                     'offset.src_id' : offset}}
                                  )

and getting 
{
        ""_id"" : ""de03fe65a6765caa8c91343acc62cffc"",
        ""total_count"" : 1,
        ""src_id"" : [
                ""a3c1b98d5606be7c5f0c5d14ffb0b741""
        ],
        ""offset"" : {
                ""a3c1b98d5606be7c5f0c5d14ffb0b741"" : [
                        512
                ],
                ""src_id"" : [
                        512,
                        1024,
                        1536,
                        2048,
                        2560,
                        3072,
                        3584,
                        4096,
                        4608
                ]
        },
        ""per_source_count"" : {
                ""a3c1b98d5606be7c5f0c5d14ffb0b741"" : 1
        }
}

I don't want ""src_id"" in the offset document I want to add to the a3c1b98d5606be7c5f0c5d14ffb0b741 key. I am using python3.5 and pymongo version 3.2.2. Thanks!
",173
38986261,38986305,2,"You can just have a nested dictionary:
collection.update_one({'_id': md5_hash}, 
                      {'$addToSet': {'offset': {src_id: offset}}})

Or, you can dynamically make the field via string formatting or concatenation:
collection.update_one({'_id': md5_hash}, 
                      {'$addToSet': {'offset.%s' % src_id: offset}})

",76
38986341,38986421,2,"At the time of passing the parameters, the parameters are not initialised
>>> def a(b=1,c=b):
...     print(c,b)
... 
Traceback (most recent call last):
    File ""<stdin>"", line 1, in <module>
NameError: name 'b' is not defined

so you need to send len of myByteArray as another variable.
So what you could do is,
def circularFind(myByteArray, searchVal, start=0, end=-1):
    if end == -1:
        end = len(myByteArray)
    #reset of code here.

",110
38986341,38986430,2,"Python default arguments are evaluated when the function is defined. Rather, you want something like this:
def circularFind(myByteArray, searchVal, start=0, end=None):
    """"""
    Return the first-encountered index in bytearray where searchVal 
    is found, searching to the right, in incrementing-index order, and
    wrapping over the top and back to the beginning if index end < 
    index start
    """"""
    if end is None:
        end = len(myByteArray)
    # continue doing what you were doing

",91
38986341,38986341,1,"I have a problem with the fact that I am calling len(myByteArray) in the input arguments to a function I am declaring. I'd like that to be a default argument, but Python doesn't seem to like it. myByteArray is of type bytearray. See documentation on bytearray here. I am accessing its built-in find function, documented here (see ""bytes.find"").
My function:
def circularFind(myByteArray, searchVal, start=0, end=len(myByteArray)):
    """"""
    Return the first-encountered index in bytearray where searchVal 
    is found, searching to the right, in incrementing-index order, and
    wrapping over the top and back to the beginning if index end < 
    index start
    """"""
    if (end >= start):
        return myByteArray.find(searchVal, start, end)
    else: #end < start, so search to highest index in bytearray, and then wrap around and search to ""end"" if nothing was found 
        index = myByteArray.find(searchVal, start, len(myByteArray))
        if (index == -1):
            #if searchVal not found yet, wrap around and keep searching 
            index = myByteArray.find(searchVal, 0, end)
        return index 

Examples to attempt to use the function above:
#-------------------------------------------------------------------
#EXAMPLES:
#-------------------------------------------------------------------
#Only executute this block of code if running this module directly,
#*not* if importing it
#-see here: http://effbot.org/pyfaq/tutor-what-is-if-name-main-for.htm
if __name__ == ""__main__"": #if running this module as a stand-alone program
    import random
    random.seed(0)

    bytes = bytearray(100)
    for i in range(len(bytes)):
        bytes[i] = int(random.random()*256)

    print(list(bytes)); print();

    print('built-in method:')
    print(bytes.find(255))
    print(bytes.find(2,10,97))
    print(bytes.find(5,97,4))

    print('\ncircularFind:')
    print(circularFind(bytes,255))
    print(circularFind(bytes,2,10,97))
    print(circularFind(bytes,5,97,4))

Error: 

NameError: name 'myByteArray' is not defined

If I just remove my default arguments (=0 and =len(myByteArray)), however, it works fine. But...I really want those default arguments there so that the start and end arguments are optional. What do I do? 
In C++ this would be easy, as argument types are specified when you write functions.
",522
38986342,38986374,2,"Something like this should do the trick:
def my_ord(c):
  alphabet = ""abcdefghijklmnopqrstuvwxyz0123456789 .,?!""
  return alphabet.index(c)

",28
38986342,38986375,2,"You can create a dict to map from characters to indices and then do lookups into that. This will avoid repeatedly searching the string as other answers are suggesting (which is O(n)) and instead give O(1) lookup time with respect to the alphabet:
my_ord_dict = {c : i for i, c in enumerate(alphabet)}
my_ord_dict['0'] # 26

At that point you can easily wrap it in a function:
def my_ord(c):
    return my_ord_dict['0']

Or use the bound method directly
my_ord = my_ord_dict.__getitem__

But you don't want to change the name that refers to a builtin function, that'll confuse everyone else trying to use it that can see your change. If you are really trying to hurt yourself you can replace my_ord with ord in the above.
",158
38986342,38986376,2,"You don't.
You're going about this the wrong way: you're making the mistake

This existing thing doesn't meet my needs. I want to make it meet my needs!

instead, the way to go about the problem is

This existing thing doesn't meet my needs. I need a thing that does meet my needs!

Once you realize that, the problem is now pretty straightforward. e.g.
DEFAULT_ALPHABET = ""abcdefghijklmnopqrstuvwxyz0123456789 .,?!""
def myord(x, alphabet=DEFAULT_ALPHABET):
    return alphabet.find(x)

",101
38986342,38986388,2,"If i've understood correctly, this is what you want:
alphabet = ""abcdefghijklmnopqrstuvwxyz0123456789 .,?!""

def crypt(c, key=97):
    return ord(c)-key

def decrypt(c, key=97):
    return chr(c+key)

dst = [crypt(c) for c in alphabet]
src = [decrypt(c) for c in dst]
print dst
print ''.join(src)

",79
38986342,38986342,1,"For example, in python, when I type in ord(""a"") it returns 97 because it refers to the ascii list. I want ord(""a"") to return zero from a string that I created such as 
alphabet = ""abcdefghijklmnopqrstuvwxyz0123456789 .,?!""

so ord(""b"") would be 1 and ord(""c"") would be 2 ect.
How would I go about doing this?
",85
38986343,38986385,2,"Assuming both Latitude and Longitude are Integers:
'<a href=""http://maps.google.com/maps?z=12&t=m&q=loc:'+str(p['latitude'])+'+'+str(p['longitude'])+'"">Click Here</a>'

Try This.
Your mistake is that you are not concatenating the strings properly.
",64
38986343,38986343,1,"I'm trying to create a hyper link in python that includes string and variables but i keep getting a syntax error, I think its probably because of quotations but I cant seen to figure it out. Thanks for the help.
'<a href=""http://maps.google.com/maps?z=12&t=m&q=loc:'str(p['latitude']+'+'+str(p['longitude']))'"">Click Here</a>'})

",86
38986344,38986563,2,"First of all, I'm guessing that you didn't really want to print that y value of 10; that you really wanted the base-10 reduction to 0.  Note that you have an extra character in the pyramid base.
Do not change the value of a loop parameter while you're inside the loop.  Specifically, don't change y within the for y loop.
Get rid of c; you can derive it from the other values.
For flexibility, make your upper limit a parameter: you have two constants (6 and 7) that depend on one concept (row limit).
Here's my version:
row_limit = 7

for y in range(1, row_limit):
    print()
    print((row_limit-y-1) * "" "", end="""")

    for x in range(y, 2*y):
        print(x%10, end="""")

    for x in range(2*(y-1), y-1, -1):
        print(x%10, end="""")

print()

Output:
     1
    232
   34543
  4567654
 567898765
67890109876


If you really want to push things, you can shorten the loops with string concatenation and comprehension, but it's likely harder to read for you.
for y in range(1, row_limit):
    print()
    print((row_limit-y-1) * "" "" + ''.join([str(x%10) for x in range(y, 2*y)]) + \
              ''.join([str(x%10) for x in range(2*(y-1), y-1, -1)]), end="""")

print()

Each of the loops is turned into a list comprehension, such as:
[str(x%10) for x in range(y, 2*y)]

Then, this list of characters is joined with no interstitial character; this forms half of the row.  The second half of the row is the other loop (counting down).  In front of all this, I concatenate the proper number of spaces.
Frankly, I prefer my first form.
",398
38986344,38986584,2,"Here's my implementation.
Python 2:
def print_triangle(n):
    for row_num in xrange(1, n + 1):
        numbers = [str(num % 10) for num in xrange(row_num, 2 * row_num)]
        num_string = ''.join(numbers + list(reversed(numbers))[1:])
        print '{}{}'.format(' ' * (n - row_num), num_string)

Python 3:
def print_triangle(n):
    for row_num in range(1, n + 1):
        numbers = [str(num % 10) for num in range(row_num, 2 * row_num)]
        num_string = ''.join(numbers + list(reversed(numbers))[1:])
        print('{}{}'.format(' ' * (n - row_num), num_string))

Input:
print_triangle(5)
print_triangle(6)
print_triangle(7)

Output:
    1
   232
  34543
 4567654
567898765
     1
    232
   34543
  4567654
 567898765
67890109876
      1
     232
    34543
   4567654
  567898765
 67890109876
7890123210987

",199
38986344,38986344,1,"Can you help to simplify this code and make it more efficient? Mine seems like it's not the best version; what can I improve?
     1
    232
   34543
  4567654
 567898765
678901109876

This is the code I made:
c = -1

for y in range(1, 7):
    print()
    print((6-y) * "" "", end="""")
    c += 1

    for x in range(1, y+1):
        print(y%10, end="""")
        y += 1

    while y - c > 2:
        print(y-2, end="""")
        y -= 1

",114
38986369,38986369,1,"I'm trying to build a client and a server using xmlrpc in python, I HAVE to use a class named FunctionWrapper which has a method and the client use it, the method's name is sendMessage_wrapper(self, message), and the server is declared in another class, I'm trying to register the method in the server but when i call the method from de client I raise and error, can you help me, please?
Cliente:
#! /usr/bin/env python
# -*- coding: utf-8 -*-
import sys
import xmlrpclib
from SimpleXMLRPCServer import SimpleXMLRPCServer

from os import path
sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))
from Constants.Constants import *

class MyApiClient:
    def __init__(self, contact_port = DEFAULT_PORT,contact_ip=LOCALHOST_CLIENT):
        self.contact_port = contact_port
        self.contact_ip = contact_ip
        self.proxy = xmlrpclib.ServerProxy(contact_ip+str(self.contact_port)+""/"")


    def sendMessage(self,message):
        self.proxy.sendMessage_wrapper(message)

a = MyApiClient()
a.sendMessage(""Hi"")
a.sendMessage(""WORKING"")

Server:
#! /usr/bin/env python
# -*- coding: utf-8 -*-
import sys
import xmlrpclib
from SimpleXMLRPCServer import SimpleXMLRPCServer

from os import path
sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))
from Constants.Constants import *

class MyApiServer:
    def __init__(self,wrapper, my_port = DEFAULT_PORT):
        self.port = my_port
        self.server = SimpleXMLRPCServer((LOCALHOST,self.port))
        self.wrapper = wrapper
        self.server.register_instance(self.wrapper)
        print(""Running"")
        self.server.serve_forever()


class FunctionWrapper:
    def __init__(self):
        self.message = None

    """"""
    Procedimiento que ofrece nuestro servidor, este metodo sera llamado
    por el cliente con el que estamos hablando, debe de
    hacer lo necesario para mostrar el texto en nuestra pantalla.
    """"""
    def sendMessage_wrapper(self, message):
        self.message = message
        self.showMessage()
    def showMessage(self):
        print (""Mensaje ""+self.message)
        #raise  NotImplementedError( ""Should have implemented this"" )


a = FunctionWrapper()
b = MyApiServer(a)

",372
38986369,38986383,2,"Here are the constants in case you need it 
#! /usr/bin/env python
# -*- coding: utf-8 -*-

#Nombres para etiquetas login local y remoto
MY_PORT_NUMBER_TITLE = ""Cual es mi puerto?""
OTHER_PORT_NUMBER_TITLE = ""Cual es el puerto de contacto?""
OTHER_IP_NUMBER_TITLE = ""Cual es la direccion ip de contacto?""
LOGIN_TITLE = ""Acceder""

#Nombres para las etiquetas del chat
CONVERSATION_TITLE = ""Conversacion""
SEND_TITLE = ""Responder""

#Titulo de las ventans GUI
LOGIN_WINDOW = ""Login""
CHAT_WINDOW = ""Chat""

#Modos de acceso  al chat, local o remoto
LOCAL = ""Local""
REMOTE = ""Remote""

#Mensajes de error
WARNING = ""¡Alerta!""
MISSING_MESSAGE = ""No hay ningun mensaje para enviar""

#Localhost
LOCALHOST = ""localhost""
DEFAULT_PORT = 5000
LOCALHOST_CLIENT = ""http://localhost:""

",155
38986403,39010456,2,"The line json=data should have been data=data. The json attribute accepts a dictionary, which that data string is not. Here is what working code looks like:
import json
import requests
url = 'https://inputtools.google.com/request?itc=ja-t-i0-handwrit&app=demopage'

data = '{""app_version"":0.4,""api_level"":""537.36"",""device"":""Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36"",""input_type"":0,""options"":""enable_pre_space"",""requests"":[{""writing_guide"":{""writing_area_width"":200,""writing_area_height"":200},""pre_context"":"""",""max_num_results"":1,""max_completions"":0,""ink"":[[[100,100],[20,180],[0,1]],[[20,180],[100,100],[2,3]]]}]}'

headers = {'content-type': 'application/json'}

r = requests.post(url, json=data, headers=headers)
print r.json() 

",196
38986403,38986403,1,"Short version:
This Python request doesn't work. The Javascript version does. Why? 
import json
import requests
url = 'https://inputtools.google.com/request?itc=ja-t-i0-handwrit&app=demopage'

data = '{""app_version"":0.4,""api_level"":""537.36"",""device"":""Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36"",""input_type"":0,""options"":""enable_pre_space"",""requests"":[{""writing_guide"":{""writing_area_width"":200,""writing_area_height"":200},""pre_context"":"""",""max_num_results"":1,""max_completions"":0,""ink"":[[[100,100],[20,180],[0,1]],[[20,180],[100,100],[2,3]]]}]}'

headers = {'content-type': 'application/json'}

r = requests.post(url, json=data, headers=headers)
print r.json()

Longer version:
I have this Javascript that successfully makes an Ajax call. I print the response to the console and can see an array of suggested characters from the input I send.
https://jsbin.com/wufesifasa/1/edit?js,console,output
var text = {
  'app_version' : 0.4,
  'api_level' : '537.36',
  'device' : window.navigator.userAgent,
  'input_type' : 0, // ?
  'options' : 'enable_pre_space', // ?
  'requests' : [ {
    'writing_guide' : {
      'writing_area_width' : 200, // canvas width
      'writing_area_height' : 200, // canvas height
    },
    'pre_context' : '', // confirmed preceding chars
    'max_num_results' : 1,
    'max_completions' : 0,
    'ink' : []
  } ]
};

// written coordinates to be sent
text.requests[0].ink = [
  [[100,100],[20,180],[0,1]],
  [[20,180],[100,100],[2,3]],
];

console.log(JSON.stringify(text))

$.ajax({
  url : 'https://inputtools.google.com/request?itc=ja-t-i0-handwrit&app=demopage',
  method : 'POST',
  contentType : 'application/json',
  data : JSON.stringify(text),
  dataType : 'json',
}).done(function(json) {
  console.log(json);
});

Output:

[""SUCCESS"", [[""fb02254b519a9da2"", [""+"", ""十"", ""t"", ""T"", ""ナ"", ""f"", ""子"",
  ""干"", ""1"", ""千""], [], [object Object] {   is_html_escaped: false }]]]

Now I'm trying to replicate that in Python. I've tried using the above code and many variations of it, but every time I receive the response 'FAILED_TO_PARSE_REQUEST_BODY'. What's different between the Ajax and Python calls that makes my request fail? 
This question is similar to this and this, but they deal with using the same key multiple times and incorrect data encoding, which I do not think applies in this case. 
",573
38986427,38989448,2,"This is the best I could come up with for now.
df.set_index('Time').groupby(pd.TimeGrouper('5T')) \
    .apply(lambda df: df.reset_index()).unstack() \
    .resample('5T').last().stack(dropna=False)

",51
38986427,38986732,2,"Simplest way is to form them into another DataFrame. Use pd.concat
frames, names = [], []

grouped = df.groupby(pd.TimeGrouper(""5Min""), as_index=False)
for name, group in grouped:
    names.extend([name])
    frames.extend([group])

pd.concat(frames, keys=names)

",59
38986427,38986427,1,"My dataframe looks like
Time,                           Id             A               B            C                                                                            
2016-06-15 08:09:26.212962  115516             3           3.238     7.790000   
2016-06-15 08:10:13.863304  115517             3           0.000     8.930000   
2016-06-15 08:11:02.236033  115518             3           0.000     9.090000   
2016-06-15 08:11:52.085754  115519             3           0.000     9.420000  

If I apply a groupby like 
grouped = df.groupby(pd.TimeGrouper(""5Min""), as_index=False)

I get group names and groups like:
2016-06-15 08:05:00
    2016-06-15 08:09:26.212962
2016-06-15 08:10:00
    2016-06-15 08:10:13.863304
    2016-06-15 08:11:02.236033
    2016-06-15 08:11:52.085754
2016-06-15 08:25:00
    2016-06-15 08:25:41.827770

So my question is how can I resample the group names formed above and fill non existent groups with None to get something like: 
2016-06-15 08:05:00
    2016-06-15 08:09:26.212962
2016-06-15 08:10:00
    2016-06-15 08:10:13.863304
    2016-06-15 08:11:02.236033
    2016-06-15 08:11:52.085754
2016-06-15 08:15:00
2016-06-15 08:20:00
2016-06-15 08:25:00
    2016-06-15 08:25:41.827770

Can this be formed as a Dataframe as well?
Regards
",133
38986525,38986525,1,"I'm working on a simple text-based trivia game as my first python project, and my program won't terminate once the score limit is reached.
def game(quest_list):
    points = 0
    score_limit = 20

    x, y = info()
    time.sleep(2)

    if y >= 18 and y < 100:
        time.sleep(1)
        while points < score_limit:
            random.choice(quest_list)(points)
            time.sleep(2)
            print(""Current score:"", points, ""points"")
        print(""You beat the game!"")
        quit()
    ...

",109
38986525,38986607,2,"It looks like the points variable is not increased. Something like this might work in your inner loop:
    while points < score_limit:
        points = random.choice(quest_list)(points)
        time.sleep(2)
        print(""Current score:"", points, ""points"")

I'm assuming that quest_list is a list of functions, and you're passing the points value as an argument? To make this example work, you'll also want to return the points from the function returned by the quest_list that's called. A perhaps cleaner way to build this would be to return only the points generated by the quest. Then you could do something like:
        quest = random.choice(quest_list)
        points += quest()

Unless points is a mutable data structure, it won't change the value. You can read more about that in this StackOverflow question. 
",162
38986527,38986527,1,"I'm currently working on a IoT project using NanoPi M1 and it's expected to notify the users when the amount of light received is not adequate during the day time. I've done the part related to light. Hence, I need to find an effective way to retrieve sunrise and sunset time in python, since the whole script is written in python. I know there are several libraries out there for other languages, I wonder what is the most convenient way to do this in python.
It will seem pretty much like this, I suppose:
if(sunrise<T<sunset) and (light<threshold):
    notifyUser()

I'd appreciate any help here, have a good one.
",136
38986527,38986561,2,"Check out astral. Here's a slightly modified example from their docs:
>>> from astral import Astral
>>> city_name = 'London'
>>> a = Astral()
>>> a.solar_depression = 'civil'
>>> city = a[city_name]
>>> sun = city.sun(date=datetime.date(2009, 4, 22), local=True)

>>> if (sun['sunrise'] < T < sun['sunset']) and (light < threshold):
>>>    notifyUser()

If you use something like this example, please remember to change the city_name and date provided to city.sun.
",120
39267793,39268611,2,"I suspect that your inner class should be called Meta, not meta.
",14
39267793,39267793,1,"Hi i am creating a simple Sign Up form with django framework and mongodb. Following is my view:
class SignUpView(FormView):
    template_name='MnCApp/signup.html'
    form_class=EmployeeForm()
    succes_url='/success/'

Following is my model:
class Employee(Document):
    designation=StringField()
    department=StringField()
    emp_name=StringField(max_length=50)
    password=StringField(max_length=10)

Following is my forms.py
class EmployeeForm(DocumentForm):
    class meta:
        desigs=(
        ('D','Director'),
        ('GM','General Manager'),
        ('AM','Assistant Manager'),
        ('A','Associates')
            )
        deptts=(
            ('HR','Human Resources'),
            ('IT','IT Support'),
            ('TT','Technical Team'),
            ('SM','Sales and Marketting'),
            ('SS','Support Staff')
            )
        document=Employee
        fields='__all__'
        widgets={
             'designation':Select(choices=desigs),
             'department':Select(choices=deptts)
             }

Following is the traceback ValueError recieved on loading SignUpview
Traceback:

File ""C:\Program Files\Python35\lib\site-packages\django\core\handlers\exception.py"" in inner
    39.             response = get_response(request)
File ""C:\Program Files\Python35\lib\site-packages\django\core\handlers\base.py"" in _get_response
    187.                 response = self.process_exception_by_middleware(e, request)
File ""C:\Program Files\Python35\lib\site-packages\django\core\handlers\base.py"" in _get_response
    185.                 response = wrapped_callback(request, *callback_args, **callback_kwargs)
File ""C:\Program Files\Python35\lib\site-packages\django\views\generic\base.py"" in view
    68.             return self.dispatch(request, *args, **kwargs)
File ""C:\Program Files\Python35\lib\site-packages\django\views\generic\base.py"" in dispatch
    88.         return handler(request, *args, **kwargs)
File ""C:\Program Files\Python35\lib\site-packages\django\views\generic\edit.py"" in get
    174.         return self.render_to_response(self.get_context_data())
File ""C:\Program Files\Python35\lib\site-packages\django\views\generic\edit.py"" in get_context_data
    93.             kwargs['form'] = self.get_form()
File ""C:\Program Files\Python35\lib\site-packages\django\views\generic\edit.py"" in get_form
    45.         return form_class(**self.get_form_kwargs())
File ""C:\Program Files\Python35\lib\site-packages\mongodbforms\documents.py"" in init
    353.                 raise ValueError('A document class must be provided.')
Exception Type: ValueError at /signup/
  Exception Value: A document class must be provided.

I am not able to find root of this problem. I am new to django and this is my first project. Also is their anyother way for creating model forms for mongo documents??
",404
39660132,41006261,2,"first, do: pip install pygame
second: make sure you have a correct import statement on top of your main app module.
",25
39660132,39660132,1,"I want to compile a game I made in Python. I searched around for compilers and I prefer Nuitka because it is cross-platform. But whenever I try to compile my code with Nuitka using nuitka --recurse-all --standalone myappname.py I get this error:
Cannot find 'pygame' as relative or absolute import.

I have pygame installed, could anyone please help me?
PS: I dont want to use ""compilers"" like cx_freeze
Thanks in advance
",84
39867462,39867462,1,"How would I replace the the hard coded 'python' with snippet['language'] from a for loop in my view?
{% highlight 'python', lineno='inline' -%}
    {{snippet['code']}}
{% endhighlight %}

",49
39867462,39869400,2,"You can simply put your variable in place of the hardcoded string like this:
{% set lang = 'python' %}
{% highlight lang %}
  from fridge import Beer
  glass = Beer(lt=500)
  glass.drink()
{% endhighlight %}

You haven't showed us your for-loop, but in principle you can do the same thing in the for-loop too:
{% for lang in ['python', 'ruby', 'scheme'] %}
  {% highlight lang %}
  from fridge import Beer
  {% endhighlight %}
{% endfor %}

",107
39867464,39867464,1,"I have a project which is making a simple breakout game with python. I am having a problem with making a button on a graphic window.
from graphics import*
win = GraphWin(""win"",200,150)
def buttons():
    rectangle = Rectangle(Point(30,85),Point(60,55))
    rectangle2 = Rectangle(Point(170,85),Point(140,55))
    rectangle.setFill(""blue"")
    rectangle2.setFill(""blue"")
    rectangle.draw(win)
    rectangle2.draw(win)

Here, How can I make those rectangles as buttons which represent the movements ""Left"",& ""Right""??
",117
39867464,41554451,2,"Below is a simple solution for a red left button, a green right button and an ""Exit"" button to quit the program.  I've rearranged the rectangles that represent the buttons such that P1 is the lower left corner and P2 is the upper right corner.  This simplifies the test to see if the clicked point was inside the button.  (You can make the code more sophisticated to remove this assumption.)
from graphics import *

WINDOW_WIDTH, WINDOW_HEIGHT = 200, 150

win = GraphWin(""Simple Breakout"", WINDOW_WIDTH, WINDOW_HEIGHT)

def buttons():
    left = Rectangle(Point(25, 55), Point(55, 85))  # points are ordered ll, ur
    right = Rectangle(Point(145, 55), Point(175, 85))
    quit = Rectangle(Point(85, 116), Point(115, 146))

    left.setFill(""red"")
    right.setFill(""green"")
    text = Text(Point(100, 133), ""Exit"")
    text.draw(win)

    left.draw(win)
    right.draw(win)
    quit.draw(win)

    return left, right, quit

def inside(point, rectangle):
    """""" Is point inside rectangle? """"""

    ll = rectangle.getP1()  # assume p1 is ll (lower left)
    ur = rectangle.getP2()  # assume p2 is ur (upper right)

    return ll.getX() < point.getX() < ur.getX() and ll.getY() < point.getY() < ur.getY()

left, right, quit = buttons()

centerPoint = Point(WINDOW_WIDTH / 2, WINDOW_HEIGHT / 2)
text = Text(centerPoint, """")
text.draw(win)

while True:
    clickPoint = win.getMouse()

    if clickPoint is None:  # so we can substitute checkMouse() for getMouse()
        text.setText("""")
    elif inside(clickPoint, left):
        text.setText(""left"")
    elif inside(clickPoint, right):
        text.setText(""right"")
    elif inside(clickPoint, quit):
        break
    else:
        text.setText("""")

win.close()

If you click the red or green buttons, you'll get ""left"" or ""right"" printed in the center of the window, otherwise no text appears:

",433
40018348,40018429,2,"Have a look at BeautifulSoup: https://www.crummy.com/software/BeautifulSoup/
You can request a website and then read the HTML source code from it:
import requests
from bs4 import BeautifulSoup

r = requests.get(YourURL)

soup = BeautifulSoup(r.content)
print soup.prettify()

If you want to read JavaScript, look into Headless Browsers.
",58
40018348,40018348,1,"What I had try are as following:
1)
response = urllib2.urlopen(url)
html = response.read()

In this way, I can't open the url in browser.
2)
webbrowser.open(url)

In this way, I can't get source code of the url.
So, how can I open an URL and get source code at the same time?
Thanks for your help.
",76
40018351,40018351,1,"I'm running on debian jessie. I'm trying to parse my pdf with tabula-py library but I get this error 
   2016 12:16:57 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont 

getawtFont  
0                                             Italic                          
1   2016 12:16:57 PM org.apache.fontbox.util.Font...                          
2                                             Italic                          
                                       Oct 13  \
0  INFO: Can't find the specified font Tahoma   
1                                      Oct 13   
2             WARNING: Font not found: Tahoma   

How can do to fix that ?
Here is my code :
import cv2
import numpy as np
# from matplotlib import pyplot as plt
from wand.image import Image
from tabula import read_pdf_table

# Converting first page into JPG
with Image(filename=""ed.pdf"", resolution=200) as pdf:
    pdf.compression_quality = 99
    pdf.save(filename=""temp.png"")

img = cv2.imread('temp.png', 0)
img2 = img.copy()
template = cv2.imread('test cust.png', 0)
imgw, imgh = img.shape[::-1]
w, h = template.shape[::-1]

methods = ['cv2.TM_CCOEFF', 'cv2.TM_CCOEFF_NORMED', 'cv2.TM_CCORR', 'cv2.TM_CCORR_NORMED', 'cv2.TM_SQDIFF', 'cv2.TM_SQDIFF_NORMED']

for meth in methods:
    img = img2.copy()
    method = eval(meth)

    # Apply template Matching
    res = cv2.matchTemplate(img, template, method)
    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)

    # If the method is TM_SQDIFF or TM_SQDIFF_NORMED, take minimum
    if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:
        top_left = min_loc
    else:
        top_left = max_loc

    bottom_right = (top_left[0] + w, top_left[1] + h)

    top = top_left[1];
    left = top_left[0];
    bottom = imgh - bottom_right[1];
    right = imgw - bottom_right[0];

    cv2.rectangle(img, top_left, bottom_right, [0,255,0], 10)

    df = read_pdf_table('ed.pdf', area=(top,left,bottom,right))
    print(df)

The error will be happened at this line 
df = read_pdf_table('ed.pdf', area=(top,left,bottom,right))
",367
40018351,41790252,2,"I'm author of tabula-py. I guess you would like to extract image based PDF, but tabula-py isn't a tool for OCR. It is assumed to extract text embedded PDF.
I think you should try OCR tools such as Google Cloud Vision API.
",49
40018351,41791796,2,"Just to add to what Chezou has said: Google Could Vision OCR does not support PDFs directly. You would first need to extract the pages (as images) with a tool like Ghostscript and then send the image of each page to the API. But if your PDF has three pages or less, you can use the free OCR.space PDF OCR api, which can take the whole PDF document as input.
",78
