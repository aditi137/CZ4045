ParentId,Id,PostTypeId,Text,Post length
4942,4942,1,"When asked to create system XYZ and you ask to do it in Python over PHP or Ruby, what are the main features you can mention when they require you to explain it?
",35
4942,5014,2,"I would consider that using python on a new project is completely dependent on what problem you are trying to solve with python.  If you want someone to agree with you that you should use python, then show them how python's features apply specifically to that problem.
In the case of web development with python, talk about WSGI and other web libraries and frameworks you could use that would make your life easier.  One note for python is that most of the frameworks for python web development can be plugged right into any current project. With ruby on rails, you're practically working in a DSL that anyone who uses your project will have to learn.  If they know python, then they can figure out what you are doing with django, etc in a day.
I'm only talking about web development because it appears that's what you are going to be working on seeing ruby, python and PHP in the same list.  The real message that's important is applying to whatever it is you like about python directly to some problem you are trying to solve.
",204
4942,15291,2,"I agree with mreggen. Tell them by working in Python you can get things done faster. Getting things done faster possibly means money saved by the client. In the least it means that you are working with a language you a more comfortable in, meaning faster development, debugging, and refactoring time. There will be less time spent looking up documentation on what function to use to find the length of a string, etc. 
",82
4942,15296,2,"It's one of the preferred languages over at Google - It's several years ahead of Ruby in terms of ""maturity"" (what ever that really means - but managers like that). Since it's prefered by Google you can also run it on the Google App Engine.
Mircosoft is also embracing Python, and will have a v2.0 of IronPython coming out shortly. They are working on a Ruby implementation as well, but the Python version is way ahead, and is actually ""ready for primetime"". That give you the possibility for easy integration with .NET code, as well as being able to write client side RIAs in Python when Silverlight 2 ships.
",127
4942,21221,2,"The best sell of Python I've ever seen was by a manager in our group who had a young daughter.  He used a quote attributed to Einstein:
If you can't explain something to a six-year-old, you really don't understand it yourself.
The next few slides of his presentation demonstrated how he was able to teach his young daughter some basic Python in less than 30 minutes, with examples of the code she wrote and an explanation of what it did.
He ended the presentation with a picture of his daughter and her quote ""Programming is fun!""
I would focus on Python's user friendliness and wealth of libraries and frameworks.  There are also a lot of little libraries that you might not get in other languages, and would have to write yourself (i.e. How a C++ developer writes Python).
Good luck!
",160
4942,9420311,2,"Give them a snippet of code in each (no more than a page) that performs some cool function that they will like. (e.g show outliers in a data set).
Show them each page. One in PHP, Ruby and Python.
Ask them which they find easiest to understand/read.
Tell them thats why you want to use Python. It's easier to read if you've not written it, more manageable, less buggy and quicker to build features because it is the most elegant (pythonic)
",98
4942,32847801,2,"Though All 3 languages are versatile and used worldwide by programmers, Python still have some advantages over the other two. Like From my personal experience :-
Non-programmers love it (most of 'em choose Python as their first computer language,check this infographic php vs python vs ruby here)
Multiple frameworks (You can automate your system tasks, can develop apps for web and windows/mac/android OSes)
Making OpenCV apps easily than MATLAB 
Testing done easy (you can work on Selenium for all kind of web testing)
OOPS concepts are followed by most languages now , so how come Python can stay behind! Inheritance, Abstraction and Encapsulation are followed by Python as well.
Python as of now is divided into two versions popularly that are not much different in terms of performance but features. Python2.x and Python 3.x both have same syntax ,except for some statements like :-
print ""..."" in Python2.x and print() in Python3.x
raw_input() in Python2.x and input() in Python3.x (for getting user input)
In the end, client only cares about money and Python helps you save a lot as compared to PHP and Ruby , because instead of hiring experienced programmers , you can make a newbie learn and use Python expertly.
",232
4942,4978,2,"This is one of those cases that really boil down to personal preference or situational details. If you're more comfortable and experienced with Python, then say so. Are they asking you to justify it because they're more comfortable with one of the other environments? After you're done, will the system be passed off to someone else for long-term maintenance?
If they ask you to use a technology or language that you're not as familiar with, then make sure they know up-front that it's going to take you longer.
",101
4942,4958,2,"Focus on the shorter time needed for development/prototype and possibly easier maintenance (none of this may apply against Ruby).
",22
5102,14791003,2,"The problem for me wasn't in Apache set up, but in understanding how mod_apache actually uses the .py files. Module-level statements (including those in a if __name__=='__main__' section) are not executed--I assumed that the stdout from running the script at the commandline would be what the server would output, but that's not how it works.
Instead, I wrote a module-level function called index(), and had it return as a string the HTML of the page. It's also possible to have other module-level functions (e.g., otherFunction()) that can be accessed as further segments in the URI (e.g., testScript/otherFunction for the file testScript.py.)
Obviously, this makes more sense than my original stdout conception. Better capability of actually using Python as a scripting language and not a humongous markup language.
",156
5102,5168,2,"Are you running Python on UNIX or Windows?
An alternative to mod_python and FastCGI is mod_wsgi. You can find out more at modwsgi
I have built and installed this on Solaris without problems. I had previously tried mod_python but ran into problems with shared libraries as part of the build. There are good install docs available.
",61
5102,5102,1,"I tried to follow a couple of googled up tutorials on setting up mod_python, but failed every time. Do you have a good, step-by step, rock-solid howto?
My dev box is OS X, production - Centos.
",43
5102,5165,2,"Yes, mod_python is pretty confusing to set up.  Here's how I did it.
In httpd.conf:
and in your application directory:
Repeat the configuration for each python program you wish to have running under mod_python.
",41
5102,5129,2,"There are two main ways of running Python on Apache. The simplest would be to use CGI and write normal Python scripts while the second is using a web framework like Django or Pylons.
Using CGI is straightforward. Make sure your Apache config file has a cgi-bin set up. If not, follow their documentation (http://httpd.apache.org/docs/2.0/howto/cgi.html). At that point all you need to do is place your Python scripts in the cgi-bin directory and the standard output will become the HTTP response. Refer to Python's documentation for further info (https://docs.python.org/library/cgi.html).
If you want to use a web framework you'll need to setup mod_python or FastCGI. These steps are dependent on which framework you want to use. Django provides clear instructions on how to setup mod_python and Django with Apache (http://www.djangoproject.com/documentation/modpython/)
",153
5909,5909,1,"I'm downloading an entire directory from a web server. It works OK, but I can't figure how to get the file size before download to compare if it was updated on the server or not. Can this be done as if I was downloading the file from a FTP server?
@Jon: thank for your quick answer. It works, but the filesize on the web server is slightly less than the filesize of the downloaded file. 
Examples:
It has anything to do with the CR/LF conversion?
",98
5909,5927,2,"The size of the file is sent as the Content-Length header. Here is how to get it with urllib:
",21
5909,5935,2,"Using the returned-urllib-object method info(), you can get various information on the retrived document. Example of grabbing the current Google logo:
It's a dict, so to get the size of the file, you do urllibobject.info()['Content-Length']
And to get the size of the local file (for comparison), you can use the os.stat() command:
",72
5909,5938,2,"Also if the server you are connecting to supports it, look at Etags and the If-Modified-Since and If-None-Match headers.
Using these will take advantage of the webserver's caching rules and will return a 304 Not Modified status code if the content hasn't changed.
",48
5909,5985,2,"I have reproduced what you are seeing:
Outputs this:
What am I doing wrong here? Is os.stat().st_size not returning the correct size?
Edit:
OK, I figured out what the problem was:
this outputs:
Make sure you are opening both files for binary read/write.
",55
5909,25502448,2,"In Python3:
",3
5909,40957594,2,"A requests-based solution using HEAD instead of GET (also prints HTTP headers):
Usage
",16
5909,46440161,2,"For a python3 (tested on 3.5) approach I'd recommend:
",13
5966,5966,1,"Basically, I've written an API to www.thetvdb.com in Python. The current code can be found here.
It grabs data from the API as requested, and has to store the data somehow, and make it available by doing:
What is the ""best"" way to abstract this data within the Tvdb() class?
I originally used a extended Dict() that automatically created sub-dicts (so you could do x[1][2][3][4] = ""something"" without having to do if x[1].has_key(2): x[1][2] = [] and so on)
Then I just stored the data by doing self.data[show_id][season_number][episode_number][attribute_name] = ""something""
This worked okay, but there was no easy way of checking if x[3][24] was supposed to exist or not (so I couldn't raise the season_not_found exception).
Currently it's using four classes: ShowContainer, Show, Season and Episode. Each one is a very basic dict, which I can easily add extra functionality in (the search() function on Show() for example). Each has a __setitem__, __getitem_ and has_key.
This works mostly fine, I can check in Shows if it has that season in it's self.data dict, if not, raise season_not_found. I can also check in Season() if it has that episode and so on.
The problem now is it's presenting itself as a dict, but doesn't have all the functionality, and because I'm overriding the __getitem__ and __setitem__ functions, it's easy to accidentally recursively call __getitem__ (so I'm not sure if extending the Dict class will cause problems).
The other slight problem is adding data into the dict is a lot more work than the old Dict method (which was self.data[seas_no][ep_no]['attribute'] = 'something'). See _setItem and _setData. It's not too bad, since it's currently only a read-only API interface (so the users of the API should only ever retrieve data, not add more), but it's hardly... Elegant.
I think the series-of-classes system is probably the best way, but does anyone have a better idea for storing the data? And would extending the ShowContainer/etc classes with Dict cause problems?
",457
5966,9080,2,"Bartosz/To clarify ""This worked okay, but there was no easy way of checking if x[3][24] was supposed to exist or not""
x['some show'][3][24] would return season 3, episode 24 of ""some show"". If there was no season 3, I want the pseudo-dict to raise tvdb_seasonnotfound, if ""some show"" doesn't exist, then raise tvdb_shownotfound
The current system of a series of classes, each with a __getitem__ - Show checks if self.seasons.has_key(requested_season_number), the Season class checks if self.episodes.has_key(requested_episode_number) and so on.
It works, but it there seems to be a lot of repeated code (each class is basically the same, but raises a different error)
",145
5966,8165,2,"I don't get this part here:
This worked okay, but there was no easy way of checking if x[3][24] was supposed to exist or not (so I couldn't raise the season_not_found exception)
There is a way to do it - called in:
what seems to be the problem with that?
",64
5966,6805,2,"I have done something similar in the past and used an in-memory XML document as a quick and dirty hierachical database for storage. You can store each show/season/episode as an element (nested appropriately) and attributes of these things as xml attributes on the elements. Then you can use XQuery to get info back out.
NOTE: I'm not a Python guy so I don't know what your xml support is like.
NOTE 2: You'll want to profile this because it'll be bigger and slower than the solution you've already got. Likely enough if you are doing some high-volume processing then XML is probably not going to be your friend.
",123
5966,6125,2,"Why not use SQLite? There is good support in Python and you can write SQL queries to get the data out. Here is the Python docs for sqlite3
If you don't want to use SQLite you could do an array of dicts.
That way you add metadata to any record and search it very easily
",59
5966,10778,2,"OK, what you need is classobj from new module. That would allow you to construct exception classes dynamically (classobj takes a string as an argument for the class name). 
this gives you:
remember that you can always get the class name through:
So, after some string mangling and concatenation, you should be able to obtain appropriate exception class name and construct a class object using that name and then raise that exception.
P.S. - you can also raise strings, but this is deprecated.
",96
12591,12591,1,"Is there a way, when I parse an XML document using lxml, to validate that document against its DTD using an external catalog file?   I need to be able to work the fixed attributes defined in a documentâ€™s DTD.
",43
12591,13040,2,"Can you give an example? According to the lxml validation docs, lxml can handle DTD validation (specified in the XML doc or externally in code) and system catalogs, which covers most cases I can think of.
",42
12591,36219,2,"It seems that lxml does not expose this libxml2 feature, grepping the source only turns up some #defines for the error handling:
From the catalog implementation in libxml2 page it seems possible that the 'transparent' handling through installation in /etc/xml/catalog may still work in lxml, but if you need more than that you can always abandon lxml and use the default python bindings, which do expose the catalog functions.
",76
12591,8391738,2,"You can add the catalog to the XML_CATALOG_FILES environment variable:
See this thread. Note that entries in XML_CATALOG_FILES are space-separated URLs. You can use Python's pathname2url and urljoin (with file:) to generate the URL from a pathname.
",45
12592,12592,1,"Is it possible to write a doctest unit test that will check that an exception is raised?
For example, if I have a function foo(x) that is supposed to raise an exception if x<0, how would I write the doctest for that? 
",51
12592,12609,2,"Yes. You can do it. The doctest module documentation and Wikipedia has an example of it.
",19
12592,33787471,2,"
ellipsis flag # doctest: +ELLIPSIS is not required to use ... in Traceback doctest 
",15
12692,21149,2,"Keep a look out for ASP.NET MVC
The IronRuby guys have got some internal builds of MVC to work with IronRuby, and IronPython 2 and IronRuby have a lot of code in common with the DLR.
I'm not sure if they'll support IronPython/IronRuby when MVC is released, but it's definitely worth keeping your eye on anyway - The old ASP.NET forms-based development model is old, busted, and the sooner it goes away the better.
",83
12692,239346,2,"Check out the Dynamic Languages in ASP.NET page on Codeplex. This has the newest IronPython bits. It doesn't give you any Visual Studio integration, other than the sample website project, but that's coming.
",40
12692,12713,2,"The current version of ASP.NET integration for IronPython is not very up-to-date and is more of a ""proof-of-concept."" I don't think I'd build a production website based on it.
Edit:: I have a very high level of expectation for how things like this should work, and might setting the bar a little high.  Maybe you should take what's in ""ASP.NET Futures"", write a test application for it and see how it works for you.  If you're successful, I'd like to hear about it.  Otherwise, I think there should be a newer CTP of this in the next six months.
(I'm a developer on IronPython and IronRuby.)
Edit 2: Since I originally posted this, a newer version has been released.
",147
12692,12692,1,"Has anyone built a website with IronPython and ASP.NET.  What were your experiences and is the combination ready for prime-time?
",22
13941,13941,1,"I'd like to have a python program alert me when it has completed its task by making a beep noise.  Currently,  I use import os and then use a command line speech program to say ""Process complete.""  I much rather it be a simple ""bell.""
I know that there's a function that can be used in Cocoa apps, NSBeep, but I don't think that has much anything to do with this.
I've also tried print(\a) but that didn't work.
I'm using a Mac, if you couldn't tell by my Cocoa comment, so that may help.
Thanks!
",122
13941,13949,2,"Have you tried :
That works for me here on Mac OS 10.5
Actually, I think your original attempt works also with a little modification:
(You just need the single quotes around the character sequence).
",40
13941,13959,2,"I had to turn off the ""Silence terminal bell"" option in my active Terminal Profile in iTerm for print('\a') to work. It seemed to work fine by default in Terminal.
You can also use the Mac module Carbon.Snd to play the system beep:
The Carbon modules don't have any documentation, so I had to use help(Carbon.Snd) to see what functions were available. It seems to be a direct interface onto Carbon, so the docs on Apple Developer Connection probably help.
",97
13941,34482,2,"If you have PyObjC (the Python - Objective-C bridge) installed or are running on OS X 10.5's system python (which ships with PyObjC), you can do
to play the system alert.
",38
13941,6110229,2,"I tried the mixer from the pygame module, and it works fine. First install the module:
Then in the program, write this:
With pygame you have a lot of customization options, which you may additionally experiment with.
",44
13941,46743047,2,"Building on Barry Wark's answer...
NSBeep() from AppKit works fine, but also makes the terminal/app icon in the taskbar jump.
A few extra lines with NSSound() avoids that and gives the opportunity to use another sound:
Standard sound files can be found via commandline locate /System/Library/Sounds/*.aiff
The file used by NSBeep() seems to be '/System/Library/Sounds/Funk.aiff'
",67
20794,20794,1,"If I call os.stat() on a broken symlink, python throws an OSError exception. This makes it useful for finding them. However, there are a few other reasons that os.stat() might throw a similar exception. Is there a more precise way of detecting broken symlinks with Python under Linux?
",58
20794,20843,2,"Can I mention testing for hardlinks without python? /bin/test has the FILE1 -ef FILE2 condition that is true when files share an inode.
Therefore, something like find . -type f -exec test \{} -ef /path/to/file \; -print works for hard link testing to a specific file.
Which brings me to reading man test and the mentions of -L and -h which both work on one file and return true if that file is a symbolic link, however that doesn't tell you if the target is missing.
I did find that head -0 FILE1 would return an exit code of 0 if the file can be opened and a 1 if it cannot, which in the case of a symbolic link to a regular file works as a test for whether it's target can be read.
",149
20794,20845,2,"I'm not a python guy but it looks like os.readlink()?  The logic I would use in perl is to use readlink() to find the target and the use stat() to test to see if the target exists.
Edit: I banged out some perl that demos readlink.  I believe perl's stat and readlink and python's os.stat() and os.readlink()are both wrappers for the system calls, so this should translate reasonable well as proof of concept code:
",94
20794,20848,2,"os.lstat() may be helpful. If lstat() succeeds and stat() fails, then it's probably a broken link.
",26
20794,20859,2,"os.path
You may try using realpath() to get what the symlink points to, then trying to determine if it's a valid file using is file.
(I'm not able to try that out at the moment, so you'll have to play around with it and see what you get)
",58
20794,26957,2,"A common Python saying is that it's easier to ask forgiveness than permission.  While I'm not a fan of this statement in real life, it does apply in a lot of cases.  Usually you want to avoid code that chains two system calls on the same file, because you never know what will happen to the file in between your two calls in your code.
A typical mistake is to write something like:
The second call (os.unlink) may fail if something else deleted it after your if test, raise an Exception, and stop the rest of your function from executing.  (You might think this doesn't happen in real life, but we just fished another bug like that out of our codebase last week - and it was the kind of bug that left a few programmers scratching their head and claiming 'Heisenbug' for the last few months)
So, in your particular case, I would probably do:
The annoyance here is that stat returns the same error code for a symlink that just isn't there and a broken symlink.
So, I guess you have no choice than to break the atomicity, and do something like
",219
20794,31102280,2,"This is not atomic but it works.
os.path.islink(filename) and not os.path.exists(filename)
Indeed by RTFM
 (reading the fantastic manual) we see
os.path.exists(path)
Return True if path refers to an existing path. Returns False for  broken symbolic links.
It also says:
On some platforms, this function may return False if permission is not granted to execute os.stat() on the requested file, even if the path physically exists.
So if you are worried about permissions, you should add other clauses.
",99
20794,40274852,2,"I had a similar problem: how to catch broken symlinks, even when they occur in some parent dir?  I also wanted to log all of them (in an application dealing with a fairly large number of files), but without too many repeats.
Here is what I came up with, including unit tests.
fileutil.py:
Unit tests:
",66
20927,20927,1,"I've got two models: Message and Attachment. Each attachment is attached to a specific message, using a ForeignKey on the Attachment model. Both models have an auto_now DateTimeField called updated. I'm trying to make it so that when any attachment is saved, it also sets the updated field on the associated message to now. Here's my code:
Will this work, and if you can explain it to me, why? If not, how would I accomplish this?
",92
20927,33449486,2,"Proper version to work is: (attention to last line self.message.save())
",15
20927,20983,2,"You would also need to then save the message.  Then it that should work.
",16
20927,72359,2,"DateTime fields with auto_now are automatically updated upon calling save(), so you do not need to update them manually. Django will do this work for you.
",31
21454,21454,1,"How do I go about specifying and using an ENUM in a Django model?
",15
21454,28408589,2,"There're currently two github projects based on adding these, though I've not looked into exactly how they're implemented:
Django-EnumField:
Provides an enumeration Django model field (using IntegerField) with reusable enums and transition validation. 
Django-EnumFields:
This package lets you use real Python (PEP435-style) enums with Django.
I don't think either use DB enum types, but they are in the works for first one.
",78
21454,22155357,2,"A the top of your models.py file, add this line after you do your imports:
",17
21454,21468,2,"From the Django documentation:
And you define a charfield in your model :
You can do the same with integer fields if you don't like to have letters
in your db.
In that case, rewrite your choices:
",42
21454,13089465,2,"http://www.b-list.org/weblog/2007/nov/02/handle-choices-right-way/
This is another nice and easy way of implementing enums although it doesn't really save enums in the database.
However it does allow you to reference the 'label' whenever querying or specifying defaults as opposed to the top-rated answer where you have to use the 'value' (which may be a number).
",61
21454,19040441,2,"Setting choices on the field will allow some validation on the Django end, but it won't define any form of an enumerated type on the database end.
As others have mentioned, the solution is to specify db_type on a custom field.
If you're using a SQL backend (e.g. MySQL), you can do this like so:
Run syncdb, and inspect your table to see that the ENUM was created properly.
",82
21454,33932,2,"Using the choices parameter won't use the ENUM db type; it will just create a VARCHAR or INTEGER, depending on whether you use choices with a CharField or IntegerField.  Generally, this is just fine.  If it's important to you that the ENUM type is used at the database level, you have three options:
Use ""./manage.py sql appname"" to see the SQL Django generates, manually modify it to use the ENUM type, and run it yourself.  If you create the table manually first, ""./manage.py syncdb"" won't mess with it.
If you don't want to do this manually every time you generate your DB, put some custom SQL in appname/sql/modelname.sql to perform the appropriate ALTER TABLE command.
Create a custom field type and define the db_type method appropriately.
With any of these options, it would be your responsibility to deal with the implications for cross-database portability.  In option 2, you could use database-backend-specific custom SQL to ensure your ALTER TABLE is only run on MySQL.  In option 3, your db_type method would need to check the database engine and set the db column type to a type that actually exists in that database.
UPDATE: Since the migrations framework was added in Django 1.7, options 1 and 2 above are entirely obsolete. Option 3 was always the best option anyway. The new version of options 1/2 would involve a complex custom migration using SeparateDatabaseAndState -- but really you want option 3.
",272
21454,334932,2,"If you really want to use your databases ENUM type:
Use Django 1.x
Recognize your application will only work on some databases.
Puzzle through this documentation page:http://docs.djangoproject.com/en/dev/howto/custom-model-fields/#howto-custom-model-fields
Good luck!
",38
21454,1530858,2,"
",0
31340,21608282,2,"One easy solution to the GIL is the multiprocessing module. It can be used as a drop in replacement to the threading module but uses multiple Interpreter processes instead of threads. Because of this there is a little more overhead than plain threading for simple things but it gives you the advantage of real parallelization if you need it.
It also easily scales to multiple physical machines.
If you need truly large scale parallelization than I would look further but if you just want to scale to all the cores of one computer or a few different ones without all the work that would go into implementing a more comprehensive framework, than this is for you.
",122
31340,31340,1,"I've been trying to wrap my head around how threads work in Python, and it's hard to find good information on how they operate. I may just be missing a link or something, but it seems like the official documentation isn't very thorough on the subject, and I haven't been able to find a good write-up.
From what I can tell, only one thread can be running at once, and the active thread switches every 10 instructions or so?
Where is there a good explanation, or can you provide one? It would also be very nice to be aware of common problems that you run into while using threads with Python.
",126
31340,31552,2,"Use threads in python if the individual workers are doing I/O bound operations. If you are trying to scale across multiple cores on a machine either find a good IPC framework for python or pick a different language.
",40
31340,31372,2,"Python's a fairly easy language to thread in, but there are caveats.  The biggest thing you need to know about is the Global Interpreter Lock.  This allows only one thread to access the interpreter.  This means two things:  1)  you rarely ever find yourself using a lock statement in python and 2) if you want to take advantage of multi-processor systems, you have to use separate processes.  EDIT:  I should also point out that you can put some of the code in C/C++ if you want to get around the GIL as well.
Thus, you need to re-consider why you want to use threads.  If you want to parallelize your app to take advantage of dual-core architecture, you need to consider breaking your app up into multiple processes.
If you want to improve responsiveness, you should CONSIDER using threads.  There are other alternatives though, namely microthreading.  There are also some frameworks that you should look into:
stackless python
greenlets
gevent
monocle
",182
31340,31358,2,"Below is a basic threading sample. It will spawn 20 threads; each thread will output its thread number. Run it and observe the order in which they print.
As you have hinted at Python threads are implemented through time-slicing. This is how they get the ""parallel"" effect. 
In my example my Foo class extends thread, I then implement the run method, which is where the code that you would like to run in a thread goes. To start the thread you call start() on the thread object, which will automatically invoke the run method...
Of course, this is just the very basics. You will eventually want to learn about semaphores, mutexes, and locks for thread synchronization and message passing.
",139
31340,1197151,2,"Try to remember that the GIL is set to poll around every so often in order to do show the appearance of multiple tasks. This setting can be fine tuned, but I offer the suggestion that there should be work that the threads are doing or lots of context switches are going to cause problems.
I would go so far as to suggest multiple parents on processors and try to keep like jobs on the same core(s).
",84
31340,31398,2,"Yes, because of the Global Interpreter Lock (GIL) there can only run one thread at a time. Here are some links with some insights about this:
http://www.artima.com/weblogs/viewpost.jsp?thread=214235
http://smoothspan.wordpress.com/2007/09/14/guido-is-right-to-leave-the-gil-in-python-not-for-multicore-but-for-utility-computing/
From the last link an interesting quote:
Let me explain what all that means. 
  Threads run inside the same virtual
  machine, and hence run on the same
  physical machine.  Processes can run
  on the same physical machine or in
  another physical machine.  If you
  architect your application around
  threads, you’ve done nothing to access
  multiple machines.  So, you can scale
  to as many cores are on the single
  machine (which will be quite a few
  over time), but to really reach web
  scales, you’ll need to solve the
  multiple machine problem anyway.
If you want to use multi core, pyprocessing defines an process based API to do real parallelization. The PEP also includes some interesting benchmarks.
",175
31412,31412,1,"I am developing a GPL-licensed application in Python and need to know if the GPL allows my program to use proprietary plug-ins. This is what the FSF has to say on the issue:
If a program released under the GPL uses plug-ins, what are the requirements for the licenses of a plug-in?
It depends on how the program invokes its plug-ins. If the program uses fork and exec to invoke plug-ins, then the plug-ins are separate programs, so the license for the main program makes no requirements for them.
If the program dynamically links plug-ins, and they make function calls to each other and share data structures, we believe they form a single program, which must be treated as an extension of both the main program and the plug-ins. This means the plug-ins must be released under the GPL or a GPL-compatible free software license, and that the terms of the GPL must be followed when those plug-ins are distributed.
If the program dynamically links plug-ins, but the communication between them is limited to invoking the ‘main’ function of the plug-in with some options and waiting for it to return, that is a borderline case. 
The distinction between fork/exec and dynamic linking, besides being kind of artificial, doesn't carry over to interpreted languages: what about a Python/Perl/Ruby plugin, which gets loaded via import or execfile?
(edit: I understand why the distinction between fork/exec and dynamic linking, but it seems like someone who wanted to comply with the GPL but go against the ""spirit"" --I don't-- could just use fork/exec and interprocess communication to do pretty much anything).
The best solution would be to add an exception to my license to explicitly allow the use of proprietary plugins, but I am unable to do so since I'm using Qt/PyQt which is GPL.
",339
31412,31420,2,"@Daniel The distinction between fork/exec and dynamic linking, besides being kind of artificial, doesn't carry over to interpreted languages: what about a Python/Perl/Ruby plugin, which gets loaded via import or execfile?
I'm not sure that the distinction is artificial. After a dynamic load the plugin code shares an execution context with the GPLed code. After a fork/exec it does not.
In anycase I would guess that importing causes the new code to run in the same execution context as the GPLed bit, and you should treat it like the dynamic link case. No?
",107
31412,31423,2,"How much info are you sharing between the Plugins and the main program? If you are doing anything more than just executing them and waiting for the results (sharing no data between the program and the plugin in the process) then you could most likely get away with them being proprietary, otherwise they would probably need to be GPL'd.
",65
31412,31421,2,"
he distinction between fork/exec and dynamic linking, besides being kind of artificial,
I don't think its artificial at all.  Basically they are just making the division based upon the level of integration.  If the program has ""plugins"" which are essentially fire and forget with no API level integration, then the resulting work is unlikely to be considered a derived work.  Generally speaking a plugin which is merely forked/exec'ed would fit this criteria, though there may be cases where it does not.  This case especially applies if the ""plugin"" code would work independently of your code as well.
If, on the other hand, the code is deeply dependent upon the GPL'ed work, such as extensively calling APIs, or tight data structure integration, then things are more likely to be considered a derived work.  Ie, the ""plugin"" cannot exist on its own without the GPL product, and a product with this plugin installed is essentially a derived work of the GPLed product.
So to make it a little more clear, the same principles could apply to your interpreted code.  If the interpreted code relies heavily upon your APIs (or vice-versa) then it would be considered a derived work.  If it is just a script that executes on its own with extremely little integration, then it may not.
Does that make more sense?
",254
38435,38718,2,"I believe this is a bug in the Oracle ODBC driver. Basically, the Oracle ODBC driver does not support the TIMESTAMP WITH (LOCAL) TIME ZONE data types, only the TIMESTAMP data type. As you have discovered, one workaround is in fact to use the TO_CHAR method.
In your example you are not actually reading the time zone information. If you have control of the table you could convert it to a straight TIMESTAMP column. If you don't have control over the table, another solution may be to create a view that converts from TIMESTAMP WITH TIME ZONE to TIMESTAMP via a string - sorry, I don't know if there is a way to convert directly from TIMESTAMP WITH TIME ZONE to TIMESTAMP.
",137
38435,38435,1,"Given an Oracle table created using the following:
Using the Python ODBC module from its Win32 extensions (from the win32all package), I tried the following:
When I run this, I get the following:
The other data types I've tried (VARCHAR2, BLOB) do not cause this problem. Is there a way of retrieving timestamps?
",66
38435,38442,2,"My solution to this, that I hope can be bettered, is to use Oracle to explicitly convert the TIMESTAMP into a string:
This works, but isn't portable. I'd like to use the same Python script against a SQL Server database, so an Oracle-specific solution (such as TO_CHAR) won't work.
",61
38508,38516,2,"
",0
38508,38524,2,"Why not throw an exception if the operation wasn't successful?  Personally, I tend to be of the opinion that if you need to return more than one value from a function, you should reconsider if you're doing things the right way or use an object.
But more directly to the point, if you throw an exception, you're forcing them to deal with the problem.  If you try to return a value that indicates failure, it's very well possible somebody could not check the value and end up with some potentially hard to debug errors.
",107
38508,38760,2,"Throwing an exception for failure is one good way to proceed, and if you're returning a lot of different values, you can return a tuple.  For the specific case you're citing, I often take an intermediate approach: return the modified string on success, and return None on failure.  I'm enough of an unreconstructed C programmer to want to return a NULL pointer to char on failure.
If I were writing a routine to be used as part of a larger library and consumed by other developers, I'd throw an exception on failure.  When I'm eating my own dogfood, I'll probably return different types and test on return.
",125
38508,38513,2,"Returning a tuple is the usual way to do this in Python.
",13
38508,38508,1,"I have a function where I need to do something to a string.  I need the function to return a boolean indicating whether or not the operation succeeded, and I also need to return the modified string.  
In C#, I would use an out parameter for the string, but there is no equivalent in Python.  I'm still very new to Python and the only thing I can think of is to return a tuple with the boolean and modified string.
Related question: Is it pythonic for a function to return multiple values?
",103
38508,38514,2,"Return a tuple.
",4
38601,11446609,2,"My head code for 1.4 version(some new and some removed)
",13
38601,38601,1,"How can I use the nifty JavaScript date and time widgets that the default admin uses with my custom view?
I have looked through the Django forms documentation, and it briefly mentions django.contrib.admin.widgets, but I don't know how to use it?
Here is my template that I want it applied on.
Also, I think it should be noted that I haven't really written a view up myself for this form, I am using a generic view. Here is the entry from the url.py:
And I am relevantly new to the whole Django/MVC/MTV thing, so please go easy...
",110
38601,39946546,2,"In Django 10.
myproject/urls.py:
at the beginning of urlpatterns
In my template.html:
",14
38601,9139017,2,"What about just assigning a class to your widget and then binding that class to the JQuery datepicker?
Django forms.py:
And some JavaScript for the template:
",29
38601,3284874,2,"(I'm trying to comment on people suggesting to roll their own Calendar widget, but either I don't see the comment button, or I don't have enough rep.)
What happened to DRY? I think it would be best to re-use the admin widget, but perhaps it should be separated from admin, and easier to use. Thanks for this information anyways.
",71
38601,72284,2,"As the solution is hackish, I think using your own date/time widget with some JavaScript is more feasible.
",20
38601,408230,2,"Yep, I ended up overriding the /admin/jsi18n/ url.
Here's what I added in my urls.py.  Make sure it's above the /admin/ url
And here is the i18n_javascript function I created.
",36
38601,719583,2,"Complementing the answer by Carl Meyer, I would like to comment that you need to put that header in some valid block (inside the header) within your template.
",32
38601,1392329,2,"I find myself referencing this post a lot, and found that the documentation defines a slightly less hacky way to override default widgets. 
(No need to override the ModelForm's __init__ method)
However, you still need to wire your JS and CSS appropriately as Carl mentions.
forms.py
Reference Field Types to find the default form fields.
",63
38601,1833247,2,"Updated solution and workaround for SplitDateTime with required=False:
forms.py
form.html
urls.py
",12
38601,38916,2,"The growing complexity of this answer over time, and the many hacks required, probably ought to caution you against doing this at all. It's relying on undocumented internal implementation details of the admin, is likely to break again in future versions of Django, and is no easier to implement than just finding another JS calendar widget and using that.
That said, here's what you have to do if you're determined to make this work:
Define your own ModelForm subclass for your model (best to put it in forms.py in your app), and tell it to use the AdminDateWidget / AdminTimeWidget / AdminSplitDateTime (replace 'mydate' etc with the proper field names from your model):
Change your URLconf to pass 'form_class': ProductForm instead of 'model': Product to the generic create_object view (that'll mean ""from my_app.forms import ProductForm"" instead of ""from my_app.models import Product"", of course).
In the head of your template, include {{ form.media }} to output the links to the Javascript files.
And the hacky part: the admin date/time widgets presume that the i18n JS stuff has been loaded, and also require core.js, but don't provide either one automatically.  So in your template above {{ form.media }} you'll need:
You may also wish to use the following admin CSS (thanks Alex for mentioning this):
This implies that Django's admin media (ADMIN_MEDIA_PREFIX) is at /media/admin/ - you can change that for your setup.  Ideally you'd use a context processor to pass this values to your template instead of hardcoding it, but that's beyond the scope of this question.
This also requires that the URL /my_admin/jsi18n/ be manually wired up to the django.views.i18n.javascript_catalog view (or null_javascript_catalog if you aren't using I18N).  You have to do this yourself instead of going through the admin application so it's accessible regardless of whether you're logged into the admin (thanks Jeremy for pointing this out).  Sample code for your URLconf:
Lastly, if you are using Django 1.2 or later, you need some additional code in your template to help the widgets find their media:
Thanks lupefiasco for this addition.
",411
38601,2396907,2,"The below will also work as a last resort if the above failed
Same as 
put this in your forms.py from django.forms.extras.widgets import SelectDateWidget
",24
38601,2716963,2,"I finally managed to get this widget working on the dev server, only to have it break on deployment.  I finally decided it wasn't worth shoehorning into my site, and wrote my own widget.  It's not as flexible, but it will probably work well for many: http://www.copiesofcopies.org/webl/?p=81
",59
38601,2818128,2,"Starting in Django 1.2 RC1, if you're using the Django admin date picker widge trick, the following has to be added to your template, or you'll see the calendar icon url being referenced through ""/missing-admin-media-prefix/"".
",43
38987,40677646,2,"You can use toolz.merge([x, y]) for this.
",14
38987,18665968,2,"** creates an intermediary dict, which means that the total number of copies
is actually higher doing the dict(one, **two) form, but all that happens in C
so it's still generally faster than going to itertools, unless there are a huge number of copies (or, probably, if the copies are very expensive). As always if you actually care about speed you should time your use case.
Timing on Python 2.7.3 with an empty dict:
With 10,000 (tiny) items:
With 100,000 items:
With 1,000,000 items:
",104
38987,19279501,2,"In python3, the items method no longer returns a list, but rather a view, which acts like a set. In this case you'll need to take the set union since concatenating with + won't work:
For python3-like behavior in version 2.7, the viewitems method should work in place of items:
I prefer this notation anyways since it seems more natural to think of it as a set union operation rather than concatenation (as the title shows).
Edit:
A couple more points for python 3. First, note that the dict(x, **y) trick won't work in python 3 unless the keys in y are strings.
Also, Raymond Hettinger's Chainmap answer is pretty elegant, since it can take an arbitrary number of dicts as arguments, but from the docs it looks like it sequentially looks through a list of all the dicts for each lookup:
Lookups search the underlying mappings successively until a key is found.
This can slow you down if you have a lot of lookups in your application:
So about an order of magnitude slower for lookups. I'm a fan of Chainmap, but looks less practical where there may be many lookups.
",224
38987,19950727,2,"
",0
38987,20358548,2,"The problem I have with solutions listed to date is that, in the merged dictionary, the value for key ""b"" is 10 but, to my way of thinking, it should be 12.
In that light, I present the following:
Results:
",50
38987,20394520,2,"I have a solution which is not specified here(Man I LOVE python) :-)
This will not update x as well as y. Performance? I don't think it will be terribly slow :-)
NOTE: It is supposed to be 'or' operation and not 'and' operation. Edited to correct the code.
",64
38987,22122836,2,"It's so silly that .update returns nothing.
I just use a simple helper function to solve the problem:
Examples:
",23
38987,26111877,2,"A union of the OP's two dictionaries would be something like:
Specifically, the union of two entities(x and y) contains all the elements of x and/or y.
Unfortunately, what the OP asks for is not a union, despite the title of the post.
My code below is neither elegant nor a one-liner, but I believe it is consistent with the meaning of union.
From the OP's example:
Whether one wants lists could be changed, but the above will work if a dictionary contains lists (and nested lists) as values in either dictionary.
",110
38987,26853961,2,"
How can I merge two Python dictionaries in a single expression?
Say you have two dicts and you want to merge them into a new dict without altering the original dicts:
The desired result is to get a new dictionary (z) with the values merged, and the second dict's values overwriting those from the first.
A new syntax for this, proposed in PEP 448 and available as of Python 3.5, is 
And it is indeed a single expression. It is now showing as implemented in the release schedule for 3.5, PEP 478, and it has now made its way into What's New in Python 3.5 document.
However, since many organizations are still on Python 2, you may wish to do this in a backwards compatible way. The classically Pythonic way, available in Python 2 and Python 3.0-3.4, is to do this as a two-step process:
In both approaches, y will come second and its values will replace x's values, thus 'b' will point to 3 in our final result.
Not yet on Python 3.5, but want a single expression
If you are not yet on Python 3.5, or need to write backward-compatible code, and you want this in a single expression, the most performant while correct approach is to put it in a function:
and then you have a single expression:
You can also make a function to merge an undefined number of dicts, from zero to a very large number:
This function will work in Python 2 and 3 for all dicts. e.g. given dicts a to g:
and key value pairs in g will take precedence over dicts a to f, and so on.
Critiques of Other Answers
Don't use what you see in the formerly accepted answer:
In Python 2, you create two lists in memory for each dict, create a third list in memory with length equal to the length of the first two put together, and then discard all three lists to create the dict. In Python 3, this will fail because you're adding two dict_items objects together, not two lists - 
and you would have to explicitly create them as lists, e.g. z = dict(list(x.items()) + list(y.items())). This is a waste of resources and computation power. 
Similarly, taking the union of items() in Python 3 (viewitems() in Python 2.7) will also fail when values are unhashable objects (like lists, for example). Even if your values are hashable, since sets are semantically unordered, the behavior is undefined in regards to precedence. So don't do this:
This example demonstrates what happens when values are unhashable:
Here's an example where y should have precedence, but instead the value from x is retained due to the arbitrary order of sets:
Another hack you should not use:
This uses the dict constructor, and is very fast and memory efficient (even slightly more-so than our two-step process) but unless you know precisely what is happening here (that is, the second dict is being passed as keyword arguments to the dict constructor), it's difficult to read, it's not the intended usage, and so it is not Pythonic. 
Here's an example of the usage being remediated in django.
Dicts are intended to take hashable keys (e.g. frozensets or tuples), but this method fails in Python 3 when keys are not strings.
From the mailing list, Guido van Rossum, the creator of the language, wrote:
I am fine with
  declaring dict({}, **{1:3}) illegal, since after all it is abuse of
  the ** mechanism.
and 
Apparently dict(x, **y) is going around as ""cool hack"" for ""call
  x.update(y) and return x"". Personally I find it more despicable than
  cool.
It is my understanding (as well as the understanding of the creator of the language) that the intended usage for dict(**y) is for creating dicts for readability purposes, e.g.:
instead of 
Response to comments
Despite what Guido says, dict(x, **y) is in line with the dict specification, which btw. works for both Python 2 and 3. The fact that this only works for string keys is a direct consequence of how keyword parameters work and not a short-comming of dict. Nor is using the ** operator in this place an abuse of the mechanism, in fact ** was designed precisely to pass dicts as keywords. 
Again, it doesn't work for 3 when keys are non-strings. The implicit calling contract is that namespaces take ordinary dicts, while users must only pass keyword arguments that are strings. All other callables enforced it. dict broke this consistency in Python 2:
This inconsistency was bad given other implementations of Python (Pypy, Jython, IronPython). Thus it was fixed in Python 3, as this usage could be a breaking change.
I submit to you that it is malicious incompetence to intentionally write code that only works in one version of a language or that only works given certain arbitrary constraints.
Another comment:
dict(x.items() + y.items()) is still the most readable solution for Python 2. Readability counts. 
My response: merge_two_dicts(x, y) actually seems much clearer to me, if we're actually concerned about readability. And it is not forward compatible, as Python 2 is increasingly deprecated.
Less Performant But Correct Ad-hocs
These approaches are less performant, but they will provide correct behavior.
They will be much less performant than copy and update or the new unpacking because they iterate through each key-value pair at a higher level of abstraction, but they do respect the order of precedence (latter dicts have precedence)
You can also chain the dicts manually inside a dict comprehension:
or in python 2.6 (and perhaps as early as 2.4 when generator expressions were introduced):
itertools.chain will chain the iterators over the key-value pairs in the correct order:
Performance Analysis
I'm only going to do the performance analysis of the usages known to behave correctly. 
The following is done on Ubuntu 14.04
In Python 2.7 (system Python):
In Python 3.5 (deadsnakes PPA):
Resources on Dictionaries
My explanation of Python's dictionary implementation, updated for 3.6.
Answer on how to add new keys to a dictionary
Mapping two lists into a dictionary
The official Python docs on dictionaries 
The Dictionary Even Mightier - talk by Brandon Rhodes at Pycon 2017
Modern Python Dictionaries, A Confluence of Great Ideas - talk by Raymond Hettinger at Pycon 2017
",1249
38987,28753078,2,"Python 3.5 (PEP 448) allows a nicer syntax option:
Or even 
",14
38987,29177685,2,"
",0
38987,31812635,2,"Simple solution using itertools that preserves order (latter dicts have precedence)
And it's usage:
",18
38987,44151666,2,"In python 3:
Out:
Docs: https://docs.python.org/3/library/collections.html#collections.ChainMap:
",14
38987,33999337,2,"
This should solve your problem.
",6
38987,34899183,2,"Be pythonic. Use a comprehension:
",7
38987,36263150,2,"(For Python2.7* only; there are simpler solutions for Python3*.)
If you're not averse to importing a standard library module, you can do
(The or a bit in the lambda is necessary because dict.update always returns None on success.)
",47
38987,18114065,2,"Abuse leading to a one-expression solution for Matthew's answer:
You said you wanted one expression, so I abused lambda to bind a name, and tuples to override lambda's one-expression limit. Feel free to cringe.
You could also do this of course if you don't care about copying it:
",57
38987,46439094,2,"Pythonic way (Python 2)
",6
38987,46356150,2,"If you don't mind mutating x,
Simple, readable, performant. You know update() always returns None, which is a false value. So it will always evaluate to x.
Mutating methods in the standard library, like update, return None by convention, so this trick will work on those too.
If you're using a library that doesn't follow this convention, you can use a tuple display and index to make it a single expression, instead of or, but it's not as readable.
If you don't have x in a variable yet, you can use lambda to make a local without using an assignment statement. This amounts to using lambda as a let expression, which is common technique in functional languages, but rather unpythonic.
If you do want a copy, PEP 448 is best {**x, **y}. But if that's not available, let works here too.
",175
38987,37304637,2,"I know this does not really fit the specifics of the questions (""one liner""), but since none of the answers above went into this direction while lots and lots of answers addressed the performance issue, I felt I should contribute my thoughts.
Depending on the use case it might not be necessary to create a ""real"" merged dictionary of the given input dictionaries.  A view which does this might be sufficient in many cases, i. e. an object which acts like the merged dictionary would without computing it completely.  A lazy version of the merged dictionary, so to speak.
In Python, this is rather simple and can be done with the code shown at the end of my post.  This given, the answer to the original question would be:
When using this new object, it will behave like a merged dictionary but it will have constant creation time and constant memory footprint while leaving the original dictionaries untouched.  Creating it is way cheaper than in the other solutions proposed.
Of course, if you use the result a lot, then you will at some point reach the limit where creating a real merged dictionary would have been the faster solution.  As I said, it depends on your use case.
If you ever felt you would prefer to have a real merged dict, then calling dict(z) would produce it (but way more costly than the other solutions of course, so this is just worth mentioning).
You can also use this class to make a kind of copy-on-write dictionary:
Here's the straight-forward code of MergeDict:
",299
38987,39251599,2,"For Python 2 :
For Python 3:
It gives output:{'a': 1, 'c': 11, 'b': 10}
",28
38987,39736284,2,"In Python 3.5 you can use unpack ** in order to create new dictionary. 
This method has no been showed in past answers. Also, it's better to use {} instead of dict(). Because {} is a python literal and dict() involves a function call.
",56
38987,31478567,2,"This can be done with a single dict comprehension:
In my view the best answer for the 'single expression' part as no extra functions are needed, and it is short.
",34
38987,44262317,2,"The question is tagged python-3x but, taking into account that it's a relatively recent addition and that the most voted, accepted answer deals extensively with a Python 2.x solution, I dare add a one liner that draws on an irritating feature of Python 2.x list comprehension, that is name leaking...
I'm happy to say that the above doesn't work any more on any version of Python 3.
",76
38987,17587183,2,"Here is some code, it seems to work ok:
",11
38987,16769722,2,"Using  a dict comprehension, you may
gives
Note the syntax for if else in comprehension 
",16
38987,17738920,2,"Drawing on ideas here and elsewhere I've comprehended a function:
Usage (tested in python 3):
You could use a lambda instead.
",27
38987,38987,1,"I have two Python dictionaries, and I want to write a single expression that returns these two dictionaries, merged.  The update() method would be what I need, if it returned its result instead of modifying a dict in-place.
How can I get that final merged dict in z, not x?
(To be extra-clear, the last-one-wins conflict-handling of dict.update() is what I'm looking for as well.)
",81
38987,38989,2,"
For items with keys in both dictionaries ('b'), you can control which one ends up in the output by putting that one last.
",28
38987,38990,2,"In your case, what you can do is:
This will, as you want it, put the final dict in z, and make the value for key b be properly overridden by the second (y) dict's value:
If you use Python 3, it is only a little more complicated.  To create z:
",63
38987,39437,2,"An alternative:
",3
38987,44512,2,"I wanted something similar, but with the ability to specify how the values on duplicate keys were merged, so I hacked this out (but did not heavily test it).  Obviously this is not a single expression, but it is a single function call.
",50
38987,49492,2,"This probably won't be a popular answer, but you almost certainly do not want to do this.  If you want a copy that's a merge, then use copy (or deepcopy, depending on what you want) and then update.  The two lines of code are much more readable - more Pythonic - than the single line creation with .items() + .items().  Explicit is better than implicit.
In addition, when you use .items() (pre Python 3.0), you're creating a new list that contains the items from the dict.  If your dictionaries are large, then that is quite a lot of overhead (two large lists that will be thrown away as soon as the merged dict is created).  update() can work more efficiently, because it can run through the second dict item-by-item.
In terms of time:
IMO the tiny slowdown between the first two is worth it for the readability.  In addition, keyword arguments for dictionary creation was only added in Python 2.3, whereas copy() and update() will work in older versions.
",209
38987,228366,2,"In a follow-up answer, you asked about the relative performance of these two alternatives:
On my machine, at least (a fairly ordinary x86_64 running Python 2.5.2), alternative z2 is not only shorter and simpler but also significantly faster.  You can verify this for yourself using the timeit module that comes with Python.
Example 1: identical dictionaries mapping 20 consecutive integers to themselves:
z2 wins by a factor of 3.5 or so.  Different dictionaries seem to yield quite different results, but z2 always seems to come out ahead.  (If you get inconsistent results for the same test, try passing in -r with a number larger than the default 3.)
Example 2: non-overlapping dictionaries mapping 252 short strings to integers and vice versa:
z2 wins by about a factor of 10.  That's a pretty big win in my book!
After comparing those two, I wondered if z1's poor performance could be attributed to the overhead of constructing the two item lists, which in turn led me to wonder if this variation might work better:
A few quick tests, e.g.
lead me to conclude that z3 is somewhat faster than z1, but not nearly as fast as z2.  Definitely not worth all the extra typing.
This discussion is still missing something important, which is a performance comparison of these alternatives with the ""obvious"" way of merging two lists: using the update method.  To try to keep things on an equal footing with the expressions, none of which modify x or y, I'm going to make a copy of x instead of modifying it in-place, as follows:
A typical result:
In other words, z0 and z2 seem to have essentially identical performance.  Do you think this might be a coincidence?  I don't....
In fact, I'd go so far as to claim that it's impossible for pure Python code to do any better than this.  And if you can do significantly better in a C extension module, I imagine the Python folks might well be interested in incorporating your code (or a variation on your approach) into the Python core.  Python uses dict in lots of places; optimizing its operations is a big deal.
You could also write this as
as Tony does, but (not surprisingly) the difference in notation turns out not to have any measurable effect on performance.  Use whichever looks right to you.  Of course, he's absolutely correct to point out that the two-statement version is much easier to understand.
",474
38987,39858,2,"Another, more concise, option:
Note: this has become a popular answer, but it is important to point out that if y has any non-string keys, the fact that this works at all is an abuse of a CPython implementation detail, and it does not work in Python 3, or in PyPy, IronPython, or Jython. Also, Guido is not a fan. So I can't recommend this technique for forward-compatible or cross-implementation portable code, which really means it should be avoided entirely.
",96
38987,7770473,2,"While the question has already been answered several times,
this simple solution to the problem has not been listed yet.
It is as fast as z0 and the evil z2 mentioned above, but easy to understand and change.
",42
38987,8247023,2,"If you think lambdas are evil then read no further.
As requested, you can write the fast and memory-efficient solution with one expression:
As suggested above, using two lines or writing a function is probably a better way to go.
",45
38987,8310229,2,"Recursively/deep update a dict
Demonstration:
Outputs:
Thanks rednaw for edits.
",13
38987,11804613,2,"Even though the answers were good for this shallow dictionary, none of the methods defined here actually do a deep dictionary merge.
Examples follow:
One would expect a result of something like this:
Instead, we get this:
The 'one' entry should have had 'depth_2' and 'extra' as items inside its dictionary if it truly was a merge.
Using chain also, does not work:
Results in:
The deep merge that rcwesick gave also creates the same result.
Yes, it will work to merge the sample dictionaries, but none of them are a generic mechanism to merge.  I'll update this later once I write a method that does a true merge.
",128
38987,11825563,2,"
Among such shady and dubious answers, this shining example is the one and only good way to merge dicts in Python, endorsed by dictator for life Guido van Rossum himself!  Someone else suggested half of this, but did not put it in a function.
gives:
",51
38987,12926103,2,"Two dictionaries
n dictionaries
sum has bad performance. See https://mathieularose.com/how-not-to-flatten-a-list-of-lists-in-python/
",13
38987,16259217,2,"In Python 3, you can use collections.ChainMap which groups multiple dicts or other mappings together to create a single, updateable view:
",24
38987,3936548,2,"The best version I could think while not using copy would be:
It's faster than dict(x.items() + y.items()) but not as fast as n = copy(a); n.update(b), at least on CPython. This version also works in Python 3 if you change iteritems() to items(), which is automatically done by the 2to3 tool.
Personally I like this version best because it describes fairly good what I want in a single  functional syntax. The only minor problem is that it doesn't make completely obvious that values from y takes precedence over values from x, but I don't believe it's difficult to figure that out.
",132
39086,315088,2,"Here's another example that was tested, and will match search & replace patterns:
Example use:
",19
39086,23426834,2,"Expanding on @Kiran's answer, which I agree is more succinct and Pythonic, this adds codecs to support the reading and writing of UTF-8:
",28
39086,21857132,2,"If you're wanting a generic function that replaces any text with some other text, this is likely the best way to go, particularly if you're a fan of regex's:
",35
39086,18676598,2,"A more pythonic way would be to use context managers like the code below:
You can find the full snippet here.
",23
39086,13641746,2,"Based on the answer by Thomas Watnedal. 
However, this does not answer the line-to-line part of the original question exactly. The function can still replace on a line-to-line basis 
This implementation replaces the file contents without using temporary files, as a consequence file permissions remain unchanged.
Also re.sub instead of replace, allows regex replacement instead of plain text replacement only.
Reading the file as a single string instead of line by line allows for multiline match and replacement.
",86
39086,11784227,2,"if you remove the indent at the like below, it will search and replace in multiple line.
See below for example.
",24
39086,1388570,2,"This should work: (inplace editing)
",8
39086,290494,2,"The shortest way would probably be to use the fileinput module. For example, the following adds line numbers to a file, in-place:
What happens here is:
The original file is moved to a backup file
The standard output is redirected to the original file within the loop
Thus any print statements write back into the original file
fileinput has more bells and whistles. For example, it can be used to automatically operate on all files in sys.args[1:], without your having to iterate over them explicitly. Starting with Python 3.2 it also provides a convenient context manager for use in a with statement.
While fileinput is great for throwaway scripts, I would be wary of using it in real code because admittedly it's not very readable or familiar. In real (production) code it's worthwhile to spend just a few more lines of code to make the process explicit and thus make the code readable.
There are two options:
The file is not overly large, and you can just read it wholly to memory. Then close the file, reopen it in writing mode and write the modified contents back.
The file is too large to be stored in memory; you can move it over to a temporary file and open that, reading it line by line, writing back into the original file. Note that this requires twice the storage.
",258
39086,23123426,2,"Using hamishmcn's answer as a template I was able to search for a line in a file that match my regex and replacing it with empty string.
",29
39086,39110,2,"I guess something like this should do it. It basically writes the content to a new file and replaces the old file with the new file:
",28
39086,39089,2,"Create a new file, copy lines from the old to the new, and do the replacing before you write the lines to the new file.
",28
39086,39113,2,"As lassevk suggests, write out the new file as you go, here is some example code:
",19
39086,39086,1,"I want to loop over the contents of a text file and do a search and replace on some lines and write the result back to the file. I could first load the whole file in memory and then write it back, but that probably is not the best way to do it.
What is the best way to do this, within the following code?
",70
39104,39104,1,"I've written a Python package that includes a bsddb database of pre-computed values for one of the more time-consuming computations.  For simplicity, my setup script installs the database file in the same directory as the code which accesses the database (on Unix, something like /usr/lib/python2.5/site-packages/mypackage/).
How do I store the final location of the database file so my code can access it?  Right now, I'm using a hack based on the __file__ variable in the module which accesses the database:
dbname = os.path.join(os.path.dirname(__file__), ""database.dat"")
It works, but it seems... hackish.  Is there a better way to do this?  I'd like to have the setup script just grab the final installation location from the distutils module and stuff it into a ""dbconfig.py"" file that gets installed alongside the code that accesses the database.
",160
39104,39295,2,"That's probably the way to do it, without resorting to something more advanced like using setuptools to install the files where they belong.
Notice there's a problem with that approach, because on OSes with real a security framework (UNIXes, etc.) the user running your script might not have the rights to access the DB in the system directory where it gets installed.
",72
39104,39659,2,"Try using pkg_resources, which is part of setuptools (and available on all of the pythons I have access to right now):
There's more discussion about using pkg_resources to get resources on the eggs page and the pkg_resources page.
Also note, where possible it's probably advisable to use pkg_resources.resource_stream or pkg_resources.resource_string because if the package is part of an egg, resource_filename will copy the file to a temporary directory.
",78
39104,9918496,2,"Use pkgutil.get_data. Itâ€™s the cousin of pkg_resources.resource_stream, but in the standard library, and should work with flat filesystem installs as well as zipped packages and other importers.
",31
41547,4815619,2,"its possible by default, by doing the following steps, ensure you have added the context 'django.contrib.auth.context_processors.auth' in your settings. By default its added in settings.py, so its looks like this 
And you can access user object like this,
For more information, refer here http://docs.djangoproject.com/en/1.2/topics/auth/#authentication-data-in-templates 
",55
41547,1064621,2,"There is no need to write a context processor for the user object if you already have the ""django.core.context_processors.auth"" in TEMPLATE_CONTEXT_PROCESSORS and if you're using RequestContext in your views. 
if you are using django 1.4 or latest the module has been moved to django.contrib.auth.context_processors.auth
",48
41547,269249,2,"@Ryan: Documentation about preprocessors is a bit small
@Staale: Adding user to the Context every time one is calling the template in view, DRY
Solution is to use a preprocessor
A: In your settings add
B: In myapp/processor_file_name.py insert
From now on you're able to use user object functionalities in your templates.
",61
41547,41560,2,"@Dave
To use {{user.username}} in my templates, I will then have to use 
requestcontext rather than just a normal map/hash: http://www.djangoproject.com/documentation/templates_python/#subclassing-context-requestcontext
So I guess there are no globals that the template engine checks.
But the RequestContext has some prepopulate classes that I can look into to solve my problems. Thanks.
",64
41547,41558,2,"In a more general sense of not having to explicitly set variables in each view, it sounds like you want to look at writing your own context processor.
From the docs:
A context processor has a very simple interface: It's just a Python function that takes one argument, an HttpRequest object, and returns a dictionary that gets added to the template context. Each context processor must return a dictionary.
",78
41547,11878636,2,"The hints are in every answer, but once again, from ""scratch"", for newbies:
authentication data is in templates (almost) by default -- with a small trick:
in views.py:
in index.html:
From here: https://docs.djangoproject.com/en/1.4/topics/auth/#authentication-data-in-templates
This template context variable is not available if a RequestContext is
  not being used.
",64
41547,41555,2,"If you can hook your authentication into the Django authentication scheme you'll be able to use request.user.
I think this should just be a case of calling authenticate() and login() based on the contents of your Cookie.
Edit: @Staale - I always use the locals() trick for my context so all my templates can see request and so request.user.  If you're not then I guess it wouldn't be so straightforward.
",85
41547,41547,1,"I am working on a small intranet site for a small company, where user should be able to post. I have imagined a very simple authentication mechanism where people just enter their email address, and gets sent a unique login url, that sets a cookie that will always identify them for future requests.
In my template setup, I have base.html, and the other pages extend this. I want to show logged in or register button in the base.html, but how can I ensure that the necessary variables are always a part of the context? It seems that each view just sets up the context as they like, and there is no global context population. Is there a way of doing this without including the user in each context creation?
Or will I have to make my own custom shortcuts to setup the context properly?
",158
48458,67533,2,"I think the first option is considered the best practice. And make the code folder your first package. The Rietveld project developed by Guido van Rossum is a very good model to learn from. Have a look at it: http://code.google.com/p/rietveld
With regard to Django 1.0, I suggest you start using the Django trunk code instead of the GAE built in django port. Again, have a look at how it's done in Rietveld.
",83
48458,12535000,2,"I implemented a google app engine boilerplate today and checked it on github. This is along the lines described by Nick Johnson above (who used to work for Google).
Follow this link gae-boilerplate
",37
48458,48467,2,"I am not entirely up to date on the latest best practices, et cetera when it comes to code layout, but when I did my first GAE application, I used something along your second option, where the code and templates are next to eachother.
There was two reasons for this - one, it kept the code and template nearby, and secondly, I had the directory structure layout mimic that of the website - making it (for me) a bit easier too remember where everything was.
",96
48458,3105295,2,"I like webpy so I've adopted it as templating framework on Google App Engine.
My package folders are typically organized like this:
Here is an example.
",30
48458,48458,1,"I started an application in Google App Engine right when it came out, to play with the technology and work on a pet project that I had been thinking about for a long time but never gotten around to starting.  The result is BowlSK.  However, as it has grown, and features have been added, it has gotten really difficult to keep things organized - mainly due to the fact that this is my first python project, and I didn't know anything about it until I started working.
What I have:
Main Level contains:
all .py files (didn't know how to make packages work)
all .html templates for main level pages
Subdirectories:
separate folders for css, images, js, etc.
folders that hold .html templates for subdirecty-type urls
Example:
http://www.bowlsk.com/ maps to HomePage (default package), template at ""index.html""
http://www.bowlsk.com/games/view-series.html?series=7130 maps to ViewSeriesPage (again, default package), template at ""games/view-series.html""
It's nasty.  How do I restructure?  I had 2 ideas:
Main Folder containing: appdef, indexes, main.py?
Subfolder for code.  Does this have to be my first package?
Subfolder for templates.  Folder heirarchy would match package heirarchy
Individual subfolders for css, images, js, etc.
Main Folder containing appdef, indexes, main.py?
Subfolder for code + templates.  This way I have the handler class right next to the template, because in this stage, I'm adding lots of features, so modifications to one mean modifications to the other.  Again, do I have to have this folder name be the first package name for my classes?  I'd like the folder to be ""src"", but I don't want my classes to be ""src.WhateverPage""
Is there a best practice?  With Django 1.0 on the horizon, is there something I can do now to improve my ability to integrate with it when it becomes the official GAE templating engine?  I would simply start trying these things, and seeing which seems better, but pyDev's refactoring support doesn't seem to handle package moves very well, so it will likely be a non-trivial task to get all of this working again.
",415
48458,70271,2,"First, I would suggest you have a look at ""Rapid Development with Python, Django, and Google App Engine""
GvR describes a general/standard project layout on page 10 of his slide presentation.  
Here I'll post a slightly modified version of the layout/structure from that page. I pretty much follow this pattern myself. You also mentioned you had trouble with packages. Just make sure each of your sub folders has an __init__.py file. It's ok if its empty.
Boilerplate files
These hardly vary between projects
app.yaml: direct all non-static requests to main.py 
main.py: initialize app and send it all requests 
Project lay-out
static/*: static files; served directly by App Engine
myapp/*.py: app-specific python code
views.py, models.py, tests.py, __init__.py, and more
templates/*.html: templates (or myapp/templates/*.html)
Here are some code examples that may help as well:
main.py
myapp/views.py
myapp/models.py
I think this layout works great for new and relatively small to medium projects. For larger projects I would suggest breaking up the views and models to have their own sub-folders with something like:
Project lay-out
static/: static files; served directly by App Engine
js/*.js
images/*.gif|png|jpg
css/*.css
myapp/: app structure
models/*.py
views/*.py
tests/*.py
templates/*.html: templates
",222
48458,153862,2,"My usual layout looks something like this:
app.yaml
index.yaml
request.py - contains the basic WSGI app
lib
__init__.py - common functionality, including a request handler base class
controllers - contains all the handlers. request.yaml imports these.
templates
all the django templates, used by the controllers
model
all the datastore model classes
static
static files (css, images, etc). Mapped to /static by app.yaml
I can provide examples of what my app.yaml, request.py, lib/init.py, and sample controllers look like, if this isn't clear.
",97
48562,48575,2,"What exactly are you trying to do?
Of course, there are numerous places to learn about svn pre-commit hooks (e.g.  here ,  here, and in the Red Book) but it depends what you're trying to do and what is available on your system.  
Can you be more specific? 
",57
48562,50507,2,"I think you can avoid a commit hook script in this case by using the svn:eol-style property as described in the SVNBook:
End-of-Line Character Sequences
Subversion Properties
This way SVN can worry about your line endings for you.
Good luck!
",45
48562,48562,1,"I was wondering if anyone here had some experience writing this type of script and if they could give me some pointers.
I would like to modify this script to validate that the check-in file does not have a Carriage Return in the EOL formatting. The EOL format is CR LF in Windows and LF in Unix. When a User checks-in code with the Windows format. It does not compile in Unix anymore. I know this can be done on the client side but I need to have this validation done on the server side. To achieve this, I need to do the following:
1) Make sure the file I check is not a binary, I dont know how to do this with svnlook, should I check the mime:type of the file? The Red Book does not indicate this clearly or I must have not seen it.
2) I would like to run the dos2unix command to validate that the file has the correct EOL format. I would compare the output of the dos2unix command against the original file. If there is a diff between both, I give an error message to the client and cancel the check-in.
I would like your comments/feedback on this approach.
",227
48777,48777,1,"I'm trying to get Google AppEngine to work on my Debian box and am getting the following error when I try to access my page:
The same app works fine for me when I run it on my other Ubuntu box, so I know it's not a problem with the app itself. However, I need to get it working on this Debian box. It originally had python 2.4 but after AppEngine complained about it I installed the python2.5 and python2.5-dev packages (to no avail). 
I saw on this Google Group post that it may be due to the version of AppEngine and just to reinstall it, but that didn't work. Any ideas?
Edit 1: Also tried uninstalling python2.4 and 2.5 then reinstalling 2.5, which also didn't work.
Edit 2: Turns out when I made AppEngine into a CVS project it didn't add the core directory into my project, so when I checked it out there literally was no module named core.exceptions. Re-downloading that folder resolved the problem.
",190
48777,48806,2,"core.exceptions is part of django; what version of django do you have installed? The AppEngine comes with the appropriate version for whatever release you've downloaded (in the lib/django directory). It can be installed by going to that directory and running python setup.py install
",49
48884,74335,2,"pyaws seems to be the best one out there.  I used it here (my source code)  It worked fine for me.
",25
48884,49222,2,"The only other library I'm aware of is pyAmazon, which is the predecessor of pyaws.  If you're familiar with the Amazon API (or are willing to become so), you could probably put together something yourself with ZSI.
",45
48884,1084257,2,"pyaws is the best in my opinion.  The most available version is 0.2.0, but there is also a version 0.3.0 that is somewhat harder to find.  The best maintained version of it that I have found though, which is based on 0.3.0, is on bitbucket.
",51
48884,48884,1,"What Python libraries do folks use for querying Amazon product data? (Amazon Associates Web Service - used to be called E-Commerce API, or something along those lines).
Based on my research, PyAWS seems okay, but still pretty raw (and hasn't been updated in a while).  Wondering if there's an obvious canonical library that I'm just missing.
",70
48884,1533430,2,"PyAWS is no longer hosted on SourceForge. The latest version (0.3.0) is available via the authors website.
Make sure you also grab the patch for Amazons latest API changes, mentioned in the comments.
",39
48884,2448578,2,"There is now another alternative: python-amazon-product-api. It supports API version 2009-11-01 2010-12-01.
",15
48884,10076757,2,"If what you are looking for is a simple, object oriented access to Amazon products (lookup and search), try python-amazon-simple-product-api. Its a new project i've just released:
http://github.com/yoavaviram/python-amazon-simple-product-api
Its the new kid on the block!
",45
48884,1087797,2,"How about boto? Anyone have any experience with it? I just started looking for a Python package for Amazon and boto looks up to date (v1.8c release 28-Jun-2009), active and complete (has a long list of supported interfaces).
",46
48884,4251893,2,"I'm using Bottlenose, Dan Loewenherz's ""super awesome Python wrapper for the Amazon Product Advertising API"". It doesn't parse the XML, so I'm using lxml.objectify:
",34
49137,49148,2,"Boost has a python interface library which could help you.
Boost.Python
",12
49137,49319,2,"Embeding the Python interpreter inside your C++ app will let you run Python programs using your application run Python scripts. It will also make it easier possible for those scripts to call C++ functions in your application. If this is what you want then the Boost library mentioned previously may be what you want to make it easier to create the link. In the past I have used SWIG to generate Python interfaces to C++ code. It was not clear from your question whether you wanted the Python scripts to call your C++ program or whether you just wanted the C++ to call Python.
Many of the Python functions use modules which are not built into the Python interpreter. If your Python scripts call these functions then you will either need to have your users install Python or include the python runtime files with your application. It will depend on what modules you import in you Python scripts.
",166
49137,49439,2,"Boost is probably the best choice, however if you're wanting something that's more standalone, and if this is for use with Windows (which seems feasible given that they are the people least likely to have Python installed), then you can use py2exe to create a DLL with entry points suitable for COM objects.  You can then interface with the library via COM.  (Obviously this is not at all useful as a cross-platform solution).
",85
49137,328451,2,"
I would like to call python script files from my c++ program.
This means that you want to embed Python in your C++ application. As mentioned in Embedding Python in Another Application:
Embedding Python is similar to
  extending it, but not quite. The
  difference is that when you extend
  Python, the main program of the
  application is still the Python
  interpreter, while if you embed
  Python, the main program may have
  nothing to do with Python â€” instead,
  some parts of the application
  occasionally call the Python
  interpreter to run some Python code.
I suggest that you first go through Embedding Python in Another Application. Then refer the following examples
Embedding Python in C/C++: Part I
Embedding Python in C/C++: Part II
Embedding Python in Multi-Threaded C/C++ Applications
If you like Boost.Python, you may visit the following links:
Embedding Python with Boost.Python Part 1
",160
49137,49137,1,"I would like to call python script files from my c++ program. 
I am not sure that the people I will distribute to will have python installed.
Basically I'm looking for a .lib file that I can use that has an Apache like distribution license.
",49
49146,15667090,2,"Use cx_Freeze to make exe your python program
",8
49146,49155,2,"py2exe is probably what you want, but it only works on Windows.
PyInstaller works on Windows and Linux.
Py2app works on the Mac.
",27
49146,49179,2,"See a short list of python packaging tools on FreeHackers.org.
",11
49146,2245736,2,"I found this presentation to be very helpfull.
How I Distribute Python applications on Windows - py2exe & InnoSetup
From the site:
There are many deployment options for
  Python code. I'll share what has
  worked well for me on Windows,
  packaging command line tools and
  services using py2exe and InnoSetup.
  I'll demonstrate a simple build script
  which creates windows binaries and an
  InnoSetup installer in one step. In
  addition, I'll go over common errors
  which come up when using py2exe and
  hints on troubleshooting them. This is
  a short talk, so there will be a
  follow-up Open Space session to share
  experience and help each other solve
  distribution problems.
",122
49146,3384483,2,"Not on the freehackers list is gui2exe which can be used to build standalone Windows executables, Linux applications and Mac OS application bundles and plugins starting from Python scripts.
",31
49146,10241354,2,"Also known as Frozen Binaries but not the same as as the output of a true compiler- they run byte code through a virtual machine (PVM). Run the same as a compiled program just larger because the program is being compiled along with the PVM. Py2exe can freeze standalone programs that use the tkinter, PMW, wxPython, and PyGTK GUI libraties; programs that use the pygame game programming toolkit; win32com client programs; and more. 
The Stackless Python system is a standard CPython implementation variant that does not save state on the C language call stack. This makes Python more easy to port to small stack architectures, provides efficient multiprocessing options, and fosters novel programming structures such as coroutines. Other systems of study that are working on future development: Pyrex is working on the Cython system, the Parrot project, the PyPy is working on replacing the PVM altogether, and of course the founder of Python is working with Google to get Python to run 5 times faster than C with the Unladen Swallow project. In short, py2exe is the easiest and Cython is more efficient for now until these projects improve the Python Virtual Machine (PVM) for standalone files.
",221
49146,49146,1,"I've used several modules to make EXEs for Python, but I'm not sure if I'm doing it right.
How should I go about this, and why?  Please base your answers on personal experience, and provide references where necessary.
",47
49146,49150,2,"py2exe:
py2exe is a Python Distutils extension which converts Python scripts into executable Windows programs, able to run without requiring a Python installation.
",26
49164,49169,2,"Setuptools is the software that creates .egg files. It's an extension of the distutils package in the standard library.
The process involves creating a setup.py file, then  python setup.py bdist_egg creates an .egg package.
",39
49164,49175,2,"Also, if you need to get an .egg package off a single .py file app, check this link: EasyInstall - Packaging others projects as eggs.
",29
49164,49164,1,"How do I turn a python program into an .egg file?
",12
49195,49195,1,"The first language I learnt  was PHP, but I have more recently picked up Python. As these are all 'high-level' languages, I have found them a bit difficult to pick up. I also tried to learn Objective-C but I gave up.
So, what language should I learn to bridge between Python to C
",60
49195,50673,2,"Pascal! Close enough syntax, still requires you to do some memory management, but not as rough for beginners.
",22
49195,49502,2,"Languages are easy to learn (especially one like C)... the hard part is learning the libraries and/or coding style of the language.  For instance, I know C++ fairly well, but most C/C++ code I see confuses me because the naming conventions are so different from what I work with on a daily basis.
Anyway, I guess what I'm trying to say is don't worry too much about the syntax, focus on said language's library.  This isn't specific to C, you can say the same about c#, vb.net, java and just about every other language out there.
",115
49195,49295,2,"try to learn a language which you are comfortable with, try different approach and the basics.
",18
49195,49285,2,"I generally agree with most of the others - There's not really a good stepping stone language.
It is, however, useful to understand what is difficult about learning C, which might help you understand what's making it difficult for you.
I'd say the things that would prove difficult in C for someone coming from PHP would be :
Pointers and memory management This is pretty much the reason you're learning C I imagine, so there's not really any getting around it. Learning lower level assembly type languages might make this easier, but C is probably a bridge to do that, not the other way around.
Lack of built in data structures PHP and co all have native String types, and useful things like hash tables built in, which is not the case in C. In C, a String is just an array of characters, which means you'll need to do a lot more work, or look seriously at libraries which add the features you're used to.
Lack of built in libraries Languages like PHP nowadays almost always come with stacks of libraries for things like database connections, image manipulation and stacks of other things. In C, this is not the case other than a very thin standard library which revolves mostly around file reading, writing and basic string manipulation. There are almost always good choices available to fill these needs, but you need to include them yourself.
Suitability for high level tasks If you try to implement the same type of application in C as you might in PHP, you'll find it very slow going. Generating a web page, for example, isn't really something plain C is suited for, so if you're trying to do that, you'll find it very slow going.
Preprocessor and compilation Most languages these days don't have a preprocessor, and if you're coming from PHP, the compilation cycle will seem painful. Both of these are performance trade offs in a way - Scripting languages make the trade off in terms of developer efficiency, where as C prefers performance.
I'm sure there are more that aren't springing to mind for me right now. The moral of the story is that trying to understand what you're finding difficult in C may help you proceed. If you're trying to generate web pages with it, try doing something lower level. If you're missing hash tables, try writing your own, or find a library. If you're struggling with pointers, stick with it :)
",475
49195,49217,2,"I'm feeling your pain, I also learned PHP first and I'm trying to learn C++, it's not easy, and I am really struggling, It's been 2 years since I started on c++ and Still the extent of what I can do is cout, cin, and math.
If anyone reads this and wonders where to start, START LOWER.
",70
49195,49247,2,"My suggestion is to get a good C-book that is relevant to what you want to do. I agree that K & R is considered to be ""The book"" on C, but I found ""UNIX Systems Programming"" by Kay A. Robbins and Steven Robbins to be more practical and hands on. The book is full of clean and short code snippets you can type in, compile and try in just a few minutes each.
There is a preview at http://books.google.com/books?id=tdsZHyH9bQEC&printsec=frontcover (Hyperlinking it didn't work.)
",103
49195,49237,2,"Java might actually be a good option here, believe it or not.  It is strongly based on C/C++, so if you can get the syntax and the strong typing, picking up C might be easier.  The benefit is you can learn the lower level syntax without having to learn pointers (since memory is managed for you just like in Python and PHP).  You will, however, learn a similar concept... references (or objects in general).
Also, it is strongly Object Oriented, so it may be difficult to pick up on that if you haven't dealt with OOP yet....  you might be better off just digging in with C like others suggested, but it is an option.
",136
49195,49234,2,"Learning any language takes time, I always ensure I have a measurable goal; I set myself an objective, then start learning the language to achieve this objective, as opposed to trying to learn every nook and cranny of the language and syntax. 
C is not easy, pointers can be hard to comprehend if youâ€™re not coming assembler roots. I first learned C++, then retro fit C to my repertoire but I started with x86 and 68000 assembler.
",86
49195,49227,2,"The best place to start learning C is the book ""The C Programming Language"" by Kernighan and Ritchie.
You will recognise a lot of things from PHP, and you will be surprised how much PHP (and Perl, Python etc) do for you.
Oh and you also will need a C compiler, but i guess you knew that.
",67
49195,49202,2,"It's not clear why you need a bridge language. Why don't you start working with C directly? C is a very simple language itself. I think that hardest part for C learner is pointers and everything else related to memory management. Also C lang is oriented on structured programming, so you will need to learn how to implement data structures and algorithms without OOP goodness. Actually, your question is pretty hard, usually people go from low level langs to high level and I can understand frustration of those who goes in other direction.
",104
49195,49248,2,"I think C++ is a good ""bridge"" to C.  I learned C++ first at University, and since it's based on C you'll learn a lot of the same concepts - perhaps most notably pointers - but also Object Oriented Design.  OO can be applied to all kinds of modern languages, so it's worth learning.  
After learning C++, I found it wasn't too hard to pick up the differences between C++ and C as required (for example, when working on devices that didn't support C++).
",100
49195,49241,2,"Python is about as close to C as you're going to get.  It is in fact a very thin wrapper around C in a lot of places.  However, C does require that you know a little more about how the computer works on a low level.  Thus, you may benefit from trying an assembly language.
LC-3 is a simple assembly language with a simulated machine.
Alternatively, you could try playing with an interactive C interpreter like CINT.
Finally, toughing it out and reading K&R's book is usually the best approach.
",105
49195,49245,2,"Forget Java - it is not going to bring you anywhere closer to C (you have allready proved that you don't have a problem learning new syntax).
Either read K&R or go one lower: Learn about the machine itself. The only tricky part in C is pointers and memory management (which is closely related to pointers, but also has a bit to do with how functions are called). Learning a (simple, maybe even ""fake"" assembly) language should help you out here.
Then, start reading up on the standard library provided by C. It will be your daily bread and butter.
Oh: another tip! If you really do want to bridge, try FORTH. It helped me get into pointers. Also, using the win32 api from Visual Basic 6.0 can teach you some stuff about pointers ;)
",162
49195,49246,2,"C is a bridge onto itself.
K&R is the only programming language book you can read in one sitting and almost never pick it up again ... 
",30
49307,65903,2,"If x and y are column vectors, you can do:
(with row vectors, just use x and y).
Here is an example run:
",30
49307,49307,1,"Using the zip function, Python allows for loops to traverse multiple sequences in parallel.  
for (x,y) in zip(List1, List2):
Does MATLAB have an equivalent syntax? If not, what is the best way to iterate over two parallel arrays at the same time using MATLAB?
",58
49307,49514,2,"If I'm not mistaken the zip function you use in python creates a pair of the items found in list1 and list2. Basically it still is a for loop with the addition that it will retrieve the data from the two seperate lists for you, instead that you have to do it yourself.
So maybe your best option is to use a standard for loop like this:
or whatever you have to do with the data.
If you really are talking about parallel computing then you should take a look at the Parallel Computing Toolbox for matlab, and more specifically at parfor
",109
49307,51137,2,"Tested only in octave... (no matlab license). Variations of arrayfun() exist, check the documentation.
Yields...
",24
49307,138886,2,"for loops in MATLAB used to be slow, but this is not true anymore.
So vectorizing is not always the miracle solution. Just use the profiler, and tic and toc functions to help you identify possible bottlenecks.
",42
49307,218618,2,"I would recommend to join the two arrays for the computation:
This will work great if your functions can work with vectors. Then again, many functions can even work with matrices, so you wouldn't even need the loop.
",44
49455,49485,2,"A plugin for GSview for viewing encrypted PDFs is here.
If this works for you, you may be able to look at the source.
",27
49455,110110,2,"xpdf is probably a good reference implementation for this sort of problem.  I have successfully used them to open encrypted pdfs before.
",24
49455,107838,2,"If I remember correctly, there is a fixed padding string of 32 (?) bytes to apply to any password. All passwords need to be 32 bytes at the start of computing the encryption key, either by truncating or adding some of those padding bytes.
If no user password was set you simply have to pad with all 32 bytes of the string, i.e. use the 32 padding bytes as the starting point for computing the encryption key.
I have to admit it's been a while since I've done this, I do remember that the encryption part of the PDF is an absolute mess as it got changed significantly in nearly every revision, requiring you to cope with a lot of cases to handle all PDF's.
Good luck.
",143
49455,49455,1,"Although the PDF specification is available from Adobe, it's not exactly the simplest document to read through.  PDF allows documents to be encrypted so that either a user password and/or an owner password is required to do various things with the document (display, print, etc).  A common use is to lock a PDF so that end users can read it without entering any password, but a password is required to do anything else.
I'm trying to parse PDFs that are locked in this way (to get the same privileges as you would get opening them in any reader).  Using an empty string as the user password doesn't work, but it seems (section 3.5.2 of the spec) that there has to be a user password to create the hash for the admin password.
What I would like is either an explanation of how to do this, or any code that I can read (ideally Python, C, or C++, but anything readable will do) that does this so that I can understand what I'm meant to be doing.  Standalone code, rather than reading through (e.g.) the gsview source, would be best.
",222
51412,51415,2,"Put the asterisks before the kwargs variable. This makes Python pass the variable (which is assumed to be a dictionary) as keyword arguments.
",27
51412,63943,2,"As an aside: When using functions instead of methods, you could also use functools.partial:
The last line will define a function ""bar"" that, when called, will call foo with the first argument set to ""argvalue"" and all other functions just passed on:
will call
Unfortunately that will not work with methods.
",62
51412,51412,1,"Say I have the following methods:
In methodA I wish to call methodB, passing on the kwargs. However, it seems that if I define methodA as follows, the second argument will be passed on as positional rather than named variable arguments.
How do I make sure that the **kwargs in methodA gets passed as **kwargs to methodB?
",64
51412,51414,2,"Some experimentation and I figured this one out:
def methodA(arg, **kwargs):
    methodB(""argvalue"", **kwargs)
Seems obvious now...
",29
51502,51570,2,"Rather than focusing on making your existing structures more readable, you should focus on making more logical structures.  Make smaller blocks, try not to nest blocks excessively, make smaller functions, and try to think through your code flow more.
If you come to a point where you can't quickly determine the structure of your code, you should probably consider refactoring and adding some comments.  Code flow should always be immediately apparent -- the more you have to think about it, the less maintainable your code becomes.
",97
51502,3054853,2,"
Need I say more? :)
Seriously, PEP 8, 'Blank lines', ยง4 is the official way to do it.
",25
51502,52111,2,"I would look in to understanding more details about Python syntax. Often times if a piece of code looks odd, there usually is a better way to write it. For example, in the above example:
While it is a small change, it might help the readability. Also, in all honesty, I've never used a while loop, so there is a good change you would end up with a nice concise list comprehension or for loop instead. ;)
",90
51502,52090,2,"Part of learning a new programming language is learning to read code in that language.  A crutch like this may make it easier to read your own code, but it's going to impede the process of learning how to read anyone else's Python code.  I really think you'd be better off getting rid of the end of block comments and getting used to normal Python.
",72
51502,51551,2,"I like to put blank lines around blocks to make control flow more obvious. For example:
",18
51502,51531,2,"Perhaps the best thing would be to turn on ""show whitespace"" in your editor.  Then you would have a visual indication of how far in each line is tabbed (usually a bunch of dots), and it will be more apparent when that changes.
",50
51502,51505,2,"You could try increasing the indent size, but in general I would just say, relax, it will come with time. I don't think trying to make Python look like C is a very good idea.
",41
51502,51502,1,"I've been really enjoying Python programming lately. I come from a background of a strong love for C-based coding, where everything is perhaps more complicated than it should be (but puts hair on your chest, at least). So switching from C to Python for more complex things that don't require tons of speed has been more of a boon than a bane in writing projects.
However, coming from this land of brackets and parentheses and structs as far as the naked eye can see, I come across a small problem: I find Python difficult to read.
For example, the following block of text is hard for me to decipher unless I stare at it (which I dislike doing):
The problem occurs at the end of that if block: all the tabbing and then suddenly returning to a jarring block feels almost disturbing. As a solution, I've started coding my Python like this:
And this, for some odd reason, makes me more able to read my own code. But I'm curious: has anyone else with my strange problem found easier ways to make their tabbed-out code more readable? I'd love to find out if there's a better way to do this before this becomes a huge habit for me.
",239
51520,51520,1,"Given a path such as ""mydir/myfile.txt"", how do I find the absolute filepath relative to the current working directory in Python? E.g. on Windows, I might end up with:
",36
51520,43691204,2,"
",0
51520,51523,2,"
Also works if it is already an absolute path:
",10
51520,51539,2,"
",0
51520,58417,2,"Better still, install the path.py module, it wraps all the os.path functions and other related functions into methods on an object that can be used wherever strings are used:
",32
51520,15325066,2,"Today you can also use the unipath package which was based on path.py: http://sluggo.scrapping.cc/python/unipath/
I would recommend using this package as it offers a clean interface to common os.path utilities.
",34
51520,26539947,2,"You could use the new Python 3.4 library pathlib. (You can also get it for Python 2.6 or 2.7 using pip install pathlib.) The authors wrote: ""The aim of this library is to provide a simple hierarchy of classes to handle filesystem paths and the common operations users do over them.""
To get an absolute path in Windows:
Or on UNIX:
Docs are here: https://docs.python.org/3/library/pathlib.html
",78
51520,38813098,2,"I prefer to use glob
here is how to list all file types in your current folder:
here is how to list all (for example) .txt files in your current folder:
here is how to list all file types in a chose directory:
hope this helped you
",52
51553,51976,2,"You need to increase postgres' caches to the point where the whole working set fits into memory before you can expect to see perfomance comparable to doing it in-memory with a program.
",34
51553,51553,1,"I need a real DBA's opinion. Postgres 8.3 takes 200 ms to execute this query on my Macbook Pro while Java and Python perform the same calculation in under 20 ms (350,000 rows):
Is this normal behaviour when using a SQL database?
The schema (the table holds responses to a survey):
I wrote some tests in Java and Python for context and they crush SQL (except for pure python):
Even sqlite3 is competitive with Postgres despite it assumping all columns are strings (for contrast: even using just switching to numeric columns instead of integers in Postgres results in 10x slowdown)
Tunings i've tried without success include (blindly following some web advice):
So my question is, is my experience here normal, and this is what I can expect when using a SQL database?  I can understand that ACID must come with costs, but this is kind of crazy in my opinion.  I'm not asking for realtime game speed, but since Java can process millions of doubles in under 20 ms, I feel a bit jealous. 
Is there a better way to do simple OLAP on the cheap (both in terms of money and server complexity)?  I've looked into Mondrian and Pig + Hadoop but not super excited about maintaining yet another server application and not sure if they would even help.
No the Python code and Java code do all the work in house so to speak.  I just generate 4 arrays with 350,000 random values each, then take the average.  I don't include the generation in the timings, only the averaging step.  The java threads timing uses 4 threads (one per array average), overkill but it's definitely the fastest.
The sqlite3 timing is driven by the Python program and is running from disk (not :memory:)
I realize Postgres is doing much more behind the scenes, but most of that work doesn't matter to me since this is read only data.
The Postgres query doesn't change timing on subsequent runs.
I've rerun the Python tests to include spooling it off the disk.  The timing slows down considerably to nearly 4 secs.  But I'm guessing that Python's file handling code is pretty much in C (though maybe not the csv lib?) so this indicates to me that Postgres isn't streaming from the disk either (or that you are correct and I should bow down before whoever wrote their storage layer!)
",463
51553,51668,2,"I don't think that your results are all that surprising -- if anything it is that Postgres is so fast.
Does the Postgres query run faster a second time once it has had a chance to cache the data?  To be a little fairer your test for Java and Python should cover the cost of acquiring the data in the first place (ideally loading it off disk).
If this performance level is a problem for your application in practice but you need a RDBMS for other reasons then you could look at memcached.  You would then have faster cached access to raw data and could do the calculations in code.
",118
51553,51933,2,"Those are very detailed answers, but they mostly beg the question, how do I get these benefits without leaving Postgres given that the data easily fits into memory, requires concurrent reads but no writes and is queried with the same query over and over again.
Is it possible to precompile the query and optimization plan? I would have thought the stored procedure would do this, but it doesn't really help.
To avoid disk access it's necessary to cache the whole table in memory, can I force Postgres to do that?  I think it's already doing this though, since the query executes in just 200 ms after repeated runs.
Can I tell Postgres that the table is read only, so it can optimize any locking code?
I think it's possible to estimate the query construction costs with an empty table (timings range from 20-60 ms) 
I still can't see why the Java/Python tests are invalid.  Postgres just isn't doing that much more work (though I still haven't addressed the concurrency aspect, just the caching and query construction)
UPDATE: 
I don't think it's fair to compare the SELECTS as suggested by pulling 350,000 through the driver and serialization steps into Python to run the aggregation, nor even to omit the aggregation as the overhead in formatting and displaying is hard to separate from the timing.  If both engines are operating on in memory data, it should be an apples to apples comparison, I'm not sure how to guarantee that's already happening though.
I can't figure out how to add comments, maybe i don't have enough reputation?
",304
51553,51745,2,"I would say your test scheme is not really useful. To fulfill the db query, the db server goes through several steps:
parse the SQL
work up a query plan, i. e. decide on which indices to use (if any), optimize etc.
if an index is used, search it for the pointers to the actual data, then go to the appropriate location in the data or
if no index is used, scan the whole table to determine which rows are needed
load the data from disk into a temporary location (hopefully, but not necessarily, memory)
perform the count() and avg() calculations
So, creating an array in Python and getting the average basically skips all these steps save the last one. As disk I/O is among the most expensive operations a program has to perform, this is a major flaw in the test (see also the answers to this question I asked here before). Even if you read the data from disk in your other test, the process is completely different and it's hard to tell how relevant the results are.
To obtain more information about where Postgres spends its time, I would suggest the following tests:
Compare the execution time of your query to a SELECT without the aggregating functions (i. e. cut step 5)
If you find that the aggregation leads to a significant slowdown, try if Python does it faster, obtaining the raw data through the plain SELECT from the comparison.
To speed up your query, reduce disk access first. I doubt very much that it's the aggregation that takes the time.
There's several ways to do that:
Cache data (in memory!) for subsequent access, either via the db engine's own capabilities or with tools like memcached
Reduce the size of your stored data
Optimize the use of indices. Sometimes this can mean to skip index use altogether (after all, it's disk access, too). For MySQL, I seem to remember that it's recommended to skip indices if you assume that the query fetches more than 10% of all the data in the table.
If your query makes good use of indices, I know that for MySQL databases it helps to put indices and data on separate physical disks. However, I don't know whether that's applicable for Postgres.
There also might be more sophisticated problems such as swapping rows to disk if for some reason the result set can't be completely processed in memory. But I would leave that kind of research until I run into serious performance problems that I can't find another way to fix, as it requires knowledge about a lot of little under-the-hood details in your process.
Update:
I just realized that you seem to have no use for indices for the above query and most likely aren't using any, too, so my advice on indices probably wasn't helpful. Sorry. Still, I'd say that the aggregation is not the problem but disk access is. I'll leave the index stuff in, anyway, it might still have some use.
",584
51553,53713,2,"Are you using TCP to access the Postgres? In that case Nagle is messing with your timing.
",19
51553,53333,2,"I retested with MySQL specifying ENGINE = MEMORY and it doesn't change a thing (still 200 ms).  Sqlite3 using an in-memory db gives similar timings as well (250 ms).
The math here looks correct (at least the size, as that's how big the sqlite db is :-)
I'm just not buying the disk-causes-slowness argument as there is every indication the tables are in memory (the postgres guys all warn against trying too hard to pin tables to memory as they swear the OS will do it better than the programmer)
To clarify the timings, the Java code is not reading from disk, making it a totally unfair comparison if Postgres is reading from the disk and calculating a complicated query, but that's really besides the point, the DB should be smart enough to bring a small table into memory and precompile a stored procedure IMHO.
UPDATE (in response to the first comment below):
I'm not sure how I'd test the query without using an aggregation function in a way that would be fair, since if i select all of the rows it'll spend tons of time serializing and formatting everything.  I'm not saying that the slowness is due to the aggregation function, it could still be just overhead from concurrency, integrity, and friends.  I just don't know how to isolate the aggregation as the sole independent variable.
",263
51553,53303,2,"I'm a MS-SQL guy myself, and we'd use DBCC PINTABLE to keep a table cached, and SET STATISTICS IO to see that it's reading from cache, and not disk. 
I can't find anything on Postgres to mimic PINTABLE, but pg_buffercache seems to give details on what is in the cache - you may want to check that, and see if your table is actually being cached.
A quick back of the envelope calculation makes me suspect that you're paging from disk. Assuming Postgres uses 4-byte integers, you have (6 * 4) bytes per row, so your table is a minimum of (24 * 350,000) bytes ~ 8.4MB. Assuming 40 MB/s sustained throughput on your HDD, you're looking at right around 200ms to read the data (which, as pointed out, should be where almost all of the time is being spent). 
Unless I screwed up my math somewhere, I don't see how it's possible that you are able to read 8MB into your Java app and process it in the times you're showing - unless that file is already cached by either the drive or your OS.
",218
51553,52179,2,"Thanks for the Oracle timings, that's the kind of stuff I'm looking for (disappointing though :-)
Materialized views are probably worth considering as I think I can precompute the most interesting forms of this query for most users.
I don't think query round trip time should be very high as i'm running the the queries on the same machine that runs Postgres, so it can't add much latency?
I've also done some checking into the cache sizes, and it seems Postgres relies on the OS to handle caching, they specifically mention BSD as the ideal OS for this, so I thinking Mac OS ought to be pretty smart about bringing the table into memory.  Unless someone has more specific params in mind I think more specific caching is out of my control.
In the end I can probably put up with 200 ms response times, but knowing that 7 ms is a possible target makes me feel unsatisfied, as even 20-50 ms times would enable more users to have more up to date queries and get rid of a lots of caching and precomputed hacks.
I just checked the timings using MySQL 5 and they are slightly worse than Postgres.  So barring some major caching breakthroughs, I guess this is what I can expect going the relational db route.
I wish I could up vote some of your answers, but I don't have enough points yet.
",263
51553,52006,2,"Postgres is doing a lot more than it looks like (maintaining data consistency for a start!)
If the values don't have to be 100% spot on, or if the table is updated rarely, but you are running this calculation often, you might want to look into Materialized Views to speed it up.
(Note, I have not used materialized views in Postgres, they look at little hacky, but might suite your situation).
Materialized Views
Also consider the overhead of actually connecting to the server and the round trip required to send the request to the server and back.
I'd consider 200ms for something like this to be pretty good, A quick test on my oracle server, the same table structure with about 500k rows and no indexes, takes about 1 - 1.5 seconds, which is almost all just oracle sucking the data off disk.
The real question is, is 200ms fast enough?
-------------- More --------------------
I was interested in solving this using materialized views, since I've never really played with them. This is in oracle.
First I created a MV which refreshes every minute.
While its refreshing, there is no rows returned
Once it refreshes, its MUCH faster than doing the raw query
If we insert into the base table, the result is not immediately viewable view the MV.
But wait a minute or so, and the MV will update behind the scenes, and the result is returned fast as you could want.
This isn't ideal. for a start, its not realtime, inserts/updates will not be immediately visible. Also, you've got a query running to update the MV whether you need it or not (this can be tune to whatever time frame, or on demand). But, this does show how much faster an MV can make it seem to the end user, if you can live with values which aren't quite upto the second accurate.
",380
51553,51817,2,"One other thing that an RDBMS generally does for you is to provide concurrency by protecting you from simultaneous access by another process.  This is done by placing locks, and there's some overhead from that.
If you're dealing with entirely static data that never changes, and especially if you're in a basically ""single user"" scenario, then using a relational database doesn't necessarily gain you much benefit.
",78
58294,8848856,2,"
",0
58294,2646313,2,"Using the address suggested in the source of http://whatismyip.com
",11
58294,256358,2,"import socket
s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
s.connect((""msn.com"",80))
s.getsockname()
",22
58294,58299,2,"The only way I can think of that's guaranteed to give it to you is to hit a service like http://whatismyip.com/ to get it.
",28
58294,58296,2,"This isn't possible without cooperation from an external server, because there could be any number of NATs between you and the other computer. If it's a custom protocol, you could ask the other system to report what address it's connected to.
",48
58294,58294,1,"When I call socket.getsockname() on a socket object, it returns a tuple of my machine's internal IP and the port. However, I would like to retrieve my external IP. What's the cheapest, most efficient manner of doing this?
",48
58294,8305740,2,"https://github.com/bobeirasa/mini-scripts/blob/master/externalip.py
",3
58622,58622,1,"I like doxygen to create documentation of C or PHP code. I have an upcoming Python project and I think I remember that Python doesn't have /* .. */ comments and also has its own self-documentation facility which seems to be the pythonic way to document.
Can I just use doxygen? Anything particular to be aware of?
I have done some coding in Python but so far only on small projects where I was to lazy to document at all (yeah, I know ... but let's just pretend that's OK for now).
",103
58622,35377654,2,"In the end, you only have two options:
You generate your content using Doxygen, or you generate your content using Sphinx*.
Doxygen: It is not the tool of choice for most Python projects. But if you have to deal with other related projects written in C or C++ it could make sense. For this you can improve the integration between Doxygen and Python using doxypypy.
Sphinx: The defacto tool for documenting a Python project. You have three options here: manual, semi-automatic (stub generation) and fully automatic (Doxygen like). 
For manual API documentation you have Sphinx autodoc. This is great to write a user guide with embedded API generated elements.
For semi-automatic you have Sphinx autosummary. You can either setup your build system to call sphinx-autogen or setup your Sphinx with the autosummary_generate config. You will require to setup a page with the autosummaries, and then manually edit the pages. You have options, but my experience with this approach is that it requires way too much configuration, and at the end even after creating new templates, I found bugs and the impossibility to determine exactly what was exposed as public API and what not. My opinion is this tool is good for stub generation that will require manual editing, and nothing more. Is like a shortcut to end up in manual.
Fully automatic. This have been criticized many times and for long we didn't have a good fully automatic Python API generator integrated with Sphinx until AutoAPI came, which is a new kid in the block. This is by far the best for automatic API generation in Python (note: shameless self-promotion).
There are other options to note:
Breathe: this started as a very good idea, and makes sense when you work with several related project in other languages that use Doxygen. The idea is to use Doxygen XML output and feed it to Sphinx to generate your API. So, you can keep all the goodness of Doxygen and unify the documentation system in Sphinx. Awesome in theory. Now, in practice, the last time I checked the project wasn't ready for production.
pydoctor*: Very particular. Generates its own output. It has some basic integration with Sphinx, and some nice features.
",423
58622,58701,2,"This is documented on the doxygen website, but to summarize here:
You can use doxygen to document your Python code. You can either use the Python documentation string syntax:
In which case the comments will be extracted by doxygen, but you won't be able to use any of the special doxygen commands.
Or you can (similar to C-style languages under doxygen) double up the comment marker (#) on the first line before the member:
In that case, you can use the special doxygen commands. There's no particular Python output mode, but you can apparently improve the results by setting OPTMIZE_OUTPUT_JAVA to YES.
Honestly, I'm a little surprised at the difference - it seems like once doxygen can detect the comments in ## blocks or """""" blocks, most of the work would be done and you'd be able to use the special commands in either case. Maybe they expect people using """""" to adhere to more Pythonic documentation practices and that would interfere with the special doxygen commands?
",195
58622,59018,2,"An other very good documentation tool is sphinx. It will be used for the upcoming python 2.6 documentation and is used by django and a lot of other python projects.
From the sphinx website:
Output formats: HTML (including Windows HTML Help) and LaTeX, for printable PDF versions
Extensive cross-references: semantic markup and automatic links for functions, classes, glossary terms and similar pieces of information
Hierarchical structure: easy definition of a document tree, with automatic links to siblings, parents and children
Automatic indices: general index as well as a module index
Code handling: automatic highlighting using the Pygments highlighter
Extensions: automatic testing of code snippets, inclusion of docstrings from Python modules, and more
",130
58622,59955,2,"Sphinx is mainly a tool for formatting docs written independently from the source code, as I understand it.
For generating API docs from Python docstrings, the leading tools are pdoc and pydoctor. Here's pydoctor's generated API docs for Twisted and Bazaar.
Of course, if you just want to have a look at the docstrings while you're working on stuff, there's the ""pydoc"" command line tool and as well as the help() function available in the interactive interpreter.
",93
58622,497322,2,"The doxypy input filter allows you to use pretty much all of Doxygen's formatting tags in a standard Python docstring format.  I use it to document a large mixed C++ and Python game application framework, and it's working well.
",44
58711,335400,2,"Declarative is not necessarily more (or less) pythonic than functional IMHO. I think a layered approach would be the best (from buttom up):
A native layer that accepts and returns python data types.
A functional dynamic layer.
One or more declarative/object-oriented layers.
Similar to Elixir + SQLAlchemy.
",57
58711,58917,2,"You could actually pull this off, but it would require using metaclasses, which are deep magic (there be dragons). If you want an intro to metaclasses, there's a series of articles from IBM which manage to introduce the ideas without melting your brain.
The source code from an ORM like SQLObject might help, too, since it uses this same kind of declarative syntax.
",74
58711,58990,2,"Maybe not as slick as the Ruby version, but how about something like this:
Like Justin said, to implement this you would need to use a custom metaclass on class App, and a bunch of properties on Para and Button. This actually wouldn't be too hard.
The problem you run into next is: how do you keep track of the order that things appear in the class definition? In Python 2.x, there is no way to know if t should be above b or the other way around, since you receive the contents of the class definition as a python dict.
However, in Python 3.0 metaclasses are being changed in a couple of (minor) ways. One of them is the __prepare__ method, which allows you to supply your own custom dictionary-like object to be used instead -- this means you'll be able to track the order in which items are defined, and position them accordingly in the window.
",178
58711,60563,2,"This could be an oversimplification, i don't think it would be a good idea to try to make a general purpose ui library this way. On the other hand you could use this approach (metaclasses and friends) to simplify the definition of certain classes of user interfaces for an existing ui library and depending of the application that could actually save you a significant amount of time and code lines.
",76
58711,62780,2,"With some Metaclass magic to keep the ordering I have the following working. I'm not sure how pythonic it is but it is good fun for creating simple things. 
",32
58711,334828,2,"I have this same problem. I wan to to create a wrapper around any GUI toolkit for Python that is easy to use, and inspired by Shoes, but needs to be a OOP approach (against ruby blocks).
More information in: http://wiki.alcidesfonseca.com/blog/python-universal-gui-revisited
Anyone's welcome to join the project.
",58
58711,334938,2,"The only attempt to do this that I know of is Hans Nowak's Wax (which is unfortunately dead).
",22
58711,335077,2,"The closest you can get to rubyish blocks is the with statement from pep343: 
http://www.python.org/dev/peps/pep-0343/
",18
58711,58711,1,"I have been playing with the Ruby library ""shoes"". Basically you can write a GUI application in the following way:
This made me think - how would I design a similarly nice-to-use GUI framework in Python? One that doesn't have the usual tyings of basically being wrappers to a C* library (In the case of GTK, Tk, wx, QT etc etc)
Shoes takes things from web devlopment (like #f0c2f0 style colour notation, CSS layout techniques, like :margin => 10), and from ruby (extensively using blocks in sensible ways)
Python's lack of ""rubyish blocks"" makes a (metaphorically)-direct port impossible:
No where near as clean, and wouldn't be nearly as flexible, and I'm not even sure if it would be implementable.
Using decorators seems like an interesting way to map blocks of code to a specific action:
Basically the decorator function stores the function, then when the action occurred (say, a click) the appropriate function would be executed.
The scope of various stuff (say, the la label in the above example) could be rather complicated, but it seems doable in a fairly neat manner..
",225
58711,335358,2,"If you really want to code UI, you could try to get something similar to django's ORM; sth like this to get a simple help browser:
The idea would be to interpret some containers (like windows) as simple classes, some containers (like tables, v/hboxes) recognized by object names, and simple widgets as objects.
I dont think one would have to name all containers inside a window, so some shortcuts (like old-style classes being recognized as widgets by names) would be desirable.
About the order of elements: in MyWindow above you don't have to track this (window is conceptually a one-slot container). In other containers you can try to keep track of the order assuming that each widget constructor have access to some global widget list. This is how it is done in django (AFAIK).
Few hacks here, few tweaks there... There are still few things to think of, but I believe it is possible... and usable, as long as you don't build complicated UIs.
However I am pretty happy with PyGTK+Glade. UI is just kind of data for me and it should be treated as data. There's just too much parameters to tweak (like spacing in different places) and it is better to manage that using a GUI tool. Therefore I build my UI in glade, save as xml and parse using gtk.glade.XML().
",265
58711,335132,2,"I was never satisfied with David Mertz's articles at IBM on metaclsses so I recently wrote my own metaclass article.  Enjoy.
",24
58711,336583,2,"If you use PyGTK with glade and this glade wrapper, then PyGTK actually becomes somewhat pythonic. A little at least.
Basically, you create the GUI layout in Glade. You also specify event callbacks in glade. Then you write a class for your window like this:
Here, I'm assuming that I have a GTK Button somewhere called button1 and that I specified button_click_event as the clicked callback. The glade wrapper takes a lot of effort out of event mapping.
If I were to design a Pythonic GUI library, I would support something similar, to aid rapid development. The only difference is that I would ensure that the widgets have a more pythonic interface too. The current PyGTK classes seem very C to me, except that I use foo.bar(...) instead of bar(foo, ...) though I'm not sure exactly what I'd do differently. Probably allow for a Django models style declarative means of specifying widgets and events in code and allowing you to access data though iterators (where it makes sense, eg widget lists perhaps), though I haven't really thought about it.
",211
58711,335443,2,"Personally, I would try to implement JQuery like API in a GUI framework.
",15
58711,335887,2,"This is extremely contrived and not pythonic at all, but here's my attempt at a semi-literal translation using the new ""with"" statement.
The hardest part is dealing with the fact that python will not give us anonymous functions with more than one statement in them.  To get around that, we could create a list of commands and run through those...
Anyway, here's the backend code I ran this with:
",80
58711,336089,2,"Here's an approach that goes about GUI definitions a bit differently using class-based meta-programming rather than inheritance.
This is largley Django/SQLAlchemy inspired in that it is heavily based on meta-programming and separates your GUI code from your ""code code"".  I also think it should make heavy use of layout managers like Java does because when you're dropping code, no one wants to constantly tweak pixel alignment.  I also think it would be cool if we could have CSS-like properties.
Here is a rough brainstormed example that will show a column with a label on top, then a text box, then a button to click on the bottom which shows a message.
from happygui.controls import *
MAIN_WINDOW = Window(width=""500px"", height=""350px"",
    my_layout=ColumnLayout(padding=""10px"",
        my_label=Label(text=""What's your name kiddo?"", bold=True, align=""center""),
        my_edit=EditBox(placeholder=""""),
        my_btn=Button(text=""CLICK ME!"", on_click=Handler('module.file.btn_clicked')),
    ),
)
MAIN_WINDOW.show()
def btn_clicked(sender): # could easily be in a handlers.py file
    name = MAIN_WINDOW.my_layout.my_edit.text
    # same thing: name = sender.parent.my_edit.text
    # best practice, immune to structure change: MAIN_WINDOW.find('my_edit').text
    MessageBox(""Your name is '%s'"" % ()).show(modal=True)
One cool thing to notice is the way you can reference the input of my_edit by saying MAIN_WINDOW.my_layout.my_edit.text.  In the declaration for the window, I think it's important to be able to arbitrarily name controls in the function kwargs.
Here is the same app only using absolute positioning (the controls will appear in different places because we're not using a fancy layout manager):
from happygui.controls import *
MAIN_WINDOW = Window(width=""500px"", height=""350px"",
    my_label=Label(text=""What's your name kiddo?"", bold=True, align=""center"", x=""10px"", y=""10px"", width=""300px"", height=""100px""),
    my_edit=EditBox(placeholder="""", x=""10px"", y=""110px"", width=""300px"", height=""100px""),
    my_btn=Button(text=""CLICK ME!"", on_click=Handler('module.file.btn_clicked'), x=""10px"", y=""210px"", width=""300px"", height=""100px""),
)
MAIN_WINDOW.show()
def btn_clicked(sender): # could easily be in a handlers.py file
    name = MAIN_WINDOW.my_edit.text
    # same thing: name = sender.parent.my_edit.text
    # best practice, immune to structure change: MAIN_WINDOW.find('my_edit').text
    MessageBox(""Your name is '%s'"" % ()).show(modal=True)
I'm not entirely sure yet if this is a super great approach, but I definitely think it's on the right path.  I don't have time to explore this idea more, but if someone took this up as a project, I would love them.
",560
58711,336525,2,"
It would be relatively easy to do in python with a bit of that metaclass python magic know how. Which I have. And a knowledge of PyGTK. Which I also have. Gets ideas?
",38
61151,61151,1,"If you're writing a library, or an app, where do the unit test files go?  
It's nice to separate the test files from the main app code, but it's awkward to put them into a ""tests"" subdirectory inside of the app root directory, because it makes it harder to import the modules that you'll be testing.  
Is there a best practice here?
",75
61151,63645,2,"When writing a package called ""foo"", I will put unit tests into a separate package ""foo_test"". Modules and subpackages will then have the same name as the SUT package module. E.g. tests for a module foo.x.y are found in foo_test.x.y. The __init__.py files of each testing package then contain an AllTests suite that includes all test suites of the package. setuptools provides a convenient way to specify the main testing package, so that after ""python setup.py develop"" you can just use ""python setup.py test"" or ""python setup.py test -s foo_test.x.SomeTestSuite"" to the just a specific suite.
",114
61151,103610,2,"I also tend to put my unit tests in the file itself, as Jeremy Cantrell above notes, although I tend to not put the test function in the main body, but rather put everything in an
block.  This ends up adding documentation to the file as 'example code' for how to use the python file you are testing.
I should add, I tend to write very tight modules/classes.  If your modules require very large numbers of tests, you can put them in another, but even then, I'd still add:
This lets anybody reading your source code know where to look for the test code.
",118
61151,62527,2,"For a file module.py, the unit test should normally be called test_module.py, following Pythonic naming conventions.
There are several commonly accepted places to put test_module.py:
In the same directory as module.py.
In ../tests/test_module.py (at the same level as the code directory).
In tests/test_module.py (one level under the code directory).
I prefer #1 for its simplicity of finding the tests and importing them. Whatever build system you're using can easily be configured to run files starting with test_. Actually, the default unittest pattern used for test discovery is test*.py.
",105
61151,61820,2,"In C#, I've generally separated the tests into a separate assembly.
In Python -- so far -- I've tended to either write doctests, where the test is in the docstring of a function, or put them in the if __name__ == ""__main__"" block at the bottom of the module.
",59
61151,61531,2,"I use a tests/ directory, and then import the main application modules using relative imports. So in MyApp/tests/foo.py, there might be:
to import the MyApp.foo module.
",31
61151,61518,2,"I've recently started to program in Python, so I've not really had chance to find out best practice yet.
But, I've written a module that goes and finds all the tests and runs them.
So, I have:
app/
 appfile.py
test/
 appfileTest.py
I'll have to see how it goes as I progress to larger projects.
",65
61151,61169,2,"A common practice is to put the tests directory in the same parent directory as your module/package. So if your module was called foo.py your directory layout would look like:
Of course there is no one way of doing it. You could also make a tests subdirectory and import the module using absolute import.
Wherever you put your tests, I would recommend you use nose to run them. Nose searches through your directories for tests. This way, you can put tests wherever they make the most sense organizationally.
",97
61151,61168,2,"I don't believe there is an established ""best practice"".
I put my tests in another directory outside of the app code. I then add the main app directory to sys.path (allowing you to import the modules from anywhere) in my test runner script (which does some other stuff as well) before running all the tests. This way I never have to remove the tests directory from the main code when I release it, saving me time and effort, if an ever so tiny amount.
",97
61151,128616,2,"We use 
app/src/code.py
app/testing/code_test.py 
app/docs/..
In each test file we insert ""../src/"" in sys.path. It's not the nicest solution but works. I think it would be great if someone came up w/ something like maven in java that gives you standard conventions that just work, no matter what project you work on.
",59
61151,39740835,2,"Every once in a while I find myself checking out the topic of test placement, and every time the majority recommends a separate folder structure beside the library code, but I find that every time the arguments are the same and are not that convincing. I end up putting my test modules somewhere beside the core modules. 
The main reason for doing this is: refactoring.
When I move things around I do want test modules to move with the code; it's easy to lose tests if they are in a separate tree. Let's be honest, sooner or later you end up with a totally different folder structure, like django, flask and many others. Which is fine if you don't care.
The main question you should ask yourself is this:  
Am I writing:  
a) reusable library or  
b) building a project than bundles together some semi-separated modules?
If a: 
A separate folder and the extra effort to maintain its structure may be better suited. No one will complain about your tests getting deployed to production. 
But it's also just as easy to exclude tests from being distributed when they are mixed with the core folders; put this in the setup.py:
If b: 
You may wish — as every one of us do — that you are writing reusable libraries, but most of the time their life is tied to the life of the project. Ability to easily maintain your project should be a priority. 
Then if you did a good job and your module is a good fit for another project, it will probably get copied — not forked or made into a separate library — into this new project, and moving tests that lay beside it in the same folder structure is easy in comparison to fishing up tests in a mess that a separate test folder had become. (You may argue that it shouldn't be a mess in the first place but let's be realistic here).
So the choice is still yours, but I would argue that with mixed up tests you achieve all the same things as with a separate folder, but with less effort on keeping things tidy.    
",403
61151,37122327,2,"I recommend you check some main Python projects on GitHub and get some ideas.
When your code gets larger and you add more libraries it's better to create a test folder in the same directory you have setup.py and mirror your project directory structure for each test type (unittest, integration, ...)
For example if you have a directory structure like:
After adding test folder you will have a directory structure like:
Many properly written Python packages uses the same structure. A very good example is the Boto package.
Check https://github.com/boto/boto
",102
61151,23386287,2,"Only 1 test file
If doesn't have many test files, putting it in a top-level directory is nice (I think this is a pythonic (recommended) way):
Many test files
If has many test files, put it in a tests folder:
but if you put the tests in tests folder, test can't import ..lib in CLI because __main__  can't import relative modules, so instead we can use nose, or we can add a parent directory to the python import path, and for that I will create a
env.py
in
and import env before test import module
test_module.py
",111
61151,22704148,2,"From my experience in developing Testing frameworks in Python, I would suggest to put python unit tests in a separate directory. Maintain a symmetric directory structure. This would be helpful in packaging just the core libraries and not package the unit tests. Below is implemented through a schematic diagram. 
In this way when you package these libraries using an rpm, you can just package the main library modules (only). This helps maintainability particularly in agile environment. 
",86
61151,2363162,2,"How I do it...
Folder structure:
Setup.py points to src/ as the location containing my projects modules, then i run:
Which adds my project into site-packages, pointing to my working copy. To run my tests i use:
Using whichever test runner I've configured.
",52
61151,815212,2,"We had the very same question when writing Pythoscope (http://pythoscope.org), which generates unit tests for Python programs.  We polled people on the testing in python list before we chose a directory, there were many different opinions.  In the end we chose to put a ""tests"" directory in the same directory as the source code. In that directory we generate a test file for each module in the parent directory.  
",81
61151,382596,2,"I prefer toplevel tests directory. This does mean imports become a little more difficult. For that I have two solutions:
Use setuptools. Then you can pass test_suite='tests.runalltests.suite' into setup(), and can run the tests simply: python setup.py test
Set PYTHONPATH when running the tests: PYTHONPATH=. python tests/runalltests.py
Here's how that stuff is supported by code in M2Crypto:
http://svn.osafoundation.org/m2crypto/trunk/setup.py
http://svn.osafoundation.org/m2crypto/trunk/tests/alltests.py
If you prefer to run tests with nosetests you might need do something a little different.
",93
61151,77145,2,"If the tests are simple, simply put them in the docstring -- most of the test frameworks for Python will be able to use that:
For other more involved tests, I'd put them either in ../tests/test_module.py or in tests/test_module.py.
",44
67631,43602557,2,"I have come up with a slightly modified version of @SebastianRittau's wonderful answer (for Python > 3.4 I think), which will allow you to load a file with any extension as a module using spec_from_loader instead of spec_from_file_location:
The advantage of encoding the path in an explicit SourceFileLoader is that the machinery will not try to figure out the type of the file from the extension. This means that you can load something like a .txt file using this method, but you could not do it with spec_from_file_location without specifying the loader because .txt is not in importlib.machinery.SOURCE_SUFFIXES.
",107
67631,37611448,2,"Here is some code that works in all Python versions, from 2.7-3.5 and probably even others.
I tested it. It may be ugly but so far is the only one that works in all versions.
",39
67631,27127448,2,"The best way, I think, is from the official documentation (29.1. imp â€” Access the import internals):
",23
67631,34570493,2,"It may be obvious but in interactive shell:
",9
67631,32905959,2,"To import a module from a given filename, you can temporarily extend the path, and restore the system path in the finally block reference:
",27
67631,29589414,2,"This area of Python 3.4 seems to be extremely tortuous to understand! However with a bit of hacking using the code from Chris Calloway as a start I managed to get something working. Here's the basic function.
This appears to use non-deprecated modules from Python 3.4. I don't pretend to understand why, but it seems to work from within a program. I found Chris' solution worked on the command line but not from inside a program.
",86
67631,26995106,2,"In Linux, adding a symbolic link in the directory your python script is located works.
ie: 
ln -s /absolute/path/to/module/module.py /absolute/path/to/script/module.py
python will create /absolute/path/to/script/module.pyc and will update it if you change the contents of /absolute/path/to/module/module.py
then include the following in mypythonscript.py
from module import *
",48
67631,25827116,2,"You can use the pkgutil module (specifically the walk_packages method) to get a list of the packages in the current directory. From there it's trivial to use the importlib machinery to import the modules you want:
",41
67631,8721254,2,"This should work
",3
67631,6284270,2,"I made a package that uses imp for you. I call it import_file and this is how it's used:
You can get it at:
http://pypi.python.org/pypi/import_file
or at
http://code.google.com/p/import-file/
",36
67631,129374,2,"The advantage of adding a path to sys.path (over using imp) is that it simplifies things when importing more than one module from a single package.  For example:
",32
67631,68628,2,"
",0
67631,67708,2,"You can also do something like this and add the directory that the configuration file is sitting in to the Python load path, and then just do a normal import, assuming you know the name of the file in advance, in this case ""config"".
Messy, but it works.
",56
67631,67705,2,"Import package modules at runtime (Python recipe) 
http://code.activestate.com/recipes/223972/
",12
67631,67693,2,"You can use the 
method from imp module.
",9
67631,67692,2,"For Python 3.5+ use:
For Python 3.3 and 3.4 use:
(Although this has been deprecated in Python 3.4.)
Python 2 use:
There are equivalent convenience functions for compiled Python files and DLLs.
See also. http://bugs.python.org/issue21436.
",46
67631,67672,2,"I believe you can use imp.find_module() and imp.load_module() to load the specified module.  You'll need to split the module name off of the path, i.e. if you wanted to load /home/mypath/mymodule.py you'd need to do:
...but that should get the job done.
",54
67631,37339817,2,"It sounds like you don't want to specifically import the configuration file (which has a whole lot of side effects and additional complications involved), you just want to run it, and be able to access the resulting namespace. The standard library provides an API specifically for that in the form of runpy.run_path:
That interface is available in Python 2.7 and Python 3.2+
",69
67631,67715,2,"Do you mean load or import?
You can manipulate the sys.path list specify the path to your module, then import your module. For example, given a module at:
You could do:
",37
67631,30605451,2,"I'm not saying that it is better, but for the sake of completeness, I wanted to suggest the exec function, available in both python 2 and 3.
exec allows you to execute arbitrary code in either the global scope, or in an internal scope, provided as a dictionary.
For example, if you have a module stored in ""/path/to/module"" with the function foo(), you could run it by doing the following:
This makes it a bit more explicit that you're loading code dynamically, and grants you some additional power, such as the ability to provide custom builtins. 
And if having access through attributes, instead of keys is important to you, you can design a custom dict class for the globals, that provides such access, e.g.:
",148
67631,67631,1,"How can I load a Python module given its full path? Note that the file can be anywhere in the filesystem, as it is a configuration option.
",30
68477,37142773,2,"I am trying to test django rest api and its working for me:
",14
68477,68477,1,"Is there a way to send a file using POST from a Python script?
",15
68477,75158,2,"You may also want to have a look at httplib2, with examples. I find using httplib2 is more concise than using the built-in HTTP modules.
",28
68477,525193,2,"Chris Atlee's poster library works really well for this (particularly the convenience function poster.encode.multipart_encode()). As a bonus, it supports streaming of large files without loading an entire file into memory. See also Python issue 3244.
",44
68477,7969778,2,"The only thing that stops you from using urlopen directly on a file object is the fact that the builtin file object lacks a len definition. A simple way is to create a subclass, which provides urlopen with the correct file. 
I have also modified the Content-Type header in the file below.
",56
68477,10234640,2,"From http://docs.python-requests.org/en/latest/user/quickstart/#post-a-multipart-encoded-file
Requests makes it very simple to upload Multipart-encoded files:
That's it. I'm not joking - this is one line of code. File was sent. Let's check:
",40
68477,31305207,2,"Looks like python requests does not handle extremely large multi-part files.
The documentation recommends you look into requests-toolbelt.
Here's the pertinent page from their documentation.
",29
68477,36078069,2,"
",0
68477,68502,2,"Yes. You'd use the urllib2 module, and encode using the multipart/form-data content type. Here is some sample code to get you started -- it's a bit more than just file uploading, but you should be able to read through it and see how it works:
",52
942148,945160,2,"Just wanted to post the solution I came up with.  The problem was in this line:
I'm not really a 100% why either, so I just hard coded the url like this:
",38
942148,944440,2,"Edit: I used your example, and had to change to not use keyword parameters.
Named parameters do work, as long as both uid and token are defined.  If either are not defined or blank I get the same error you do:
",47
942148,942169,2,"This is a problem I figured out myself not 10 minutes ago. The solution is to add the post_change_redirect value to the dictionary of arguments you are passing to the password_reset view.
So this is what mine now look like:
I hope that does it for you! I agree that the documentation for this particular feature is lacking somewhat, but this solved the exact same issue for my project.
Edit: I really should have scrolled across - you've included that already. Apologies for that, but I hope you get it sorted :)
",103
942148,942148,1,"I'm trying to use the password reset setup that comes with Django, but the documentation is not very good for it.  I'm using Django 1.0 and I keep getting this error:
in my urlconf I have something like this:
The problem seems to be in this file: 
on line 7
I'm at loss as to what's going on, so any help would be appreciated.
Thanks
",76
942148,6640223,2,"I've struggled with this for over an hour trying everything on this page and every other page on the internet. Finally to solve the problem in my case i had to delete 
from the top of my password_reset_email.html template.
Also note, ""uidb36=uid"" in the url script. Here's my full password_reset_email.html template, I hope it saves someone some time:
",68
7825361,7825523,2,"
Edit:  Updated for commment: ""Thanks can i print page_id and title at same time? Means 31239628 - Title""
",23
7825361,7825769,2,"The element.get() method is used to retrieve option attribute values in a tag:
",16
7825361,7825361,1,"I'm reading XML file using Etree module. Im using following code to print the value of <page> and <title> tags. My code working fine. But I want little change. If the <page id='...'>  attribute id is exists then print the value of tag. Is it possible? thanks 
Here is my xml File.
",69
7825395,7825517,2,"Yes, it's possible. The ""how"" part depends on the GUI library you choose for which there are many options, but most people will recommend the following two: wxPython or PySide which is Qt for Python.
PySide has good documentation and tutorials.
What you will want to do is create a QMainWindow instance and set the WindowFlags to your requirements. You probably want the following combination Qt::Window | Qt::CustomizeWindowHint | Qt::WindowStaysOnTopHint.
Something like this: 
Note, that there is a limit to the ""staying on top"" nature of such windows. There are Win32-specific ways to fight it and get even higher, but such requirement would be a design error. 
",131
7825395,7825395,1,"Sorry for the vague title, couldn't come up with anything more informative %)
What I want is a 5px horizontal panel on the top of the screen that I can draw on (and, possible, handle clicks on too).
One of the following features would be awesome (although I understand it's probably not really possible to combine both of them):
the panel should be just like the Windows's own taskbar, i.e., maximized windows should not overlap it, but start below it instead
the panel should show in fullscreen apps too
Is it possible to do this in Python?
Thanks.
",116
7825991,7828399,2,"Sikuli might be a perfect for this. It is based on Jython, thus can be extended with Python or Java tools.
Sikuli is a visual technology to automate and test graphical user
  interfaces (GUI) using images (screenshots). Sikuli includes Sikuli
  Script, a visual scripting API for Jython, and Sikuli IDE, an
  integrated development environment for writing visual scripts with
  screenshots easily.
Edit
If the image is that well defined as in your example, that can be easily matched without much image processing. Here I used matplotlib (small image must be exact cropped version of large image).
Target image (65x173):
Image to be matched (29x29):
",127
7825991,7995971,2,"I take it the images are always the same so the 6 is at the same offset and covered by the same  tag. Clicking the image at this point will follow the URL defined in the  href attribute. So follow that link.
",45
7825991,7825991,1,"In a html5 page using image map, I would like to use python (or Perl, Ruby, C) to crawl it to find a particular image (those are separated by a transparency area) in another one and click it when this image is found. The image are the same all the times. What is the best way to achieve this ? 
Edit: for french readers, the trick is implemented, see http://www.sputnick-area.net/?p=572
",86
7828717,7828784,2,"one is defined inside the if __name__=='__main__' block.
Consequently, one will get defined only if test.py is run as a script (rather than imported). 
For the module new to access one from the test module, you'll need to pull one out of the if __name__ block:
test.py:
Then access one by the qualified name test.one:
new.py:
",68
7828717,7828789,2,"No, you can't. The closest you can get is to pass the thing you need in to the constructor:
",23
7828717,7828775,2,"If you want your New class to use the instance of Test you created, you have to pass it in as part of the constructor.
new.py
test.py
Playing around with globals is a great way to break your code without realizing how you did it. It is better to explicitly pass in the reference you want to use.
",62
7828717,7828717,1,"I have two files, one of the test.py is
and new.py
Now when i run python test.py I get this error,
But I want to use this instance of one in my New!!
Can I do this??
edit:
I want to access the variable in test.py in new.py to do some process and give them back to test.py. Isn't this possible?
",71
7828867,7828867,1,"
a & b should be considered equal, because they have exactly the same elements, only in different order.
The thing is, my actual lists will consist of objects (my class instances), not integers.
",41
7828867,41433671,2,"If the list contains items that are not hashable (such as a list of objects) you might be able to use the Counter Class and the id() function such as:
",35
7828867,40016570,2,"https://docs.python.org/3.5/library/unittest.html#unittest.TestCase.assertCountEqual
assertCountEqual(first, second, msg=None)
Test that sequence first contains the same elements as second, regardless of their order. When they donâ€™t, an error message listing the differences between the sequences will be generated.
Duplicate elements are not ignored when comparing first and second. It verifies whether each element has the same count in both sequences. Equivalent to: assertEqual(Counter(list(first)), Counter(list(second))) but works with sequences of unhashable objects as well.
New in version 3.2. 
or in 2.7:
https://docs.python.org/2.7/library/unittest.html#unittest.TestCase.assertItemsEqual
",114
7828867,34694504,2,"If the comparison is to be performed in a testing context, use assertCountEqual(a, b) (py>=3.2) and assertItemsEqual(a, b) (2.7<=py<3.2).
Works on sequences of unhashable objects too.
",47
7828867,28089750,2,"I hope the below piece of code might work in your case :- 
This will ensure that all the elements in both the lists a & b are same, regardless of whether they are in same order or not.
For better understanding, refer to my answer in this question
",53
7828867,7829554,2,"Let a,b lists
No need to make them hashable or sort them.
",15
7828867,7829388,2,"O(n):  The Counter() method is best (if your objects are hashable):
O(n log n):  The sorted() method is next best (if your objects are orderable):
O(n * n): If the objects are neither hashable, nor orderable, you can use equality:
",65
7828867,7829249,2,"If you know the items are always hashable, you can use a Counter() which is O(n)
If you know the items are always sortable, you can use sorted() which is O(n log n)
In the general case you can't rely on being able to sort, or has the elements, so you need a fallback like this, which is unfortunately O(n^2)
",79
7828867,7828964,2,"The best way to do this is by sorting the lists and comparing them. (Using Counter won't work with objects that aren't hashable.) This is straightforward for integers:
It gets a little trickier with arbitrary objects. If you care about object identity, i.e., whether the same objects are in both lists, you can use the id() function as the sort key.
(In Python 2.x you don't actually need the key= parameter, because you can compare any object to any object. The ordering is arbitrary but stable, so it works fine for this purpose; it doesn't matter what order the objects are in, only that the ordering is the same for both lists. In Python 3, though, comparing objects of different types is disallowed in many circumstances -- for example, you can't compare strings to integers -- so if you will have objects of various types, best to explicitly use the object's ID.)
If you want to compare the objects in the list by value, on the other hand, first you need to define what ""value"" means for the objects. Then you will need some way to provide that as a key (and for Python 3, as a consistent type). One potential way that would work for a lot of arbitrary objects is to sort by their repr(). Of course, this could waste a lot of extra time and memory building repr() strings for large lists and so on.
If the objects are all your own types, you can define __lt__() on them so that the object knows how to compare itself to others. Then you can just sort them and not worry about the key= parameter. Of course you could also define __hash__() and use Counter, which will be faster.
",346
7828867,7828896,2,"You can sort both:
A counting sort could also be more efficient (but it requires the object to be hashable).
",24
7828891,7833202,2,"2.1 GB of data should take between 21 (@ 100 MB/s) to 70 (@ 30 MB/s) seconds just to read. You're then formatting that into and writing data which is perhaps five times as large. This means a total of 13 GB to read and write requiring 130-420 seconds.
Your sampling shows that reading takes 24 seconds. Writing should therefore require about two minutes. The reading and writing times can be improved using an SSD for example.
When I convert files (using programs I write in C) I assume that a conversion should take no more time than it takes to read the data itself, a lot less is usually possible. Overlapped reads and writes can also reduce the I/O time. I write my own custom formatting routines since printf is usually far too slow.
How much is 24 seconds? On a modern CPU at least 40 billion instructions. That means that in that time you can process every single byte of data with at least 19 instructions. Easily doable for a C program but not for an interpreted language (Python, Java, C#, VB).
Your 525 second processing (549-24) remainder indicates that Python is spending at least 875 billion instructions processing or 415 instructions per byte of data read. That comes out to 22 to 1: a not uncommon ratio between interpreted and compiled languages. A well-constructed C program should be down around ten instructions per byte or less.
",272
7828891,7831219,2,"Given that formatString is the slowest operation, try this:
",11
7828891,7829202,2,"To more precisely attack the problem, I suggest measuring the file read operation by making 'convertXYZ' a no-op function and timing the result.  And measuring the convert function, by changing the 'read' to always return a simple point, but calling the conversion and output the same number of times as if you were really reading the file.  (And probably another run where the final post-conversion output is a no-op.)  Depending on where the time is going, it may make a lot more sense to attack one or the other.
You might be able to get the local OS to do some interleaving for you by writing the output to the Python's stdout, and having the shell do the actual file IO.  And similarly by streaming the file into stdin (e.g., cat oldformat | python conversion.py > outputfile)
What sort of storage are the input and output files on?  The storage characteristics may have a lot more to do with the performance than the Python code.
Update: Given the output is the slowest, and your storage is pretty slow and shared between both reads and writes, try adding some buffering.  From the python doc you should be able to add some buffering by adding a third argument to the os.open call.  Try something pretty large like 128*1024?
",242
7828891,7828891,1,"Right now, I'm trying to convert a large quantity of binary files of points in latitude longitude altitude format to text based ECEF cartesian format (x, y, z). The problem right now is that the process is very very very slow. 
I have over 100 gigabytes of this stuff to run through, and more data could be coming in. I would like to make this bit of code as fast as possible.
Right now my code looks something like this: 
convertXYZ() is basically the conversion formula here:
http://en.wikipedia.org/wiki/Geodetic_system
I was wondering if it would be faster to read things in chunks of ~4MB with one thread, put them in a bounded buffer, have a different thread for conversion to string format, and have a final thread write the string back into a file on a different harddisk. I might be jumping the gun though...
I'm using python right now for testing, but I wouldn't be opposed to switching if I can work through these files faster.
Any suggestions would be great. Thanks
EDIT:
I have profiled the code with cProfile again and this time split the string format and the io. It seems I'm actually being killed by the string format... Here's the profiler report
Is there a faster way to format strings?
",246
7829007,7829376,2,"One way to fix it is to modify the helper function to:
",13
7829007,7829185,2,"Update: The link I gave you below was to Python 3.0, my apologies I forgot your tag.
The 2.7 docs do not mention blank lines in values, so I suspect they are not supported at all.
See also this SO question (which looks like Python 3): How to read multiline .properties file in python
From the documentation:
Values can also span multiple lines, as long as they are indented
  deeper than the first line of the value. Depending on the parserâ€™s
  mode, blank lines may be treated as parts of multiline values or
  ignored.
I don't know what 'parser mode' this is referring to, but not sure if what you want is doable.
On the other hand, the docs also mention the empty_lines_in_values option, which seems to indicate that blank lines are supported.
Seems somewhat contradictory to me.
",159
7829007,7829007,1,"The following is the file parsed by ConfigParser:
As described by the official Python wiki for ConfigParser examples, here is the helper function:
The resulting value is:
The expected value was:
How do I fix this?
",42
7829188,7829294,2,"The if thing in somelist is the preferred and fastest way.
Under-the-hood that use of the in-operator translates to somelist.__contains__(thing) whose implementation is equivalent to:  any((x is thing or x == thing) for x in somelist).
Note the condition tests identity and then equality.
",56
7829188,7829222,2,"the ""if x in thing:"" format is strongly preferred, not just because it takes less code, but it also works on other data types and is (to me) easier to read.
I'm not sure how it's implemented, but I'd expect it to be quite a lot more efficient on datatypes that are stored in a more searchable form. eg. sets or dictionary keys. 
",78
7829188,7829220,2,"In your simple example it is of course better to use in.
However... in the question you link to, in doesn't work (at least not directly) because the OP does not want to find an object that is equal to something, but an object whose attribute n is equal to something.
One answer does mention using in on a list comprehension, though I'm not sure why a generator expression wasn't used instead:
But this is hardly much of an improvement over the other approaches, such as this one using any:
",104
7829188,7829336,2,"
The above is a terrible way to test whether an item exists in a collection. It returns True from the function, so if you need the test as part of some code you'd need to move this into a separate utility function, or add thingWasFound = False before the loop and set it to True in the if statement (and then break), either of which is several lines of boilerplate for what could be a simple expression.
Plus, if you just use thingIAmLookingFor in list, this might execute more efficiently by doing fewer Python level operations (it'll need to do the same operations, but maybe in C, as list is a builtin type). But even more importantly, if list is actually bound to some other collection like a set or a dictionary thingIAmLookingFor in list will use the hash lookup mechanism such types support and be much more efficient, while using a for loop will force Python to go through every item in turn.
Obligatory post-script: list is a terrible name for a variable that contains a list as it shadows the list builtin, which can confuse you or anyone who reads your code. You're much better off naming it something that tells you something about what it means.
",233
7829188,7829188,1,"I'm fairly new to python and have found that I need to query a list about whether it contains a certain item.
The majority of the postings I have seen on various websites (including this similar stackoverflow question) have all suggested something along the lines of
However, I have also found from one lone forum that 
works.  
I am wondering if the if thing in list method is shorthand for the for i in list method, or if it is implemented differently.
I would also like to which, if either, is more preferred.
",104
7829200,7829200,1,"I am just installing TurboGears2 on Windows 7 running a virtual environment (python 2.7). I am not seeing the paster quickstart option when I run 
Instead, here's that output:
So, it's no surprise that when I run :
I get: 
My questions: Why is it missing, and how do I get it?
",64
7829200,7829286,2,"You get into this situation when you've installed the package required to run a TurboGears app (TurboGears2), but not the one required to develop a TurboGears app (tg.devtools).
Running the following command while in your virtualenv should install the correct package:
",49
7829212,7829212,1,"The example in Simple wrapping of C code with cython describes nicely how to evaluate a function written in C on an array passed from numpy and return the result in a numpy array.
How would one go about doing the same thing but returning a 2D array? I.e. I'd like to evaluate a C function on a grid defined by two numpy arrays, and return the result as a numpy 2D array.
It would be something like this (using same functions as in the link above). Obviously one can't use double z[] now, but I'm not sure how to pass a 2D numpy array to C.
This is the original .pyx file (see below).
Many thanks.
",135
7829212,7829463,2,"You can use a normal array for a 2D Matrix. You need only give the length of the dimension to the function.
In the C file do something as that: 
(z is now an array of length N*N)
In Python you need to do the same, so you can use a 1D Array with N*N elements instead of an 2D Matrix.
Update 3D case
(z is now an array of length N*N*N)
",81
7829279,7829279,1,"I have a bit of a problem with using the Python YouTube API to get the title of the videos in the playlist. I have the enviroment configured correctly, also when I copied the example from the API documentation it works for the added playlist id, but when I try to use one from an other playlist I get an error.
Here is some code I wrote: (In this example I try to get the titles of the videos from here)
Here is the error I get: 
I'm quite frustrated with this and would appreciate some help. Thank you!
",110
7829279,7829295,2,"Remove PL in your request URI:
I'm not sure why YouTube needs it to be in that format, but it needs to be.
You can also just do a .replace('playlists/PL', 'playlists/') on your string.
",45
7829311,7829311,1,"The goal is just to retrieve a specific file without downloading the entire contents, using the HTTP range method as described:
http://www.codeproject.com/KB/cs/remotezip.aspx
",26
7829311,7852229,2,"You can solve this a bit more generally with less code.  Essentially, create enough of a file-like object for ZipFile to use.  So you wind up with z = ZipFile(HttpFile(url)) and it dynamically downloads just the portion needed.  The advantage with this is you write less code, and it applies to more than just zip files.  (In fact, I wonder if there is something like this already... I'm not finding it though.)
Using the same idea, you could also create a caching wrapper for HttpFile to avoid repeated downloads.
And here's the code: (note the lack of error-handling)
",122
7829311,7843535,2,"Since there was no such library I have written a small module myself, most code and logic is is from zipfile with the seek/reads translated to HTTP range requests.
Feel free to review and suggest improvements:
The code:
",42
7833533,7833533,1,"Example table:
I would like somehow to temper it with Gnumeric and produce matching cells across columns:
Real example if with string values instead numbers but it is easier to explain with numbers I think
If this is not trivial and someone has idea how this can be done with Python lists instead table columns in Gnumeric please post a Python solution.
",65
7833533,7833642,2,"It's quite easy to do in Python:
Seems the most direct if you're not worried about the speed.
I also just noticed my answer to Joining multiple iteratorars by a key sort of applies:
You can adapt that if you need performance to scale linearly.
",51
7833668,7835936,2,"If you do not want to change Primay key values or do not to add new objects to the table, which could be duplicate of the old info - then you need some kind of data which you can use as lookup parameters in your database. 
If you have model which represents this data, then its really easy using 
But why include django in this anyway? If you have CSV table, then updating this info is really a case of writing queries. and programs like Excel and openoffice make this very easy. 
If you already have data in CSV format, then just open the data as spreadsheet and use excels/openoffice's Concactenate function to create update queries
If you used openoffice for this, then openoffice has this nifty Text to columns function (under data in program menu), which turns concactenated values into string. Then you can copypaste those strings into command prompt or phppgadmin and run.. and voila, you get updated data in your database.
Edit (In response to you comment.):
Look into this: https://docs.djangoproject.com/en/dev/ref/models/querysets/#get-or-create
If you want to use django for this, then use get_or_create. But you need to remember here, that if any of the parameters you use in get_or_create method have changed, then new object will be created. Thats why i said in the beginning of the post, that you need some kind of data, which will not change.
for example (taken from the link above)
will create new obj(Person) when used first time. But if used 2nd time and the date has changed, then new Person with same name and last name but new date will be created.
So to avoid this, you'll still need to do something like
if you want to have more control over the data, that could have been changed.
Alan.
",344
7833668,7833668,1,"I have a table which already contains some data in it. Now i want to upload new data from a csv file and want to update some of the previous values of that table. I am using django 1.3 and sqlite3 as database. But i am not able to update the table.
",56
7833715,7833827,2,"If the program works and the speed is acceptable, I wouldn't change it.
Otherwise, you could try unutbu's answer.
Generally, I would leave away the 
stuff as I don't see any purpose in not using the strings directly.
And better test for "".png"" instead of ""png"".
An even better solution would be to define 
somewhere centally and use that in
.
",75
7833715,7833781,2,"Since you are recursing through subdirectories, use os.walk:
",10
7833715,7833715,1,"I'm fairly new to Python, but I have gotten this code to work, and in fact, do what it's intended to do.
However, I'm wondering if there is a more efficient way to code this, perhaps to enhance the processing speed.
Right now, there are about 8000 files, and it takes quite some time to process every file and check if it indeed ends in png or jpg.  
",81
7833807,35569519,2,"we can use ffmpeg to get the duration of any video or audio files.
To install ffmpeg follow this link
",21
7833807,7842081,2,"
",0
7833807,7833963,2,"The duration is equal to the number of frames divided by the framerate (frames per second):
Regarding @edwards' comment, here is some code to produce a 2-channel wave file:
If you play the resultant file in an audio player, you'll find that is 40 seconds in duration. If you run the code above it also computes the duration to be 40 seconds. So I believe the number of frames is not influenced by the number of channels and the formula above is correct. 
",95
7833807,41617943,2,"A very simple method is to use pysoundfile, https://github.com/bastibe/PySoundFile
Here's some example code of how to do this:
The output for that particular file is:
This aligns with soxi:
",36
7833807,7833807,1,"I'm looking for a way to find out the duration of a audio file (.wav) in python. So far i had a look at python wave library, mutagen, pymedia, pymad i was not able to get the duration of the wav file. Pymad gave me the duration but its not consistent. 
Thanks in advance. 
",64
7833831,7833831,1,"I have a VB6 application running on a number of old 486 Windows 95 machines and sometimes the application is upgraded. The only way to accomplish this today is to use Hyperterminal and send the file over a null modem cable. Replacing the machines is not an option at this point.
I want to write an application that can take care of transferring the updating app over null modem without rewriting the VB6 app. This means I'm free to use anything I see fit. What alternatives are there?
These are the ones I can think of but I'd like to know if I'm wrong and any pros/cons. Also, I'd prefer to avoid C/C++ if at all possible.
Python with py2exe
Another VB6 app
C/C++
Edit: Some clarifications after reading the comments:
I want to make the process as easy as possible, today we have to remove and dismantle the computer, connect a keyboard and then fire up Hyperterminal to get going. That's why I want something more automatic. I'm open to suggestion of existing solutions but given the specific needs I didn't think there were any.
There is no ethernet on some of the computers either so the solution needs to be able to run RS232.
And again: Replacing the machines is not an option at this point. Just trust me on this.
",250
7833831,7835341,2,"If you must use a null modem, how about the built in serial line networking support?
Then you can just use normal network methods (psexec, file share, etc) methods to do the update.
",40
7833831,7835687,2,"We've got a few testing laboratories in a similar situation-the labs make money for the company so no touching the ancient computers that run the tests under pain of death.  :-)
Anyway, pySerial seems like it'd work for this application, maybe take a look at their wxPython examples for some ideas on a GUI.
",62
7833831,7835783,2,"I guess the answer is pretty simple if you are happy using VB6 and the other app is already VB6, then use it. 
That will do whatever serial comms you require quite adequately. Remember though you may want to update the application you write to do the updating in which case you are back to using hyperterminal!
",61
7833831,7849945,2,"Network Method
I would first get network cards installed in everything.  If you want something that you can just plugin and go, look for any card compatible with NE2000.  That card will work out of the box on Windows 3.11 and 95, no problem.  In particular, you can find the 3Com EtherLink II or the 3C509B for very cheap online.  It's an excellent card.  (The Google Shopping results list several for under $20.)
From there, just enable the Windows File/Print Sharing service over TCP/IP, and you're good to go!  After you've done this, you can remotely manage and upgrade these machines, saving you a lot of headache later on.
Serial-Port Method
Deanna's suggestion of using the serial port as a network device and transferring files normally will work as well.  However, there is a bit of setup involved, and it can be a hassle if you've never done it.  There are several other software options.  I recommend LapLink.  It's fairly painless.
You could even go all-out and pickup a multi-port serial interface for fairly cheap these days, and manage these computers centrally.  RS232 is very robust and can go a long distance over the proper cabling.
Networking over Ethernet is the way to go though.  If at all possible, choose that option.
",249
7833919,7833919,1,"I am looking for a regexp that returns only three matched groups for the string ""A   :B C:D""
where A,B,C,D are words examples (\w+)
The following Python code prints unwanted (None,None).
I just want ('A',None) (None,'B') and ('C','D') using one regexp (No added python code for filtering).
",84
7833919,7834115,2,"This might do the trick:
(\w*)(?::(\w*))? describes the structure you want, but it has a problem that it also matches empty string; thus we have to assure that there is at least one non-space character at the start (which will get matched by the greedy operators), and the lookahead at the start does it.
Edit: wrong paste :)
",78
7833919,7834326,2,"
yields
",1
7834907,7835084,2,"The following snippet should do what you want:
The regular expression there begins with (?s), which turns on the DOTALL option, meaning that . matches newlines as well as any other character.  The two instances of .*? are non-greedy matches of any charcter - i.e. they will find the shortest stretch of zero or more characters until the rest of the expression can be matched.
",75
7834907,7834907,1,"I'm using etree module. I'm trying to extract the information around <text ...> tag. Here is my XML file. I want if <text ..."">{{Infobox film start with Infobox film then copy all the text between {{ }}. Is it possible? thanks
Update: XML file updated
",63
7834990,7834990,1,"Currently i filter by some option in django's admin interface. For instance lets say i filter by 'By status'. Is it possible to select multiple statuses to filter results from? Here is the screenshot of the filter:
Can i select multiple items from this list? 
",52
7834990,7835497,2,"Not in the admin UI, but if you modify the URL, you can make the filtering criterion more complex.
For instance, now the URL (after you click on a filter) probably ends with something like
You can change this to
in order to see both statuses a and m. The %2C encodes a comma.
",62
7834990,34410509,2,"You can also add the following query to the URL of your list page.
in my case if i have multiple option.
",24
7834990,29198828,2,"You can also add the following query to the URL of your list display page
for numeric fields. That way you can have a ranged selection. 
Basically you can use any query you would also use in your code. 
",42
7835030,7835030,1,"I'm currently writing a basic dispatch model server based on the Python Eventlet library (http://eventlet.net/doc/). Having looked at the WSGI docs on Eventlet (http://eventlet.net/doc/modules/wsgi.html), I can see that the eventlet.wsgi.server function logs the x-forwarded-for header in addition to the client IP address. 
However, the way to obtain this is to attach a file-like object (the default which is sys.stderr) and then have the server pipe that to that object.
I would like to be able to obtain the client IP from within the application itself (i.e. the function that has start_response and environ as parameters). Indeed, an environ key would be perfect for this. Is there a way to obtain the IP address simply (i.e. through the environ dictionary or similar), without having to resort to redirecting the log object somehow?
",158
7835030,7839576,2,"What you want is in the wsgi environ, specifically environ['REMOTE_ADDR']. 
However, if there is a proxy involved, then REMOTE_ADDR will be the address of the proxy, and the client address will be included (most likely) in HTTP_X_FORWARDED_FOR. 
Here's a function that should do what you want, for most cases (all credit to SÃ¦var):
You can easily see what is included in the wsgi environ by writing a simple wsgi app and pointing a browser at it, for example:
And combining the two ...
",102
7838241,7838241,1,"I'm building a PyGTK application with several widgets that when changed, need to notify other widgets about the change. I would like to avoid code like this:
And do something like this instead:
The status bar, current color pane and current tool would subscribe to that notification event and act accordingly. From what I can tell, the GObject signaling mechanism only allows me to register a callback on a particular widget, so each object that wants to receive a notification has to be aware of that widget.
Does GTK provide such a system or should I build it myself? Developers of Shotwell, a photo organization application for GNOME, had to build their own signaling mechanism, if I understand their design doc correctly. Searching here on SO didn't turn out any definitive answers.
Edit:
Clarification why I think GObject signaling is not what I need (or just a part of what I need). With GObject, I need to explicitly connect an object to another object, like so:
So in my application, I would have to do this:
In other words, I have to pass the application object or some other object that knows about the color_pallete to other objects in my application so that they connect to color_pallette signals. This is the kind of coupling that I want to avoid.
",248
7838241,7840285,2,"For one, you could create a custom subclass of GObject, which provides some custom signals. The following example is a slightly adapted version of the one given in the linked article:
Another approach would be to take advantage of one of the many event/signalling packages already on PyPI, for example:
Zope Event
Louie
PyDispatcher
Darts Events
Trellis
",63
7838241,7840578,2,"GObjects don't have to be widgets. For example, your application class can be a GObject which defines signals that other widgets connect to.
Also, I don't think you understood the Shotwell design document correctly. It looks to me like their signalling system is 100% GObject signalling system, just with particular guarantees about the order in which signals are handled. As they say in their design document, such things are possible in plain GObject, but Vala makes it easier to code it their way.
",96
7838290,7838290,1,"I have data in XML format. Example is shown as follow. I want to extract data from <text> tag. 
Here is my XML data.
I need only The 40-Year-Old Virgin is a 2005 American buddy comedy film about a middle-aged man's journey to finally have sex. Is it possible? thanks
",59
7838290,7838352,2,"Whenever you find yourself looking at XML data and thinking about regular expressions, you should stop and ask yourself why you aren't considering a real XML parser. The structure of XML makes it perfectly suited for a proper parser, and maddeningly frustrating for regular expressions.
If you must use regular expressions, the following should do. Until your document changes!
Spoiler: regular expressions might be appropriate if this is just a one-off problem that needs a quick and dirty solution. Or they  might be appropriate if you know the input data will be fairly static and can't tolerate the overhead of a parser.
",114
7838290,7838395,2,"Use an XML parser to parse XML. Using lxml:
yields
",12
7838290,7838328,2,"Don't use regular expression to parse XML/HTML. Use a proper parser like BeautifulSoup or lxml in python.
",20
7838290,7838436,2,"Here is an example using xml.etree.ElementTree:
And there you go!
",12
7838290,7838410,2,"Here is how you could do this using ElementTree:
",10
7838356,7841601,2,"Read this document:
http://docs.celeryproject.org/en/latest/internals/app-overview.html
The App is an instance of the Celery library, you can subclass app to override almost any part and corner of how Celery behaves.
",33
7838356,7838356,1,"Can somebody please explain me what is celery Application object, noted here and what purposes it should be used for?
",22
7838401,7841692,2,"Thanks Prusse,It works now - I have increased time limit from 1 to 200 on function queue(s) 
",22
7838401,7838401,1,"I'm using a python tool called ""Ajaxterm""(http://wiki.kartbuilding.net/index.php/Ajaxterm). After setting it up. The problem  , I face is,when type few chars - they appear as jumbled character.For example,When i type ""abcde"" it may come as ""cdbea"" - but no character missing.   From my understanding - I tried to debug the python code,it seems like ajaxterm internal server (qweb - QWebRequest Request Handler) processes the request it.There seems to be nothing wrong with it.
The problem is (I assume) - From .js file (javascript) to ajaxterm.py , for every character i type a connect() is initiated and thus  characters typed later may be arriving sooner than the previously typed character. 
How should I make sure,character appear in the order it was typed. 
(The problem is does (mostly) happen only when network is slow)
Thanks for any help.
",174
7838564,7838636,2,"You can use the getpass module. This doesn't exactly answer the question because the getpass function doesn't output anything to the console except for the prompt. The reason for this is that it's an extra layer of security. If someone is watching over your shoulder, they won't be able to figure out how long your password is.
Here's an example of how to use it:
This example will display ""Enter your password: "" and then you can type in your password.
",95
7838564,7838660,2,"The getpass module is written in Python. You could easily modify it to do this. In fact, here is a modified version of getpass.win_getpass() that you could just paste into your code:
You might want to reconsider this, however. The Linux way is better; even just knowing the number of characters in a password is a significant hint to someone who wants to crack it.
",75
7838564,16670956,2,"kindall's answer is close, but it has issues with backspace not erasing the asterisks, as well as backspace being able to go back beyond the input prompt.
Try:
Note mscvrt.putwch does not work with python 2.x, you need to use mscvrt.putch instead.
",49
7838564,7838564,1,"I'm writing a console program with Python under Windows.
The user need to login to use the program, when he input his password, I'd like they to be echoed as ""*"", while I can get what the user input.
I found in the standard library a module called getpass, but it will not echo anything when you input(linux like).
Thanks.
",75
7838606,7838606,1,"Edit 3: I replaced __file__ with sys.argv[0], when I need to know the location of my script/executable. This is not exactly the same, but in my case it seems to run fine (at least on executable version...). Now everything is working fine, in one-file mode, with use of accepted answer's function to access resource files!
Edit 2: as shown in accepted answer's comments, problem is coming from path resolution in my script; I try to use __file__ to get the location of the script, so that I can access to its resource files. This does not work once packaged, as __file__ will return filename from Python.dll to the script, so quite always no path and just a file name. So I have to find another trick to make access to resource files; a work-around for the moment is to move current directory to the executable path.
By the way, this means that the ConfigParser should report problem when accessing the file, and not that a section is missing.
I'll update this question with the way I resolved this path resolution question.
I've got problems with pyinstaller, and as it's the first time I'm using it, it's sure that I've done something wrong.
So, here's the problem: pyisntaller runs smoothly on a script I wrote, and generates some stuff in dist folder. Ok, so now I want to execute it to see if all went well, and here's what I get:
My first idea was that the logging.conf file was missing, so I added it (and some other resource files) in the p_tool.spec file, but this is not better.
Python version: 2.6.6, under WinXP. I'm using pyinstaller as I will need it to package files for a Solaris workstation.
So, anyone did have this problem? The only topic related is the following question: PyInstaller Problem, really close to my problem, but hopelessly it got no answer.
Edit3: details about logging removed, as not really related to the problem.
",394
7838606,7878083,2,"The error message ConfigParser.NoSectionError: No section: 'formatters' suggests that it's not a missing file but a file with a missing section that you should be looking for.
",32
7838606,7878766,2,"I had a similar problem but couldn't find a elegant fix so far. The 'hack' I use that got me trough, say my project is located in '~/project/project_root', first in the .spec file:
Here a is the Analysis object, basically I remove all of my projects files from the PYZ so no import is passed there and the logger's relative paths won't be computed from there. After this, create a Tree object from the project.
Then add this Tree to the list of TOC that is passed to COLLECT, so :
You should have your project folder added to the dist folder. The problem is that you'll end up distributing the pyc's of your project too, but couldn't find a better way so far. Very interested in the valid solution. 
",151
7838606,7887341,2,"Firstly, it might be wise to do a print config_file / os.path.exists(config_file) before reading it, so you can be sure where the file is and if python can find it. 
As to actually accessing it, os.path.split(__file__) looks almost correct, but I'm not sure it works properly under pyinstaller - the proper way of packing files is to add them to the .spec file, pyinstaller will then load them at compile time and unpack them to $_MEIPASS2/ at run time. To get the _MEIPASS2 dir in packed-mode and use the local directory in unpacked (development) mode, I use this:
",117
7838629,7838629,1,"My possibilities are limited, as I do have a nice host but can just use the normal server plan. Which means, only a normal server on port 80.
I have tried to read up some on WebSockets and/or Comet, and they mostly seem to require a second server running on another port.
Is there a way to get a stable Comet-like behaviour that scales nicely. My solution up to now is a script that sends a GET request every 5 seconds, which is not a good way to make a web chat. And I am afraid it might kill my server when a few dozen people are online.
So how can I get a reliable comet-like behaviour?
",128
7838629,7839447,2,"I've had some success using socket.io for asynchronous web stuff (comet). For Django in particular, I don't have any personal experience, but I found a nice article about combining Gevent, Socket.io, and Django. Some other resources on Socket.io and Gevent can be found on my in a couple of my blog articles as well as a slideshare presentation.
",69
7838667,7838667,1,"I created a simple bookmarking app using django which uses sqlite3 as the database backend.
Can I upload it to appengine and use it? What is ""Django-nonrel""?
",32
7838667,7838935,2,"Unfortunately, no you can't. Google App Engine does not allow you to write files, and that is needed by SQLite.
Until recently, it had no support of SQL at all, preferring a home-grown solution (see the ""CAP theorem"" as for why). This motivated the creation of projects like ""Django-nonrel"" which is a version of Django that does not require a relational database.
Recently, they opened a beta service that proposes a MySQL database. But beware that it is fundamentally less reliable, and that it is probably going to be expensive.
EDIT: As Nick Johnson observed, this new service (Google Cloud SQL) is fundamentally less scalable, but not fundamentally less reliable.
",135
7838758,7838875,2,"In Fedora python is already installed. Just run in the command line:
",14
7838758,7838758,1,"How do I run a simple python program in fedora? 
I am very much new to fedora and i don't know how exactly to start in fedora. Do we need to install any software in fedora to make python programs work? Please tell me step by step how to do it. I am familiar with IDLE but not in fedora.
",66
7838758,7838878,2,"Put 
at the top of your file. Then on the command line do 
(I use $ to indicate a shell prompt, don't type that.) 
You can run the file with
You can also just do
and don't need the #!/usr.... or $ chmod ... stuff, but the first approach is the natural way to do things in unix.
",71
7839723,7839723,1,"Version1
Version 2
These two versions of code basically do the same task, however, the version 1 code will use more than 800ms to perform a simple put operation (a yellow or red light) CPU time on google app engine. In the contract, the version 2 code only use about 300ms. (Both test on HRD datastore) 
On M/S Datastore, the version 1 code will use about 400ms and version 2 code will use about 150ms.
I can image that the version 1 will be slower compare to version 2, since it use more key index.  However, it is hard to believe that the difference is so huge. It is also surprising that Google app engine cannot handle such a easy task. 
Does that mean we cannot expect GAE to perform insert on data with more than 10 properties
or do I misunderstand anything?
thx
",163
7839723,7884415,2,"Your first model has 13 indexed properties, while your second one has only 5. It shouldn't be surprising that the first takes more time - you can reduce it by setting properties as unindexed, as Dave suggests.
Does that mean we cannot expect GAE to perform insert on data with
  more than 10 properties or do I misunderstand anything?
App Engine performs the insert just fine - you just have to be prepared for it to spend more time and cost more operations.
",91
7839723,7871059,2,"Set index=False on all properties that you don't need indexed (i.e., properties that you won't use in a query). This cuts down the number of index writes it takes to save an entity.
See http://code.google.com/appengine/docs/python/datastore/queries.html#Introduction_to_Indexes for an explanation.
",50
7839786,7843962,2,"Both execnet and Pyro mention PyPy <-> CPython communication. Other packages from Python Wiki's Parallel Processing page are probably suitable too.
",26
7839786,7840185,2,"Parallel Python might be worth a look, it works on Windows, OS X, and Linux (and I seem to recall I used it on a UltraSPARC Solaris 10 machine a while back).  I don't know if it works with PyPy, but it does seem to work with Psyco.
",57
7839786,7840047,2,"Native objects don't get shared between processes (due to reference counting).  
Instead, you can pickle them and share them using unix domain sockets, mmap, zeromq, or an intermediary such a sqlite3 that is designed for concurrent accesses.
",46
7839786,7839786,1,"What would be an inter-process communication (IPC) framework\technique with the following requirements:
Transfer native Python objects between two Python processes
Efficient in time and CPU (RAM efficiency irrelevant)
Cross-platform Win\Linux
Nice to have: works with PyPy
UPDATE 1: the processes are on the same host and use the same versions of Python and other modules
UPDATE 2: the processes are run independently by the user, no one of them spawns the others
",81
7839786,7839930,2,"Use multiprocessing to start with.
If you need multiple CPU's, look at celery.
",17
7843786,7843786,1,"I am using a package and it is returning me an array. When I print the shape it is (38845,). Just wondering why this ','.
I am wondering how to interpret this.
Thanks.
",43
7843786,7843805,2,"Python has tuples, which are like lists but of fixed size. A two-element tuple is (a, b); a three-element one is (a, b, c). However, (a) is just a in parentheses. To represent a one-element tuple, Python uses a slightly odd syntax of (a,). So there is only one dimension, and you have a bunch of elements in that one dimension.
",83
7843786,7843917,2,"
Just wondering why this ','.
Because (38845) is the same thing as 38845, but a tuple is expected here, not an int (since in general, your array could have multiple dimensions). (38845,) is a 1-tuple.
",50
7843786,7843818,2,"It seems you're talking of a Numpy array.
shape returns a tuple with the same size as the number of dimensions of the array. Each value of the tuple is the size of the array along the corresponding dimensions, or, as the tutorial says: 
An array has a shape given by the number of elements along each axis.
Here you have a 1D-array (as indicated with a 1-element tuple notation, with the coma (as @Amadan) said), and the size of the 1st (and only dimension) is 38845.
For example (3,4) would be a 2D-array of size 3 for the 1st dimension and 4 for the second.
You can check the documentation for shape here: http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html
",138
7843786,7843795,2,"It sounds like you're using Numpy. If so, the shape (38845,) means you have a 1-dimensional array, of size 38845.
",28
7843891,7843891,1,"I need to classify some values between two classes.
I have about 30 values that I can use as a training set and each value has 10 different dimensions.
I am using libSVM (in Python) and it seems that it works quite well.
I am trying also to give an interpretation to the model computed by libSVM, because I think that some dimensions are more ""important"" than others in the classification process.
For instance, consider the following example:
It is clear that the second dimension of x list's elements is useless to classify this data set.
My question is:
is there any systematic way to detect these kind of situations analyzing the model generated by libSVM?
",131
7843891,10064716,2,"A little bit late, but:
It is your responsibility to check if a feature is important or not - so you have to choose your features manually that they meet your application's requirements. The SVM tries to get the best result with the features you put in - it wouldn't make much sense to ignore given data just because the choice will get clearer (but maybe more wrong).
Only you can know which features are good, and which not. You have to find them by hand/brain.
",97
7844118,31131547,2,"In the case of integers that are included at the string, if you want to avoid casting them to int individually you can do:
It is called list comprehension, and it is based on set builder notation.
ex: 
",43
7844118,7844141,2,"
",0
7844118,7844128,2,"You can use the str.split method.
If you want to convert it to a tuple, just
If you are looking to append to a list, try this:
",31
7844118,7844118,1,"Given a string that is a sequence of several values separated by a commma:
How do I convert the string to a list?
",25
7844187,7864302,2,"After seeing the tenor of the answers here and remembering the Zen of Python, I'm going to answer my own dang question by saying, ""I probably should have just thought harder about it.""  
I will restate my own question as the answer.  Suppose I have this tiny program:
The program will return ""Fail"" for Foo.using() and ""Pass"" for Bar.using().  Upon actually thinking about what I'm doing, ""Foo.holding = Item()"" and ""Bar.holding = Item()"" are clearly different instances.  I even ran this dumpy program to prove it worked as I surmised it did, and no surprises to you pros, it does.  So I withdraw my question on the basis that I wasn't actually using my brain when I asked it.  The funny thing is, with the program I've been working on, I was already doing it this way but assuming it was the wrong way to do it.  So thanks for humoring me.
",190
7844187,7844236,2,"You have to create new instances of the Tool for each Student.
",13
7844187,7844233,2,"Just call Tool('hammer') every time you want to create a new tool.
",17
7844187,7844222,2,"You could change your lines like this:
That'll produce a distinct instance of your Tool class for each instance of the Student class.  the trouble with your posted example code is that you haven't ""called the Tool into being"" (to use your words) more than once.
",55
7844187,7844187,1,"As an example, just a couple of dummy objects that will be used together.  FWIW this is using Python 2.7.2.
That's probably enough code, you see where I'm going with this.  If I call Hammer.break(), I'm calling it on the same instance of the object; if Billy's hammer is broken, so is Tommy's (it's really the same Hammer after all).
Now obviously if the program were limited to just Billy and Tommy as instances of Students, the fix would be obvious - instantiate more Hammers.  But clearly I'm asking because it isn't that simple, heh.  I would like to know if it's possible to create objects which show up as unique instances of themselves for every time they're called into being.
EDIT: The kind of answers I'm getting lead me to believe that I have a gaping hole in my understanding of instantiation.  If I have something like this:
Each time I use Moo(), is that a separate instance, or do they both reference the same object?  If they're separate, then my whole question can be withdrawn, because it'll ahve to make way for my mind getting blown.
",230
7844187,7845174,2,"Oh wait, I forgot, Python does have magic.
But I still say you should use magic sparingly.
",21
7844187,7844281,2,"
I'll try to be brief. Well.. I always try to be brief, but my level of success is pretty much random.randint(0, never). So yeah.
Lol. You even failed to be brief about announcing that you will try to be brief.
First, we need to be clear about what ""called into being"" means. Presumably you want a new hammer every time self.tool = object happens. You don't want a new instance every time, for example, you access the tool attribute, or you'd always a get a new, presumably unbroken, hammer every time you check self.tool.broken.
A couple approaches.
One, give Tool a copy method that produces a new object that should equal the original object, but be a different instance. For example:
Then in Student's init you say
Option two, use a factory function.
I can't think any way in Python that you can write the line self.tool = object and have object automagically make a copy, and I don't think you want to. One thing I like about Python is WYSIWYG. If you want magic use C++. I think it makes code hard to understand when you not only can't tell what a line of code is doing, you can't even tell it's doing anything special.
Note you can get even fancier with a factory object. For example:
",263
7845274,7845274,1,"I just found out about the following code I can enter into the command line:
Where can I find more documentation about the modules and what I can do with this?
",33
7845274,7845286,2,"Mostly in the source itself. There is almost no documentation on which can be used this way, and not all of them do anything useful.
",28
7845274,7845288,2,"In the main Python documentation, there is a note at the bottom of the SimpleHTTPServer documentation. Other modules such as pdb and timeit have similar notes.
",29
7845274,7845405,2,"The overall -m command line feature is documented here and here.  You can use it for you own modules. As noted, for individual standard lib modules it's best to consult their source code.
",38
7845274,7845599,2,"Try with:
Or in Python 3:
",8
7849077,7849077,1,"I have a 3D (time, X, Y) numpy array containing 6 hourly time series for a few years. (say 5). I would like to create a sampled time series containing 1 instance of each calendar day randomly taken from the available records (5 possibilities per day), as follows.
Jan 01: 2006
Jan 02: 2011
Jan 03: 2009
...
this means I need to take 4 values from 01/01/2006, 4 values from 02/01/2011, etc.
I have a working version which works as follows:
Reshape the input array to add a ""year"" dimension (Time, Year, X, Y)
Create a 365 values array of randomly generated integers between 0 and 4
Use np.repeat and array of integers to extract only the relevant values:
Example:
This seems to work, but I was wondering if this is the best/fastest approach to solve my problem? Speed is important as I am doing this in a loop, adn would benefit from testing as many cases as possible.
Am I doing this right?
Thanks
EDIT
I forgot to mention that I filtered the input dataset to remove the 29th of feb for leap years.
Basically the aim of that operation is to find a 365 days sample that matches well the long term time series in terms on mean etc. If the sampled time series passes my quality test, I want to export it and start again.
",266
7849077,7849200,2,"The year 2008 was 366 days long, so don't reshape.
Have a look at scikits.timeseries:
Now you can access the t data with day/month/year objects:
so for example:
Play with the attributes of t to make it work with leap years too.
",49
7849077,7849448,2,"I don't see a real need to reshape the array, since you can embed the year-size information in your sampling process, and leave the array with its original shape.
For example, you can generate a random offset (from 0 to 365), and pick the slice with index, say, n*365 + offset.
Anyway, I don't think your question is complete, because I didn't quite understand what you need to do, or why.
",88
7849117,7854400,2,"Source: http://code.activestate.com/recipes/215418-watching-a-directory-tree-on-unix/
The watch_directories() function takes a list of paths and a callable object, and then repeatedly traverses the directory trees rooted at those paths, watching for files that get deleted or have their modification time changed. The callable object is then passed two lists containing the files that have changed and the files that have been removed.
This recipe is useful where you'd like some way to send jobs to a daemon, but don't want to use some IPC mechanism such as sockets or pipes. Instead, the daemon can sit and watch a submission directory, and jobs can be submitted by dropping a file or directory into the submission directory.
Locking is not taken into account. The watch_directories() function itself doesn't really need to do locking; if it misses a modification on one pass, it'll notice it on the next pass. However, if jobs are written directly into a watched directory, the callable object might start running while a job file is only half-written. To solve this, you can use a lockfile; the callable must acquire the lock when it runs, and submitters must acquire the lock when they wish to add a new job. A simpler approach is to rely on the rename() system call being atomic: write the job into a temporary directory that isn't being watched, and once the file is complete use os.rename() to move it into the submission directory.
",274
7849117,7849117,1,"Ex, I need to catch remove and add files events on some directory on linux os. I found libs like inotify and python wrappers for them, but if I want to use clear python code should I watch for os.listdir(path) output every sec or are there some ways to accomplish such task?
",59
7849145,7877517,2,"You already have the proper 'Jo達o', methinks. The difference between >>> 'Jo\xc3\xa3o' and >>> print 'Jo\xc3\xa3o' is that the former calls repr on the object, while the latter calls str (or probably unicode, in your case). It's just how the string is represented.
Some examples might make this more clear:
Notice how the second and third result are identical. The original ldap_username currently is an ASCII string. You can see this on the Python prompt: when it is displaying an ACSII object, it shows as 'ASCII string', while Unicode objects are shown as u'Unicode string' -- the key being the leading u.
So, as your ldap_username reads as 'Jo\xc3\xa3o', and is an ASCII string, the following applies:
Summed up: you need to determine the type of the string (use type when unsure), and based on that, decode to Unicode, or encode to ASCII.
",180
7849145,7849145,1,"EDIT: 
The following print shows my intended value.
(both sys.stdout.encoding and sys.stdin.encoding are 'UTF-8').
Why is the variable value different than its print value? I need to get the raw value into a variable.
Original question:
I'm having an issue querying a BD and decoding the values into Python.
I confirmed my DB NLS_LANG using
where 
I've both tried (which return the same)
where
how to get the queries result back to the proper 'João' ?
",91
7849169,7849169,1,"I am working with four tables:
Unfortunatly I need to pass an onChange event to the form element of FileAlarm.alarm_type (which means the queryset in __init__ wont work) so I need to populate the choices= variable of ChoiceField. I'm just wondering how to do that since I only want alarms that are active and linked to the same products as the File which the FileAlarm is linked.
",73
7849169,7849859,2,"It turns out the best way round it was:
",10
7850908,7850908,1,"I'm going through and writing a setup doc for other developers at work for a python project and I've been reading up on the PYTHONPATH environment variable. I'm looking at my current development system and think I have a few things set wrong that is causing my IDE (IntelliJ) to behave incorrectly when looking up the python libraries.
I've looked at the documentation here and here and I'm still unsure of what should actually be in the PYTHONPATH environment variable. 
I have PYTHONHOME pointed to `C:\Python27'.
My current PYTHONPATH is set to PYTHONHOME. Should I also add the directories from sys.path?
UPDATE:
Based on the below information, PYTHONPATH does not need to be set unless there are non-standard libraries that you want python to be able to find by default. For instance, when I install wxPython from the installer it will add its libraries to PYTHONPATH. I do set PYTHONHOME to the root of the python installation so that I can add it to my system PATH environment variable so that I can run python from any where.
",200
7850908,7850960,2,"You don't have to set either of them.  PYTHONPATH can be set to point to additional directories with private libraries in them.  If PYTHONHOME is not set, Python defaults to using the directory where python.exe was found, so that dir should be in PATH.
",50
7850908,7851585,2,"For most installations, you should not set these variables since they are not needed for Python to run.  Python knows where to find its standard library.
The only reason to set PYTHONPATH is to maintain directories of custom Python libraries that you do not want to install in the global default location (i.e., the site-packages directory).
Make sure to read: http://docs.python.org/using/cmdline.html#environment-variables
",73
7854945,7854945,1,"I need to be able to manage the supervisord setup programmatically. Furthermore, not any user on the system should be able to to gain access to configuration of supervisord. For this reason, communication needs to be secured somehow.
I know that supervisord offers programmatic access in the form of XML-RPC. I read the documentation and attempted to work with it in several ways, but I keep running into problems.
On the Introduction page, the documentation recommends running an HTTP server for the XML-RPC interface and using the Python standard library xmlrpclib to communicate with it. There are two problems here:
The inet_http_server directive for supervisord.conf only includes username, password, and port as settings. There is no option to encrypt the connection.
xmlrpclib doesn't even support usernames and passwords. When I use the syntax username:password@host:port, I get IOError: unsupported XML-RPC protocol. As you can see in the example on the documentation page, no authentication occurs.
Since UNIX sockets are secure, I figured that connecting to the [unix_http_server] with xmlrpclib would be a good idea. Still, I don't know how authentication would work, and furthermore, xmlrpclib only supports network HTTP/HTTPS servers.
Another page in the documentation mentions a supervisor.rpcinterface module. I have no access to such a thing in Python, though. To glean more information as to why that is, I re-installed supervisord with Pip. sudo pip install --upgrade supervisor. In the pip output, I see the line Skipping installation of /usr/local/lib/python2.6/dist-packages/supervisor/__init__.py (namespace package). I don't know why it would skip installation of the namespace package.
How am I supposed to communicate programmatically and securely with supervisord?
",316
7854945,7932112,2,"Supervisor supports options to set permissions on the Unix domain socket.
http://supervisord.org/configuration.html#unix-http-server-section-example
I don't know the details but you should be able to call the xmlrpc interface over UNIX domain socket the same way that supervisorctl.py does.  It's calling options.getServerProxy() to get an xmlrpclib.ServerProxy object.
https://github.com/Supervisor/supervisor/blob/master/supervisor/supervisorctl.py#L188
",61
7855099,7855099,1,"How to define that?
What should be used instead of
get_by_key_name
SELECT __key__
key().name()
",20
7855099,7855122,2,"
get_by__id()
no change; it's still a key whether it has an ID or a name
key().id() 
",25
7855229,40452621,2,"To create the plot you want, we need to use matplotlib's plot_surface to plot Z vs (X,Y) surface, and then use the keyword argument facecolors to pass in a new color for each patch.
",42
7855229,7855229,1,"I am looking for a way to create four-dimensional plots (surface plus a color scale) using Python and matplotlib.  I am able to generate the surface using the first three variables, but I am not having success adding the color scale for the fourth variable.  Here is a small subset of my data below.  Any help would be greatly appreciated.  Thanks
Data Subset         
",70
7855235,8017058,2,"One way to achieve what you want to do is to use regex in your url and check for the attribute in your methods handler.
Example of urls to map
And an example of the corresponding handler
I would map 'my_param' options to a dictionary to keep things clear and to avoid me to dive into the handler if I need to change these values or if I want to add new urls.
",76
7855235,7865558,2,"You can use @ee_vin's answer to do this. However, in this situation, why not create two handlers? It's much simpler:
Anyone posting to the first URL or GETting the second would get a method not supported error.
",46
7855235,7855235,1,"I'm trying to implement some sort of API on tornado and I have such question:
is it possible to route two urls to one handler separating by method.
So if someone trying to send POST to the first url he should see error message
",47
7855237,7855326,2,"It's possible that the helper function better fits in at the module level rather than the class.
If you don't agree that this is the case, there is a staticmethod decorator that you can use on functions inside of the class. Simply put, a static method behaves the same between object instantiations of the same class. It does not rely on instance data.
For this reason, the staticmethod decorator renders behavior on the function such that it does not take an implicit first argument (typically self) as stated in the documentation).
",104
7855237,7855829,2,"When deciding where to put helper functions the question I ask is, ""Is it only for this class?""  If it can help in other places, then it goes at the module level; if it is indeed only for this class, then it goes in the class with either staticmethod (needs no class data to do its job) or classmethod (uses some class, but not instance, data to do its job).
Another python code checker is pyflakes.
",91
7855237,7855237,1,"In Python, if some methods of a class need a helper function, but the helper function itself doesn't use anything in the class, should I put the helper function inside or outside the class?
I tried putting it inside but PyLint was complaining that this function could have been put outside... 
Also, are there any good books talking about this kind of stuff (it doesn't have to be Python)? 
Thanks for your help!
Jack
@Karl:
The class is a software upgrader and the helper function creates a new folder if the folder doesn't exist yet. The class is in a module having pretty much only the code for the class as of now. Other classes may be added later on.
Thanks,
Jack
",141
7855293,7855379,2,"It will get serious. See PEP 4 for the details. But DeprecationWarnings tell you that some functionality is about to change in the next Python version.
",29
7855293,7855701,2,"No worries.  The warning is about something in the standard library that was already fixed in Python 2.7.  You can safely ignore it :-)
The sets.py is part of the standard library.  Line 85 is just a warning that the sets module is deprecated in favor of the set() builtin method but that won't disappear until Python3.0.
",66
7855293,7855293,1,"I ran some python code and got this error message:
C:\Python26\lib\sets.py:85: DeprecationWarning: functions overriding warnings.showwarning() must support the 'line' argument
  stacklevel=2)
I am not sure if this is some warning that I can just ignore or if this is serious? Any input will be appreciated. Thanks
",57
7855343,7855343,1,"I want to try playing around with gevent as a web server and application framework.  I don't see any way to ""restart"" the server or update the application code without killing and starting the whole python application again.
Is this just how it's done?  Maybe it's just a matter of me understanding a different paradigm to the apache way.
Also, as a semi-related question, is it even a good idea to run a web server AND the site/service itself through gevent.  I've seen other implementations using gunicorn for the server and gevent for the application but from the benchmarks I've seen, gevent far outperforms gunicorn as a server when it comes to scaling.
",129
7855343,7857201,2,"Gunicorn has 3 gevent workers:
-k gevent   (using gunicorn's HTTP parser)
-k gevent_pywsgi   (using gevent.pywsgi module)
-k gevent_wsgi     (using gevent.wsgi module)
gevent.wsgi is a fast HTTP server based on libevent.
gevent.pywsgi is WSGI server implemented in Python.
The reason for existence of gevent.pywsgi is libevent-http having a few limitations, such as not supporting keep-alive, streaming, ssl and websockets.
Note, that the new alpha version (1.0a3) of gevent uses libev and does not  include a WSGI server based on libevent-http. Currently, gevent.wsgi here is an alias for gevent.pywsgi.
The server classes in gevent don't have any features related to process management, restart, reload and so on. Those features are necessary for deployment though. Gunicorn provides that for gevent's WSGI servers. Use it.
",149
7855534,7855534,1,"I have a list of objects and for each of them I want to call their foo function, passing it the argument bar. I want to parallelize this operation, so right now I'm looking at using Pool.map from the multiprocessing package. I'm not sure how to use map to run an object method, though. How can I do that? Or is there a better way to do this in parallel?
",80
7855534,7855729,2,"Define a helper function and pass the object method and its arguments to that function via Pool.map. The helper function would look something like this:
And you would use it like this:
Note that the helper function must be directly importable from its containing module.
",49
7856843,7856843,1,"I've been following a tutorial but keep getting the following error
AttributeError: Worm instance has no attribute 'move'
I'm not sure exactly what it means or how to fix it. The error refers to line 44 towards the bottom the line is w.move()(this one's solved look below)
----------Change --------
code:
and Error - 
",72
7856843,14293714,2,"dir_x and dir_y are vx and vy you should change them... to vx and vy...
",17
7856843,7856877,2,"AttributeError indicates that you attempted to access a property or method on an object that was not defined in the object's class definition.
It just seems like you have not progressed far enough in the tutorial code to have defined the Worm.move() method.  It occurs at line 43 of the tutorial, just before Worm.draw().  You are headed for another AttributeError on the draw() method, as you've not yet defined that one either. Just add both of these to the Worm class definition.
Update
You're now receiving the AttributeError on Worm.vx  because you're missing that property (also vy) from Worm.__init__(). Compare your code to the code under the heading The improved game on the tutorial page. When you encounter further errors, compare your class definition to the tutorial's.
Add to __init__()
",158
7856853,7857662,2,"Here's how I'd write it:
If input is ""The cat said my name"", match will be ""The cat said"".
If input is ""The cat never mentioned my name"", match will be None.
I really like the fact that Python makes it possible to compile a regular expression and assign the particular method of interest to a variable in one line.
",74
7856853,7856915,2,"Does this work for you?
",6
7856853,7856926,2,"Use the group method on the match object:
",9
7856853,7856853,1,"Is there a more pythonic way than doing:
For exampile assume my original string is ""The cat said hi"" and my compiled regex is ""The.*said"" I would pull the text ""The cat said""
The above code looks ugly but that's how i've been doing it
",56
7856918,7856918,1,"I have just picked up Python to develop a tool and I am so far really enjoying the language, however have one issue I am not entirely sure how to solve.
I am looking to use a few external libraries in my project, at the moment cherryPy and Cheetah however I am not sure how to package up my application so that these libraries are included. Coming from a .NET world the compiler used to do pretty much everything for me.
Have done a bit of googling but have not been able to find any solution, so I must be missing something fundamental. Is this something I need to configure distutils for? Do I need to copy the libs in to my application folder structure anywhere? Both?
Appreciate any advice please. :)
",144
7856918,7862053,2,"I think what you're asking is how to create what Python calls a Built Distribution.  This can be done with distutils and it is tedious.  And again I'm guessing at the question, but I think you'd benefit from the docs about describing extension modules.
Besides that, I don't think a broad answer would be complete without pointing at py2exe and py2app -- utilities to create standalone executables for Windows and OS X, respectively.
",84
7856949,7876050,2,"It's documented in WTForms documentation of the SelectField quoted here for convenience:
Select fields keep a choices property which is a sequence of (value,
  label) pairs.
I'm not sure about form.parent.choices syntax but the code looks like:
",45
7856949,7856949,1,"I am testing out Python framework Flask and Flask-MongoAlchemy with MongoDB (of course). As I'm building multiple documents in my test app, I like to get the forms validated us WTForms.  
Can anyone share with me an example on how to create the object references in a SelectField()?
Any suggestion would be great!  Or link to an online example.  Thanks!
",72
7857000,7857701,2,"(.*) doesn't mean anything specific in Python. However, it can mean specific things to certain functions when a part of a string. Hence '(.*)' might mean something to a function, although it means nothing to Python itself. Since
Two functions that do take strings containing (.*) are glob.glob, fnmatch.fnmatch and the re modules functions.
In glob and fnmatch it is '*' that has special meaning, it means ""anything"". You typically use it to match filenames:
And you can also list everything with a specific ending:
Hence, in these modules '(.*)' would mean anything starts with (. and ends with ) with anything in between it.
In the re module you handle regular expressions. regular expressions is a highly magical text matching language. There '.' means ""any character (except newlines, unless you set a special flag to make it mean newlines as well)"", and '*' means ""zero to infinite amount of repetitions of the previous match"". Hence '.*' means ""pretty much anything"", and is a common thing to stick into regular expressions, as 'foobar.*' would mean anything that start with foobar.
Parenthesis means ""groups"", and to know what that mean you'll just have to read the documentation. You probably won't get it without some experimentation so you see what happens.
Basically '(.*)' matches anything. I'm assuming your regular expression has text before and after it as well?
",300
7857000,7857078,2,"This is a really weird way of explaining it but:    .* when used in files means a group of files... like if you used 
fruits.* instead of fruits.apple, it would be like saying fruits.apples, fruits.oranges, fruits.bananas, and any other files in the fruits group. 
",54
7857000,7857000,1,"I was reading a learning python book, and this was in one of the examples so I was wonder if this meant something.
",25
7861208,7861235,2,"decimal.Decimal allows you to use a specific precision.
",9
7861208,7861208,1,"I have a string:
And I want it to convert it to digits. However, I have used int, float, and others but I only get 12.0 and i want to keep all the zeroes. Please help!
I want x = 12.000 as a result.
",52
7861208,7861255,2,"You may be interested by the decimal python lib.
You can set the precision with getcontext().prec.
",21
7861208,7862381,2,"If you really want to perform calculations that take precision into account, the easiest way is to probably to use the uncertainties module.  Here is an example
The uncertainties module transparently handles uncertainties (precision) for you, whatever the complexity of the mathematical expressions involved.
The decimal module, on the other hand, does not handle uncertainties, but instead sets the number of digits after the decimal point: you can't trust all the digits given by the decimal module.  Thus,
whereas 100*(12.1±0.1) = 1210±10 (not 1210.0±0.1):
Thus, the decimal module gives '1210.0' even though the precision on 100*(12.1±0.1) is 100 times larger than 0.1.
So, if you want numbers that have a fixed number of digits after the decimal point (like for accounting applications), the decimal module is good; if you instead need to perform calculations with uncertainties, then the uncertainties module is appropriate.
(Disclaimer: I'm the author of the uncertainties module.)
",187
7861299,7861299,1,"I tested out a simple hello world line to test out JPype:
jpype.java.lang.System.out.println(""hello world"")
It works great inside the context of a ""main"" python program.  However, inside the context of a running thread, I get the following response:
",50
7861299,7861440,2,"citing from the JPype documentation:
Python Threads
For the most part, python threads based on OS level threads (i.e posix threads), will work without problem. The only thing to remember is to call jpype.attachThreadToJVM() in the thread body to make the JVM usable from that thread. For threads that you do not start yourself, you can call isThreadAttachedToJVM() to check.
HTH
",74
8022337,8022337,1,"I'm working with 3-dimensional arrays (for the purpose of this example you can imagine they represent the RGB values at X, Y coordinates of the screen).
What I would like to do, is to set to an arbitrary value the G channel for those pixels whose G channel is already below 5. I can manage to isolate the pixel I am interested in using:
but I am struggling to understand how to assign a new value to the G channel only. I tried:
...but it seems not to produce any effect. I also tried:
...(failing to understand what is happening). Finally I tried:
I suspect I am missing something fundamental on how NumPy works (this is the first time I use the library). I would appreciate some help in how to achieve what I want as well as some explanation on what happened with my previous attempts.
Many thanks in advance for your help and expertise!
EDIT: The outcome I would like to get is:
",190
8022337,8022415,2,"
a[:,:,1] gives you G channel, I subsetted it by a[:,:,1] < 5 using it as index.  then assigned value 9 to that selected elements.
",39
8022337,8022408,2,"there is no need to use where, you can directly index an array with the boolean array resulting from your comparison operator.
you do not list the expected output in your question, so I am not sure this is what you want.
",46
8022342,8022342,1,"Counter objects are subclasses of dict so they have the method setdefault.
If I do:
everything seems pretty nice. But:
Is this a bug?. Should not give c.setdefault('castles') a value/key error instead of silently accept a key without value ? Or maybe a repr method taking into account None values?
",61
8022342,8022354,2,"The  c.setdefault('castles') line directly assigns c['castles'] = None.  This likely isn't what you intended.
If you intended to make castles show in the __repr__, use c['castles'] = 0 instead.
For a Counter to behave as designed, the keys can be anything you want to count and the values need to be a number.  As you've seen, the sort-step in __repr__ expects that the values are all numbers  and it won't work if one of the values is set to None.
It might seem that setdefault would be used to give the counter default values or assign a factory function, but that isn't what setdefault does.  And you don't need to do that step at all since Counter objects automatically return default value of zero for you.  No extra work is required.
Here is how it all works, simply and easily:
",171
8022342,8022365,2,"Yes, looks like a bug. The problem is that setdefault without a value argument assumes the value is None, while in the case of a Counter it should really insert either one or zero, or fail by raising some exception.
In Python 2.7, your snippet works, btw., although it still inserts a None value, violating Counter's invariants.
Mind you, this isn't the first bug/design flaw that I encounter with collections.Counter.
",84
8121142,8135762,2,"In case anyone needs it, I created Whooshstore, which is essentially a Whoosh-based, pure Python clone of GNU id utils that provides incremental updates, pagination and a Python API.
The command line client works like this:
(-b is for batch updating, which is faster but requires more memory. For the full CLI syntax use --help.)
It does not come close to the speed of GNU id utils, but by updating the index using several incremental batch (in-memory) updates it's fast enough for us.
",100
8121142,8121142,1,"I am trying to create a web interface for searching through a large number of huge configuration files (approx 60000 files, each one with a size between 20 KByte to 50 MByte). Those files are also updated frequently (~3 times/day).
Requirements:
Concurrency
Must identify the line numbers for each matching line
Good update performance
What I have looked into:
Lucene: To identify a line number, each line must be stored in a separate Lucene document, each containing two fields (the line number and the line). This makes updates hard/slow.
SOLR and Sphinx: Both based on Lucene, they have the same problem and do not allow for identifying the line number.
SQL table with a fulltext index: Again, no way to show the line number.
SQL table with each line in a separate row: Tested this with SQLite or MySQL, and update performance was the worst of all options. Updating a 50 MB document took more than an hour.
eXist-db: We converted each text file to XML like this: <xml><line number=""1"">test</line>...</xml>. Updates take ~5 minutes, which somewhat works but we are still not happy with it.
Whoosh for Python: Pretty much like Lucene. I have implemented a prototype that sort-of works by dropping/re-importing all lines of a given file. Updating a 50MB document takes about 2-3 minutes using this method.
GNU id utils: Suggested by sarnold, this is blazingly fast (50MB document is updated in less then 10 seconds on my test machine) and would be perfect if it had pagination and an API.
How would you implement an alternative?
",316
8121142,8121230,2,"You might wish to investigate the GNU idutils toolkit. On a local copy of the Linux kernel sources, it can give output like this:
Rebuilding the index from a cold cache is reasonably quick:
Rebuilding the index from a warm cache is much faster:
The index only takes 46 megabytes for my 2.1 gigs of data -- which is tiny in comparison to yours, but the ratio feels good.
Finding 399 occurrences of foo took only 0.039 seconds:
Update
Larsmans was curious about the performance of git grep on the kernel sources -- which is an excellent way to show how much performance gain gid(1) provides.
On a cold cache, git grep foo (which returned 1656 entries, far more than idutils):
Once the cache was warm, git grep foo runs much faster:
Because my dataset fits entirely in RAM once the cache is warm, git grep is pretty amazing: it's only seven times slower than the gid(1) utility and certainly it would be more than fast enough for interactive use. If the dataset in question cannot be entirely cached (which is probably where things actually get interesting) then the performance benefit of the index is unmistakable.
The two complaints about idutils:
No pagination. This is definitely a downside, though in my experience it runs quickly enough to simply store the results of the search elsewhere. If the search is going to return an appreciable percentage of the original dataset, then storage of partial results is definitely going to be annoying.
No API: true enough, there's no API. But the source is available; src/lid.c function report_grep() takes a linked list of files that match the output. A little fiddling with this function should even offer pagination. (It would take some doing.) At the end of the day, you'd have a C API, which might still not be ideal. But customizing it doesn't look awful.
However, the weakness that is probably worst is the lack of an incremental database update. If all files are updated three times per day, this is not a big deal. If some files are updated three times a day, it is doing needless work. If a handful of files are updated three times a day, there must be a better solution.
",436
15670525,15693931,2,"Gensim has a semi-well-hidden function that can kind of do this for you:
http://radimrehurek.com/gensim/matutils.html#gensim.matutils.Sparse2Corpus
""class gensim.matutils.Sparse2Corpus(sparse, documents_columns=True)
    Convert a matrix in scipy.sparse format into a streaming gensim corpus.""
I've had some success with it using a corpus extracted with CountVectorizer, then loaded into gensim.
",59
15670525,15670525,1,"I have X as a csr_matrix that I obtained using scikit's tfidf vectorizer, and y which is an array
My plan is to create features using LDA, however, I failed to find how to initialize a gensim's corpus variable with X as a csr_matrix. In other words, I don't want to download a corpus as shown in gensim's documentation nor convert X to a dense matrix, since it would consume a lot of memory and the computer could hang.
In short, my questions are the following,
How do you initialize a gensim corpus given that I have a csr_matrix (sparse) representing the whole corpus?
How do you use LDA to extract features?
",129
15670559,15675120,2,"Installing a module from a git repository in the usual ways does require a setup.py, but it can be a bare minimum one rather than a fuller one like you'd use for a PyPI module. For example:
If you drop something like that into each of your lib repositories then you can make your requirements file point to the git repo using -e git://git.example.com/foo.git#egg=foo. If you install this into a virtualenv then it will appear at src/foo inside the virtualenv directory, and it'll be added to sys.path automatically when running Python from that virtualenv.
The -e option also accepts a local directory as a parameter, so if you guarantee that the current working directory will always be the root of your project when you install from requirements.txt (so that the relative path resolves correctly) it should work just fine to write things like -e lib1 in there, and then you can still reference the libraries as git submodules if you want.
If you are determined not to create a setup.py then your task is to emulate what would happen when running python setup.py develop. This command does two things (as of Python 2.7, at least):
It creates an egg link in the site-packages lib directory in your virtualenv (or system-wide if you don't have a virtualenv) that points to your target source directory.
It adds a line to easy-install.pth, which is also in the site-packages directory, which also points to your target source directory.
You could choose to do both of the above actions by a means other than running setup.py editable and get the same effect for current versions of Python, but of course the implementation of editable distributions may change in newer versions of Python.
",318
15670559,15670559,1,"I've got a directory structure like this:
The intent if to have scripts/foo/ and scripts/bar/ be directories of runnable python scripts that make use of the modules in lib1, lib2, lib3. Scripts, lib1, lib2, and lib3 are all separate internal git repositories under active development. There isn't a static interface or published version to depend on. Basically, they are all being written nearly at the same time by a small team.
I've played with making lib[1-3] submodules and I really hate the workflow. What I think I would like is to be able to do ""import lib1"" from foo/a.py and have it use the current code in lib1. Once things mature we will likely version everything and work to product proper packages. 
One way to do this would to muck with sys.path in each of the scripts to explicitly look in '../../' or something. I was wondering if there was something more elegant. Could I get something like pip install -r requirements.txt to do this work for me? I don't want to make an official pypi setup.py, I just want to get a pointer to the current contents of the lib[1-3] directory. The reason I like the requirements.txt approach is that as the libs mature, I'll end up putting version and git URLs in there. 
Or, is there a completely different way to do this?
",262
15670586,15670672,2,"please see this path of how to setup the environment in order to use cpython. 
fully detailed tutorial for Linux
A small github project contains all the relevant files to use on MAC OSx.
use python-config in order to get the paths to Python.h and to the static lib of python, which you should link your code with.
",62
15670586,34204023,2,"Find the path with:
Once you have the path, add it to:
If needed, add:
to:
",22
15670586,15670586,1,"I apologize if this is a silly question. But I tried to google this and I couldn't find anything to point me in the right direction. I'd just like to understand what I need to do to 'set-up' cdt to 'understand' my python.h include.
the erroneous statement is this:
but I also tried
And CDT responds with an error sign on the side stating:
I am using Eclipse CDT Juno on Ubuntu 12.04. Any pointers would be great!
",89
15670586,41693140,2,"after installing the python-devel, locate On Terminal(ctrl+shift+t) by locate Python.h > the result is the file , copy the path and add it to eclipse by the following instructions.
personaly i like to get in the gist of the code so im adding to the compiler includes. but you can add to the linker as well, as mentioned above.
On eclipse : 
Project > Properties > C/C++ Build > Settings > ...Compiler(*) > Includes > Include paths (-l)
Directory : paste the path you've located in terminal.
for example/usr/include/python2.7 
press OK
see it was added to the list
press OK
.
enjoy
(*)note: if you are compiling c project choose the include under '.. GCC Copmiler'
for C++ '..G++ Compiler'
",141
15670751,15670778,2,"Python will evaluate the arguments left to right in a function call, then the function itself is executed.
General evaluation order is left to right.
",28
15670751,15670818,2,"Python uses strict (eager) evaluation strategy: the arguments to a function are always evaluated completely before the function is applied. The evaluation order is left to right (except when evaluating assignment):
Not descending into attribute lookups:
Evaluate text.find (we'll name the result F1)
Evaluate 'zip' → A1
Evaluate text.find → F2
Evaluate 'zip' → A2
Call F2.__call__(A2) (we'll call the return value R1) (text.find('zip'))
Evaluate 1 → A3
Call R1.__add__(A3) (returns R2) (R1 + 1)
Call F1.__call__(A1, R2) (returns the final result) (text.find('zip', R2))
",129
15670751,15691577,2,"It is called operator precedence and evaluation order. Within an expression, operator precedence applies, and per expression the evaluation order is used.
The text.find() call comes before the + addition operator because it has a higher precedence.
For operators of equal priority, evaluation goes from left to right. In a function call, each argument is a separate expression and these are thus evaluated from left to right.
",78
15670751,15670751,1,"I'm learning python and here's one code I can't quite get:
Now, I know this is a shortcut of accomplishing:
I was wondering, how the does
works and in what order does Python evaluates these expressions? Is there a name for this kind of 'order' of execution?
",57
15670760,15670760,1,"I wish to check if in a text file of points (x,y,z, etc) where is an header (True) or not (False). I wish to know if there is a built-in function in Python or a better method respect my own function.
i wrote this function 
example
",59
15670760,15671103,2,"If you can have columns named, e.g., ""3.5"", your code obviously won't work, so I'll assume you can't.
And that means the whole thing is a bit overcomplicated. Really, all you need to do is see if the first character is a valid float starting character for a float:
For an empty file, this will return True instead of raising an exception, but otherwise, it should work for exactly the same use cases as your original code.
I normally wouldn't even mention this, but since you tagged your question ""optimization"", I guess you care: This code is theoretically faster than yours for reasons that should be pretty obvious, but in real life, it will almost always make no difference. According to %timeit on my machine, the part after the read/readline takes 244ns instead of 2.6us. That's more than 10x as fast, as you'd expect. But the read/readline part takes 13.1us vs. 13.2us for a file is in the OS disk cache, or 39.7ms vs. 39.7ms for a file on a remote drive. The I/O cost of reading a block from a file into a buffer, even in the best case, swamps the cost of processing it (both the extra processing in readline, and the extra processing in your code).
",250
15670760,15670984,2,"Plaintext files don't really have headers in traditional sense. It's just a stream of characters.
If this were a binary format you could have a strict header and any reader would have to adhere to that format. I assume this is a custom format that you've created, if that's the case you've already got a good solution.
If you want to learn more about headers, you should look at the JPEG header specification, which is simple.
http://www.fastgraph.com/help/jpeg_header_format.html
See this post for an example of python code that reads the binary jpeg header.
Python: Check if uploaded file is jpg
",116
15674026,15674727,2,"
",0
15674026,15674322,2,"i think you want to search for a list whose pattern got matched.. .
output:
",16
15674026,15674235,2,"I think you are confused about what any means. It is used to check a sequence of values, and see if any of them is ""true"". That's not related to finding out if a value is ""any number"" or ""any of these possibilities"".
If you have a fixed, finite set of possibilities that you want to consider, then what you really want to know is whether your candidate value is in that set:
But ""any number"" is not a finite set. First off, you need to define what you mean by number; and then you need to perform the appropriate test. It sounds like what you are trying to do is check whether the value is an integer. In other words, you are concerned with the type of the values in the list.
If you already know they're all integers, then there's nothing to test; if you don't care what the value is, then just don't consider it when you make your checks. But if you need to be sure it's an integer, then the way to do that is
But maybe you have confused me, by giving an example ""to-search list"" that happens to be the same length as your ""pattern"", when you actually want to look for the pattern at any point in a longer list.
In that case, you can make a function that does an exact match of the pattern against a list of the same length; and then use any to check whether any pattern-lengthed sublist matches. any is designed to be used with generator expressions, and it looks like this:
There are more efficient ways to match, depending on the details of your pattern, that will be inspired by string search algorithms and regular expression engines. But that is getting into much more difficult material - the above should get you started.
",357
15674026,15674195,2,"
Or more code for easy to understand:
",8
15674026,15674186,2,"
How is y really defined. Obviously you can't have n in there as a placeholder? Could your use None perhaps?
",24
15674026,15674026,1,"I'm trying to work out a function to find specific patterns in a list.  For example if we take the list 
I want to then check if the pattern y shows up in the list x:
where n represents any number.  So in my example it would return True.
I've looked into the any() function and I haven't been able to work much out. 
",74
15674026,15674541,2,"This type of problem is well suited to Numpy masked arrays:
Of course, the use here may not merit installing and importing numpy, but it has advantages in some situations.
",34
15674037,15674037,1,"So I wrote this code a while back but now I have to write it recursively. This program takes the input and adds it up.
For example input=55 the answer should be 10. If the input=2645 the answer should be 17
The def sumD(num) function has to call itself. I'm not sure how to do this. 
",65
15674037,15674057,2,"
",0
15674410,15676432,2,"You wanted the while loop to end when the path length reached the number of squares on the board- using and instead of or in your while loop it will end when either this expression:
or this expression:
evaluates to False. Using or, as long as one of those expressions is true, the loop would keep running. So your fixed code would be:
",70
15674410,15674410,1,"So I am trying to plan a path on a 9x9 grid, so boardSize is 9. The while loop should stop path list has a length of 81 or more so why is it possible that it can get to a length of 3531when the creature is at 7,5 and the goal is at 5,2 and elevations are 0? Is my while loop wrong or do you think it might be elsewhere?
",76
15674412,15674412,1,"When I run this code, it responds with UnboundLocalError: local variable 'hhh' referenced before assignment. However, the global string 'temp' does not respond with such an error despite being defined in a similar manner. Any help would be fantastic, thank you.
",50
15674412,15674761,2,"Yes,you should take a look at this question Using global variables in a function other than the one that created them
Briefly speaking,if it is only reading from a name, and the name doesn't exist locally, it will try to look up the name in any containing scopes.That's what happens to temp,which will be found in the global scope.But with hhh,you do writing,which will make Python believe that hhh is a local variable.  
And another thing,but more important,it is not recomended using global.You could invoke actualcrawl() in start(),and pass in hhh,temp,which is the way most people do.
EDIT
It is simple:
I don't know what language you use before Python,but you really don't need to declare a variable like in C/C++.Because when you assign to a variable, you are just binding the name to an object.See this python variables are pointers?
",178
15674576,15674576,1,"I'm trying to implement an OAuth2 authentication server and for the client part i wanted to send a json request to the server (from a Django view) and i found several libraries to do that tho' the most common are httplib2 and urllib2 i was wondering which is the difference between them and which is the best library for this purpose.
Thanks in advance.
Edit:
After searching, i found an extremely useful library called Requests and i use this one since then. (http://docs.python-requests.org/en/latest/)
",96
15674576,15674649,2,"urllib2 handles opening and reading URLs. It also handles extra stuff like storing cookies.
httplib handles http requests, its what happens behind the curtain when you open a url. 
you can send json request with urllib2 so you should use that.
see this.
",49
15674601,15676278,2,"models.py
forms.py
views.py
upload.html
",4
15674601,15674601,1,"So I am trying to upload and save a csv file to a variable via a POST to a url.  I have looked through the django documentation on file uploads found here.  I just don't understand the use of a form?  What's the purpose in this situation?  
They use an example with a form: 
Upload html:
",64
15674602,16370556,2,"I have never try MySQLdb on python 3k. You can read the dev blog:
Python 3 compatibility is needed now more than ever. I thought I could
  do this in 1.2.4, but I would have to sacrifice compatibility for
  Python < 2.7. So MySQLdb-1.2.4 will be a bugfix release and fully
  Python 2.7 compatible (and should be Python 2.8 compatible), and very
  soon thereafter there will be a 1.3.0 which will require Python 2.7 or
  newer and be compatible with Python 3.
",91
15674602,15674602,1,"I'm about to go nuts with python 3.2.3 
Do you see something wrong with the following statement? 
because as you can see by visiting my webpage at http://superhost.gr it produces an error and I don't know why. 
I'm using MySQLdb.
Î™'m using '?' or '%s'; the latter used to work flawlessly with python 2.6, but it does not in python 3.2.3 
Both these commands fail in python 3.2.3
Any idea why?
",88
15674701,15674890,2,"Here are a few possibilities. Your question is quite vague and your code isn't even close to working, so it's difficult to understand the question
",29
15674701,15674701,1,"I am trying to read all the elements in a dictionary one by one. my dictionary is as given below ""test"".
i want to do as given in below sample code.
thank you 
",38
15674701,15674767,2,"You can use a nested comprehension:
Since dictionaries are unsorted in Python, your tuples will be unsorted as well. 
",22
15674701,15674713,2,"Try this:
if you need a list:
If you want to print each record from dict than:
",20
15674701,15674719,2,"
Analyzing your code
You can't index a dictionary. A dictionary is not a sequence like a list
You don;t use parenthesis to index. It turns to be a function call
To iterate a dictionary, you can either iterate the keys or the values. 
for key in test to iterate a dictionary by keys
for key in test.values() to iterate a dictionary by values
",72
15675469,15675469,1,"I am writing a simple program that pulls up an image (BackgroundFinal.png) and displays it in a window. I want to be able to press a button on the window to move the picture down by 22 pixels. Everything works except the button does not do anything. 
If I am not mistaken, the button should change the global 'b' value, therefore changing the y position of the label. I really appreciate any help, sorry for my god-awful conventions. Thanks in advance!
",93
15675469,15736052,2,"Thanks for the reply but, It was not really what I was looking for. I'll post what I found worked best here for anybody else with the same problem. 
Essentially, It is much better, in this case, to use a Canvas instead of a label. With canvases, you can move objects with canvas.move, here is a simple example program 
my code may not be perfect (sorry!) but that is the basic idea. Hope I help anybody else with this problem 
",94
15675469,15675742,2,"You have a few problems here.  
First, you're using pack and place.  In general, you should only use 1 geometry manager within a container widget.  I don't recommend using place.  That's just too much work that you need to manage.
Second, you're calling the callback movedown when you construct your button.  That's not what you want to do -- You want to pass the function, not the result of the function:
Third, globals returns a dictionary of the current namespace -- It's not likely to have an integer key in it.  To get the reference to the object referenced by b, you'd need globals()[""b""].  Even if it did, changing the value of b in the global namespace won't change the position of your label because the label has no way of knowing that change.  And in general, if you need to use globals, you probably need to rethink your design.
Here's a simple example of how I would do it...
",197
15678988,15678988,1,"So I have the following case:
I have a module that has several function definitions. I want to create a class that has those functions available to it.
I just wanted to ask which way is best?
Option1
Option2
I know option 2 is against PEP 8 so I assume people will say option1, however, what if I wanted everything to be available as:
instead of:
How can I do that with option1?
",82
15678988,15679105,2,"You generally wouldn't do this.
You normally would create a base class in baseclass_module instead:
Then use that in other modules:
",25
15679067,15679819,2,"With standard PyDev
It seems that this option is only available in Eclipse's Java Editor.
The Java editor allows you to create ""profiles"" for the code formatter, while PyDev's options for the code formatter are very limited.
However,
You can hack this. PythonTidy.py is an awesome script that cleans up Python code to make it follow PEP8 conventions, and that can be tweaked with your own settings. 
PythonTidy (code cleanup & formatting)
Get here (homepage) the source for PythonTidy. 
You will see inside the file, at the beginning of the code and just after the comments, that many settings are defined. 
The first one of these is COL_LIMIT with its default value set to 72. Now you can use PythonTidy to format your code the way you want.
Integration with PyDev
Now you have to make this work with PyDev's formatting. This blog post will explain it really better than me, but still I'll sum up the steps :
Write a Jython interface betwenn PyDev's editor (PyEdit) and PythonTidy. This blog's author already wrote a wrapper script in the public domain available in the above link or here in case the link goes 404.
Save this file anywhere you want, with the name pyedit_pythontidy.py, along with the PythonTidy.py file.
Configure PyDev to use this script as its Code Formatter. You can do this in Preferences > PyDev > Scripting PyDev
Note #1: I really recommend reading the original blog post to have a better understanding
Note #2: The wrapper script author did not implement Code Block formatting, so this means you can only format a full file. This should not be that hard to implement, up to you.
Note #3: All credits goes to bear330 for the PyDev integration part.
",336
15679067,15679067,1,"I'm using Pydev for Python programming in Eclipse.
I would like to know if there is a way for auto-formatting the code so that a maximum number of characters per line is respected.
Thanks
",37
15679162,15689093,2,"There is a module for openerp 6.1 to remove the create and edit option(in the openerp apps site search for web remove) from the default selection of many2one field. You can use this as an example and create you own module. or you can modify the base codes goto your server, then navigate to openerp/addons/web/static/src/js/view_form.js and remove the quick create functionality defined from the line number 2860.
This is the Same answer that I have given in  openerp help site.
",88
15679162,20212921,2,"In v7 you can use the answer as suggested in http://help.openerp.com/question/16498/how-to-disable-create-and-edit-from-from-a-menu/
I had this problem in v6.1 though, so I created a new option so that I could apply it to only some fields (not all fields as suggested by @Bipin)
and changed web/static/src/js/view_form.js
I want to make this into a module, as editing the source code isn't very maintainable.
",69
15679162,15679162,1,"Friends,
Need to remove this option from pop up manyone fields. (not in all fields.some fields need to remove this feature).i used widget=""selection"".then my domain filter not working.so please help me to find a solution.
",45
15679162,19246498,2,"I have faced the same problem, but I solved it easily.
You need to change your web add-ons.
Please follow the step:
Go to: web/static/src/js
open the file: view_form.js
Go to line number 2958 or you can find label: _t (""Create and
Edit...""),
comment it
Enjoy, you can now see in your many2one fields don't have 'Create and Edit'
Note : This will affect every many2one field.
",83
15679261,15679261,1,"I have a directory containing 450 folders, all unique names. Inside this folder I need to create a sub-folder called Metadata, so it looks like the following:
Folder1/Metadata
Folder2/Metadata
Folder3/Metadata
Is there a way using Python to create this example?
",45
15679261,15679335,2,"The functions you are looking for are all in the os module:
",13
15679272,15679272,1,"By ""reasonable"" environment I mean that it should not require the user to manually install any dependencies of the application, but a working Python installation can be required. Additionally I would like the application to work on Windows, OSX, and popular Linux distributions. If I can package a Python interpreter as well, that's better. Size is not really a concern. A good example of what I want to accomplish is the SublimeText editor. 
Is there an established way of doing this?
",94
15679272,15679428,2,"Yes, python comes with setup utilities, and there are packages which will put your complete application in a platform specific binary(exe on windows, .app on osx).
Some of the packages I would recommend looking at would be:
cx_freeze
py2app
py2exe
",48
15679359,15679476,2,"urllib does not support parsing a .pac file. The page you see is probably the Apache page for the server serving that .pac configuration file instead.
.pac files contain javascript code that present your browser with proxy rules. You can try and open the file directly and see what proxy would be configured for the Python Challenge site instead. See http://en.wikipedia.org/wiki/Proxy_auto-config for more details on the file format.
Once you figured out what proxy server would be used, configure that as server in the proxies mapping instead.
",96
15679359,15679359,1,"I'm currently working my way through the excellent Python Challenge (http://www.pythonchallenge.com/). The current problem I'm tackling involves the use of the urllib library but I'm having issues.  I'm attempting to use this library to connect to the site through my company's firewall.  Let's start with some code:
This yields an http response, but strangely its the Apache HTTP server test page:
...Red Hat Enterprise Linux Test Page... This page is used to test the proper operation of the Apache HTTP server after it has been installed, etc...
So, I appear to be successfully acheiving an http connection outside our firewall but getting a different http resposne than my browser.  Another clue (or not) is when I try to connect to the about.php page:
This, however, yields:
404 Not found... Apache 2.2.3 Red Hat Server at www.pythonchallenge.com Port 80
Both addresses above work just fine in my browser (using the same proxy).  Any ideas where I'm going wrong?
",191
15679406,15679406,1,"I try to put a listing of subdirectories of a directory into a list.
I use that code :
The problem is that I have not all my subdirectories into my list ""liste""
Would you have any solutions ?
Thanks
",43
15679406,15679423,2,"You need to call liste.append inside the loop.
",9
15679454,15680848,2,"Use sys.argv. Which gives you a list of the items passed on the command line
Script b
sys.argv will be a list that contains ['./scriptB.py', '10'] in this case.
",36
15679454,15679454,1,"Hey guys I've got a problem. 
I'm trying to send variable x that is found in script a to script b and then execute script b with that variable. 
Example:
Script a
Script b
Any ideas on how I could do this?
Thanks
",49
15679467,15679531,2,"Just create a loop over os.listdir():
",9
15679467,15679467,1,"I'm parsing XML in python by ElementTree
I wish to parse all the 'xml' files in a given directory. The user should enter only the directory name and I should be able to loop through all the files in directory and parse them one by one. Can someone tell me the approach. I'm using Linux. 
",62
15679526,15680860,2,"If you want to make list2 identical to list1, you don't need to mess with order or re-arrange anything, just replace list2 with a copy of list1:
list() takes any iterable and produces a new list from it, so we can use this to copy list1, thus creating two lists that are exactly the same.
It might also be possible to just do list2 = list1, but do note that this will cause any changes to either to affect the other (as they point to the same object), so this is probably not what you want.
If list2 is referenced elsewhere, and thus needs to remain the same object, it's possible to replace every value in the list using list2[:] = list1.
In general, you probably want the first solution.
",153
15679526,15679667,2,"or may be just check everything is perfect then..copy list a for b
",13
15679526,15679526,1,"I have two lists in python.
What i need is to order b according to a. Is there any methods to do it so simply without any
loops?
",31
15679526,39667373,2,"Sort b based on items' index in a, with all items not in a at the end.
",20
15679526,15679634,2,"The easiest way to do this would be to use zip to combine the elements of the two lists into tuples:
sorted will compare the tuples by their first element (the element from a) first; zip(*...) will ""unzip"" the sorted list.
",52
15679672,15680677,2,"I'm posting this without further comments, for learning purposes (in the real life please do use a library). Note that there's no error checking (a homework for you!)
Feel free to ask if there's something you don't understand.
",50
15679672,15679672,1,"im trying to parse lines in the form:
Where OP is a symbol for a logical gate (AND, OR, NOT) and something is the thing i want to evaluate.
The output im looking for is something like:
Where a condition itself can be a dict/list pair itself (nested conditions). So far i tried something like:
I tried other ways aswell but i just can't wrap my head around this whole recursion thing so im wondering if i could get some pointers here, i looked around the web and i found some stuff about recursive descent parsing but the tutorials were all trying to do something more complicated than i needed.
PS: i realize i could do this with existing python libraries but what would i learn by doing that eh?
",146
15679719,15688085,2,"You are basically reinventing the indexing scheme of a multidimensional array. It is relatively easy to code, but you can use the two functions unravel_index and ravel_multi_index to your advantage here.
If your grid is of M rows and N columns, to get the idx and idy of a single item you could do:
This also works if, instead of a single index, you provide an array of indices:
So if cells has the indices of several cells you want to find neighbors to:
You can get their neighbors as:
Or, if you prefer it like that:
The nicest thing about going this way is that ravel_multi_index has a mode keyword argument you can use to handle items on the edges of your lattice, see the docs.
",141
15679719,15679719,1,"Let's suppose I have a set of 2D coordinates that represent the centers of cells of a 2D regular mesh. I would like to find, for each cell in the grid, the two closest neighbors in each direction.
The problem is quite straightforward if one assigns to each cell and index defined as follows:
idx_cell = idx+N*idy
where N is the total number of cells in the grid, idx=x/dx and idy=y/dx, with x and y being the x-coordinate and the y-coordinate of a cell and dx its size.
For example, the neighboring cells for a cell with idx_cell=5 are the cells with idx_cell equal to 4,6 (for the x-axis) and 5+N,5-N (for the y-axis).
The problem that I have is that my implementation of the algorithm is quite slow for large (N>1e6) data sets.
For instance, to get the neighbors of the x-axis I do
[x[(idx_cell==idx_cell[i]-1)|(idx_cell==idx_cell[i]+1)] for i in cells]
Do you think there's a fastest way to implement this algorithm?
",203
15679734,15679734,1,"I am using python minidom to parse a xml, but not able to get it working for below xml. I want to select the first server tag and want the value of name tag , in this case ""Server1""
",43
15679734,15680192,2,"You'll have to reference the XML DOM documentation and grit your teeth.
To get the first <server> element, then its <name>:
",29
15679762,15679762,1,"In python 3 I have a line asking for input that will then look in an imported dictionary and then list all their inputs that appear in the dictionary. My problem is when I run the code and put in the input it will only return the last word I input.
For example
the dictionary contains (AIR, AMA)
and if I input (AIR, AMA) it will only return AMA.
Any information to resolve this would be very helpful!
The dictionary: 
The Code:
",94
15679762,15680564,2,"Ok, the code seems somewhat confused. Here's a simpler version that seems to do what you want:
And here's an example of use:
The code sample you provided actually does what was expected, if you gave it space separated quote names.
Hope this helps.
",53
15679782,15679782,1,"What is the difference between these fetching.?
please give me a examples for reference site to get clear idea.still i'm confuse with it
cr is the current row, from the database cursor  (OPENERP 7 )
ex : 
",41
15679782,15679995,2,"cr.dictfetchall() will give you all the matching records in the form of ** list of dictionary** containing key, value. 
cr.dictfetchone() works same way as cr.dictfetchall() except it returns only single record.
cr.fetchall() will give you all the matching records in the form of list of tupple.
cr.fetchone() works same way as cr.fetchall() except it returns only single record.
In your given query, if you use:
cr.dictfetchall() will give you [{'reg_no': 123},{'reg_no': 543},].
cr.dictfetchone() will give you {'reg_no': 123}.
cr.fetchall() will give you '[(123),(543)]'.
cr.fetchone() will give you '(123)'.
",150
15680583,15680787,2,"This could be due to network congestion in your LAN. If it is just happening for a little while like 1 minute. 
",24
15680583,15680583,1,"I've a problem: I created a small Python script to read data from an Omron PLC memory on LAN. Delphi program run batch file that run Python script periodically (every 6 seconds).
This script runs on 2 Win 7 PCs and 1 Win XP PC.
My problem is: no data transfer between Win XP PC and PLC for a random period of time (around 1 minute, sometimes more), but Win 7 PCs have no problem to communicate with same PLC.
I use UDP protocol.
LAN seems ""fall to sleep"". This is LOG File: (08:41:13 -> 08:42:30. Expectation 08:41:13 -> 08:41:19, 08:41:25, 08:41:31, ...)
What could be the problem?
",135
15680593,15680781,2,"Notice you are not only working with 1D arrays:
So, b is a 2D array.
You also see this in the output of b.shape: (1,3) indicates two dimensions as (3,) is one dimension.
The behaviour of np.dot is different for 1D and 2D arrays (from the docs):
For 2-D arrays it is equivalent to matrix multiplication, and for 1-D
  arrays to inner product of vectors
That is the reason you get different results, because you are mixing 1D and 2D arrays. Since b is a 2D array, np.dot(b, b) tries a matrix multiplication on two 1x3 matrices, which fails.
With 1D arrays, np.dot does a inner product of the vectors:
With 2D arrays, it is a matrix multiplication (so 1x3 x 3x1 = 1x1, or 3x1 x 1x3 = 3x3):
",160
15680593,15680593,1,"I try to understand how to handle 1D array (vector in linear algebra) with numpy. In the following example, I generate two numpy.array a and b:
For me, a and b have the same shape according linear algebra definition: 1 row, 3 columns, but not for numpy.
Now, the numpy dot product:
I have three different output. What's the difference between dot(a,a) and dot(b,a)? Why dot(b,b) doesn't work?
I also have some differencies with those dot products:
",110
15680809,15682527,2,"Personally, I'd store an rrule object from python-dateutil (http://labix.org/python-dateutil) rather than inventing your own recurrence format. Then you can just define some methods that use rrule. between(after, before) to generate instances of your event object for a given range.
One catch though, dateutil's rrule object doesn't pickle correctly, so you should define your own mechanism of serialising the object to the database. I've generally gone with a JSON representation of the keyword arguments for instantiating the rrule. The annoying edge case is that if you want to store stuff like '2nd Monday of the month', you have to do additional work with MO(2), because the value it returns isn't useful. It's hard to explain, but you'll see the problem when you try it.
I'm not aware of any efficient way to find all eligible events within a range though, you'll have to load in all the Event models that potentially overlap with the range. So you'll always be loading in potentially more data than you'll eventually use. Just make sure relatively smart about it to reduce the burden. Short of someone adding recurrence handling to databases themselves, I'm not aware of any way to improve this.
",239
15680809,15680809,1,"I've been working on a event based AJAX application that stores recurring events in the a table in the following format (Django models):
The rec_type stores data in the following format:
For example:
This works fine, and allows many events to be transmitted concisely, but I now have the requirement to extract all events that occur during a given range. For example on a specific date, week or month and I am a bit lost as to how best to approach. 
In particular, I am stuck with how to check if an event with a given recurrence pattern is eligible to be in the results.
What is the best approach here?
",125
15680836,15680836,1,"I am trying to run a piece of code using netrc lib in python. I got examples from the Internet but they have all failed at the first line. 
",31
15680836,16031745,2,"The problem is the name of your script file: netrc.py which is the same as the module name. Rename it.
Providing filename to netrc.netrc() causes only to use the specific netrc file in place of the default ~\.netrc.
",44
15684178,15684178,1,"I count lines in python for a text like this:
The same text goes into a textBox control.
I would like to scroll to the last line but the line count does not help , because the textBox wraps long lines making them 2 or more lines.
I can go to the last line by scrolling to -1, but then I cannot scroll up anymore. I need to know the (actual) max line count so I can scroll to any position i want.
",92
15684178,32752419,2,"A little late but just in-case someone else could use this, here is my solution. This example will dynamically size the height of the curses textpad widget.
",30
15684178,15735445,2,"you said a long line is divided in 2 lines or more,right?
you could count lines this way
where string is the text you are dealing with, and MaxLen is the maximum number of characters that the textbox can show in a line
but this just doesn't solve the problem if you don't know how to get MaxLen,and i actually don't...
",71
15684335,15684335,1,"I'm trying to use tastypie filtering but when I try to get a resource through filtering I receive a 404.
code
I'm using urlopen to access the resource:
How do I make it so I don't get a 404 when trying to get a resource through filtering with Tastypie?
",54
15684335,15685030,2,"If there are no results in a list that you query on, TastyPie would just send back Json with zero elements (something like the following): 
So it seems that if you're getting a 404, you don't have something set up correctly. 
The following things could resolve your issue:
Make sure you have ?format=json appended to your url before the &
Make sure you've registered the APIResource
Make sure you've set up the appropriate urls.py if anything is different.
",91
15684473,15684473,1,"I have the following function that takes 3 pieces of information (name, age, hometown) for 3 people and saves it in a txt file.
I am now trying to create a function that will read the text file and print a persons name if their hometown is 'Oxford'. So far I have the following just to read the text from the file but im not sure how to skip back a line and print the name if the town is Oxford.
Thanks for any help!
",94
15684473,15686380,2,"I used the following for anyone that is interested:
",10
15684557,15684557,1,"I am trying to use Form Wizard but I can't figure out where to set the choices for the fields.
I see an empty form that looks like the admin panel for adding an object.
I would like to be able to pass a question to the form and have the question field filled out and not editable and preferable not submitted.
If I do 
the function get_form_list has no length
Quiz_id is unknown.
so now I am trying to pass quiz_id to the view function and generate the list of question forms to be used in the form wizard
urls.py
views.py
I am getting the error message
Update based on Rohan's answer:
With this code I am getting an error 
Here is my models.py
",132
15684557,15684711,2,"You should call it using the class instead of object. So change your call to
Update:
The wizard view takes form class list as parameters not the form instance. You are creating form instances in question_forms and passing it to view.
If you want to pass instance for the form in each step, you can pass instance_dict.
Something like ...
",66
15684605,15684605,1,"I am writing a simple Python for loop to prnt the current character in a string. However, I could not get the index of the character. Here is what I have, does anyone know a good way to get the current index of the character in the loop?
",53
15684605,15684860,2,"Do you want to iterate over characters or words?
For words, you'll have to split the words first, such as
This prints the index of the word.
For the absolute character position you'd need something like
",42
15684605,15684617,2,"Use the enumerate() function to generate the index along with the elements of the sequence you are looping over:
",22
15684662,15684679,2,"You'd read the lines into memory, into a list and then index into that list:
",18
15684662,15684662,1,"I have a list which contains a series of numbers. This list of numbers corresponds to a line in a .dat file. How can I use the list say [0,1,2,3,4,5,6,9,4] and then print out the line in the .dat file which each number corresponds to.
",50
15684662,15685001,2,"
",0
15684687,15684687,1,"I am working on inserting a local image into a table on my mysql server. I can insert data but when I download it, it isn't a valid jpg. 
Here is a sample like what I am using. Am I doing something wrong when I format the data?
The code completes as expected but the jpg is no good.
",66
15684687,15702263,2,"I figured out how to make it work. From this:
To this:
",15
15684842,15688667,2,"Assignment itself does nothing. rc here is your client. Doing rc[0], or any sort of indexing, generates and returns a DirectView object that's a view with whatever engines you specify in []. This is a shorthand for generating the views: it's not actually just getting a specific object.
Thus, those views aren't unique. The best way to explain it, I think, is with an example. Say you have 2 engines. You want to run some tasks on only engine one, and want the tasks to block. You want to run others on only engine one, but don't want them to block. You want to run yet more on engines 1 and 2, and don't want them to block. Then you could do:
Then, you can use these to run tasks in whatever way you'd like, eg
There's no actual magic being used here. When you run rc[0] twice, it generates two views. The second view is not the same as the first. When you assign rc[0] to a variable, and then use that variable, you're working with one view, and not creating a new one. 
iPython, like Numpy and Scipy, has quite a few shorthand notations that don't necessarily fit Python's idioms perfectly. This is especially the case with [] and getitem. A purer Python way of writing this could would be to use the much more unwieldy rc.direct_view(1), and so on, which would make clear that this wasn't just getting an item, and was actually creating a view.
",312
15684842,15684842,1,"I'm playing around with IPython and remote control:
There is a behavior difference between
and
I realy can't understand what append ? Why is that ? what do assignement do ?
",34
15684881,15685014,2,"The python copy module can reuse the pickle module interface for letting classes customize copy behaviour.
The default for instances of custom classes is to create a new, empty class, swap out the __class__ attribute, then for shallow copies, just update the __dict__ on the copy with the values from the original. A deep copy recurses over the __dict__ instead.
Otherwise, you specify a __getstate__() method to return internal state. This can be any structure that your class __setstate__() can accept again.
You can also specify the __copy__() and/or __deepcopy__() methods to control just copy behaviour. These methods are expected to do all the copying themselves, the __deepcopy__() method is passed a memo mapping to pass on to recursive deepcopy() calls.
An example could be:
This example defines custom copy hooks to prevent self.spam being copied too, as a new instance will calculate it anew.
",173
15684881,15684881,1,"It is in most of the situations easy to implement copy constructors (or overloaded assignment operator) in C++ since there is a concept of pointers. However, I'm quite confused about how to implement shallow and deep copy in Python.
I know that there are special commands in one of the libraries but they don't work on classes that you have written by yourself. So what are the common ways to implement?
P.S. Showing process on some basic data structures (linked list or tree) will be appreciated.
EDIT: Thanks, they worked, it was my mistake in syntax.
I am very interested in overwriting these functions with __copy__() and __deep_copy()__. For example. how can I make a deep copy without knowing which type of information is in a data structure?
",153
15685631,15685898,2,"I think it certainly sounds like a viable option. As far as I know, the only ""interaction"" the official Python installer has with Windows is to add registry keys associating .py and .pyw files with the proper executables and possibly modifying the PATH variable. As long as the user has the correct .dll files to which the .exe's are linked, you could just zip up c:\Python33 or whichever version you're using and distribute that with your application. Before you do that, though, clone the directory and go through c:\clonedPython\libs\site-packages and get rid of any modules that aren't required for your application. Don't delete any necessary dependencies!
Portable Python is a possibility, but there may be some issues with certain modules not working properly, and it's not available yet for Python 3.3 (3.2.1 is the latest version, as well as 2.7.3), so if you have version-dependent syntax that might not be the best choice.
",180
15685631,15685854,2,"When you install python on Windows, select ""for current user only"" rather than ""for all users of this system"" when you're asked. And select the installation target to some custom directory, e.g. D:\mypython\
This kind of installation will package all necessary binaries and DLL files (e.g. msvcr90.dll) to this specified dir, with which you can deploy easily to another system (with same CPU-bit and operating system).
(I got this solution from a Chinese website http://www.oschina.net/question/23734_13481 - comment 1)
",100
15685631,15685631,1,"I'd like to ""install"" a version of Python locally, which doesn't touch anything on the system (Windows in this case) except the directory I extract it into. I would run it by specifying that particular python.exe. 
This is for the end-user. Essentially, I want to be able to extract a Python into a directory and start using it immediately, without requiring the user to even know that my program is using Python. I'm looking into py2exe and PyInstaller as well, but I'd like to know if this option is viable.
",107
15685631,15686095,2,"Use Portable Python - it is a version of python modified to do exactly what you want.
",18
15685682,15685682,1,"I have a main object with a Pyglet window as an attribute. Pylget's window class has a method called push handlers, which lets me push methods to the event stack. The following code works:
The above code will spawn a new window at the default size and attach the on_mouse_press() and on_draw event handlers to it. That works well and good - however, trying to call on the push_handlers() method in other classes doesn't seem to work.
The above code spawns a new window, but it doesn't attach the menu class's handlers. Is there a reason for this, or a workaround I can use? Thanks!
",125
15685682,17002763,2,"When you call pyglet.app.run(), you enter the pyglet loop and it doesn't come back until the pyglet window is closed, so your m.menu() is called only when the pyglet loop ends. If you remove the pyglet.app.run line from Main and call it like this:
It works.
",56
15688744,15688744,1,"i have to read a line in which i looking for pattern like 
following  found the required pattern 
in above code i have hard-coded the regex pattern for width
i have stored all four variables in array key_word = ['width', 'height', 'left', 'right']
i want to search for pattern for all these variable like
the problem is how can i make this key a raw string which will be a pattern like
",81
15688744,15688986,2,"You don't need regular expressions for this task. See other answers.
However if you insist, you can create one dynamically using re.escape:
",27
15688744,15688879,2,"You can also just use a generic regex:
matches will be a list of (key, value) tuples.
",22
15688744,15688847,2,"I would do something like the following:
",8
15688744,15688783,2,"Why not use split (or partition) and strip?
If you're really needing to use regular expressions, you can format them, too:
Or for each key:
",33
15688887,15688887,1,"It sounds reasonable that the os/rtos would schedule an ""Idle task"". In that case, wouldn't it be power consuming? (it sounds reasonable that the idle task will execute: while (true) {} )
",43
15688887,15690680,2,"This answer is specific to Windows NT-based OS.
Idle thread functioality
Tasks may vary between architectures, but generally these are the tasks performed by idle threads:
Enable interrupts to allow pending interrupts be delivered
Disable interrupts (using STI or CLI instructions, more on wiki)
On the DEBUG (or checked) builds, query if a kernel debugger is attached and allow breakpoints if been requested
Handle deferred procedure calls
Check if there are any runnable threads ready for execution. If there is one, update the idle processor control block with a pointer to the thread
Check the queues of other processors, if possible schedule thread awaiting execution on the idle processor
Call a power management routine, which may halt a processor or downgrade CPU tick rate and do other similar power saving activities
Additional info
When there are no runnable threads for a logical processor, Windows executes a kernel-mode idle thread. There is only 1 Idle process that has as many idle threads as there are logical processors. So on a Quad core machine with 4 logical/physical processors, there will be 1 Idle process and 4 idle threads.
In Windows, Idle process has ID = 0, so do all the Idle threads. These objects are represented by standard EPROCESS/KPROCESS and ETHREAD/KTHREAD data structures. But they are not executive manager processes and threads objects. There are no user-land address space and no user-land code is executed..
Idle process is statically allocated at system boot time before the process manager and object manager are set up. Idle thread structures are allocated dynamically as logical processors are brought live.
Idle thread priority is set to 0. However, this value doesn't actually matter as this thread only gets executed when there are no other threads available to run. Idle thread priority is never compared with priority of any other threads.
Idle threads are also special cases for preemption. The idle thread main routine KiIdleLoop (implementation from reactos) performs several tasks that are not interrupted by other threads. When there are no runnable threads available to run on a processor, that processor is marked as idle in a processor control block. Then if a runnable threads arrives to the queue scheduled for execution, that thread's address pointer is stored in the NextThread pointer of the idle processor control block. During the run of an idle thread, this pointer address gets checked on every iteration inside a while loop.
Source: Windows Internals. M. Russinovich. 6-th edition. Part 1, p.453 - 456.
",460
15688887,15689206,2,"Historically it's been a lot of different schemes, especially before reducing power consumption in idle was an issue.
Generally there is an ""idle"" process/task that runs at the lowest priority and hence always gets control when there's nothing else to do.  Many older systems would simply have this process run a ""do forever"" loop with nothing of consequence in the loop body.  One OS I heard of would run machine diagnostics in the idle process.  A number of early PCs would run a memory refresh routine (since memory needed to be cycled regularly or it would ""evaporate"").
(A benefit of this scheme is that 100% minus the % CPU used by the idle process gives you the % CPU utilization -- a feature that was appreciated by OS designers.)
But the norm on most modern systems is to either run a ""halt"" or ""wait"" instruction or have a special flag in the process control block that even more directly tells the processor to simply stop running and go into power-saving mode.
",196
15688887,15688958,2,"There's always code to run, the idle task is the code if there's nothing else. It may execute a special CPU instruction to power down the CPU until a hardware interrupt arrives. On x86 CPUs it's hlt (halt).
",47
15688887,15688937,2,"This depends on the OS and the CPU architecture. On x86 (Intel compatible) the operating system might execute HLT instructions, making the CPU wait until something interesting happens, such as a hardware interrupt. This supposedly consumes very little power. Operating systems report the time spent doing this as ""idle"" and may even assign it to a fictional ""idle"" process.
So, when in Windows task manager you see that the System Idle Process is consuming 90% CPU what it really means is that the CPU does not have an actual a program to run 90% of the time.
Here's a good article on the subject: What does an idle CPU do?
",129
15688889,15688943,2,"It's equivalent to 
",4
15688889,15688889,1,"I'm following this tutorial:
http://zetcode.com/tutorials/pyqt4/firstprograms/
and on the very first example I don't understand why does the application show only after the command:
Also, I modified the code slightly to test things
If I run this in the terminal and comment out the last line it will only show the window after running the last line.
I don't quite understand what the last line does.
EDIT:
I'm also confused as to why there isn't a reference of app and w i'd expect some kind of indication that w is a child or something of app.
I'm running it in Spyder and an IPython interpreter.
",121
15688889,15688946,2,"The last line is two functions. First it runs app.exec_() (which shows the main window of your application), then when that function ends, it passes the return value to as a parameter to sys.exit, which ends the program and sends the return value to the operating system (you can see this on *nix systems with echo $? after the program ends).
The reason it doesn't immediately exit is that Python can't execute sys.exit until it knows the value of the parameter to it, and it won't know that until app.exec_() finishes.
",110
15688898,15689294,2,"Buildpacks are the mechanism Heroku uses to build your application including installing dependencies. Pip is not installed by default, the Python buildpack itself pulls this dependency in. You could use the multi-buildpack which allows you to include several buildpacks. 
Multi buildpack - https://github.com/ddollar/heroku-buildpack-multi
Python buildpack - https://github.com/heroku/heroku-buildpack-python
Ruby buildpack - https://github.com/heroku/heroku-buildpack-ruby
",61
15688898,15688898,1,"I have an rails app deployed on heroku. This app depends on one python module, https://github.com/clips/pattern. Based on documentation of pattern, I could install by two ways:
1: cd pattern-2.5; python setup.py install
creating /usr/local/lib/python2.7/site-packages/pattern error: could
  not create '/usr/local/lib/python2.7/site-packages/pattern': Read-only
  file system
2: pip install pattern
bash: pip: command not found
Please, advise me how to install pattern. I found a similar question, How to install python module on Heroku cedar stack with Rails, but it just doesn't work for me.
",103
15688954,15692895,2,"You can't unload C extension modules at all.  There is just no way to do it, and I know for sure that most of the standard extension modules would leak like crazy if there was.
",39
15688954,15688954,1,"The CPython headers define a macro to declare a method that is run to initialize your module on import: PyMODINIT_FUNC
My initializer creates references to other python objects, what is the best way to ensure that these objects are properly cleaned up / dereferenced when my module is unloaded?
",52
15689151,15689238,2,"You can set the root to use your FE url patterns like this:
If you wanna forcibly redirect to /frontend/ then you will need a view to handle the redirect.
Maybe look at the Redirect Generic view: https://docs.djangoproject.com/en/1.1/ref/generic-views/#django-views-generic-simple-redirect-to
",46
15689151,15689151,1,"I'trying to redirect the / of my domain to point to a index in my ""frontend"" app.
I tried a lot of ways and all of them work. 
The problem is that my index_view is being called twice for every redirect.
Here is my top urls.py
And here is my frontend/urls.py
Every time I go to / is calling my views.index twice and I can't see why =/
Am I doing the redirecting wrong ?
Thanks in advance for any help!
",88
15689237,15689944,2,"Generally speaking, thinking of the ""readability counts"" mantra, the actual operator should always be your preferred choice. Using the operator versions has a place, when you can replace lambda a, b: a < b with the more compact operator.lt, but not much outside of that. And you really shouldn't be using explicit calls to the corresponding ufunc, unless you want to use the out parameter to store the calculated values directly in an existing array.
That said, if what you are worried is performance, you should do fair comparisons, because as you say, all your calls are eventually handled by numpy's less ufunc.
If your data is already in a numpy array, then you have already shown that they are all performing similarly, so go with the < operator for clarity.
What if your data is in a python object, say a list? Well, here are some timings for you to ponder:
Not sure why operator.lt is so slow, but you clearly want to stay away from it. If you want to get a numpy array as output from a Python object input, then this will probably be the fastest:
Note that ufuncs operating on numpy arrays are much faster than any of the above:
",236
15689237,15689237,1,"I'm curious about the benefits and tradeoffs of using numpy ufuncs vs. the built-in operators vs. the 'function' versions of the built-in operators.
I'm curious about all ufuncs.  Maybe there are times when some are more useful than others.  However, I'll use < for my examples just for simplicity.
There are several ways to 'filter' a numpy array by a single number to get a boolean array.  Each form gives the same results, but is there a preferred time/place to use one over the other?  This example I'm comparing an array against a single number, so all 3 will work.
Consider all examples using the following array:
'<' operator
operator.lt
numpy.less
Note that all of them achieve pretty much the equivalent performance and exactly the same results.  I'm guessing that all of these calls actually end up in the same function anyway since < and operator.lt both map to __lt__ on a numpy array, which is probably implemented using numpy.less or the equivalent?
So, which is more 'idiomatic' and 'preferred'?
",197
15689237,15689361,2,"In this case, the preferred form is x < 5000 because it is simpler and you are already using a numpy array.
ufuncs are meant to allow these operations to be done on any type of data (not only numpy arrays)
On Python 3, this last comparison will raise an error.
",57
15689260,15689260,1,"I have a large application that has many small windows.  I wanted to use traitsui's file dialog to open some files from these windows.  However, when I do, the file dialog correctly spawns and picks a file, but it also consistently switches the active window to an undesirable window after it is finished.  I am really confused why.
Here is a simplified test which displays the same problem:
When open_file returns and selects the desired file, the active window is switched to the BigApplication window and not back to the Subwindow window (so that the user can select some additional options before clicking OK).
",117
15689260,17602727,2,"I found a hacky workaround, as per usual.  But this behavior is still a bug.
The workaround is to dispose() of the old window and then to call edit_traits() on it.  This edits the File trait and also happens to make it the active window.  Disposing of the window manually has to be done inside the handler and is a little trickier than might be expected.
",76
15689290,15689331,2,"You're looking for:
This doesn't come with the overhead of instantiating the str list (though with 5 elements it's hardly an issue). Whenever I encounter this, though, I generally just use str(y) within the body of the loop.
",51
15689290,15689290,1,"Real simple question. I'm trying to do this in python: 
so that y will hold an integer, and x will hold the string representation of said integer. I realize I could 2 line it pretty easily - just wanting a shortcut.
Also, I'm curious as to what exactly I'm asking. I'm sure this has been answered a bazillion times throughout the forum, but I have no idea what the heck to search for to find it.
",89
15689290,15689317,2,"You can use list comprehension for achieving this:
",9
15689290,15689346,2,"An easy way is to use the map() function:
The map() function takes an iterable and passes each elem through the first arg which is a callable.
",33
15689290,15689351,2,"You want to search for List Comprehensions most likely.
This will give you tuples of strings and ints as a list, then iterate through that list like you would any other
",33
15689290,15689400,2,"Your question is unclear, since it relies upon a piece of code that doesn't actually compile.
The above construct will bind x and y to a str and an int, respectively, for each loop iteration. The output is:
",45
15690071,15690071,1,"How do I save the list of items if 'boa' is in href to a list? I don't want to print them using get() but instead convert them to a list in their own variable (it seems these are in a dictionary?), preferably to boat_links. Thanks!
",57
15690071,17243193,2,"I am not sure if you want a list or dictionary or a dictionary of lists so here are all of them
Here is a dictionary with the a tags text as the key and the href as the value
Here is a dictionary of lists based on the a.tags (what if you have multiple links with the same text)
",62
15690185,15690185,1,"I know this question is pretty basic, and I'm sorry for posting it, but I literally have spent the last hour googling and I still havent found an answer.
I coded a blackjack game using pygame, and want to convert it to wxpython mostly as a way to learn wxpython. In pygame the graphics are easy, I have a base image I display (basically just the table), and then I can just display other images on top of it as the action unfolds, for example the cards. Each time the player hits I just draw a new card in a different position. When The hand is over and I want to go to the next hand, I just display the base image again, and it covers everything up and viola! Its really simple all I have to use is blit() and pygame.display.update().
I cant seem to find anyway to do this in wxpython. All the examples I find are for drawing items in new widgets, or drawing vector graphics over an image, or opening new frames with images, etc.
any help is greatly appreciated, thank you very much.
--Daniel
",218
15690185,15690383,2,"The AlphaDrawing example in the wxPython demo shows how to overlap multiple drawings. This uses wx.GraphicContext / wx.GCDC. If you look at the documentation, you'll note that it has CreateBitmapFromImage and CreateBitmap methods that probably apply to what you want to do. See also the DragImage demo and the following links for related information:
wx python card game
http://rummy-py.sourceforge.net/ (look at the original that was done in wx)
",78
15690185,15724125,2,"I found this post, which reading the comments, does a good job of showing me want I was looking for. 
Delete image in wxpython?
",28
15690201,15690201,1,"I have a server that I'd like to use to maintain persistent connections with a set of devices, just so that they can pass simple messages back and forth. It's a trivial task, but selecting a server-side platform has been suprrisingly difficult (especially since I have no administrative privileges - it's a dedicated commercial server).
My best idea so far is to write a TCP server in Python. The Twisted platform seems suitable for the task, and has a lot of good reviews. However, my server has Python 2.7 but not Twisted, and the admins have been reluctant to install it for me.
Is there any way that I can just upload a Twisted package to the server and reference it in my libraries without installing it as a framework?
",146
15690201,15705793,2,"Use virtualenv to create your private Python libraries installation.
",10
15690201,15692480,2,"I'm not sure what you mean by ""installing it as a framework"".  If you are using an OS X server hosting environment, then maybe you're talking about Framework with a Capital F.  However, OS X server hosting isn't a very common environment so I'm guessing that's not it.
If you just want to know how to install a Python library in your home directory, then the general answer is:
This Just Worksâ„¢ on Python 2.7 (assuming the package uses distutils, which Twisted does, and you unpack the source .tar.gz and change your working directory to the directory that is the root of the contents of that .tar.gz), so you should be done after that.
",133
15690224,15690224,1,"I've recently decided to start using .format() instead of % (see this question). Instead of the {0}, {1} syntax, I'm wondering if the following is an acceptable use:
I like the straightforwardness of it - local variables go into the string - but am wondering if there's any reason I shouldn't do the above.
",71
15690224,15690479,2,"As I commented above, this should not be used on untrusted sources. Also it may not me explicit enough to be Pythonic.
One can also define a function to do that, but to access the right locals, it needs to do some frame handling
This kind of pattern is not nice and things like Pypy can't optmize this kind of code.
I would use this code (unless you need Python 2.6 support so you must add the indexes):
",88
15690224,15690429,2,"The major problem with passing locals() (or globals()) to format (or %) is that often, format strings can come from untrusted sources, and you risk exposing variables you didn't want to. If you're just formatting a literal string, that isn't an issue, but if you ever may have untrusted format strings, you have to think very carefully about what you're doing—and it's easier to just not do it.
The more minor problem is that some of your code's readers won't understand locals, or the ** syntax, and will have a hard time figuring out what it does or why it works. This isn't much of an argument. In fact, you could even say that a lot of Python's design decisions come down to making sure this is almost never a good argument—the language is exactly big enough that it's reasonable to expect your readers to understand/learn anything pythonic you write. But it's still worth thinking about.
Then there's the style issue. Every few months, someone comes along suggesting that the language should make it easier to do this, and it starts an argument on the mailing lists. Some people definitely think that this feels ""implicit rather than explicit"". Others disagree. I think it's pretty well settled that magic locals here would not be pythonic… but if you have to explicitly pass locals(), maybe that's fine and maybe it isn't. Like most style arguments that haven't gathered a consensus, it's really up to you. (By the way, the format API ultimately came out of an argument like this, where the original suggestion was for more-perl-like string interpolation with an implicit locals.)
But ultimately, you have to consider what you're saving. Compare this:
to:
Your version isn't clearer, more explicit, easier to type, or even shorter. So, I'd say the risk of making it a little harder for novices to read, and annoying to some segment of the community (even if it's for bad reasons), isn't worth it if there's no benefit.
",408
15693448,15701844,2,"The response to your login request will contain a Set-Cookie header looking something like this:
You need to send that cookie with your curl request so that the session data is available for processing, you can add additional headers to curl requests with -H, or specify the cookie explicitly:
Browsers will handle this for you of course, but curl is totally stateless and wont parse and store the Set-Cookie header for you by default, though if you're performing the login using curl, you can tell it to store the cookie in a cookie jar with -c <file>, and then you can read from it on your next request with -b file
HTTP Cookie wiki page
Curl cookie docs
Curl man page
",132
15693448,15693448,1,"I am building a RESTful application with Flask which will use sessions to track the logged in user. Here is the login code which I adapted from this Flask tutorial
When a user first logs in, I store their userid in the session, so that when the same user requests a resource, the data is tailored to them:
If I issue this command after logging in:
I receive:
instead of the address information for my logged in user.
Can anyone tell me how I can use the session in subsequent curl calls to return data that is specific to the userid stored in the cookie?
",114
15693529,16034387,2,"There are a number of ways you can achieve what you want. For example, one way is to make a custom initialiser for your handler:
Note that I moved the logging.yml to be adjacent to the script. The logmaker is the custom initialiser. Specify it in the YAML as follows:
If you run the Python script, you should find that the bot.log is created adjacent to the script and YAML file. The same message is printed to the console and bot.log:
N.B. The script could be a little tidier, but it illustrates my point.
Update: As per the documentation, the use of () as a key in dictionary indicates that the value is a callable which is essentially a custom constructor for the handler.
",140
15693529,15693529,1,"In my Python program I have the following code:
and this is my logging.yml configuration file:
The problem is that the bot.log file is created where the program is launched. I want it to always be created in the project's folder, i.e. in the same folder as my Python program.
For an example, launching the program with ./bot.py would create the log file in the same folder. But launching it with python3 path/bot.py would create the log file a level above the Python program in the file hierarchy.
How should I write the filename in the config file to solve this? Or do I need to write a custom handler? If so, how? Or is this not possible to solve using dictConfig?
",137
15693565,15953727,2,"I solved my own problem and even though it's a bit of a corner case, I'll still answer it here. The script that Jenkins was launching itself launched a threaded Python server using ThreadedTCPServer implemented more or less exactly from here. That threaded server wasn't properly exiting, so it left some pipes open. Although the server process died, the leaky pipes made it impossible for Jenkins to properly determine that the process was over (Jenkins waits for an EOF from the child process to determine if the process completed). The solution was a reimplementation of the socket server that exited properly. Hope this helps someone in the future!
",122
15693565,15693722,2,"To debug this:
Add set -x towards the top of your shell script.
Set a PS4 which prints the line number of each line when it's invoked: PS4='+ $BASH_SOURCE:$FUNCNAME:$LINENO:'
Look in particular for any places where your scripts assume environment variables which aren't set when Hudson is running.
If your Python scripts redirect stderr (where logs from set -x are directed) and don't pass it through to Hudson (and so don't log it), you can redirect it to a file from within the script: exec 2>>logfile
There are a number of tools other than Jenkins for kicking off jobs across a number of machines, by the way; MCollective (which works well if you already use Puppet), knife ssh (which you'll already have if you use Chef -- which, in my not-so-humble opinion, you should!), Rundeck (which has a snazzy web UI, but shouldn't be used by anyone until this security bug is fixed), Fabric (which is a very good choice if you don't have mcollective or knife already), and many more.
",219
15693565,15693565,1,"I have a ton of scripts I need to execute, each on a separate machine. I'm trying to use Jenkins to do this. I have a Python script that can execute a single test and handles time limits and collection of test results, and a handful of Jenkins jobs that run this Python script with different args. When I run this script from the command line, it works fine. But when I run the script via Jenkins (with the exact same arguments) the test times out. The script handles killing the test, so control is returned all the way back to Jenkins and everything is cleaned up. How can I debug this? The Python script is using subprocess.popen to launch the test.
As a side note, I'm open to suggestions for how to do this better, with or without Jenkins and my Python script. I just need to run a bunch of scripts on different machines and collect their output.
",180
15693597,15693597,1,"I have three lists (L1, L2, L3), something like :
I want to compute the product of L1*L2*L3, id est,
but I want to take or not in account some lists; hence,[1,], [ 1, 'a' ] would be a part of the result, like ['a',], ['a', 'B'] and so on.
Any idea to help me ? Thanks !
",86
15693597,15693791,2,"Use the powerset function given in the itertools examples. powerset([L1,L2,L3]) will give you all subsets of the set of 3 lists. For each subset you can take the cartesian product, and then chain them all together.
",49
15693605,15707814,2,"A more conventional way to store ""meta"" data along with a dict would be either:
to maintain two dicts with same set of keys, one for the actual data, one for ""meta""
to have a dict with (""raw"") keys, and values are 2-tuples: ( value, item-meta-data )
Both are simple and require no special magic.  You would also avoid problems like the one you describe in your question (and others to come).
",90
15693605,15707644,2,"I've made a slight modification to your base code:
Then if you want to retrieve the instance of DictKey in the set of keys you can do the following:
Hope it helps.
",36
15693605,15693605,1,"I have created a class that will allow me to store meta-data with an arbitrary dictionary key and still pass the in test with the original object type:
produces the output:
Now, given the string 'hello', I need to get the instance of DictKey that was created from said string in constant time:
",59
15693633,15693633,1,"I'm trying to create a sqlite3 table using python. My code is given below:
This code runs fine the first time I run it and my database is created. However, if I run it a second time, I get the following error:
I have no idea what's going wrong. Can anybody help?
",62
15693633,15699000,2,"Dropping modules first makes the foreign key constraint in modulesfiles invalid.
Drop the child table first.
",18
15693703,15693703,1,"I am using os.system() to run a python program and am trying to log its output to the file. This works fine. 
The problem is that myprogram.py calls another python program which gives me the output i need but doesn't finish- I can even see the prompt become different as in the picture below
Is there a way to kill the child process when I get to the next line of my program 
I tried using os.system(""quit()"") and subprocess.popen(""quit()"", shell=False) and that didn't do anything.
I can't really use exit() because that just kills python all together.
Btw this stalls 
",126
15693703,15694075,2,"The program that is being called by myprogram.py lands you in the python prompt. Why that happens we cannot tell you unless you show us the code.
Using the subprocess module (which is more versatile) is preferred to using os.system.
But you're not using subprocess correctly. Try it like this:
The with statement will close the file once subprocess.call is done. The program and its arguments should be given as a list of strings. Redirection is achieved by using the std... arguments.
After myprogram.py finishes, rc contains its return code.
If you want to capture the output of the program, use subprocess.check_output() instead.
",122
15693735,15693735,1,"How can I generate all the paths to text strings in a HTML document, preferably using BeautifulSoup? 
I have f.e. this code:
I'd like to divide HTML code into paths to text strings, like
or
or  
I've studied BeautifulSoup documentation, but I can't figure out how to do it. Do you have any ideas?
",65
15693735,15694498,2,"
Output
",1
15693756,15693756,1,"How can I convert Extended ASCII characters such as: ""æ, ö or ç"" into non-extended ASCII characters (a,o,c) using python? The way it works should be that if it takes ""A, Æ ,Ä"" as input, It returns A for all of them.
",59
15693756,15693802,2,"Unidecode might be of use to you.
",8
15693808,15693808,1,"I know that in order to install a package I need to execute:
But how can I know what is the name of package -  I should give as an argument.
I found in pypi a package I want to install - pcapy:
https://pypi.python.org/pypi/pcapy/0.10.3
I tried:
It didn't work...
What is the right way to install this package?
Thank you very much!
",72
15693808,15693858,2,"That package isn't in the PyPI. There's a page, but the source code is hosted elsewhere for some reason (this is the first time I've seen it):
Ubuntu has a (probably old) package:
You can also build it from source:
",52
15693859,15693859,1,"I have a bash script myscript.sh:
calling a python program myprog.py
The ssh command that is called by subprocess executes without error, the output is correct. But when called like this the loop in myscript.sh only runs through the first line of input and then exits with status 0.  If I replace the subprocess.check_output(...) call with a subprocess.Popen(...) and don't subsequently call Popen.wait() then the outer loop works as expected and the output from the ssh command is dumped to standard out some time after any output from the bash script.  With the Popen.wait() behavior is the same as with check_output: bash loop only goes through one iteration before exiting without error.  
If instead of ssh another command, e.g. ls, is called with check_output then the bash loop works as expected.
Can anyone help me understand why the code as shown isn't working as expected?
Note: this is a simplified version of what I am trying to do, though I do experience the same behavior with this code.  In reality I am doing something with ""$line"" in the bash script and the subprocess call is wrapped in a try/except block.
",221
15693859,15693954,2,"As @larsmans guessed the ssh call was consuming stdin, breaking the outer bash loop.  Adding the -n option to the ssh command resolved the issue:
",29
15693880,15707208,2,"You need to use the Text widget's get method to get all of the text from '1.0' (line 1, character 0) to END. 
Here's a modified version of your code that does this in a write_text function. I also added scroll bars and switched to using grid instead of pack.
",59
15693880,15693880,1,"Hi so i'm having a hard time getting Text from my program on Python to convert to a string so i can write it to a file without it just writing numbers in the file. i put this code :
so thats the code to write the obj locFile to the file Test.txt but when i type something in the Text box on the program and hit the writButton it will write to the file just not what i typed in it puts .50132192 so i wanted to know how i could convert it to a String? 
",100
15693900,15694036,2,"Seeing that the text is the only large blob, and everything else is barely larger than a pixel, a simple morphological opening should suffice
You can do this in opencv
or with imagemagic
Afterwards the white rectangle should be the only thing left in the image. You can find it with opencvs findcontours, with the CvBlobs library for opencv or with the imagemagick -crop function
Here is your image with 2 steps of erosion followed by 2 steps of dilation applied:
You can simply plug this image into the opencv findContours function as in the Squares tutorial example to get the position
",107
15693900,15699181,2,"Using your test image I was able to remove all the noises with a simple erosion operation.
After that, a simple iteration on the Mat to find for the corner pixels is trivial, and I talked about that on this answer. For testing purposes we can draw green lines between those points to display the area we are interested at in the original image:
At the end, I set the ROI in the original image and crop out that part. 
The final result is displayed on the image below:
I wrote a sample code that performs this task using the C++ interface of OpenCV. I'm confident in your skills to translate this code to Python. If you can't do it, forget the code and stick with the roadmap I shared on this answer.
",147
15693900,15693900,1,"I asked a similar question here but that is focused more on tesseract. 
I have a sample image as below. I would like to make the white square my Region of Interest and then crop out that part (square) and create a new image with it. I will be working with different images so the square won't always be at the same location in all images. So I will need to somehow detect the edges of the square. 
What are some pre-processing methods I can perform to achieve the result?
",99
15693938,17378072,2,"I shouldn't give full answers as JES is an application designed for students, but I think that three months later one can give a full working sample which can be used as reference for the others...
This should be close to what you attempted to do:
Note : your approach of a simple double loop over x and y was the right one.
Output (Painting by Antoni Tapies):
......From......
Here is a more detailed thread about greyscale.
",90
15693938,15693938,1,"My goal is to double the picture size then change the left half to grayscale, then change the green value of the top right half and the blue value of the bottom right half. I have values that I found in my textbook for the grayscale but im not sure if thats what I actually use. And I also am unsure if I program each of these different values using for loops or just something different 
So far my code is:
",85
15695846,15695918,2,"Your add function doesn't have anything that will increment self.size. So it's whatever you set it to in __init__, which is presumably 0.
So, when the list actually is empty, __len__ returns 0 because self.head is None.
And after you add an element, it still returns 0 because self.size is 0.
Also, your code has at least one other problem in it. Look at this:
Clearly, newNode.prev is going to end up pointing at itself, rather than the previous tail.
There are lots of things that can help judge the correctness of code—unit tests, code reviews by someone who didn't work on it, stepping through it with an interactive visualizer, formal proofs, etc.—but the number of hours you worked on it is not one of those things.
",149
15695846,15695846,1,"I cant seem to get my len function to work, ive been trying heaps of stuff but im a complete beginner so im pretty sure im missing something completely obvious. This is my code...
My thinking behind this is simple. If the head of the doubly linked list is None, then it must be empty so return 0, otherwise, just return the size of the list.
However, i get an asssertion error saying...
Any help is appreciated, thanks in advance.
EDIT: This is the code thats running my function...
EDIT2: this is my add function, im pretty sure its right, i spend 2 hours on it...
",124
15699551,15699587,2,"These are format specifiers. The %d specifier refers specifically to a decimal (base 10) integer. The %s specifier refers to a Python string.
",30
15699551,15699580,2,"It is used for string formatting. The random number will be converted to an integer (since %d is being used), then it will be converted to a string and inserted into the given string.
For example:
There are several other letters that correspond to different types. They can be found here (in the second table of that section).
",69
15699551,15699578,2,"%d stands for ""decimal"". ! is there with the rest of the string. %d means that a decimal number will be outputted there. Synonim for %d is also %i. same goes for instance with %s and strings
",48
15699551,15699551,1,"I was looking at this python tutorial and in the generator section they had something like this:
What I want to know is exactly what does that mean and is there any other letters because I've seen %s before. I've googled everything I could think of and nothing gave me an answer.
",58
15699566,15774990,2,"The cleanest solution would be inotify in many ways - this is more or less exactly what it's intended for, after all. If the log file was changing extremely rapidly then you could potentially risk being woken up almost constantly, which wouldn't necessarily be particularly efficient - however, you could always mitigate this by adding a short delay of your own after the inotify filehandle returns an event. In practice I doubt this would be an issue on most systems, but I thought it worth mentioning in case your system is very tight on CPU resources.
I can't see how the sleep() approach would miss file updates except in cases where the file is truncated or rotated (i.e. renamed and another file of the same name created). These are tricky cases to handle however you do things, and you can use tricks like periodically re-opening the file by name to check for rotation. Read the tail man page because it handles many such cases, and they're going to be quite common for log files in particular (log rotation being widely considered to be good practice).
The downside of sleep() is of course that you'd end up batching up your reads with delays in between, and also that you have the overhead of constantly waking up and polling the file even when it's not changing. If you did this, say, once per second, however, the overhead probably isn't noticeable on most systems.
I'd say inotify is the best choice unless you want to remain compatible, in which case the simple fallback using sleep() is still quite reasonable.
EDIT:
I just realised I forgot to mention - an easy way to check for a file being renamed is to perform an os.fstat(fd.fileno()) on your open filehandle and a os.stat() on the filename you opened and compare the results. If the os.stat() fails then the error will tell you if the file's been deleted, and if not then comparing the st_ino (the inode number) fields will tell you if the file's been deleted and then replaced with a new one of the same name.
Detecting truncation is harder - effectively your read pointer remains at the same offset in the file and reading will return nothing until the file content size gets back to where you were - then the file will read from that point as normal. If you call os.stat() frequently you could check for the file size going backwards - alternatively you could use fd.tell() to record your current position in the file and then perform an explicit seek to the end of the file and call fd.tell() again. If the value is lower, then the file's been truncated under you. This is a safe operation as long as you keep the original file position around because you can always seek back to it after the check.
Alternatively if you're using inotify anyway, you could just watch the parent directory for changes.
Note that files can be truncated to non-zero sizes, but I doubt that's likely to happen to a log file - the common cases will be being deleted and replaced, or truncated to zero. Also, I don't know how you'd detect the case that the file was truncated and then immediately filled back up to beyond your current position, except by remembering the most recent N characters and comparing them, but that's a pretty grotty thing to do. I think inotify will just tell you the file has been modified in that case.
",669
15699566,15699566,1,"I'm writing a Python script that needs to tail -f a logfile.
The operating system is RHEL, running Linux 2.6.18.
The normal approach I believe is to use an infinite loop with sleep, to continually poll the file.
However, since we're on Linux, I'm thinking I can also use something like pyinotify (https://github.com/seb-m/pyinotify) or Watchdog (https://github.com/gorakhargosh/watchdog) instead?
What are the pros/cons of the this?
I've heard that using sleep(), you can miss events, if the file is growing quickly - is that possible? I thought GNU tail uses sleep as well anyhow?
Cheers,
Victor
",122
15699666,15699666,1,"I'm using gevent to build a server which do some redis stuff and return the result to client. But the performance is bad. After some research I found that there is only one connection to redis. It looks like there is only one greenlet spawned. Here is my program:
Is there something wrong with my program? Why there is only one connection to redis?
",72
15699666,19117266,2,"Have a look at http://gehrcke.de/2013/01/highly-concurrent-connections-to-redis-with-gevent-and-redis-py/
I'm not 100% is your monkey-patching is doing the trick but I'd replace it with:
You could also go and create your own pool with gevent supported connection to redis...
",42
15699666,19120904,2,"What makes you think your only have one connection to redis? Actually my little test shows that your server is indeed opening lots of connections to redis.
To make the test more clear, I modified your print statement a bit:
Then run this script to make some requests:
And here's what I got:
It can be seen that at peek time you have 10 greenlets running simultaneously, waiting for sockets. Your code looks perfectly fine to me. Why 'the performance is bad' is another story. It could be your sorted set of 'online' is tooo large. Or more likely you are using a blocking client to test the server, in which case you'll see only one connection to redis.
",136
15699814,15704532,2,"For the template part everything looks absolutely ok and there should not be problem if what you do is what you showed.
Is Your list is dict() or actually list()? 
Because your problem is here:
This syntax will work only if List is dictionary.
In case of list you should write:
",60
15699814,15699814,1,"Here is what i want 
tmpl1.jinja
tmpl2.jinja
tmpl3.jinja
Basically i have a user block that exists across site with only the action(one or more link but with quiet a few html like image etc) changing. What can i do. 
Thanks 
",45
15699836,15699836,1,"I have seen a number of questions relating to writing files & creating new directories using Python and GAE, but a number of them conclude (not only on SO) by saying that Python cannot write files or create new directories. Yet these commands exist and plenty of other people seem to be writing files and opening directories no problem.
I'm trying to write to .txt files and create folders and getting the following errors:
Case #1:
produces ""IOError: [Errno 30] Read-only file system: 'aardvark.txt'"".  But i've checked and it's def-o not a read only file.
Case #2:
produces ""OSError: [Errno 38] Function not implemented: 'C:\project\folder'""
What am i missing?
",142
15699836,31651345,2,"AppEngine can now write to a local ""ephemeral"" disk storage when using Managed-VM which is not supported when using the sandbox method as specified on this documentation:
https://cloud.google.com/appengine/docs/managed-vms/tutorial/step3
",33
15699836,15699948,2,"Appengine does not support any write operations to the filesystem (amongst other restrictions).
The BlobStore does have a file like api, but you cannot rewrite/append to existing blob store entities.  The dev server also presents these restrictions to emulate production environment.
You should probably have a read of the some of the docs about appengine.
The overview doc https://developers.google.com/appengine/docs/python/overview explicitly states you can't write.  
",76
15699964,15699964,1,"I am facing a strange error while executing a python code. The following code is a small snippet of the python code I am executing:
While executing samplecode.py I am facing an error showing the following:
My logging.py which contains the code that needs to be imported while execution. The following is the code:
we can see that the class Dynamic is created yet the import error is thrown.
The strangest thing is I did few examples of importing files and it worked well. I have tried hard but still cannot figure it out. I would like to know why this error was thrown and why suddenly for this and not in previous samples?
",124
15699964,15700053,2,"Python already has a built-in logging module, which is being located before yours (you're appending your folder to the end of the path).
Rename your logging.py file to something else.
",36
15700109,15700109,1,"I am trying to parse a large xml file and print the tags to an output file. I am using minidom, my code is working fine for 30Mb files but for larger ones it is getting memory error. So I used bufferred reading the on file but now I am unable to get the desired output.
    XML File
Desired Output
Sony, Burger, Apple
Samsung, Pizza, HTC
Bravia, Pasta, BlackBerry  
When reading with buffer its giving me an output saying :-
Sony, Burger, Apple
Samsung,Piz
Bravia, Pasta, BlackBerry  
I tried with seek() but still I was unable to get the desired output. 
",121
15700109,15700565,2,"You're reading in 2048 byte at once, which put the reading cursor in the middle of a line. In the next read, the rest of that line is discard because it doesn't start with a tag.
Instead of rolling your own parser, consider using iterparse. An even faster version of iterparse is included with lxml
Here's an example
",67
15700109,15883599,2,"Thanks for your support and i have finally written my code and its working great here it is   
",18
15700128,15700128,1,"I have a dataframe and I grouped it by two keys df.groupby(['key1',key2']). For each key2 entry, how do I display the its percent of key1 values?
",38
15700128,15702317,2,"call groupby twice for ""k1"" and (""k1"", ""k2""), and then do div:
output:
",25
15700128,15703898,2,"Here's an alternative method using one groupby statement.
Group by k1, select column k2 and apply a lambda function.  The lambda gets frequency counts for each level of k2 within k1 and then we divide by the count of k1:
Performance:
HYRY's method:
My method:
",54
15701312,15701312,1,"I'm trying to get celery's official tutorial work but kept getting this error:
D:\test>celery -A tasks worker --loglevel=info
   -------------- celery@BLR122S v3.0.17 (Chiastic Slide)
  ---- **** -----
  --- * *  * -- [Configuration]
  -- * - **** --- . broker:      amqp://guest@localhost:5672//
  - ** ---------- . app:         tasks:0x2a76850
  - ** ---------- . concurrency: 2 (processes)
  - ** ---------- . events:      OFF (enable -E to monitor this worker)
  - ** ----------
  - * --- * --- [Queues]
  -- ******* ---- . celery:      exchange:celery(direct) binding:celery
  --- ***** -----
  [Tasks]
    . tasks.add
  [2013-03-29 17:50:52,533: WARNING/MainProcess] celery@BLR122S ready.
  [2013-03-29 17:50:52,568: INFO/MainProcess] consumer: Connected to amqp://guest@
  127.0.0.1:5672//.
  [2013-03-29 17:51:32,496: INFO/MainProcess] Got task from broker: tasks.add[8345
  9233-ce54-40ed-a2a8-ee0d60768006]
  [2013-03-29 17:51:32,562: ERROR/MainProcess] Task tasks.add[83459233-ce54-40ed-a
  2a8-ee0d60768006] raised exception: Task of kind 'tasks.add' is not registered,
  please make sure it's imported.
  Traceback (most recent call last):
File ""C:\Python27\lib\site-packages\billiard\pool.py"", line 293, in worker
result = (True, func(*args, **kwds))
      File ""C:\Python27\lib\site-packages\celery\task\trace.py"", line 320, in _fast_trace_task
      return _tasks[task].__trace__(uuid, args, kwargs, request)[0]
      File ""C:\Python27\lib\site-packages\celery\app\registry.py"", line 20, in __missing__
      raise self.NotRegistered(key)
      NotRegistered: 'tasks.add'
I installed celery==3.0.17 and rabbitMQ.
Then start celery by ""D:\test>celery -A tasks worker --loglevel=info""
tasks.add seems to be in [Tasks], but calling by:
got the failure above. Does anyone have the same problem?
Edit:
Here is my tasks.py copying from tutorial.
",371
15701312,15701595,2,"try to import tasks first, I recommend you implement your work in a interactive python environment, like a python IDE, and then you do this:
import tasks
before you write tasks.add
",35
15701356,15702117,2,"Ok, i just had to inherit the account_invoice class and define a new invoice_validate method
@user1576199 is this what you meant ?
",24
15701356,15701356,1,"I want to add a small process after an user validate an invoice in Openerp 7 (just some recording for the salesperson) but i dont know how to 'intercept' the action ?
",35
15705434,15705434,1,"Right now I'm having trouble with the code restarting. It restarts but it doesn't go back to the beginning. It just keeps asking me if I want to restart.
For example it says 
I type in Y and it keeps saying
Can anyone please fix it so that it will restart. - or tell me how to my code is below.
",68
15705434,15708619,2,"fun little game, I removed the second dealer for simplicity, but it should be easy enough to add back in.  I changed input to raw_input so you could get a string out of it without entering quotes.  touched up the logic a bit here and there, redid formating and added comments.
",57
15705439,41648497,2,"I found that using os.seteuid and os.setegid didn't actually drop the root privileges.  After calling them I was still able to do things that required root privileges.  The solution I found that worked was to use os.setresuid and os.setresgid instead:
",44
15705439,15705439,1,"In my Python script, I perform a few operations that need root privileges. I also create and write to files that I don't want to be owned exclusively by root but by the user who is running my script.
Usually, I run my script using sudo. Is there a way to do the above?
",61
15705439,15706858,2,"http://linux.die.net/man/8/sudo quote: 
The real and effective uid and gid are set to match those of the target user 
So, your only option of knowing which user to use is to read the target user from either a config file or a cmdline option, or someway of heuristical guessing.
A good idea is the so called rights shedding: Start with root privilegs, then do what you nedd them for. Then become a less privileged user. 
You would use the os module for that:
http://docs.python.org/2/library/os.html#os.setuid
",98
15705439,15707075,2,"You can switch between uid's using os.seteuid(). This differs from os.setuid() in that you can go back to getting root privileges when you need them.
For example, run the following as root:
This creates file1 and file3 as root, but file2 as the user with uid 501.
If you want to determine which user is calling your script, sudo sets two environment variables:
Respectively the username and the uid of the user who called sudo. So you could use int(os.environ['SUDO_UID']) to use with os.seteuid().
",108
15705491,15718449,2,"You are attempting to use both grid and pack for the same containing widget. You cannot do that. You either need to use grid for the text and scrollbars or use pack for the buttons.
",39
15705491,15705491,1,"the follwing code in a serparate file is working fine. It is creating a text area and adding a scrollbar to it.
But exactly same code is not woking when it was merged with other code (main.py)
and when i running main.py file from cmd prompt, it just hanging. what is going wrong here ?
",61
15705511,15707037,2,"you can modify query._whereclause directly, but I'd seek to find a way to not have this issue in the first place - whereever it is that the Query is generated should be factored out so that the non-whereclause version is made available.
",45
15705511,15705511,1,"If I've been given a Query object that I didn't construct, is there a way to directly modify its WHERE clause?  I'm really hoping to be able remove some AND statements or replace the whole FROM clause of a query instead of starting from scratch.  
I'm aware of the following methods to modify the SELECT clause:
Query.with_entities(), Query.add_entities(), Query.add_columns(), Query.select_from()
which I think will also modify the FROM.  And I see that I can view the WHERE clause with Query.whereclause, but the docs say that it's read-only.  
I realize I'm thinking in SQL terms, but I'm more familiar with those concepts than the ORM, at this point.  Any help is very appreciated.
",141
15705546,15724609,2,"Here's a way of doing it without collections.Counter, as requested in chat:
",15
15705546,15705622,2,"Use collections.Counter() to convert x and y to multi-sets, then subtract to see if all of y's letters can be found in x:
Subtracting multi-sets removes characters when their count falls to 0. If this results in an empty multi-set, it becomes False in a boolean context, like all 'empty' python types. not turns that into True if that is the case.
Demo:
",75
15705546,15705546,1,"For instance I have 
So I think I would need to use a while loop as a counter for each of the letter in y, and then I can use itetools to cycle through each of x, each cycle It would check to see if x == y, if it is it would remove it then check the next letter in o.
Is there a more simple way to do this?
",76
15705577,15705955,2,"Pass in your lists and the key that you want to check values on. 
",15
15705577,15705577,1,"I have a dataset like this:
I need to delete the first elements of each subview of the data as defined by the first column. So first I get all elements that have 0 in the first column, and delete the first row: [0,1]. Then I get the elements with 1 in the first column and delete the first row [1,5], next step I delete [2,8] and so on and so forth. In the end, I would like to have a dataset like this:
EDIT: Can this be done in numpy? My dataset is very large so for loops on all elements take at least 4 minutes to complete.
",127
15705577,15705717,2,"You want to use itertools.groupby() with a dash of itertools.islice() and itertools.chain:
The groupby() call groups the input list into chunks where the first item is the same (itemgetter(0) is the grouping key). 
The islice(group, 1, None) call turns the groups into iterables where the first element will be skipped.
The chain.from_iterable() call takes each islice() result and chains them together into a new iterable, which list() turns back into a list.
Demo:
",101
15705577,15705725,2,"
",0
15705577,15706171,2,"As requested, a numpy solution:
(above is edited to incorporate @Jaime's solution, here is my original masking solution for posterity's sake)
Interestingly, the mask seems to be faster:
",38
15705577,15705828,2,"My answer is :  
output is:
",7
15705630,15705630,1,"I hope I can find help for my question. I am searching for a solution for the following problem:
I have a dataFrame like:
My objective is to get the result rows whose count is max between the groups, like :
Somebody knows how can I do it in pandas or in python?
UPDATE
I didn't give more details for my question. For my problem, I want to group by ['Sp','Mt'].  Let take a second example like this :
For the above example, I want to get ALL the rows where count equals max in each group e.g :
",115
15705630,44960833,2,"Easy solution would be to apply : idxmax() function to get indices of rows with max values. 
This would filter out all the rows with max value in the group.
",34
15705630,40629420,2,"You can sort the dataFrame by count and then remove duplicates. I think it's easier:
",18
15705630,31185210,2,"For me, the easiest solution would be keep value when count is equal to the maximum. Therefore, the following one line command is enough : 
",28
15705630,21709413,2,"Having tried the solution suggested by Zelazny on a relatively large DataFrame (~400k rows) I found it to be very slow.  Here is an alternative that I found to run orders of magnitude faster on my data set.
",42
15705630,15705958,2,"
To get the indices of the original DF you can do:
Note that if you have multiple max values per group, all will be returned.
Update
On a hail mary chance that this is what the OP is requesting:
",43
15705719,15705719,1,"I installed lxml on a mac and trying to use it in my code and I get errors importing tostring and tounicode. Python cannot see it at all. Any ideas what I am doing wrong?
Here is the code that is causing problems - 
I get an unresolved import error 
Also my IDE (eclipse) is able to see the init.py file for lxml.etree module. Here is what it sees ---
Thanks for any help.
EDIT:
The only log I see is 
    Unresolved import: tostring
    Unresolved import: tounicode
And when I add the following line before the import of tostring it works no errors --
    import etree from lxml
Also to give you some more background on what I am trying to do. I got the readability code from here (https://github.com/buriy/python-readability) and trying to use it in my project. 
EDIT2: I fixed the problem but still do not understand why. I wanted to use the code from the readability project directly without installing the package using easy_install. The idea was step into the code so that I understand what its doing. But when I copy the code into my project I get the above mentioned error in the readability code. If I install the package using easy_install then all I can simply import the class exported by readability and use it.
So can some one tell me what the difference is between using the code directly and the package installed? What is a .egg file? How to create one?
",275
15705719,46108414,2,"In lxml's code, it dynamically load modules. That makes IDE fails to analyze reference as IDE just analyzes raw code.
",24
15705745,15787278,2,"This is based off of Oblivion's answer but I edited it so it worked for me.
I made a new listbox class that was based off of the original one but had a new function that I got from Oblivion's code. I then call that function and it makes the listbox an appropriate size.
",59
15705745,26504193,2,"Resetting the listbox width worked for me. I used the Oblivion's answer and noticed that only width is always zero.
I also recommend to reset root window geometry after reloading content of the list. If user manually extends the window the window would stop accommodate size of its content.
",54
15705745,15705745,1,"I am writing a program to run batch files for various servers and so far everything is going fine. I mean the programs works and uses a simple GUI and all is well. Apart from when I give it a slightly longer name to display in the listbox it clips the end off. The code that Tkinter uses is below.
So basically it all works perfectly fine, I'm not getting any errors but it is a bit annoying the way it doesn't properly fit and I can't scroll. I know I can add scroll bars and make the window re-sizable and make the listbox fit the space it's given, but I would like it to work without having to resize stuff. I know it's not that important but it's just one of those things that I would love to work but can't figure out :/ .
",162
15705745,15712127,2,"tkListAutoWidth.py shows one way to do it:
http://svn.effbot.org/public/stuff/sandbox/tkinter/
edit:
So you might have something along the lines of,
",23
16131208,16131255,2,"np.argsort(A) is sorting each row of A separately. For example,
Instead, you want to flatten your array into a 1-dimensional array of values, then argsort that. That can be done by setting the axis parameter to None (thanks to @Akavall for pointing this out):
Then use np.unravel_index to recover the associated index in A.
Note, for NumPy version 1.5.1 or older, np.unravel_index raises a ValueError if passed an array-like object for its first argument. In that case, you could use a list comprehension:
",101
16131208,16131208,1,"I'm aware of numpy.argsort(), but what it does is return indices of elements in an array that would be sorted along a certain axis.
What I need is to sort all the values in an N-dimensional array and have a linear list of tuples as as result.
Like this:
P.S. I don't even understand what the output of argsort is trying to tell me for this array.
",77
16301356,16301356,1,"I'm using Cygwin and Python2.7 and I have already installed some third party modules (PySide, PyGame) on my computer and when I run them using the normal GUI they work fine. But when I run python inside Cygwin it doesn't recognize the modules I already have installed. I'm using Windows 7 for this. Is there anyway I can get Cygwin to recognize these libraries? 
",74
16301356,16640744,2,"You're question is not clear (e.g. what do you mean by ""the normal GUI""), but it sounds like you installed Windows versions of Python and those extensions; those binaries won't work with the Cygwin version of Python.
",47
23500040,23500083,2,"You could use the built-in sum function:
The argument to sum is a generator expression, and the result is the sum of the values that are greater than x.
",32
23500040,23500040,1,"I was wondering what is a nice, elegant pythonic way to run a sumif on values of keys in a dictionary that adhere to a certain condition. For example, this dict:
lets say I want the sum of all the values of the keys, given that the value is >= a certain number x.
How would you go about it? Anonymous function maybe?
Thanks in advance for your help
",78
23500173,23501543,2,"The documentation says:
""Returns the approximate processor time used by the process since the beginning of an implementation-defined era related to the program's execution. To convert result value to seconds divide it by CLOCKS_PER_SEC.""
That's pretty vague. CLOCK_PER_SEC is set to 10^6 and the approximate stands for poor resolution, not that the current clocks tick over 1000 faster and the results are rounded. That might be not a very technical term, but it is appropriate. The actual resolution everywhere I tested was about 100Hz = 0,01s. It's been like that for years. Note date here http://www.guyrutenberg.com/2007/09/10/resolution-problems-in-clock/. 
Then the doc follows with: ""On POSIX-compatible systems, clock_gettime with clock id CLOCK_PROCESS_CPUTIME_ID offers better resolution.""
So:
It's CPU time only. But 2 threads = 2*CPU time. See the example on cppreference.
It is not suited for fine grain measurements at all, as explained above. You were on the verge of its accuracy.
IMO measuring wall-clock is the only sensible thing, but its a rather personal opinion. Especially with multithreaded applications and multiprocessing in general. Otherwise results of system+user should be similar anyways.
EDIT: At 3. This of course holds for computational tasks. If your process uses sleep or give up execution back to system, it might be more feasible measuring CPU time. Also regarding the comment that clock resolution is erm... bad. It is, but to be fair one could argue you should not measure such short computations. IMO its too bad, but if you measure times over few seconds I guess its fine. I would personally use others available tools.
",305
23500173,23500173,1,"I'm interested in comparing CPU times some code portions written C++ vs Python (running on Linux). Will the following methods produce a ""fair"" comparison between the two? 
Python
Using the resource module:
which allows for timing like so:
Then I test like:
C++
Using the ctime lib:
which yields 0.002.
Concerns:
I've read that C++ clock() measures CPU time which is what I'm after, but I can't seem to find if it includes both user and system times.
Results from C++ are much less precise. Why is that?
Overall fairness of comparison as mentioned.
Update
updated the c++ code as per David's suggestion in the comments:
Running:
So I suppose getrusage gets me a little bit better resolution, but I'm not sure how much I should read into it. Setting the optimization flag certainly makes a big difference.
",168
23500173,23504062,2,"
Setting the optimization flag certainly makes a big difference.
C++ is a language that begs to be compiled optimized, particularly so if the code in question uses containers and iterators from the C++ standard library. A simple ++iterator shrinks from a good-sized chain of function calls when compiled unoptimized to one or two assembly statement when optimization is enabled.
That said, I knew what the compiler would do to your test code. Any decent optimizing compiler will make that for (int i=0; i<N; i++) continue; loop vanish. It's the as-if rule at work. That loop does nothing, so the compiler is free to treat it as if it wasn't even there.
When I look at the CPU behavior of a suspect CPU hog, I write a simple driver (in a separate file) that calls the suspect function a number of times, sometimes a very large number of times. I compile the functionality to be tested with optimization enabled, but I compile the driver with optimization disabled. I don't want a too-smart optimizing compiler to see that those 100,000 calls to function_to_be_tested() can be pulled out of the loop and then further optimize the loop away.
There are a number of solid reasons for calling the test function a number of times between the single call to start timer and stop timer. This is why python has the timeit module.
",261
23500202,23500202,1,"I'm trying to find a blendshape deformer from a target mesh in the python maya api. I'm pretty sure I have to iterate through the dependency graph to get the blendshape.
This is what i'm trying:
Unfortunately I get no blendshape deformer.
The same example with maya.cmds:
Any help would be greatly appreciated!
",62
23500202,23501184,2,"you don't want the future flag if you're working from the deformable object:
To get the actual shapes, you'd add
",25
23500202,23502118,2,"So here's the solution I got working i believe:
The trick is that I needed to pass the shape node and to only use OpenMaya.MItDependencyGraph.kPlugLevel). In this example it finds the base object of the blendshape.
",41
23503326,23503414,2,"Seems you're using an incorrect import, try:
instead.
",12
23503326,23503326,1,"I have this code:
I am using Python 3.3 and latest version of Pillow. But when I run this code I get:
I uninstall PIL. Can somebody help me?
",34
23503486,23503575,2,"This should be a unicode string:
Edit: A simple python script as an example:
",17
23503486,23503486,1,"I have a list of surnames and it isn't formatted the right way. Every name is written in cAmEl sTyLe -.-
I'm trying to make it look more clean with title() method.
This one works fine. But when I have non-ascii letter in the name:
the letter that follows this non-ascii remains in Upper-case. Even if I change the string:
I still get the wrong result. 
Why does it behave this way? How can I make this string become 'Kröger'?
",94
23503486,23616980,2,"I finally found a way to do what I want. God bless generators:
",15
23503486,23503597,2,"You could decode UTF 8 before the title:
",9
23503667,23503667,1,"I can test the rank of a matrix using np.linalg.matrix_rank(A) . But how can I test if all the rows of A are orthogonal efficiently?
I could take all pairs of rows and compute the inner product between them but is there a better way?
My matrix has fewer rows than columns and the rows are not unit vectors.
",65
23503667,23504241,2,"This answer basically summarizes the approaches mentioned in the question and the comments, and adds some comparison/insights about them
Approach #1 -- checking all row-pairs
As you suggested, you can iterate over all row pairs, and compute the inner product. If A.shape==(N,M), i.e. you have N rows of size M each, you end up with a O(M*N^2) complexity.
Approach #2 -- matrix multiplication
As suggested in the comments by @JoeKington, you can compute the multiplication A.dot(A.T), and check all the non-diagonal elements. Depending on the algorithm used for matrix multiplication, this can be faster than the naive O(M*N^2) algorithm, but only asymptotically better. Unless your matrices are big, they would be slower.
The advantages of approach #1:
You can ""short circuit"" -- quit the check as soon as you find the first non-orthogonal pair
requires less memory. In #2, you create a temporary NxN matrix.
The advantages of approach #2:
The multiplication is fast, as it is implemented in the heavily-optimized linear-algebra library (BLAS of ATLAS). I believe those libraries choose the right algorithm to use according to input size (i.e. they won't use the fancy algorithms on small matrices, because they are slower for small matrices. There's a big constant hidden behind that O-notation).
less code to write
My bet is that for small matrices, approach #2 would prove faster due to the fact the LA libraries are heavily optimized, and despite the fact they compute the entire multiplication, even after processing the first pair of non-orthogonal rows.
",309
23503667,23512809,2,"It seems that this will do
",6
23503667,23552362,2,"Approach #3: Compute the QR decomposition of AT
In general, to find an orthogonal basis of the range space of some matrix X, one can compute the QR decomposition of this matrix (using Givens rotations or Householder reflectors). Q is an orthogonal matrix and R upper triangular. The columns of Q corresponding to non-zero diagonal entries of R form an orthonormal basis of the range space.
If the columns of X=AT, i.e., the rows of A, already are orthogonal, then the QR decomposition will necessarily have the R factor diagonal, where the diagonal entries are plus or minus the lengths of the columns of X resp. the rows of A.
Common folklore has it that this approach is numerically better behaved than the computation of the product A*AT=RT*R. This may only matter for larger matrices. The computation is not as straightforward as the matrix product, however, the amount of operations is of the same size.
",175
23503725,23503725,1,"I am new to Python, I have seen many examples but not a good summary of examples. Here are the 4 things that I want to get done.  Thanks for your help.
I wanted to use the following to access the dict elements:
How do I get the following output:
Case 1: (by key ascending)
Case 2: (by key descending) 
Case 3: (by value ascending)
Case 4:  (by value descending)
",88
23503725,23504182,2,"
",0
23503725,23503832,2,"See the documentation for sorted.
Ascending by key:
Descending by key:
Ascending by value:
Descending by value:
You could also use key=lambda x: x[1] instead of defining a get_key function if you wanted.
",43
23507320,23507320,1,"Say I have:
and I want 
How can this be achieved? the way I read paths is to go from 0 to 1, you need to visit 3. But to break that down, to go from 0 to 1. You need to go from 0 to 3 (0,3) and then from 3 to 1 (3, 1)
",67
23507320,23507328,2,"You can use zip and Explain Python's slice notation:
",11
23507365,23507365,1,"Why is it that I cannot access the attribute of an object as seen in the following example? (raises an error)
Error raised: AttributeError: 'X' object has no attribute 'variable'
Shouldn't variable be accessible to the instance of Y that is created when X is initialized?
",55
23507365,23513195,2,"You're instantiating before adding/initializing the variable attribute. It will have no knowledge of the variable unless again instantiated or swapped as the other answer mentioned.
",28
23507365,23507444,2,"Move self.variable = 5 to before self.y = Y(self). When Y is initialized, variable has yet to be set in the parent. Since it is not a class attribute (it is defined only for the instance) it does not exist until then.
",51
23507408,28179286,2,"It took me a long time to get this working for PHP, after a lot of communication with Google it was finally revealed to me that in your app.yaml file you need to have a line that reads:
In order for the pipeline to successfully pick up and deploy your git push (I use sourcetree, but command line git has the same end result) that line must be present.  If it's omitted or set to true the pipeline won't be able to deploy it.
I wanted to throw this answer on here in case anyone stumbled on this thread looking for help.  One of my projects has ""randomly broken"" and after 3 months of successfully using my release pipeline for multiple commits per day it suddenly no longer deploys when I push.  Ultimately giving the extremely helpful error message of ""Unable to get deployment status"" - and now none of my changes can be applied to the live site.  Copying the entire source code, changing the app name, and pushing to a new GAE project with release pipeline works fine, but I need the original site to start working again.
",210
23507408,26578198,2,"Google App Engine pipelines do not like .gitignore file. Try if it works without that file. It fixed the problem for me.
",25
23507408,23507408,1,"So since last week suddenly git push origin master doesn't work anymore to ""push to deploy"". It sure pushes the sources to remote repository at Google, and the code is there but it never deploys. Read about it here: GAE: Trouble with push to deploy
It seems things are changing over at Google and this week there is new stuff in the Google Developer Console, in the ""Cloud Development/Releases"" section; ""Configure Release Pipeline""
There are three settings: the pipeline name, pipeline tasks, and then an optional setting to have deploy notifications sent by email.
I just enter a random name like ""mydevpipeline"", select ""Deploy source only"", and check the email box. But I just get this error: ""Failed to create the pipeline."". I also tried unchecking the email box, still same error. Tried it over and over.
No where to go from there...
Anyone been able to create this pipeline and get it all working?
It seems that this pipeline configuration must go through in order for push to deploy from now. I haven't seen any news or notification about this change...
Fwiw, the documentation https://developers.google.com/appengine/docs/push-to-deploy states nothing about pipelines. It's just outdated I guess.
Update:
What do you know... I went on trying to configure this pipeline on the live GAE project (the one described above is the dev GAE project I'm using)... and it worked. I could configure a pipeline ok. After that, I could once more push-to-deploy, alas only on the live version so far. I might try creating a new dev project, it seems existing projects ""break"" from time to time... I have had similar problems before and creating a new project DOES solve things from time to time.....
",342
23507481,23507481,1,"Trying to install ez_setup.py on windows xp. Python2.7 is installed, and proper path variables are established.
I tried following this, but get the following error below:
",31
23507481,23513725,2,"You can try that same without the 'install', i have never used that install -command. Or one likely option is that you ez_setup.py broken or bad, i have been using this: https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py 
",39
23507481,23598378,2,"For anyone who happens upon this question: I used anaconda installer, and now everything works perfectly. I would avoid the path I took and go directly to anaconda.
",32
23507492,23507492,1,"So I have this line of code in views.py
I want to get the value from the html file and so that I can get a newpin depends on the users query and able to display the layer from the map..
just sample form above...
",46
23507492,23507634,2,"You can get the value from html form to view use request arg,
",14
23507666,23507666,1,"Given a Python list whose elements are either integers or lists of integers (only we don't know how deep the nesting goes), how can we find the sum of each individual integer within the list?
It's fairly straightforward to find the sum of a list whose nesting only goes one level deep, 
but what if the nesting goes two, three, or more levels deep?
I know the best approach is recursion, but this is a challenge wherein I have to do it without recursion.
Please help!!
",100
23507666,23507786,2,"
Basically you iterate over the outer list and unpack the first level of any inner lists until there are no inner lists left
",23
23507666,23507904,2,"Here is one solution:
Basically, it pops first element of input list. If it's Int, add into sum; if it's List, extend to the end of input list
",36
23507666,23508122,2,"One mostly-readable (and presumably performant, though I haven't tested it) way to iteratively flatten a list:
Uses deques to avoid nasty O(n**2) behavior from list.pop(0).  You can get equivalent results by making a reversed copy and popping from the end, but I find the code a little easier to follow if you just use deques and popleft.  On a similar note, it's a line or two less code if you want to mutate the list in-place but way slower (for the same reason; popping from the head of the list is O(n) since every element in the underlying array has to be shifted).
After it's flat, summing is (hopefully) trivial :-)
",140
23507731,23507848,2,"Create a simple algorithm that would generate those characters, by using the math patterns that would be necessary to create those characters. For that example, notice you can use a pattern for the digits you gave.
For example (say the user input is i):
The first line XXXXXX is six characters. The example with an input of i=8 is eight characters. So why not output the the character X for i times?
print ""X""*i
That's your first line. Now continue with the rest of the lines.
The second line is X    X. There are four spaces for an input of i=6, six spaces for an input of i=8. So there's a pattern there, clearly. So why output an X, then i-2 spaces, then another X?
print ""X"" + "" ""*(i-2) + ""X""
Complete the third line the same way:
print ""X""*(i-2)/2 + "" ""*(i/2-2) + ""X""*(i/2)/2
You should be able to complete the rest from here, to reverse the character and try out various inputs. Think about what happens for other integer values that aren't i=6, 8, 10, 12, etc. You can use round, floor, or a variation on these patterns in order to handle inputs such as i=7 and 9.
",264
23507731,23507731,1,"I have a question regarding Python ASCII Art.
How would I craft something like this in Python 3, using input from the user?
Could someone point me in the right direction. I just started learning python at home. This is one of the questions in the bonus quiz section of a book I purchased from Amazon. 
",62
23507779,23511975,2,"I assume your purpose is to create a program that starts two threads, one (client thread) receives keyboard input and sends to the other (server thread), the server thread prints out everything it received.
Based on my assumption, you first need to start a ServerThread listen to a port (it's not like what your 'ClientThread' did). Here's an example:
And these are the output:
You may check here for more details about Python socket: https://docs.python.org/2/library/socket.html?#example
",97
23507779,23507779,1,"so right now in order to receive your message you need to receive one 
my teachers instructions are (in the main)""Modify the loop so that it only listens for keyboard input and then sends it to the server.""
I did the rest but don't understand this, ... help?
",57
23507883,23507935,2,"The most straightforward approach would be to get the first item from mock.call_args_list and check if it is called with 1:
call_args_list
This is a list of all the calls made to the mock object in sequence
  (so the length of the list is the number of times it has been called).
where call is imported from mock: from mock import call.
Also, mock_calls would work in place of call_args_list too.
Another option would be to use assert_any_call():
",89
23507883,23507883,1,"eg in t.py
then:
I get:
",8
23508577,23509541,2,"xlabel and ylabel are on my plot when I run your example, just a bit outside the display depending on how it is sized.
Try adding the line:
or simply stretching the display window.
",38
23508577,23508577,1,"How is it possible to make xlabel and ylabel visible in the following plot?
",15
23508582,23508582,1,"With a row of QListWidgets I wonder if there is a simple way to label each so the user would be able to say which List is which.
Here is a dialog's screenshot a code posted below results.
I avoid using any widgets outside of QListWidgets(). Ideal solution would be solved utilizing QListWidgets itself. It would be great if there is a way to place a text line-label similar to those available for QGroupBox with .setTitle('myString'). At very least an ability to place a label as a first list item would be sufficient too... 
",107
23508582,23509644,2,"Here is my attempt to achieve it without abandoning QListWidget()... utilizing layout's .insertLayout() method to attach QLabel without losing GUI space usually taken by QGroupBox()...
",34
23508582,23509288,2,"Unfortunately there's no (Qt-native) way to label a QListView (on which QListWidget is based). If your really don't want additional widgets, I would instead use a single-column QTableWidget, and put the list title in the column header. QTableWidget and QListWidget work pretty similarly, so this probably won't break too much of your existing code.
An example based on yours:
",73
23508596,23508834,2,"interesting! Let's take a look.
The design is pretty simple. Read the file into a dictionary and perform manipulation on the dict, then write out the files.
mapping is now {""a"":1, ""b"":3, ""c"":5, ""d"":-4} Let's read in our files.
Sheesh that's probably the ugliest code I've ever written, but values is now {""sample1"": {""a"":12, ""b"":10, ""c"":4, ""d"":6}, ""sample2"": ...}
Now for the manipulation. This is actually easy. Let's tack file write onto it, since this step is rather elementary
",138
23508596,23508596,1,"I have two text files:
file1.txt:
and file2.txt:
what I want to do is to subtract a value for a given letter in file1.txt from the corresponding letter in all the samples in file2.txt and create multiple files so the output looks like:
First file for sample1, sample1.txt
and then separate file for sample2, sample2.txt:
and the same for sample3.
I tried looping over the file2.txt, but as my original file2.txt has over 1000 samples it takes a long time, is there a quicker pythonic way to do so?
Cheers,
Kate
",103
23508732,23521792,2,"You could this something like this :
And this is the content of desired xml file :
Is very important include the following line # -*- coding: utf-8 -*- line above in the beginning of your file to avoid problems  with the accent , see SyntaxError: Non-ASCII character '\xa3' in file when function returns '£' for more details.
",63
23508732,23509208,2,"Actually i used both beautiful soup and element tree(for xml parsing)
fetch all elements in <span> 
",21
23508732,23508732,1,"I am trying to get the deli title, and then under the deli title get the two menu items Made to Order Deli Core and Turkey Chipotle Petite Wrap? I'm using beautiful soup 4 to do this and its not working. And the same is true for the entree times?
or if I could get it into a XML format like this:
Thank you very much in advance, I really appreciate you taking the time to help me.
",86
23512010,23522076,2,"I use pyinstaller to create stand alone exe for windows.  http://www.pyinstaller.org/.
",15
23512010,23512010,1,"I have been developing an application using Python 3.3 and PyQt4 and I would like to be able to distribute it as a standalone app. My development environment is OS X and I have been able to create a standalone OS X app using cx_Freeze and py2app.
My question is, how would I go about creating an executable file for windows, considering I do not have access to a Windows operating system for development? 
I have tried using cx_Freeze's build_exe on OS X and running it on windows but it will not run. 
Thanks in advance :)
",105
23512030,23520198,2,"Have you considered just creating the certificate for the IP address? That wouldn't be much more fragile (probably less fragile, actually) than having to manually add the domain name to hosts files. See https://stackoverflow.com/a/11710762/138772 and https://stackoverflow.com/a/8444863/372643 for more info on that.
An alternative, but one probably requiring more work, would be to include a local DNS server in your app that redirects the domain name to your IP address. I can't really say what the best one to use for that would be, though.
",101
23512030,23512030,1,"I am using windows 7 and python 2.7 I created local https server with redirect url to server as its IP address. I created cert file for https using openssl.
Then I mapped my local system IP(172.16.17.84) to myapp.nobies.in in hosts file of windows.
So my server redirect url becomes https://myapp.nobies.in:443.
By doing this IP mapping in host file, I am not getting SSL error. 
But, I want to distribute my app to others, so, writing in host file through python code is not desirable, as it needs administrative privileges.
So, is there any way to assign/map this IP with hostname instead of making an entry in hosts file.
",127
23512095,23512095,1,"Hi i am new to GAE and python. I am trying to create small web application in GAE using python. I intend to use Unirest module in application.
I followed instruction in this post and got following error. 
no module named poster.encode
How can i install unirest successfully in GAE  ?
Thanks
",56
23512095,23513568,2,"Add import sys to your main.py and add these lines.
Create the lib folder in the root of your project and copy the unirest folder in lib folder.
You can now import unirest and use your library.
But, why not use the standard method and library provide by Google App Engine ?
",56
23512160,23512160,1,"I am trying to pass class element to method. Element is formed dynamically inserting current time in it. Mine class looks something like this:
So, I create object of this class and now I want to insert it as method argument:
When I run this script, I get this type of error:
TypeError: can only concatenate list (not ""str"") to list
So, does anyone knows where am I getting it wrong?
",85
23512160,23512230,2,"givenName seems to be a list. You can append another list like this:     
",15
23512160,23512250,2,"contact.givenName is a list, and extra_text is a string. in python you can add string to string, or list to list. you can't add string to list.
if you'd like to add string to a list, use list.append method.
",48
23512276,23512276,1,"I want to find out if there is a way to count the number of iterations that have occurred with the code below:
",24
23512276,23512299,2,"You can do that using enumerate:
Here i will be the number of iterations that you have run. More correctly, i will be the index of line1 and line2 in the zipped list which for your purpose is essentially the same. Note however that on the first iteration, i will be 0 not 1. More generally, on the nth iteration, the value of i will be n-1
",75
23512339,23512339,1,"When using the drop_duplicates() method I reduce duplicates but also merge all NaNs into one entry. How can I drop duplicates while preserving rows with an empty entry (like np.nan, None or  '')?
",40
23512339,23512486,2,"Well, one workaround that is not really beautiful is to first save the NaN and put them back in:
",21
23512339,35860429,2,"Try
The result is :
",5
23512452,23512759,2,"You're over-thinking this and confusing yourself. This appears to be a representation of a directed graph: it looks like edges is a list of Edges, and each Edge has a headNodeId and a tailNodeId. Each of those IDs refers to a Node in the nodes list.
So all that's happening when you say nodes[ edges[-1].headNodeId ] is ""give me the item from 'nodes' whose index is the head node ID of the last item in 'edges'"". Then, the code is simply calling addInEdge on that Node.
A much more verbose version of this would be:
",115
23512452,23512452,1,"I came across these two lines while reading a file. I don't quite understand what it means exactly. Here nodes, edges, headnodeId, tailnodeId are lists. I really appreciate your help
",37
23512469,23512469,1,"Consider the following minimum working example:
When the resulting svg is viewed graphically, the newline has been transformed into a single space, because the XML does not respect the newline. 
How can I get a newline to display in the SVG?
",46
23512469,23512836,2,"Section 10.1 of the SVG spec gives three options:
Each ‘text’ element causes a single string of text to be rendered. SVG
  performs no automatic line breaking or word wrapping. To achieve the
  effect of multiple lines of text, use one of the following methods:
The author or authoring package needs to pre-compute the line breaks and use multiple ‘text’ elements (one for each line of text).
The author or authoring package needs to pre-compute the line breaks and use a single ‘text’ element with one or more ‘tspan’ child
  elements with appropriate values for attributes ‘x’, ‘y’, ‘dx’ and
  ‘dy’ to set new start positions for those characters which start new
  lines. (This approach allows user text selection across multiple lines
  of text -- see Text selection and clipboard operations.)
Express the text to be rendered in another XML namespace such as XHTML embedded inline within a ‘foreignObject’ element. (Note:
  the exact semantics of this approach are not completely defined at
  this time.)
",201
23512479,23512479,1,"I'm trying to write an alembic migration to add a datetime column to a table. I want all existing rows to have a default time of right now and future rows to default to the time they were created.  I've tried server_default='now()', which sets all existing rows to right now, but new rows seem to get this same time.  How do I set the default so new rows get the current time?
",83
23512479,24624328,2,"The issue is that the default is set to the result of now(), not the execution of it, so the default will be the exact time it was set, rather than the current time at insert.  Use sa.func.current_timestamp() to set it to that function, rather than the result of that function.
",61
23512509,23533589,2,"that would work:
otherwise, you can set x and y in ABC (by doing self.x = ...) and then access them by c.x
",27
23512509,23512509,1,"I am developing a plugin in qgis. I have one interface (MMMM.py) with several buttons and one of them opens a new interface (ABC.py) where I introduce values. My objective is to read these values in the main interface (the first one). So I have a script to each interface but when I import the variables, I have several errors. I have troubles to import these variables.
second script named ABC.py
first (main) script named MMMM.py
I tried some ways to import the values x and y to main interface but I have always errors. I am working in qgis.
What I am doing wrong?
",122
23512525,23512525,1,"I am trying to pass class element to method. Element is formed dynamically inserting current time in it. Mine class looks something like this:
So, I create object of this class and now I want to insert it as method argument:
When I run this script, I get this type of error:
TypeError: list indices must be integers, not str
So, does anyone knows where am I getting it wrong?
",81
23512525,23512551,2,"You have a list of one dictionary [{}] instead of {}. The following will work:
Alternatively, you could change your self['tel'] to a dictionary instead of a list of a dictionary. Here is what it would look like:
Then, your original new_contact.insert_phone(contact.tel['value']) would work
",65
23513305,23513305,1,"I have made an egg from django (1.6) project (lets say pialon) with application (web_interface_app).
And I have Scrapy project, with virtualenv. I install django app via pip install /path/to/egg.tar.gz
All I want to do is - to use Django ORM in Scrapy pipeline. Basically, I do something like this:
pipelines.py:
But this thorws me an error:
Any ideas how to fix this error?
",79
23513305,23513720,2,"If you want to use Django's ORM without using the full Django suite, then you need to configure it before you run.
You can create a django settings file and point the DJANGO_SETTINGS_MODULE environment variable to it, or you can call settings.configure.
The documentation explains this here. It's pretty straightforward:
Note that if you're only interested in default settings, you probably don't actually need to pass any settings in there.
",82
23516515,23516515,1,"This question regards a homework assignment I'm doing for my programming class. I'm a total beginner when it comes to programming and up until now I've been fine but, in this instance, I'm lost. So, while answering this question, please assume that I'm clueless/stupid as hell because that's how  I feel right now lol.
So I'm making a game of ""In-Between"" and I'm having an issue.
Example of what happens:
Would you like to play in-between [y|n]? y
Die 1: 1   Die 2: 1
Number of chips: 100
  Place your bet: 50
Even-steven!
Even-steven! Higher or lower [h|l]? h
Die 3: 10
* You win! *
Die 3: 10
* Sorry - You lose! *
You now have 100 chips!
  Would you like to play in-between [y|n]? 
Here's my code:
I'm sorry that it's really long but I'm just so clueless r/n and would really appreciate ANY help you can give. Allow me to reiterate that I'm very new to this and I don't understand much programming terminology at all so please give me the simplest answers you can. Thanks so much!!!
",233
23516515,23516788,2,"A common pattern with these kind of restarts is to assume that the player wants to play the first time and then ask if he/she wants to continue before ending.
Hopefully that should show you how to get back on track.
Edit: sorry I saw now that this functionality was already in place. The issue you are having is that you get stuck in loops when you look at what value guess is, instead of looping you should use a if statement when checking what the guess is.
",94
23516520,23516633,2,"You need to find all script tags that apply; you only looked for the first. Use soup.find_all():
This finds all <script> tags that do not have a src attribute.
",37
23516520,23516520,1,"The following is the a simple BeautifulSoup code which has the two internal JavaScript( dont blame about the JavaScript it is just for testing purpose).
when I ran this code it remove only one <script>...</script> from the document(Dom tree) but it don't remove all other script tags. How can we remove all the <script>, <style>(internal and inline) tags which are existing in the document 
",86
23516520,23544567,2,"
another alternative, you can replace 'script' with ['script', 'style'] to get rid of styles as well.
",24
23516527,23516595,2,"__EVENTVALIDATION is probably not static, you need to load the login page in python, get the __EVENTVALIDATION field and then do the login.
Something like this should work:
You need the requests module and beautifulsoup4. Or you can just rewrite it to not use libraries.
Edit:
You probably need __VIEWSTATE as a POST value.
",62
23516527,23516527,1,"I have searched all over the Internet, looking at many examples and have tried every one I've found, yet none of them are working for me, so please don't think this is a duplicate - I need help with my specific case.
I'm trying to log into a website using Python (in this instance I'm trying with v2.7 but am not opposed to using a more recent version, it's just I've been able to find the most info on 2.7).
I need to fill out a short form, consisting simply of a username and password.
The form of the webpage I need to fill out and log in to is as follows (it's messy, I know):
From searching around online, the best Python code I have found to fill out this form and log into the website is as follows:
Note: This is not my code, I got it from this question/example, where many people have said they've found it to work well.
Unfortunately, this is not working for me and I'm unable to figure out why. If someone could please please please look over the code and tell me how I could improve it so as it works as it should. It would be so greatly appreciated!
Thanks in advance for all help I receive :)
",251
23516574,23559559,2,"Note: I'm making assumptions here since I'm unfamiliar with .H5 files and the Python code the accesses them.
I think that what is happening is that when you ""load"" the array, you're not actually loading an array. Instead, I think that an object is constructed on top of the file. It probably reads in dimensions and information related to how the file is organized, but it doesn't read the whole file.
That object mimicks an array so good that when you later on perform the slice operation, the normal Python slice operation can be executed, but at this point the actual data is being read. That's why the slice takes so long time compared to ""loading"" all the data.
I arrive at this conclusion because of the following.
If you're reading 75 frames of 3584x3584 pixels, I'm assuming they're uncompressed (H5 seems to be just raw dumps of data), and in that case, 75 * 3.584 * 3.584 = 963.379.200, this is around 918MB of data. Couple that with you ""reading"" this in 180ms, we get this calculation:
Note, this number is for 1-byte pixels, which is also unlikely.
This speed thus seems highly unlikely, as even the best SSDs today reach way below 1GB/sec.
It seems much more plausible that an object is just constructed on top of the file and the slice operation incurs the cost of reading at least 1 frame worth of data.
If we divide the speed by 75 to get per-frame speed, we get 68MB/sec speed for 1-byte pixels, and with 24 or 32-bit pixels we get up to 270MB/sec reading speeds. Much more plausible.
",317
23516574,23516574,1,"I am working with .h5 files with little experience.
In a script I wrote I load in data from an .h5 file. The shape of the resulting array is: [3584, 3584, 75]. Here the values 3584 denotes the number of pixels, and 75 denotes the number of time frames. Loading the data and printing the shape takes 180 ms. I obtain this time using os.times().
If I now want to look at the data at a specific time frame I use the following piece of code:
The slicing takes up a lot of time (1.76 s). I understand that my 2D array is huge but at some point I would like to loop over time which will take very long as I'm performing this slice within the for loop.
Is there a more effective/less time consuming way of slicing the time frames or handling this type of data?
Thank you!
",172
23516649,23516666,2,"You should do it as:
The string.join(iterable) returns a string of all the elements in the list separated by the specified string, in this case ' ' (a whitespace).
",37
23516649,23516649,1,"My apologies if this has been answered - I suspect it's very simple - but I can't see how to do it.
It's easier to demonstrate what I want to do.
I want the output to be 
and I get (of course)
",49
23516664,23516748,2,"
yields
Alternatively, you could escape the text with saxutils.escape:
yields
",12
23516664,23516664,1,"I'm using python version 2.7.3.
test.txt:
Result:
As you can see, the parser must have changed the &lt;'s to <'s etc.
What I'd like to see:
The tag &lt;StackOverflow&gt; is good to bring up at parties.
Untouched, raw text. Sometimes I really like it raw. Uncooked.
I'd like to use this text as-is for display within HTML, therefore I don't want an XML parser to mess with it.
Do I have to re-escape each string or can there be another way?
",108
23516680,23517776,2,"You can generate the truth table using a powerset,
If you want to  make a dict of the data, change yield combo to yield tuple(combo)
Then you can store key value pairings  like:
If you want the output sorted you can use sorted() which makes a copy of the list and returns a list:
Or you can use the list method sort() which sorts the list  in place:
",80
23516680,23517153,2,"You can use itertools.product() to generate the truth table and then depending on the logical operation, determine the probability. I don't know which logical operation you would like to use so let's just create a dictionary each row:
For calculating the probability, you'll probably need a separate function to do that. For example a logical disjunction for two keys with 0 and 1 being the values is basically equivalent to max().
",84
23516680,23516680,1,"I have this list and number:
The outcome that I need for my table is:
How can I build this truth table (there can be more vairables, not only 3) and assign a number to that row's probability?
I need to build it with a dictionary, I tried with some list comprehension but I don't know how to generate dynamically the truth table, considering that there can be more/less than 3 variables.
EDIT: to be more clear my goal is to have a dictionary like this:
and I need to insert all these dictionaries into a list to represent the structure of a table, is it clearer now?
Thank you very much
",127
23516703,23516703,1,"I doing a simple python GUI using tkinter to do screen recording.Basically, I am using ffmpeg commands at the backend with tkinter as the front end triggering the ffmpeg commands.There is something that I stuck with.I dont know why my time is unable to trigger off if I program in this way.
The code below is basically the recording method.You will notice that I am actually trying to update my tkinter GUI in the while loop.This method is actually in my class named Gui_Rec() which contains other methods I need for my screen recording program.
other method in the class Gui_Rec() then this below
Strangely, if I don't put the above section of code in the the def main(), the GUI will be update with the duration of the time running when rec button is pressed.I don't really know how to go about solving this.Tried putting it in another thread yet it doesn't  work as well.Thank you everyone for your help.
",176
23516703,23517360,2,"The while loop is creating a conflict with Tkinter's mainloop. Threading or multiprocessing are solutions, but I'd recommend looking into Tkinter's after() method. Here's a simplified example of how to handle a timer using after:
",45
23516828,23516828,1,"I am having some trouble attempting to manipulate a CSV file and appending the results to a new column.
Essentially I have a csv file (delimited ;) with 5 columns currently (of Cartesian coords [X, Y] and components [dX, dY], and magnitude/ length). I wish to add the result of some equations, which differ depending on the value of my Cartesian components, to a 6th column in this csv file (the angle).
Thus far my code is this (the maths is correct [hopefully], it's just the appending that I'm having trouble with):
So essentially, I want to add the angle variable to a 6th column, for the correct line of my csv file.
I tried to create a new list and append (e.g.):
However, as you may have guessed I ended up with a line like this:
Thanks for your help in advance.
",179
23516828,23516876,2,"col is a list itself, so you'd extend angles:
where list.extend() copies over the elements into the angles list, rather than add a single reference to the col list object.
If all you do is produce a new row with one value added, just re-use col and append to it directly:
and write that to your output CSV file.
col is misnamed, really, I'd call it row instead.
",82
23516828,23516935,2,"since col is list . u can just copy list items to angles and append it
",16
23516828,23519668,2,"This answer is too late as a solution has already been accepted, but the simplest way to solve the  problem is to write the newly-constructed row directly to the output csv file without creating an intermediate list.
You could write something like:
",45
23516914,23516914,1,"I have a vector D of length N and a matrix A of shape N*M. Vector D has some zero elements. I'm doing this operation:
However I'm getting a division by zero error because of some elements in D that are zero. What I need is to put zero when there's a division by zero instead of raising an error. How to do this?
E.g. my try:
",77
23516914,23518410,2,"Why not use try-catch block? Something like
try:
    some_var = A/D
except ZeroDivisionError:
    some_var = 0
",19
23516914,23519204,2,"You could use a masked array for D, like:
and when you perform the calculations with the masked array only the non-masked values will be considered.
",29
23518088,23518088,1,"
Hi code newbie here,
Basically what I need to do is read  data from a certain file. I then need to remove the characters such as ("" ; : . , \n ' )  but only if they are at the start or end of the word. Currently my program removes every instance of these characters. For example I want to be able to turn ""cars"" into cars but ca""rs would stay as ca""rs
The next part of the program involves only selecting the words which have an alphanumeric character in every position.This part works perfectly.
Any help with this would be greatly appreciated.
",117
23518088,23518157,2,"str.strip does what you want:
There are also str.lstrip and str.rstrip if you only want to take off the left or right, respectively.  
",26
23521273,23521273,1,"I have read and searched all stack overflow .. I also found JPype class not found but it didn't help me although it is solved! I have the same problem ! I am using Mac , python 2.7.6
My both python code and A.java are on desktop. But I keep receiving this error :
Traceback (most recent call last):   File
  ""/Users/jeren/Desktop/aa.py"", line 13, in 
      A = jpype.JClass(""A"")   File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/jpype/_jclass.py"",
  line 54, in JClass
      raise _RUNTIMEEXCEPTION.PYEXC(""Class %s not found"" % name) java.lang.ExceptionPyRaisable: java.lang.Exception: Class A not found
aa.py :
    import jpype
A.java :
My mac , java and python are all 64bit ! where the problem can be? 
",135
23521273,23608516,2,"everything was ok just needed to add a 'public' to the beginning of class A:
",17
23521345,23521467,2,"Specify the branch, commit hash, or tag name after an @ at the end of the url:
This will install the version tagged with 1.7b3.
Reference: https://pip.pypa.io/en/latest/reference/pip_install.html#git
",36
23521345,23521345,1,"I want to install Django 1.7 via pip. It is currently a development version, so not in pips repositories. 
So I have installed packages from github before using:
Now looking at github, I get the clone url on the django page:
But this mentions nothing of the branch. How do I specify that I want version 1.7? 
Is it somewhere obvious on the github page? 
",74
23521361,35158283,2,"The simpliest way for python2 is to use the repr():
For python3 use str.encode() to get the bytes type.
",25
23521361,23521361,1,"I know this may sounds like a duplicate question, but that's because I don't know how to describe this question properly. 
For some reason I got a bunch of unicode string like this:
As you can see, it's actually bytes representation of a Chinese character, encoding in gbk
u'岁' is what I need, but I don't know how to convert u'\xcb\xea' to b'\xcb\xea'.
Any suggestions?
",80
23521361,23521424,2,"It's not really a bytes representation, it's still unicode codepoints. They are the wrong codepoints, because it was decoded from bytes as if it was encoded to Latin-1.
Encode to Latin 1 (whose codepoints map one-on-one to bytes), then decode as GBK:
Demo:
",54
23521463,23737310,2,"It looks like you are using PyMC2, and as far as I know, you must use some Python approach to parallel computation, like IPython.parallel.  There are many ways to do this, but all the ones I know are a little bit complicated.  Here is an example of one, which uses PyMC2, IPCluster, and Wakari.
In PyMC3, parallel sampling is implemented in the psample method, but your reference code will need to be updated to the PyMC3 format:
",90
23521463,26410417,2,"PYMC3 has merged the psample into sample.
To run in parallel set the parameter njobs > 1.
The usage for the pymc.sample function is:
sample(draws, step, start=None, trace=None, chain=0, njobs=1, tune=None,
           progressbar=True, model=None, random_seed=None)
Note if you set njobs=None, it will default to Number of CPUs - 2.
I hope this helps.
",70
23521463,23521463,1,"Could someone give some general instructions on how one can parallelize the PyMC MCMC code.  I am trying to run LASSO regression following the example given here. I read somewhere that parallel sampling is done by default, but do I still need to use something like Parallel Python to get it to work?
Here is some reference code that I would like to be able to parallelize on my machine.
",75
23521511,23531250,2,"I guess anther way, possibly faster, to achieve this is 
1) Use dict comprehension to get desired dict (i.e., taking 2nd col of each array)
2) Then use pd.DataFrame to create an instance directly from the dict without loop over each col and concat.
Assuming your mat looks like this (you can ignore this since your mat is loaded from file):
Then you can do:
",77
23521511,31097813,2,"Here is how to create a DataFrame where each series is a row.
For a single Series (resulting in a single-row DataFrame):
For multiple series with identical indices:
For multiple series with possibly different indices:
To create a DataFrame where each series is a column, see the answers by others. Alternatively, one can create a  DataFrame where each series is a row, as above, and then use df.transpose(). However, the latter approach is inefficient if the columns have different data types.
",97
23521511,23521511,1,"My current code is shown below - I'm importing a MAT file and trying to create a DataFrame from variables within it:
So within the loop I can create a series of each variable (they're arrays with two columns - so the values I need are in column 2)
My question is how do I append the series to the dataframe? I've looked through the documentation and none of the examples seem to fit what I'm trying to do.
Best Regards,
Ben
",92
23521511,23522030,2,"No need to initialize an empty DataFrame (you weren't even doing that, you'd need pd.DataFrame() with the parens). Instead make a list of Series and concat those together with df = pd.concat(series, axis=1)
Something like:
",48
23521600,23521600,1,"I have two classes that loosely take the form below:
When I try to invoke the bar method on an instance of Bar, it fails.
Result:
My understanding is that this code should work based on two other questions, why doesn't it?
",49
23521600,23521763,2,"Simple. __foo contains 2 underscores in the beginning, so it's assumed to be class-private method and it's transformed into _Classname__method. 
When you request access to the attribute named as such on Bar object it asks Bar class if it has this method (not Foo class), so self.__foo is always the same as self._Bar__foo.
From the documentation:
When an identifier that textually occurs in a class definition begins
  with two or more underscore characters and does not end in two or more
  underscores, it is considered a private name of that class. Private
  names are transformed to a longer form before code is generated for
  them. The transformation inserts the class name, with leading
  underscores removed and a single underscore inserted, in front of the
  name. For example, the identifier __spam occurring in a class named
  Ham will be transformed to _Ham__spam.
If you modify your code slightly
assert statements will not cause any AssertionErrors.
Add __getattribute__ method to Bar class to capture all requests to Bar objects:
There will be 3 lines (apart from AttributeError) in the output:
As you can see, if attribute you are requesting has 2 leading underscores, Python is renaming it on the fly.
",224
23521652,23522628,2,"Make sure, action point to proper url
I think you render the form with wrong action for submitting the form.
Your version is using action="""" and I guess, it shall be action=""/search""
So your template shall be changed like:
Do not redirect out of your result
Your existing code is processing POST, but within first loop it ends up returning with redirect
Do render your template for final report
Your code does not show in POST branch any attempt to render what you have found in the database.
Instead of print r[0], r[1]... you shall call render_template()
Something like this
",121
23521652,23521652,1,"I want to make some kind of search engine for student's information by entering their first name in html input field, but I have some troubles with my code. I am using Flask with Python though. 
Here is my project.py code:
Here is my search.html code:
When I hit Search button nothing happens, I checked database it has some data in it so it is not empty, I can't find where am I making a mistake, please help?
",89
23521847,23521890,2,"The error message in the question does not come from the subprocess. It was generated before the subprocess execution. You cannot capture that error using stderr option.
Make sure there's ding program in the path.
",41
23521847,23521847,1,"Based on the answer provided here, I wanted to save the error as a string:
However, it seems that redirecting stderr is not working:
What is the correct way to capture the error as a string?
",41
23522570,23523042,2,"To answer this specific question:
How do I take object-based inputs and turn them into passable
  variables?
You access the controls and call the appropriate methods:
To pass them to your resize, function, just pass those values to your resize_file function.
You're asking too many questions here TBH.  You should start with one, and create others as you progress.
What the heck, I have time to kill before lunch!  I'll give you a few pointers.
Let's look at this function:
Looking at this function, I really have no idea what the inputs are to the function.  It says filename and an optional filename2.  Which of these is being resized?  How do I know what each parameter does?  You did not document this function, which made me dig off into your code to try to determine what it does.
So, I dug off into your code...  And it appears that this particular function resizes and possibly appends pdfs.  Notice how in your code, you execute the same code twice?
Don't do this.  Use the DRY principle.  You should have a loop, as the algorithm is basically the same.  (Can't illustrate the loop atm, running out of time before lunch, maybe when I get back :P)
You could even get fancy and have your function take an indefinite amount of PDF files.  Check this snippet out:
Ok, I lied, I tried to squeeze this in before lunch.  The code above may not work out of the box, but it should point you in the general direction.  You should add error catching to check for when no arguments are passed (among other things).
Hope it helps!
",323
23522570,23522754,2,"
maybe? 
not sure what you are asking to be honest ...
",12
23522570,23522570,1,"I have a working script that I am attempting to port over to a GUI.  I'm new to programming so a lot of this code may be hack-ish.  I'm open to suggestions on general practices and methods!  Below is my working text-based version of the function I'd like to port:
Here's my question.  I'd like to get rid of all the text-based user interaction.  How do I take object-based inputs and turn them into passable variables?  I've been able to implement wx.TextCtrl to enter user input and wx.FilePickerCtrl to select the PDF for the input.  Now how do I:
Pass these as variables to my resize_file function? 
Set the output location?
Set the backup location + input PDF name?
Pass the wx.TextCtrl values (for scale) to my resize_file function?
This also may be my problem:
I have a InitUI function doing all the wxPython stuff
The button within InitUi function calls resize_file_main
resize_file_main just handles inputs/outputs and moving the final
files around.  It also calls resize_file.
resize_file is a function that is re-used in several other areas
of the script.  It takes in the various inputs/outputs and actually
resizes the PDF(s).
Is this a bad flow?  I wasn't sure how to combine resize_file_main and resize_file because the input/outputs with the different areas that call resize_file are different.
Thank you for your help!  I know it's convoluted!
Edit:  Thank you!  I believe I have enough information to move forward.  I appreciate the help.
",281
23522728,23522728,1,"I'm trying to run this turtle function:
But I keep getting this error: 
I have no idea what this means. 
",24
23522728,23522800,2,"you have a file named turtle.py in the same folder ... you should not name files the same as libraries ... you are importing from your local turtle.py file
rename turtle.py (in this same folder) to myturtle.py and it should be fine
",44
23522781,23523021,2,"Here is a hint.
Given a list of lists:
You can get the sum of the individual sub lists with a list comprehension:
You can then get the minimum with min:
You can get the index of the list this way:
That is 90% of what you need to solve your issue.
",59
23522781,23523061,2,"
which gives
",2
23522781,23522781,1,"I'm looking to see if I could grab a hand on how to code an issue, preferably within the language Python. What I'd like to do is from 5 separate lists with 5 values in each find what the smallest sum of values would be, where each column/row can only have a value used once or twice. 
For example in the following example:
The desired outcome is to find the smallest total value where only one p, q, r, s or t can be assigned to A, B, C, D or E. i.e. you cannot have 2 values in the p column used twice.
For example, the most simple possibility could be: 98 + 75 + 66 + 58 + 49 = ... but it must be the lowest possible total value.
I'm not sure whether this would be done with lists or arrays or what ever it may be. I really appreciate any help you can provide. 
",180
23526204,23526340,2,"
Something like this?
Edit:
Not what the poster wanted after all. I don't think it is possible to do what you want with your table structure, without using more than one query.
",38
23526204,23526204,1,"So, I have this forum I am building. I'm having trouble writing an elegant select query to get all the posts for a certain topic from the db. Here's the schema:
Table posts have the following structure:
Now, there's also a table called attachments, where post attachments are referenced. I want to be able to select the attachments of a particular post with just one query. Here's the attachments table structure:
This is not what my tables look like really. I just want a basic idea how to do it. I can't even think of the least of solutions. Now the attachment table has a foreign key that references the post table. But, a post could possibly have many attachments. So how do I select a post and all its attachment in just one query? Maybe subqueries could work?
",162
23526265,23526489,2,"onliner trivial recursive lambda:
",5
23526265,23526265,1,"Let's say I have the lists [[1,2],[3,4]] and [[5,6],[7,8]]
I expect                   [[6, 8], [10, 12]] as the result.
I'm trying to sum up numbers according to their indexes.
",57
23526265,23526331,2,"Another easy job for ndarrays:
",6
23526280,23526280,1,"I'm trying out Sublime Text 2 and have installed Package Control. Now from Package Control in the Command Pallete I'm trying to install PyLint. However, this alert message pops up and am not sure how to proceed: 
Pylinter could not automatically determined the path to lint.py.
Please provide one in the settings file using the pylint_path variable.
NOTE:
If you are using a Virtualenv, the problem might be resolved by launching Sublime Text
from correct Virtualenv.
I am using a virtualenv in my project and am not sure what it means to launch Sublime Test from the virtualenv. Any help is appreciated - thanks!
UPDATE
I think I may need to configure something with build_system variable in the *.sublime_project file to get the virtualenv working. There appears to be a range of somewhat confusing advice in this SO question.
",154
23526280,23889696,2,"If you have installed pylint into your virtualenv, editing the project-file like this should fix it
See: https://github.com/biermeester/Pylinter#project-settings and 
https://github.com/biermeester/Pylinter/issues/4
",28
23526308,23526308,1,"Decorator pattern takes your function, decorates it with something else and returns you a new decorated function, and this is something I understand. 
But when it comes to Python, I found the implementation of decorators little different. Let's say we have the following code:
My basic expectation was to call this method by saying hello() however, it somewhat worked with the following exception:
So basically, when it hits hello part of hello(), it prints the correct result and returns None as any function without return statement does in Python. () is trying to invoke a None in this case. So I did this and it worked as I expected:
I am not sure whether my expectation justifies anything here. But as a person coming from C#, Java, etc. background, I found this little different. I was expecting the same behavior of the second code sample in the first one. What am I missing? Why was this designed in this way? Why is just saying hello is enough to invoke decorator function? What good does this design have? 
Update and Clarification
I was expecting it to work without specifically returning a nested function in the decorator. So in the first example, hello() could have worked as if helloWorld was returning a nested method.
",248
23526308,23526483,2,"When you do
Python does
So hello will contain whatever helloWorld returns, in your first case you didn't return anything (which is the same as returning None). That's why calling hello() would give the error 'NoneType' object is not callable.
",50
23526309,23543166,2,"Shutil.copy/unicode wasn't the problem here; tyring to copy a non-existent file was.
",15
23526309,23526309,1,"I'm trying to use the shutil module to copy files from one drive to another.  Since this is an ArcGIS script, I store the user's choice for folder source and destination locations as:
Using arcpy.AddMessage(src) to print that out gives me:
Which is what I want! However, when I try to use shutil.copy(src,dst), I get:
What is happening here? Since I'm not spelling out the path I can't change the ""u"" to an ""r"" for raw input... 
",103
23526309,24347824,2,"As a form of file management validation, you should always check to see whether or not the directory exists.  You can do this using the os.path.exists(path) method. If your path exists, you should have no problem. If not, then create it before copying your files over.
See example code below:
",61
23526384,23526434,2,"Python bytecode is portable across platforms, but not really across Python versions.
Python 2.7 introduced new syntax, for example, resulting in different, new bytecode instructions that Python 2.6 doesn't support. Also see the warning at the top of the dis module documentation:
CPython implementation detail: Bytecode is an implementation detail of the CPython interpreter! No guarantees are made that bytecode will not be added, removed, or changed between versions of Python. Use of this module should not be considered to work across Python VMs or Python releases.
You can move .pyc bytecode cache files across platforms, regardless of word-size and OS.
",117
23526384,23526384,1,"First question - If we have 2 different versions of python(say 2.6,2.7) on the same platform. Can you execute the bytecode (generated with python 2.6 interpreter) on python 2.7 interpreter?
Second question - If we have exact same version of python say 2.7.2 on unix and windows - can you run the bytecode generated on unix machine with the python on windows machine?
",71
23526390,23526390,1,"I have a program that I'm running that listens to a queue(its not multi-threaded so I want to run several instances of it).  I've tried my best to catch errors but in the event the app crashes due to a error or bad incoming data, I want to be able to respawn the python app(after I log the stacktrace) so it continues to work.
I felt this might be a common problem for people who run python based services so I thought I would ask but I was thinking of writing some code to do a ps -ef and count the instances of the name of the python program(if less than a threshold then I would have the program relaunch it).  
Before I build this, I wanted to know if there was perhaps a better way or a existing tool/module that did this? 
Thanks!
",162
23526390,23526564,2,"You could use a supervisor. A well known one that's written in Python would be supervisord, a more recent one also in Python would be Circus, and then there are Monit or daemontools and probably many more.
",42
23526390,23526956,2,"If you are looking for something more simple, you can use the subprocess module (python default) to start and check your processes...
A basic version would look like this:
",34
23526390,23526513,2,"Checkout supervisord.  I use it regularly to launch, monitor all types of things.
Here is how I set it up to launch a wsgi app on my server:
The configuration file format is easy to understand and it even logs stdout/stderr to a file.  Above it's /var/www/quizzes.seasources.net/logs/supervisor-console.log  You can read more about configuration here.
",61
23526410,23527361,2,"I don't have any python interpreter here, but it should be something similar to this:
Note: this will work for this classification, but if you want to retrieve usernames i.e. @USERNAME you will have to tweak this a little more.
",48
23526410,23526410,1,"Pulling from a couple of different examples, I've been able to create a simple Python script that parses the JSON output from the Twitter Streaming API, and prints out the screen_name and text for each tweet.  I would like to modify my code to also classify each tweet as one of the following: 
(1) Retweet --> There is an ""RT @anyusername"" somewhere in the tweet text column
(2) Mention --> There is an ""@anyusername"" but no ""RT @anyusername"" in the tweet column
(3) Tweet --> There is no ""RT @anyusername"" nor any ""@anyusername"" in the tweet column
I can do this in Excel with the following formula, but I can figure it out in Python yet. 
=IF(IFERROR(FIND(""RT @"",B2)>0,""False""),""Retweet"",IF(IFERROR(FIND(""@"",B2)>0,""False""),""Mention"",""Tweet""))
Existing Code
",203
23526486,23526548,2,"
Could something like this work for you instead? 
",9
23526486,23526486,1,"I'm getting this error on creating a string with arguments like ""abcd%s""%(e) but I'm getting e by scraping a web page. Can anyone please tell me what is the best way to avoid this error. 
I found other similar questions but they were using %20 in the url for which they need to replace %20 with %%20 which solved their problem. But my case is different. I tried encoding e but still getting same error.
",94
23527435,23790729,2,"As @Mikko Ohtamaa mentioned, the issue where was that the project root directory contained an __init__.py
",18
23527435,23527435,1,"I am having an issue in my Django project where I am importing the same module twice. This is causing a unit test of mine to fail: this unit test checks that the view found by resolving a URL is the same view imported from within the app being tested. So, my tests file looks something like:
The above assertion is failing because the two functions are not equal. When debugging in pdb, I found that found.func.__module__ is someapp.views while index.__module__ is projectName.someapp.views. 
I was told in #django on Freenode this could be because although I had recently updated to Django 1.6, I was using the old 1.3 project structure, where the project settings.py and urls.py are in the root of the project alongside manage.py. 
I've fixed that by creating a new directory within my project root, with the same name as the directory containing the project root, and placed my urls.py and settings.py in that directory. So, my directory structure looks something like this:
When I open a shell via manage.py shell, import sys, and print sys.path, the first directory in that list is /home/joseph/myWorkspace/projectName and that seems right to me. The rest of the python path looks pretty normal, pointing to different site packages, etc.  
However, when I run my test from above via manage.py test someapp, if I print the sys.path in pdb, I see that my python path first contains /home/joseph/myWorkspace, and also /home/joseph/myWorkspace/projectName. This does not seem correct to me, and I think this may be why I am having issues with double imports. 
I am not setting PYTHONPATH in my environment variables. As far as I know, I am also not making any adjustments to sys.path within my apps or settings. 
I don't know where to go from here, can anyone lend some insight? 
",338
23527600,23527600,1,"So I am using python pandas have an the following variables:
a dataframe df with a column 'TAG' I created to tag data into
groups based on data from a column 'IDnumber'. 
regex patterns stored in arrays pattern1, pattern2,
pattern2-2, ...etc
an array group which is filled with strings (ie: 'software', 'engineering', 'marketing'...etc).
The code is filling in the column df.TAG with strings from the array group based on the regex patterns  pattern1, pattern2, pattern22, ...etc
So far I have working code but there is redundancy in having multiple for loops that look the same
I am also getting a warning.
But the code works so I would like to know if there is a way to make the code more efficient by reducing the number of for loops and remove the warning without using pd.options.mode.chained_assignment = None to suppress the warnings.
",167
23527600,23527794,2,"Your first for-loop:
can be replaced with
This might be faster, since the entire loop is being replaced with one regex pattern. A similar refactoring can be done for your second and last for-loops.
But your third and fourth for-loops perplex me: for i in range(len(pattern2-2)):. Python names can not contain hyphens. So what does pattern2-2 mean? If pattern2-2 is just another array of strings (albeit with an invalid variable name!?) then your third and fourth loops can be handled the same as shown above.
If all the patterns are simply arrays of strings, then you could refactor all the for-loops with something like
Note that whenever you have numbered variable names, such as pattern1, pattern2, etc. it is usually a sign that theses variables should be replaced by a single variable which is a list or tuple, such as patterns above. Then instead of referencing pattern1, you'd simply use patterns[0].
",184
23527680,23527680,1,"I'm trying to get an arduino board communicate with a beaglebone ( BB) white running Ubuntu using UART. I have read that the BB uart driver is already interrupt driven. 
I want to store all incoming data into a sort of buffer which I can read when required, similar to the way it's done in microcontrollers. But I'm trying to avoid kernel programming so I won't be able to use the driver's data structures. I'm looking for a complete user space solution. 
I'm planning to use two python processes, one to write all incoming data (to a shared list) and the other to read it as required so that the read is non blocking. 
I have two questions:
Is this the right approach? if yes, please suggest a simple interprocess communication method that will suffice.
What is the right way to implement this?
Note: I'm using the PyBBIO library that reads and writes directly to the /dev/mem special file.
",185
23527680,23529277,2,"You might want to use pyserial, which  uses the kernel interfaces (I don't know what PyBBIO does). It provides automatic input buffering - so you don't need an extra process. If you do want to have more processes use multiprocessing. A simpler alternative is threading, which saves you the communication part. For multiprocessing with network support use Ipython's cluster
",70
23527738,23527738,1,"I have stored a number of 2d arrays in a 3d array and I need to multiply each one with a vector. so I have stored all those vectors in a 2d array. It's like this:
and I need to multiply each A[l] by B[l] which results in a Nx1 array and the output of the whole operation would be a LxN 2d array. Is there a function that can do this or do I need a loop?
",90
23527738,23528726,2,"An option is np.einsum
This results in a (L, N) sized array containing matrix products of all the A[i].T.dot(B[i])
",32
23531496,23545561,2,"Hi All,
                 With help from a friend I figured out why I was striking out with my earlier bit of code. As @wils484 pointed out, a conditional would have solved the problem (and I tried it even before posting on here, but in vain) but the key to getting it to work was realising that values[1] came out a string and needed to be converted to an integer before it could be used to loop through;
so the bit of code that helped solve the problem is:
So that resolved my issue and the code spewed separate files with the pertinent data.
",115
23531496,23531862,2,"It depends on how big your file is, but it isn't too big something like this would probably work, using a dictionary to split the data into their respective runs:
Your code is failing because it doesn't do anything to verify that the line belongs in the file. It should work if you add something like 
The disadvantage of doing that is that you are reading the input file 40x times.
",78
23531496,23531496,1,"I am trying to break a large data file which contains behavioural data from an experiment with around 40 runs on average (varies across subjects), into textfiles corresponding to run-number with the results of that run. 
The data look like this:
Subject, run, ""Sample"", result1, result2, reaction time, feedback
now, run number 1 may have about 10 trials, and run no 2 may have 16 and so on.
I have perused stackoverflow enough to learn how to open multiple output files corresponding to the list of runs, but my code yields unhelpful results (as in all data as opposed to just what I need) when I try to allocate all the values from ""Result1"" corresponding to the trials of ""a"" run to its output file (so, all Result1 values for Run 1 should end up in Run1.txt).
This is my code:
I have tried using conditionals to see if I could pick out result1 values but I am not sure I did it correctly. I am learning python on the job and would be grateful for useful advice on this issue. 
",211
23531555,23531555,1,"I'm running into a few issues on my Emacs + Org mode + Python setup. I thought I'd put this out there to see if the community had any suggestions.
Virtualenv:
I'm trying to execute a python script within a SRC block using a virtual environment instead of my system's python implementation. I have a number of libraries in this virtual environment that I don't have on my system's python (e.g. Matplotlib). Now, I set python-shell-virtualenv-path to my virtualenv's root directory. When I run M-x run-python the shell runs from my virtual environment. That is, I can import Matplotlib with no problems. But when I import Matplotlib within a SRC block I get an import error. 
How can I have it so the SRC block uses the python in my virtual
environment and not my system's python? 
Is there any way I can set
    the path to a given virtual environment automatically when I load an
    org file?
HTML5 Export:
I'm trying to export my org-files in 'html5', as opposed to the default 'xhtml-strict'. The manual says to set org-html-html5-fancy to t. I tried searching for org-html-html5-fancy in M-x org-customize but I couldn't find it. I tried adding (setq org-html-html5-fancy t) to my init.el, but nothing happened. I'm not at all proficient in emacs-lisp so my syntax may be wrong. The manual also says I can set html5-fancy in an options line. I'm not really sure how to do this. I tried #+OPTIONS html5-fancy: t but it didn't do anything.
How can I export to 'html5' instead of 'xhtml-strict' in org version
7.9.3f and Emacs version 24.3.1?
Is there any way I can view and customize the back-end that parses
the org file to produce the html?
I appreciate any help you can offer.
",344
23531555,23557258,2,"Reads like a bug, please consider reporting it at emacs-orgmode@gnu.org
As a workaround try setting the virtualenv at the Python-side, i.e. give PYTHONPATH as argument.
Alternatively, mark the source-block as region and execute it the common way, surpassing org
",47
23531807,23531807,1,"Why does my Dragon curve not look like a dragon curve?
Here is the implementation in python with order 10:
Here is what it's supposed to look like at order 10:
EDIT: Here is what I get with order 1, with a larger scaled curve:
",52
23531807,23532032,2,"When you run
the second line replaces the Xs introduced by the first line. (Also, you're using the wrong replacement rules, but you seem to have noticed that already.)
You need to carry out these replacements in such a way that they don't interact with each other, probably by doing them in a combined step. One way to do this would be to use the more advanced substitution capabilities of the re module. Another would be to write your own replacement routine. You could also use a character other than X in the first replacement, so the second replacement doesn't pick it up, then replace that character with X in a third pass.
",129
23531812,23531812,1,"I'm trying to transpose a dataset of order about 60*75. I'm having a trouble iterating through the matrix to transpose it into 75*60 order. Each column would have different kind of data (numbers, words, mixture, URLs etc). I tried the following code. But it'd just give me the first column transposed. 
Alternaticely I tried 
None of them gave me the complete transposed matrix. 
I even tried zip(*l) and map(zip(*l)). 
I appreciate your help. 
Thank you.
",101
23531812,23531842,2,"f.read() is a giant string containing all the file's contents. It is not some sort of structured data format; in particular, for row in l iterates over raw characters rather than rows of useful data.
If you want something more useful, the csv module might help, or perhaps something like numpy.loadtxt, depending on the format the file actually contains.
",70
23531825,32658000,2,"It won't work on python3.x afaik.
Sphinx is not very python3 compatible, running __import__(module_name) AND importlib.import_module(module_name) both work in my interpreter, but not in sphinx.
I tried checking out the master branch of sphinx, changed my interpreter to python3.4 and got errors on modules that were removed in the 3.x series. You can see my issue report here:
https://github.com/sphinx-doc/sphinx/issues/2046
",75
23531825,23531825,1,"I am trying to use sphinx to document a project of mine. I have used autodoc strings within all of my modules and files. I used sphinx-apidoc to automatically generate rst files for my code. So far, so good.
The problem is that sphinx is not able to import any of my modules, even though I have added my project to sys.path.
My unit tests pass and can import my modules just fine. I'm kind of at my wit's end; I've tried all sorts of renaming and moving and reloading and reconfiguring without success and it is very frustrating to say the least.
Here is my document structure:
Here is the relevant line in the sphinx config file:
Here is the output of sphinx-build:
",140
23531825,23554166,2,"I ended up restructuring my project so that the docs directory is at the same level as my project directory that contains all the files. This works when I use sys.path.insert(0, os.path.abspath(""../"")), and it's probably a bit nicer to look at.
",54
23531825,32396276,2,"Iam a bit late for the party here is my solution:
You have to go 2 directorys up:
",20
23993681,24005429,2,"The logging in your case was triggered by str(e).
Issue was fixed in https://code.google.com/p/appengine-gcs-client/source/detail?r=172
",22
23993681,23993681,1,"I'm using the GCS client library to write files to Google Cloud Storage in my App Engine app in Python.
Before creating a file, I need to make sure it doesn't already exist to avoid overwriting.
To do this I am checking to see if the file exists before trying to create it:
cloudstorage.write() is logging (either directly or indirectly) the fact that it receives a 404 error when trying to read the non-existent file. I would like to suppress this if possible.
Thanks
edit
Here's what is logged:
12:19:32.565 suspended generator _get_segment(storage_api.py:432)
  raised NotFoundError(Expect status [200, 206] from Google Storage. But
  got status 404. Path:
  '/cs-mailing/bde24e63-4d31-41e5-8aff-14b76b239388.html'. Request
  headers: {'Range': 'bytes=0-1048575', 'x-goog-api-version': '2',
  'accept-encoding': 'gzip, *'}. Response headers:
  {'alternate-protocol': '443:quic', 'content-length': '127', 'via':
  'HTTP/1.1 GWA', 'x-google-cache-control': 'remote-fetch', 'expires':
  'Mon, 02 Jun 2014 11:19:32 GMT', 'server': 'HTTP Upload Server Built
  on May 19 2014 09:31:01 (1400517061)', 'cache-control': 'private,
  max-age=0', 'date': 'Mon, 02 Jun 2014 11:19:32 GMT', 'content-type':
  'application/xml; charset=UTF-8'}. Body: ""NoSuchKeyThe specified
  key does not exist."". Extra info: None.
",264
24982969,24982969,1,"i want know witch device connect to witch usb port in ubuntu ...
for example when i connect bluetooth dongle to usb i wnat know that Bluetooth connected tu witch usb port ...
when i run 
tail -f /var/log/messages
in can see usb port number like this :
Jul 27 20:51:58 Smart-Installer kernel: [  711.363300] usb 1-1.2: New USB device found, idVendor=0a12, idProduct=0001
  Jul 27 20:51:58 Smart-Installer kernel: [  711.363331] usb 1-1.2: New USB device strings: Mfr=0, Product=0, SerialNumber=0
i want get usb 1-1.2 programmatically via python
i know lsusb -t get me port but i want also device number and -t get me a few data
",118
24982969,25048696,2,"You can either use PyUSB (python-usb package in Ubuntu and Debian), or walk through /sys/bus/usb/devices yourself, all the info is there in plain text.
",29
24982993,28203006,2,"
",0
24982993,24982993,1,"I am reading a file in Python line by line and I need to know which line is the last one while  reading,something like this:
From the examples I found it involves seeks and complete file readouts to count lines,etc.Can I just detect that the current line is the last one? I tried to go and check for ""\n"" existence ,but in many cases the last lines is not followed by backslash N.
Sorry if my question is redundant as I didn't find the answer on SO
",97
24982993,24983342,2,"Check if line is the last line:
If you want the second last line use last = lines[-2]
Or simply:
",25
24982993,24983191,2,"You could use the itertools pairwise recipe;
",8
24982993,24983104,2,"
",0
24982993,24983057,2,"One thing you could try is to try to get the next line, and catch the exception if it arises, because AFAIK python iterators don't have inbuilt hasNext method.
",33
31239302,31364280,2,"I had the same problem and tracked it down to a combination of CONN_MAX_AGE and CELERYD_MAX_TASKS_PER_CHILD. At that point it became obvious that it must be something to do with Celery not closing connections properly when a worker is replaced and from that I found this bug report: https://github.com/celery/celery/issues/2453
Upgrading to Celery 3.1.18 seems to have solved the issue for me.
",66
31239302,31239302,1,"Using Django 1.7 and Celery on Heroku with Postgres and RabbitMQ.
I recently set the CONN_MAX_AGE setting in Django to 60 or so so I could start pooling database connections. This worked fine until I discovered a problem where if for any reason a database connection was killed, Celery would continue using the bad database connection, consuming tasks but immediately throwing the following error within each task:
I would like to keep pooling database connections, but this has happened a few times now and I obviously can't allow Celery to randomly fail. How can I get Django (or Celery) to force a new database connection only when this error is hit?
(Alternatively, another idea I had was to force the Celery worker to run with a modified settings.py that sets CONN_MAX_AGE=0 only for Celery... but that feels very much like the wrong way to do it.)
Please note that this StackOverflow question seems to solve the problem on Rails, but I haven't found an equivalent for Django:
  On Heroku, Cedar, with Unicorn: Getting ActiveRecord::StatementInvalid: PGError: SSL SYSCALL error: EOF detected
",207
31239322,31239322,1,"I'm trying to remove StreamHandler during runtime of my python code execution. 
This is working fine. But I don't like that we assume that stdout is the first handler in handler array. Is there a way to query handlers class to find which type it is? Something like this
",55
31239322,31240807,2,"What you're looking for is spelled: if isinstance(handler, StreamHandler): - but I'd really like to know why you want to do such a thing instead of using the sensible solution (ie not configuring a StreamHandler for your logger at all...). 
",52
31239329,31239929,2,"The following Python script will read your file in (assuming it looks like your example) and will create a version removing the common folders:
Using the following as input:
The output is as follows:
With one space between each column, although this could easily be changed.
",53
31239329,31284985,2,"You can use the pandas library for this. Doing so, you can leverage pandas' amazing handling of big CSV files (even in the hundreds of MB).
Code:
Result:
",36
31239329,31283857,2,"So i tried something like this
",6
31239329,31239470,2,"
You can simply use this regex over each line and replace by empty string.See demo.
https://regex101.com/r/cK4iV0/17
",19
31239329,31239445,2,"What about something like,
To write the list sl out to filenew use,
",15
31239329,31239586,2,"You can automatically detect the common prefix without the need to hardcode it. You don't really need regex for this. os.path.commonprefix can be used 
instead:
rows now has a list of lists which you can write to a file
",43
31239329,31239329,1,"I have a csv file which contains 65000 lines (Size approximately 28 MB). In each of the lines a certain path in the beginning is given e.g. ""c:\abc\bcd\def\123\456"". Now let's say the path ""c:\abc\bcd\"" is common in all the lines and rest of the content is different. I have to remove the common part (In this case ""c:\abc\bcd\"") from all the lines using a python script. For example the content of the CSV file is as mentioned.
In the above example I need the output as below
Can any of you please help me out with this?
",121
31239904,31239904,1,"Can I run a Python folder or directory as a whole to execute all the .py files in it?
Edit: I'm using Windows Powershell.
",28
31239904,31240010,2,"For bash, This was already answered at Run all Python files in a directory
You can run:
If you're on Powershell, You can use:
EDIT: Like Duncan said, This is a shorter solution on Powershell:
",43
31239904,31240193,2,"Try this:
",3
31239904,31241387,2,"In Powershell you can use:
",6
31239911,31239911,1,"my small flask project is run normally on python2, but when i upgrade to python3, meet some problems:
first, my files' tree:
run.py:
app/__init__.py:
when i use python2 to run:
no problems, but on python3:
anyone knows why? rookie to python3 and need help :)
",58
31239911,31240028,2,"You are using implicit relative imports (the Python 2 model); you need to use absolute references or explicit relative imports:
where the leading . signals that the rest is relative to the current package, or
See PEP 328 - Imports: Multi-Line and Absolute/Relative.
You probably will have other issues however. Porting is not that trivial, read up on the issues in the Porting to Python 3 book. This issue is  a common migration problem.
",85
31242920,31244618,2,"Here is another alternative. This is a generic solution to remove any unquoted text:
The output would be:
It also displays text if the final quote is missing.
",32
31242920,31243111,2,"Use re.sub
OR
",3
31242920,31242920,1,"I have a string like this:
I want to replace delhi with """". But only delhi which lies outside the double-quotes. So, the output should look like this:
The string is a sample string.The substring not necessarily be ""delhi"".The substring to replace can occur anywhere in the input string. The order and number of quoted and unquoted parts in the string is not fixed
.replace() replaces both the delhi substrings. I can't use rstrip either as it wont necessarily appear at the end of the string. How can I do this?
",107
31242920,31243216,2,"As a general way you can use re.split and a list comprehension :
The re.split() function split your text based on sub-strings that has been surrounded with "" :
Then you can replace the dehli words which doesn't surrounded with 2 double quote!  
",47
31243002,31243002,1,"Given a set of points describing some trajectory in the 2D plane, I would like to provide a smooth representation of this trajectory with local high order interpolation. 
For instance, say we define a circle in 2D with 11 points in the figure below. I would like to add points in between each consecutive pair of points in order or produce a smooth trace. Adding points on every segment is easy enough, but it produces slope discontinuities typical for a ""local linear interpolation"". Of course it is not an interpolation in the classical sense, because
the function can have multiple y values for a given x 
simply adding more points on the trajectory would be fine (no continuous representation is needed).
so I'm not sure what would be the proper vocabulary for this.
The code to produce this figure can be found below. The linear interpolation is performed with the lin_refine_implicit function. I'm looking for a higher order solution to produce a smooth trace and I was wondering if there is a way of achieving it with classical functions in Scipy? I have tried to use various 1D interpolations from scipy.interpolate without much success (again because of multiple y values for a given x).
The end goals is to use this method to provide a smooth GPS trajectory from discrete measurements, so I would think this should have a classical solution somewhere.
",256
31243002,31244679,2,"I would suggest you try to transform your cartesian coordinates into polar coordinates, that should allow you to use the standard scipy.interpolation without issues as you won't have the ambiguity of the x->y mapping anymore.
",40
31243002,31335255,2,"This is called parametric interpolation. 
scipy.interpolate.splprep provides spline approximations for such curves. This assumes you know the order in which the points are on the curve.
If you don't know which point comes after which on the curve, the problem becomes more difficult. I think in this case, the problem is called manifold learning, and some of the algorithms in scikit-learn may be helpful in that.
",75
31243014,31247817,2,"I see @user3510686 has already answered it. Posing what I tried.
",14
31243014,31243288,2,"You can use a python dictionary for this purpose
For example
now dic['1'] and dic['2'] are you arrays
",26
31243014,31243014,1,"Currently, I have this piece of code to create a numpy array
I would now like to get multiple numpy arrays X(1) , X(2) etc for different conditions. What is the best way to do this in python. In matlab I can accomplish this using matlab struct.
",57
31243044,31243189,2,"You are not setting the property. You are manipulating a mutable object.
The assignment is not on the property itself, but on a subscription, the [..] part addresses a dictionary key. You could assign the property to a new name and still manipulate that dictionary:
but you cannot set the property to a new dictionary or a different type of object altogether. This applies to all mutable objects used in a property; lists, sets, instances, etc.
",91
31243044,31243044,1,"@property defined as int
The following code is taken from Python Docs:
When I run:
I get the following output (as expected, as no setter is defined)
@property defined as dict
However, if I define self._voltage = {} the property becomes writeable:
The output is then:
Same behavior in Python 2.7.9 and Python 3.4.3. Why is the property writeable, even if no setter is explicitly defined in the code? Here it was proposed to subclass dict to get this behavior. However, it seems that this not required.
",104
31243052,31245636,2,"Yes, it's possible to do it with recursion. You can make combine_n return a list of tuples with all the combinations beginning at index cur_index, and starting with a partial combination of cur_combo, which you build up as you recurse:
output:
The for loop only goes up to len(elements)-r, because if you go further than that then there aren't enough remaining elements to fill the remaining places in the tuple. The tuples only get added to the list with append at the last level of recursion, then they get passed back up the call stack by returning the temp_lists and concatenating at each level back to the top.
",124
31243052,31243052,1,"Yesterday, I encountered a problem which requires calculating combinations in an iterable with range 5.
Instead of using itertools.combination, I tried to make a primitive function of my own. It looks like:
Then I thought maybe I can abstract it a bit, to make a combine_n function. And below is my initial blueprint:
Then I've been stuck there for a whole day, the major problem is that I can't convey a value properly inside the recursive function. I added some code that fixed one problem. But as it works for every recursive loop, new problems arose.  More fixes lead to more bugs, a vicious cycle.
And then I went for help to itertools.combination's source code. And it turns out it didn't use recursion technique.   
Do you think it is possible to abstract this combine_5 function into a combine_n function with recursion technique? Do you have any ideas about its realization?
FAILURE SAMPLE 1:
This is my recent try after a bunch of overcomplicated experiments.
  The core ideas is: if I can print them right, I can collect them into a container later.
  But the problem is, in a nested for loop, when the lower for-loop hit with an empty list.
  The temp_list.append((i,j,k,n,m)) clause of combine_5 will not work.
  But in FAILURE SAMPLE 1, it still will print the content of the upper for-loop
  like combine_n([0,1], 2) will print 2, 1, 2.
  I need to find a way to convey this empty message to the superior for-loop.
  Which I didn't figure out so far.  
",311
31243172,31450804,2,"The solution to my problem is: 
This solution is found by Swordslayer on the autodesk forum for 3ds Max
",20
31243172,31243172,1,"I used the following code based on the information given in help.autodesk.com for executing maxscript in Python:
If I print the boolean, this always print: ""false"". However the following code works (aka the print statement returns true): 
However I cannot use the latter code since it must be possible in my code that a node has multiple morphers.
Is there a better way using the Python api for maxscript (I didn't find a method) or can anyone give suggestions how the first code can be improved.
Thank you
",103
31243290,36060258,2,"Your option number 2 uses a different data set (xytest) than your version number (1), which uses xtest. Furthermore, your crossvalidation should include the training, not only the prediction.
Apart from that they should be the same, while I advice you to use pipelines.
",55
31243290,31243290,1,"Does running a standard scaler and then a classifier give the same result as using a pipeline?
Hi, I have a classification problem and trying to scale the X variables using scikit learn's StandardScaler(). I see two options of doing this, should they in theory yield the same result? Because I am getting better precision score on my test data set when I use option (1).
(1) 
(2)
",83
31243352,31243643,2,"This could be done in 2 steps, generate a new column that creates the expanded str values, then groupby on 'A' and apply list to this new column:
EDIT
OK after taking inspiration from @Jianxun Li's answer:
Also this works:
",48
31243352,31244334,2,"Another way to do it. First reshape the df using pivot_table and then apply np.repeat().tolist().
",22
31243352,31243352,1,"My initial DataFrame looks as follows:
I need to group it by 'A' and make a list of 'B' multiplied by 'quantity':
Currently I'm using groupby() and then apply():
I doubt it is an efficient way, so I'm looking for a good, ""pandaic"" method to achieve this.
",64
31243355,31243355,1,"I have some data file which is array of arrays...For example:
This file has 2.8 MiB and type >i2.. File is fits and I know that its type is int16, but what does >i2 mean in Python?
How I can convert this data file to float?
So I have three data fits files, Dark frame, flat field and image. I need create correction of image. This data files I need convert to float and make some operation (divide and difference of data files values) a then convert result to origin data type int.
I created script which make corrections but I have problem that the result correction image has 6.3 MiB and don't has 2.8 MiB:
",133
31243355,31243553,2,"From the numpy docs
Note the array dtype above of >i2. The > means ‘big-endian’ (< is little-endian) and i2 means ‘signed 2-byte integer’. For example, if our data represented a single unsigned 4-byte little-endian integer, the dtype string would be < u4 .
The  type specifiers  are listed in the structured array docs.
",66
31243376,36051405,2,"You need to pass an instance of MyTextCompleter, not the class itself, to wx.TextCtrl.AutoComplete().  Change this:
to
",23
31243376,31247746,2,"I should have warned you more thouroghly: wxPython Phoenix is the future of wxPython (because, in contrast to classic it supports Python 3, too). That said, this does not mean everything is nice and shiny. My personal advice is to keep on going with classic (or in other words: what works now in classic, will most probably also work in Phoenix). In Phoenix, you will stumble into bugs like this more often.
Luckily, in this special case, there has already been something else done:
<wx.TextCtrl>.AutoComplete(â€¦) does accept a list of strings. This already works in 2.9.0/classic. See documentation for wx.TextEntry/AutoComplete.
",127
31243376,31243376,1,"I'm using wxPython (Phoenix).
I wrote a small app with a custom autocompleter, according to these guidelines, but it fails with the following error:
This is the code:
When commenting out the basicText.AutoComplete(MyTextCompleter) it runs successfully (without the autocompletion)
",52
31243388,31243684,2,"You still need to check that the action is a POST, and that the forms are valid, and you must redirect after a successful submission.
",28
31243388,31243722,2,"The easiest way to do it is by making ajax request when you push the submit button.
Considering you have a form 'voteForm', try loading this form using django's inbuilt template as: {{voteForm.as_p}}
This will create your form for, which you have already done.
Now when you press submit button, make an ajax request with your form data in it.
The ajax request will take your data to the form and reverts back with a response which you can use to further do the processing.
A quick example for ajax request would be:
",108
31243388,31243388,1,"I am currently trying to learn django. I decided to create a small app. currently I am making a form to create VoteType and Voting candidates on one page. I created a page where u can add as many candidate fields as you want, but when I click the button nothing happenes and even if I don't click the button some data is saved. I was watching this django guide on youtube. This guy is making one simple form. He added method = POST and action = '' to  ...  and in views he used (request.POST or None). I tried to do the similar, but as my form is a bit more complicated I got really confused. 
so this is my views.py code:
and this is my create.html django template:
how do I fix this code so that my program only saves instances when I push the create button?
",165
31243476,31243859,2,"Though you could do this, it's generally recommended that you open files using with as that is designed to handle errors by closing the file no matter what happens. You can still pass it to your Labyrinth's init just by passing a reference.
Then if any error occurs, the file is still safely closed, and you've also passed your open file to the Labyrinth object anyway.
",75
31243476,31244797,2,"You are at the design phase. 
So, you have to weigh the chances of losing your data due to some error or crash against the importance of your data and the cost of ""protecting"" it.
Using with protects you from some errors. If you consider that python itself may crash (e.g.), then you still have some risk. Saving after each step is evidently safer. How useful it is depends on the volume of saved data (and there are techniques for reducing this as well), the impact of each save on performance, and the chances of such crashes.
Without any further info, and simply guessing, my answer to your specific question:
... is it ok to open the file in the init function of Labyrinth and
  close it at the end of the game...? Or is it better to open and close
  the file every time?
is that I would save after each step.
",177
31243476,31243476,1,"I have a design question. I'm doing an exercise in python (2.7) which is a simple game with a labyrinth. I need to read an write from a specific file every step of the game.
Currently I have 2 classes (Game and Labyrinth). The Labyrinth class is responsible for reading and writing the file.
My question is, is it ok to open the file in the init function of Labyrinth and close it at the end of the game within another function (which can be called from another class)? Or is it better to open and close the file every time?
The reason I don't save the file content into a string with readlines() is because I'm supposed to save to the file each step of the game.
",148
31244238,31244391,2,"If you want ""to get the details"" for output, you can try Python's Data pretty printer:
The pprint module provides a capability to “pretty-print” arbitrary
  Python data structures...
Its output is very much configurable, and you can go down the structure to an arbitrary depth, with depth=....
If you want ""to get the details"" for using it elsewhere, 
I would suggest a few things:
Try not indexing with [0], you may be losing information stored somewhere else in Attributes.
Check available methods with How do I get list of methods in a Python class?, or Finding what methods an object has.
Check type with What's the canonical way to check for type in python?.
With the information obtained on Attributes you should be able to extract any info stored in it.
",158
31244238,31244238,1,"I am using Boto in Python to connect to Amazon MWS. I have successfully connected using their scripts but am having trouble parsing the response as I don't fully understand the documentation, and there are little to no examples on the internet. I am new to Python.
Here is how I get my response from MWS:
The first product gives a response of this:
My Issue is trying to get the details from the ItemAttributes:
At this point I believe it is a dictionary object.
As I am new to this language I can't figure out how to get all the information that is located in the () such as Brand, Studio, etc.
Boto has some built in functions such as Response and ResponseFactory, but I am lost again as I keep hitting a wall trying to get info such as Brand, etc..
Thank you again for any help you can give in this.
",171
31244268,31244440,2,"Have you profiled your code and have an idea where the hotspot is? If it is not computing, it's probably just the disk IO. I doubt you get a performance boost by tricks on the IO logic. In the end it's the sequential disk access that might be the limit. If you do have a RAID system it might makes sense to have multiple threads reading from the disk, but you could do that with python standard threads. Try to ramp up from 1 to a few and measure along the way to find the sweet spot.
The reason why you saw an improvement with gevent downloading images in parallel is that the network IO throughput can be improved a lot with multiple connections. A single network connection can hardly saturate the network bandwidth when the remote server is not directly attached to your network device. Whereas a single disk IO operation can easily saturate the disk throughput.
",171
31244268,31244268,1,"I need to load ~100k files with vectors and aggregate the content in a numpy array.  This process takes ~3mins so I want to speed it up. I tried to use gevent to speed it up, but I could not gain any speedup.
I read that one should use async calls to speed up IO calls and not multiprocessing. I further read that gevent is the recommended library. I wrote an example to download images, where I could see a huge improvement in speed. Here is a simplified version of my code
Using chunks is necessary, because otherwise I would have all the vectors twice in memory.
Is there an issue in my code or do I use the wrong approach?
",132
31247430,31247604,2,"How is could be done in Python. You did not give any code, so I give the basics only, no code as well:
put the root folder in a list
pop one item off the list
list the item
append the directories in this listing to the list
delete the files in the listing.
repeat the second step incl. substeps until the list is empty
Helpful: https://docs.python.org/2/library/ftplib.html
",76
31247430,31247430,1,"I need to write a script which deletes all files on an FTP server without changing the directory structure.
Since there is no find command or similar. 
I tried using lftp but got stuck since the rm -r is not flexible enough.
Anyway it could be bash or python.
I do not have ssh access to that server. FTP only.
",66
31247458,31247535,2,"You have a typo:
{% for product in products %)
and it should be
{% for product in products %}
see difference in bracket } not )
",32
31247458,31247533,2,"You have a syntax error in your template:
should be:
",12
31247458,31247458,1,"I am new to django&python. 
Using python 3.4.2 and django 1.8.
Trying to display a list of products and encountered an error:
""Exception Type:  TemplateSyntaxError 
Exception Value:   Invalid block tag: 'endfor'
Exception Location: myvirtualenv/lib/python3.4/site-packages/django/template/base.py in
  invalid_block_tag, line 395""
Can't figure out what's wrong. Found few related questions on stackoverflow but they did't help.
Give me a hint, please. Thanks in advance.
Views:
list_item.html:
P.S Without   content, {% block title %} renders with no errors.
",99
31247460,31247751,2,"If this is for your own benefit, rather than something you need to show to others, you can use IPython notebooks and the %matplotlib nbagg backend, at least for Seaborn, e.g.:
If you don't already have IPython etc. set up, you can quickly test this out by creating a new notebook at try.jupyter.org, pasting the code into a cell, and hitting Shift + Enter to run. Since this is running on a free VM it will be slow, running the notebook locally will mean panning/zooming is much smoother.
",103
31247460,31247460,1,"I've been trying to find a way to make Seaborn and Vincent interactive so that I can, for example, zoom in/out in a specific area of the plot in real time. Is this possible to do? Alternatively, are there other recommended libraries (that are not cloud-based services) that work well for visualizing time series data?
",64
31247460,31253739,2,"I found that using Seaborn with mpld3 worked best for me, thanks.
",14
31247510,31247510,1,"I have red at the help-page of easy_install that I have an ability to do ""install in user site-package"". What does this phase mean, ""user site-package""? How does it affect functionality of the installed software?
",43
31247510,31247842,2,"User site-package refers to packages installed in ~/.local/lib[64]/python-VERSION/site-packages/
These packages are available as any other installed packages, but only to this specific user. It overrides system packages too.
",35
31247578,31247578,1,"I'm using python-social-auth for user login for my organization site. I only allow Google accounts in a white list (the people in my organization) to login to the site. However, I would like to preregister everyone in the database so that I can add their custom fields (leadership positions, etc.). Is it possible to add user accounts before their first login?
",72
31247578,31247744,2,"Try using pipelines in python-social-auth.
Create a custom pipeline by creating pipeline.py file and add your functions here.
A simple example of functions can be found here
",29
31247587,31247797,2,"How about this. 
",4
31247587,31247587,1,"I'm creating a telegram bot that searchs for words in a online dictionary, the problem comes when I need to create the command in Telegram for searching in that dictionary, at the momment I have this:
I'm using this Telegram API in Python: https://github.com/yukuku/telebot
And this API for the Dictionary: https://github.com/dialelo/rae-1
",62
31247647,31248670,2,"You need to store each imshow AxesImage in a list and inside update, loop over all of them and update each based on the slider,
",27
31247647,31247647,1,"I am quite new to Python, so please excuse if this is a stupid beginner's error. However I am struggling with it for quite some time.
I want to create a figure with n x m subplots, each subplot being np.array of shape [1024,264,264]. As I am looking for differences occuring in the stack along the 0-dimension I want to use a slider to explore all stacks in my figure simultaneously. 
The slider instance works nicely for a figure with one subplot but I can't bring them all to work.
That's the code I am using:
For this particular case I stripped it down to a 3x3 array for my figure and just create randmom (smaller) arrays.
The slider is interestinly only operable on the second last subplot. However I have no real idea how to link it to all subplots simulatenously. Perhaps someone has an idea how to do this.
Thanks a lot in advance,
Tilman
",177
31247678,31250371,2,"I cannot speak to the time efficiency of this method, but it might just get what you want done. The basic idea is to create a list to contain the lines of each text file, and then output the list to your new csv file. You save a 'delimiter' variable and then change it by checking each line as you go through the text files.
For example:
I created two text files on my Desktop. They read as follows:
delimiter_test_1.txt
test=delimiter=here 
does-it-work
I'm:Not:Sure
delimiter_test_2.txt
This:File:Uses:Colons
Pretty:Much:The:Whole:Time
does-it-work
If-Written-Correctly-yes
I then ran this script on them:
And got two csv files with the text split based on the desired delimiter. Depending on how many different lines you have for changing the delimiter you could set up a dictionary of said lines. Then your check becomes:
for example.
",170
31247678,31247678,1,"I have a text file that doesn't have a standard delimiter. I need to be able to check if the current line is equal to a certain phrase and if it is, the code should use a certain delimiter until another phrase is found. delimiters used are  ',' '-',':' and '='. 
Please help me out :) 
This is what my code is at the moment
",79
31247763,31247763,1,"I have got the following pandas data frame
IÂ´d like to round Y and X columns using pandas. 
How can I do that ?
",25
31247763,31247824,2,"You can apply round:
If you want to apply to a specific number of places:
EDIT
You assign the above to the columns you want to modify like the following:
",33
31247768,38187541,2,"I am seeing this post very late, but maybe I can help... I am not sure what the function Qobj() is doing, can you please tell me more about it.
Otherwise, there is now a new partial_transpose() function in PICOS (version released today), which hopefully does what you need.
Best,
Guillaume.
",66
31247768,35159905,2,"You need to be able to output a numpy array or sparse matrix to convert to a Qobj. I could not find anything in the picos docs that discusses this option. 
",33
31247768,31247768,1,"I need to write a semidefinite program that minimizes the trace of an operator, say R, subject to the constraint that tr_A(R)^{Tb} >>0 . That means that R represents a 3 qubit quantum system and the trace over the first system gives you an operator that represents the remaining 2 qubit systems. Taking the partial transpose with respect to one of the qubits, you get the partially transposed quantum state of the restricted 2 qubit system. It is this state that I want to make positive semidefinite. 
I am using PICOS (to write the SDP) and qutip (to do the operations). 
Problem: I need to make Rho a Qobj, for qutip to be able to understand it, but Rho above is only an instance of the Variable class. Anyone has any idea on how to do this? 
Also I looked here, http://picos.zib.de/tuto.html#variables , it became even more confusing as this function puts the instance in a dictionary and only gives you back a key. 
",195
31247820,31247911,2,"Pass the Event object to the thread target function so that they are shared between main thread and the pool thread:
",22
31247820,31247820,1,"I have a main python script which starts a thread running in the background.
poll = threading.Thread(target=poll_files, args=myargs)
I want my main script to wait until something specific happens in my poll thread. I want to use an Event object. So in my main script, I do:
trigger = threading.Event()
and when I want to wait:
trigger.wait()
My question is, inside my poll thread, how do I set the Event to True? I know I do:
trigger.set()
but do I need to also have trigger = threading.Event() inside my poll thread?
",114
31247829,32295818,2,"Finally i solved this problem, there was a broken link to one of references in windows service. This is the right config for py2exe which solved my problem:
",31
31247829,31247829,1,"I've implemented a windows service, this service has no problem before compiling with pyinstaller but after that on service start command it gives 1053 error.
Windows service code:
",32
31247829,38726274,2,"To use my win32serviceutil.ServiceFramework class in an exe pyInstaller produced, I needed to add a few .pyd files to the directory containing the exe e.g. win32service.pyd and win32event.pyd.  Assuming you've used the standard install paths for your libraries, these files are found here:
C:\Python27\Lib\site-packages\win32
",52
31247878,31248021,2,"You can get the names from elements with market_listing_item_name class name located in div elements having market_listing_row class:
Here is the contents of the output.dat file after running the script:
",32
31247878,31247878,1,"I am trying to record every item on the tf2 market place using selenium. I am trying to record the name of each item in a file on sale. This is the link to the page. I think it is this tag I just dont know how to reference and record the name in a text file with each name on a new line.
Edit 1:
I have used the solution by alecxe and it works for the first page I'm now trying to run it to select the next button then run again. But to no avail this is what I am trying.
This produces this error
Any help would be greatly appreciated even if it is links to guides
",129
31248615,31248615,1,"I was trying to use ZMQ - PUSH - PULL to build a distributed task processing system. This was east to do using JMS in Java with a Queue and a listener;Listeners which are free could take the message of the queue and execute it.Once the queue is distributed across nodes, this acts like a load balancer.
With ZMQ (using Python - don't want to use Celery now), I was trying out PUSH and PULL. With the Worker having different processing time. However even when a worker is free, tasks are going in strictly  round robin fashion. That is irrespective of if a worker is free or not, the task goes  in a round robin way.
Is there any way of simulating a distributed queue with ZMQ patterns so that, I can have a pool of workers 'polling' the queue in each node and which ever is free pulls the message from the queue and process it.
",175
31248615,34895786,2,"As pointed out by 0MQ founder Pieter Hintjens in this answer, the PUSH-PULL mechanism is not a load balancer, but rather a simple round robin distributor. That's a typo in the docs that is still there.
That said, for the load balancing pattern you need to add a broker in the middle of your architecture. As pointed out by Jason in the comments, this is well explained in the official guide. There are also examples in Python.
The main idea is to have the workers sending a small ""READY"" message to the broker whenever they are free to receive more jobs. The broker in turns, keep ""pointers"" to free workers in a queue. When he receives a new job request from a client he also propagates the request to the first free worker in the queue, which gets popped out from the queue. As you can see in the picture above, the broker exploits ROUTER sockets in order to avoid a blocking behavior and to get proper load balancing. A small additional detail is that the broker does not poll the clients if there are not free workers in the queue.
This is the simplest way I am aware of for implementing a load balancing pattern with ZeroMQ. It is not exactly like ""polling"" for new jobs in the queue, but I think this is what you need. Also please beware that this is really the simplest way, that is, it is not reliable at all and it does not scale well as is. If you also need reliability, I suggest you to thoroughly read Chapter 4 of the official guide.
As a side note, maybe you should seriously consider Celery for this task. I am really in love with ZeroMQ, however this is exactly the kind of thing that Celery is very good at, and in my opinion it is not so difficult to learn, as someone may think.
",358
31251807,31251807,1,"Recently I have been having trouble opening specific UTF-16 encoded files in Python. I have tried the following:
but I get the following error:
after trying to read the contents of the file. I have tried forcing little-endian as well, but that's no good. The file header is as follows:
Which I have read denotes UTF-16 Big Endian. I have been able to read the contents of the file into a raw string by using the following:
Which works for getting me the raw hex, but the thing is - sometimes these files will be little-endian, sometimes they will be big-endian so I essentially just want to normalize the data before I start parsing, which I was hoping codecs would be able to help me out with, but no luck..
Does anyone have an idea of what's going on here? I would provide the file(s) as reference but there is some sensitive data so unfortunately I can't. This file is used by Windows OS.
My end goal, as I mentioned above, is to be able to open/read these files and normalize them so that I can use the same parser for all of them, rather than having to write a few parsers with a bunch of error handling in case the encoding is wacky.
EDIT: As requested, the first 32 bytes of the file:
",253
31251807,31252161,2,"Looks like you have a header of 24 binary bytes before your utf16-encoded string starts. So you can read the file as binary and decode afterwards:
But probably there are other binary parts. Without knowing the exact file format, there cannot be given more help.
",51
31251808,31251808,1,"I'm running series of testcases in multiple files, but I want to run the prereq and cleanup only once through out the run, please let me know is there a way to do it?
",38
31251808,31251843,2,"py.test -> session scoped fixtures and their finalization should help you
You can use conftest.py to code your fixture.
",21
31251871,31252232,2,"One-liner using re.sub function.
",5
31251871,31251929,2,"You can use a nested list comprehension: [[f for s in sequences for f in sorted(s)] for sequences in lines]
",28
31251871,31251871,1,"My data looks like:
Each line is a sequence. For each line, if multiple numbers occur in {  }, I want to sort them in each bracket(ascending sort ), while keeping the order of the rest. At last, I want to remove the brackets. So I want my output to be like this:
My thought was converting each line into a list, but then I was totally stuck.
",81
31251876,31251876,1,"Hi I'm just learning about how decorators look in my Python flask framework and wanted to confirm if I'm reading this properly since I can get lost in nested functions. Here is a piece of the code I'm working with
The thing that confuses me when following this along is the 'f' variable.
So Login_required() is called as a decorator for my index() function. And it looks something like this -->
    login_required(index()). 
Does the 'f' variable become login_required(f()) or login_required(index(f))?
It confuses me in the login_required() function when f is returned with (*args, *kwargs) I'm not exactly sure what's happening and get lost
",141
31251876,31251948,2,"f becomes index.  This:
Is the same as this:
That is, the decorator is called with the decorated function as its argument.
",27
31251881,31251881,1,"I have code for radio buttons in tkinter. I am struggling to write the code that invokes the button command. Basically I want the user to be able to choose a time frame and a person. I have three different files that run three different data analyses, so I want the three files to run but only take data from the time frame and for that person. 
I tried
but this doesn't seem to work, nor is it very pythonic considering I have to run three files using the input from the button. 
",101
31251881,31252593,2,"First, When properly indented, an if - elif block is perfectly ok. So you could just use
In other languages there is something like a switch - case block: wikipedia There is no such construct in Python, but there is a neat little trick that helps especially when dealing with bigger code blocks:
Basically, a dictionary is defined, that maps its key values to functions. Mind the parentheses: You don't want to call the function, when the dictionary is defined.
",93
31251915,31252902,2,"HTTP communication also known as RESTful communication is the universal language of the web and is ideal because it can work between operating systems and back end frameworks such as Django and iOS etc.. etc..
You will have quite a bit of research you need to do.  Django is written in Python, so assuming you know some Python, Django should not be too difficult to pickup, but you should definitely start with it's poll app tutorial.  
Django-Rest-Framework is probably the best tool to build a restful web API for your Django project.  Your iOS app will make http calls (GET, POST, PUT) etc... to your Django API.  There is really no better documentation than Django-Rest-Framework's website to learn how to use it.  
Ultimately there is no book that will tell you step by step how to do this.  You have to learn several different areas and put the pieces together.  
",167
31251915,31251915,1,"I am newbie in Django so i'm a bit confused with some of it's features. I have to do a project for my university. It consists of a desktop-web app where employees can do stuff like handle files, record files and upload files on a MySQL database. In addition, there's an iOS native app (written in Objective-C) where users can register themselves, log in, and ask for the data that employees have saved early. 
First of all:  Is it possible to build a server app that communicates with an iOS app (sending-recieve data, notifications) using Django-frameworks (and i suppose also Django-Rest-Framework)? If yes, are there any tutorials/guide/book to better understand how this communication work and how i can really do this?
My second question is related to custumizing users on Django.
For my project, i need two classes of user:
Employee (with registration_id as primary key): this user can only log in on desktop-app
Driver : this user can only log in on ios-app
Can i have multiple users like here:
is that reasonable?
Sorry for my bad English.
I will appeciate any kind of help
Thank you
Cheers
",220
31251933,31251933,1,"sorry to ask a basic question, but it is hard to find on google.
anyway, I have a program that does math from numbers found in various .txt files. one thing I would like to add is a -show argument to show the math step by step (finding bugs in math, finding numbers from certian steps, etc.).
I have it set up in the code like so:
(show = 1 does something later on). my problem is when i don't put anything for sys.argv[3] like if i put:
then the program doesn't run, I know it is becauce it is expecting an argument and thats why its messing up, but is there a way to tell it that sys.argv[3] is not always used and can be blank?
",152
31251933,31252036,2,"On a side note, it might be good to checkout argparse module in python, from the documentation -
The argparse module makes it easy to write user-friendly command-line interfaces. The program defines what arguments it requires, and argparse will figure out how to parse those out of sys.argv. The argparse module also automatically generates help and usage messages and issues errors when users give the program invalid arguments.
It also has support for optional arguments. Maybe you can take a look at this to get started on that.
",96
31251933,31251934,2,"You should be getting an IndexError on that line when you run it without the third argument. If you're not getting an IndexError, something else is wrong and you should fix it. If you are, all is well - and you simply need to check for the length of sys.argv, taking into account in your code what should happen when that value is 3 or 4:
",73
31251933,31251935,2,"You can check the length of sys.argv, but why bother? Use try/catch:
That way variable flag always has a value, and you can write code the knows that it always has a value. The code has fewer lines than a if/else testing. It's a win all around.
",55
31252022,31267118,2,"""... decides what translation object to install in the current thread context"" (source)
Simple as it is, the language is thread-wide, since one thread serves one request, and request knows about the language.
",41
31252022,31252022,1,"I am just very curious. Django runs the following line:
it runs it from form_valid() of a class-based view. New context is created based on a regular python dictionary. No parameters are passed, that come from a view, user, session, etc... Inside of the template there is {% load i18n %} and a bunch of context/variables.
Still, Django recognizes the language of the current session and applies appropriate translations.
So, where from does it know the session language?
",96
31252037,31252122,2,"A <br/> tag is an empty tag, always. There is no text in that tag.
What you have instead is text between two <br/> tags, which may have been confusing. You can drop either tag and it'll still be valid HTML.
You can get text following a tag using the .next_sibling attribute:
Demo:
Putting that together with extracting all the data:
which produces:
",78
31252037,31252037,1,"I almost have a grip on BeautifulSoup4 in Python, but I can't seem to pull out the <br/> data for the br tags in HTML data.
Data Structure:
What I'm looking for is:
The HTML comes in through requests, that all works fine. But I'm just not getting the soup to mix correctly.
Current Code:
",68
31252107,31252343,2,"Could you try calling plt.draw after plt.vlines? plt.draw is used to interactively redraw the figure after its been modified.
",21
31252107,31254208,2,"If I understood well, you want to use the animation tools of matplotlib. An example (adapted from the doc): 
Resulting gif looks like this:
",30
31252107,31252107,1,"I have a time series plot and I need to draw a moving vertical line to show the point of interest. 
I am using the following toy example to accomplish the same. However, it prints all the lines at the same time while I wanted to show these vertical line plotting one at a time.  
",59
31252155,31252155,1,"For the life of me, I cannot find a way to solve this, I don't really know what I'm doing wrong with the inlines.
I'm using django 1.8 as well as django-nested-inline 0.3.4.
Here's the contents of the models.py file:
Here's the contents of my admin.py file:
And here's the error I'm getting:
Anyway, if anyone can identify a solution to this problem, I'll be really thankfull.
Have a good day.
EDIT:
Okay, now I have another problem entirely, and I believe it's more of a design problem.
The way I want the information to be displayed in the admin, is:
Again, I've been wrecking myself trying to get this right for a while, and any help is much appreciated.
Thank you, have a nice day.
",159
31252155,31252286,2,"Try using a string for fk_name:
As an aside, I recommend that you use the Django convention, and name your models SensorAsignado and Linea instead of sensor_asignado and linea. It will make your code easier to understand for other Django users.
",46
31252263,31252263,1,"I'm a beginner for Python and trying to print Fibonacci series recursively. When i try this, the return function doesn't print at all, hence just the argument that i pass gets printed.
Here's the code:
Can someone help me to get all the numbers in the series?
",56
31252263,31252916,2,"A basic (but not naÃ¯ve) recursive Fibonacci solution is as follows:
You can save the intermediate values in two ways: create a list and append() to it, or have your function return a tuple.
Method 1:
Method 2:
",48
31252263,31252430,2,"The inefficient way to do this is to use a loop:
However, this is inefficient because it will calculate the lower Fibonacci numbers more than once. You can use an array to store the intermediate Fibonacci numbers and improve performance:
",44
31252265,31252323,2,"The line:
should be:
As the string method .split() returns a list, whereas you are expecting it to replace the string with a list of its parts which doesn't happen.
",37
31252265,31252265,1,"
This code should read the output of df -h on an ubuntu 12.04 server, split the output of the command into a list, and then trim the '%' character off, so that the drive usage can be used as a comparator in an if statement. Then, if it's above 90%, it should email the server admins. 
However, drive_usage[x] = s2 triggers the 'str' object does not support item assignment. To my knowledge, this should not be triggering, as that line should be accessing a string in a list, not a character in a string. Any help or advice would be appreciated!
",123
31252284,31252284,1,"I am learning Django and I'm still in the first tutorial. When I try to run:
I get the following error:
",25
31252284,31252433,2,"This looks like a typo in the import statement. datatime should be datetime (a -> e).
",21
31252296,31252370,2,"One solution would be to concatenate tuples.
One benefit of doing it this way is it's an expression thus usable inside a lambda.
",26
31252296,31252386,2,"Another way is to make a list, then convert the list to a tuple by calling tuple(lst).
",22
31252296,31252422,2,"One way to do it - useful if you're going to be generating the contents of the tuple a lot - is with a generator: 
Then make a tuple from the generator results: 
With a slightly modified version you can also supply the arguments which determine what the generator will produce: 
One nice thing about doing it with a generator is that you don't have to keep multiple sets of the tuple in memory - both the contents and the logic are contained in the generator. So you can be simultaneously iterating through lots of different versions of the same data, but it's all contained in only one location and generated on the fly as you need it. 
",127
31252296,31252465,2,"Python does not provide this kind of syntactic sugar.
The easiest way to solve your problem is to use a list. Then you can use my_list.append(), and finally, when you need a tuple, make a tuple out of it: tuple(my_list). (INSTALLED_APPS = tuple(my_list))
",60
31252296,31252611,2,"You can do something like this using a generator expression: 
...or in one line:
",17
31252296,31252368,2,"You can just concatenate another tuple to the end.
",10
31252296,31252296,1,"Say I have a list x = ['a','b','c'] to conditionally add a term to this you could do:
But I can't do this for a tuple (x = ('a','b','c'))
For both cases is there a 'clean' way of conditional-alising items in a definition. Pseudo code:
The use case for this is a Django INSTALLED_APPS tuple and urlpatterns list.
",84
31252320,31252320,1,"I have this piece of inelegant code that's supposed to print out something that looks like this to the console:
However, most of the time I get this:
I'm not sure where the problem lies with my code because occasionally it works as expected and I get the right output (which drives me crazy).
I'm using Enthought Canopy as my IDE.
",71
31252320,31255642,2,"The problem likely is in the terminal emulator, resetting colors after the tab-characters (which would be a bug).  If you change the tabs so that only spaces are written, it would work around that possibility.
By the way, if your terminal was set to use hard tabs, then you could expect some other unexpected behavior (the tabbed area would be skipped and not colored).  Here is a screenshot (with the script to demonstrate):
",87
31252320,31271453,2,"I still am not sure what the problem might be, but adding a sys.stdout.flush() at the beginning of the for loop seems to have fixed things.
",30
31252353,31260634,2,"You can use the uWSGI alarm subsystem:
http://uwsgi-docs.readthedocs.org/en/latest/AlarmSubsystem.html
the --alarm-backlog function will trigger an alarm whenever the listen queue is full, this alarm can be an ad-hoc script calling aws api to scale
",38
31252353,31252353,1,"I want to add instances to the current setup based on uwsgi listen queue. If uwsgi process have a high backlog, a new machine will spin up and respond to requests.
",34
31252359,31252408,2,"You're using Python 2. In Python 2, input takes your input and tries to evaluate it. You want to use raw_input.
",26
31252359,31252413,2,"I am guessing you are using Python 2.x , in Python 2.x , input actually tries to evaluate the input before returning the result, hence if you put in some name , it will treat that as a variable and try to get its value causing the issue.
Use raw_input(). instead. Example -
",59
31252359,31252359,1,"I have a python script and I am receiving the following error. I'm a new learner to this language, so I created a simple script, called writing.py, to write participant names and scores into a text file named scores.txt. But I keep getting this error:
Here is my code:
",57
31252360,31253142,2,"You can use wand for such basic tasks.  The syntax is very easy to read unlike other ImageMagik libs.  Basically you'd do something like:
It will be along those lines. Once I leave the office I will try this out.
",46
31252360,31252776,2,"If you have PIL installed then you can create an image with Image.open and get the colors like so:
",20
31252360,31252607,2,"You can use the existing pygame module. Import a file into a Surface using pygame.image.load. You can then access the bit array from this using pygame.surfarray.array2d. Please see the Pygame docs for more information.
",38
31252360,31252360,1,"I would like to convert a PNG image to a 2 dimensional array where each array holds a list of the RGB values of that specific pixel. How could one create a program to read-in a *.png file and convert to this type of data structure?
",48
31252473,31253296,2,"The compiler doesn't use the PATH environment variable for finding header files.  Usually it uses INCLUDE.
I think you can pass this into nmake, like so
nmake INCLUDE=pathToPythonHeader -f ap24py27-win64-VC9.mk
",34
31252473,31259320,2,"Why are you trying to compile it from source code? Why aren't you using the precompiled binaries?
There is nothing additional in the latter release that you would really need on Windows, so just grab and use the latest binary version which at this time is 4.4.12.
You also can't do a 'python setup.py install' nor a 'pip install' on Windows.
If you really do for some unknown reason need to build from source code, you do at least need to modify the ap24py27-win64-VC9.mk file and override the locations for where Python and Apache is installed. The default locations are based on a non standard layout that allows me to have many different Python and Apache versions installed for different architectures at the same time.
",137
31252473,31252473,1,"I am having a LOT of trouble to start with these are my specs
I cannot for the life of me figure out how to compile mod_wsgi to make the necessary mod_wsgi.so file from the source code.
https://github.com/GrahamDumpleton/mod_wsgi/releases
I'm trying to compile the ap24py27-win64-VC9.mk and after opening Visual C++ 2008 64-bit command prompt
I already set my path to path=%path% Python27/include because that's where the Python.h file is. 
I've also tried
",82
31253148,31343406,2,"I believe the .addSample() method expects one sample at a time. Rather than using .addSample(), try
The 'assert()' is recommended because the .setField() method does not verify array dimensions like .addSample() does.
See Pybrain dataset tutorial for more info.
",54
31253148,31253148,1,"
Starting using pybrain to get a Neural Network to work on my diffusion energy data. I don't know how to get a dataset working from my X and y values. X is 35 inputs and y is 1 ouput and there are 148 samples. With this code I get the error: ""ValueError: could not broadcast input array from shape (148,35) into shape (35)""
Need to know how to properly prepare a dataset for pybrain.
",87
31253156,31253676,2,"I'd suggest you start with your DataFrame like you have already got:
Add a couple of columns with nothing in them:
Then set the values as needed with some conditional wizardry:
(Spot the 'deliberate' mistake that I put the levels and the titles the wrong way round. But you get the idea!)
",61
31253156,31253471,2,"If there are only two levels, then you can use list comprehension , like this -
",17
31253156,31253156,1,"I have this dictionary:
I want to have the following output:
So far, my code outputs this:
Basically when the first item in the list is 1, the second item in the list should go to 'level1 title', and when the item is 2, the second item should go to 'level2 title' 
",61
31253163,31257416,2,"Your main issue is probably just reshaping your data so that you have date along one dimension and time along the other.  Once you do that you can use whatever plotting you like best (here I've used matplotlib's mplot3d, but it has some quirks).
What follows takes your data and reshapes it appropriately so you can then plot a surface that I believe is what your are looking for.  The key is using the pivot method, which restructures your data by date and time.
This gives me (using your data) the following graph:
Note that I've assumed when plotting and making the ticks that your dates and times are linear (which they are in this case).  If you have data with uneven samples, you'll have to do some interpolation before plotting.
",151
31253163,31253163,1,"My csv file is, 
https://github.com/camenergydatalab/EnergyDataSimulationChallenge/blob/master/challenge2/data/total_watt.csv
I want to visualize this csv file as clusters.
My ideal result would be the following image.(Higher points (red zone) would be higher energy consumption and lower points (blue zone) would be lower energy consumption.) 
I want to set x-axis as dates (e.g. 2011-04-18), y-axis as time (e.g. 13:22:00), and z-axis as energy consumption (e.g. 925.840613752523).
I successfully visualized the csv data file as values per 30mins with the following program.
I also succeeded to visualize the csv data file as values per day with the following program.
Although I could visualize the csv file as values per 30mins and per days, I do not have any idea to visualize the csv data as clusters in 3D..
How can I program it...?
",157
31253229,31253274,2,"You're creating two, since Toplevel() is the constructor call:
Instead, create one and save it:
",22
31253229,31253229,1,"I have a program that uses Tkinter and I'm trying to assign a command to a button in my root window that opens one additional window. I'm using Toplevel(), but whenever I click the button I've assigned the command to, two windows open, one with my root window's name and one with the name of the additional window I've assigned. 
I've tried using .withdraw and .destroy, to hide or remove this extra root window, but nothing seems to be working. 
Here is my code:
If you click ""Begin Test"" in the root window, two extras pop up. I only want the one that says ""Directory."" 
Any ideas?
",132
31256091,31256091,1,"All I'm trying to make is a program that loops through a word or phrase character-by-character like this:
I wrote this:
and the output is exactly what I want:
But, if there is a repeated letter, it'll remove both at once, like this:
How can I make it just remove the one letter, without removing both? Can't think of a solution
",73
31256091,31256103,2,"You can pass count as 1 to replace to only replace one occurrence.
You could also just slice the last letter off:
",24
31256091,31256205,2,"Do you have to use a tempvariable, or are you allowed to just use string slicing?
",18
31256091,31256220,2,"Here is a solution, it's not the best. Depending of what you want to do with this, I can provide a more specific solution
EDIT
I prefer this version more
",34
31256157,31256157,1,"I'm using Python 3.4.3 and PyQt 5.4.2 and have some problems, when trying to get data from rowsInserted signal.
I want to get data from all items near the dropped item after drop operation.
Here some of my code (whole model class is too large, hope this part is enough):
This code prints valid data for all items, excluding dropped item. For dropped item it prints 'None'. After dropping in QTreeView all data is OK. What wrong with it?
Sorry for my bad English.
",98
31256157,31273784,2,"It seems like rowsInserted signal emits after rows were inserted, but before model fills data in inserted rows.
So I call printSomeData function from dropMimeData function (after data was dropped) and it works.
Here corrected code:
",42
31256159,31256227,2,"IIUC, you could use itertools.accumulate to generate a forward fill:
",12
31256159,31256231,2,"Here's some code that will do what you want in place, if you don't want it in place then just pass it list(my_list) instead of my_list.
Also, if using python2, use xrange instead of range.
",45
31256159,31256246,2,"
You also only need to set start to a value if the first element is None:
",17
31256159,31256466,2,"
OUT:
",2
31256159,31256159,1,"Given
I'd like
Currently I have brute forced it with:
Finally, I'd like to get to
by running this right to left. Currently I have
I'm not fussy about inplace or create a new list, but right now this smells to me. I can't see a way to store a temporary 'last' value and use map/lambda, and nothing else is coming to mind.
",75
31256174,31265221,2,"django-lazysignup, which you are using, allows you to deliver a custom LazyUser class (here). All you need to do is to write a subclass of lazysignup.models.LazyUser with defined is_authenticated method and set it as settings.LAZYSIGNUP_USER_MODEL.
But this is not the end of your troubles. Lots of django apps 
assume that authenticated user has some properties. Mainly, is_staff, is_superuser, permissions, groups. First and foremost, django.contrib.admin needs them to check if it can let the user in and what to show him. Look how django.contrib.auth.models.AnonymousUser mocks them and copy it. Remark: look how AnonymousUser is not subclassing any user class nor db.Model. Supplied user class only needs to quack.
",126
31256174,32236034,2,"Ended up just having to replace all calls to User.is_authenticated().
To prevent django-allauth from redirecting lazy-users from the login page, this ended up looking something like this:
Where user_has_account() was my own method for checking whether the user was actually signed in.
",50
31256174,31256174,1,"I installed django-lazysignup and am facing the challenge now of User.is_authenticated() returning True, for what are not actually authenticated users, but instead lazy-signup users.  I can update any checks for User.is_authenticated() in my code with my own function.  However, other packages like django-allauth, check this method to decide whether a user is already signed-in, and redirect from attempts to reach the login page.
Are there any recommendations that don't require replacing is_authenticated() in every package that I include?  Seems most of them expect it to function a particular way, and django-lazysignup turns that on its head.  Could I monkey patch the User model with a new is_authenticated() method?  If it's possible, where would I do this so that it's attached to the page request?
",149
31256238,31257573,2,"If your session.execute writes were not successful (they did not meet the required consistency level), then the driver will raise one of the following exceptions:
Unavailable - There were not enough live replicas to satisfy the requested consistency level, so the coordinator node immediately failed the request without forwarding it to any replicas.
Timeout - Replicas did not respond to the coordinator before cassandra timeout.
Write timeout - Replicas did not respond to the coordinator before the write timeout. Configured in cassandra.yaml. There is a similar timeout for reads, read and write timeouts are configured separately in the yaml.
Operation timeout - Operation took longer than the specified client side timeout. Configure in your application code.
You can try tracing your queries and find out what exactly happened for each write. This will show you the coordinators and the replica nodes involved in the operation and how much time the request spent in each.
",169
31256238,31256238,1,"Solved
I was testing update on 3 nodes, and the time on one of those nodes was 1 second behind, so when update a row, the write time is always behind the timestamp, cassandra would not update the rows. I sync all nodes time, and the issue fixed.
Edit:
I double checked the result, all insertions are succeed, partial updates failed. There's no error/exception messages
I have a cassandra cluster(Cassandra 2.0.13) which contains 5 nodes. Using python(2.6.6) cassandra driver(2.6.0c2) for inserting data into database.  my server systems are Centos6.X
The following code is how i connect to cassandra and get session.  I provided at most 2 nodes ip addresses, and select the keyspace.
For each row, I have 17 columns and if the key does not exist in database, I will use session insert key with the rest columns default values, and then update specific column's value.
the following is how I invoke them:
I was trying to insert 100 rows and update each row's columnX, but only partial of those 100 rows can be updated, the rest rows columnX are the default values.insert_initial_row has been invoked and initialized default values for all 100 lines, but the update_columnX does not. Event I change the consistency level to Quorum, it doesnt help at all.  ""not initialized correctly..."" never printed out, and I added a print line in update_columnX and the line is printed 100 time, so it is invoked 100 times, but not all of them updated. 
Any idea? Please help.
Thanks
",299
31256252,31257909,2,"np.linalg.solve(A, b) does not compute the inverse of A. Instead it calls one of the gesv LAPACK routines, which first factorizes A using LU decomposition, then solves for x using forward and backward substitution (see here).
np.linalg.inv uses the same method to compute the inverse of A by solving for A-1 in A·A-1 = I where I is the identity*. The factorization step is exactly the same as above, but it takes more floating point operations to solve for A-1 (an n×n matrix) than for x (an n-long vector). Additionally, if you then wanted to obtain x via the identity A-1·b = x then the extra matrix multiplication would incur yet more floating point operations, and therefore slower performance and more numerical error.
There's no need for the intermediate step of computing A-1 - it is  faster and more accurate to obtain x directly.
* The relevant bit of source for inv is here. Unfortunately it's a bit tricky to understand since it's templated C. The important thing to note is that an identity matrix is being passed to the LAPACK solver as parameter B.
",211
31256252,31256252,1,"I do not quite understand why numpy.linalg.solve() gives the more precise answer, whereas numpy.linalg.inv() breaks down somewhat, giving (what I believe are) estimates. 
For a concrete example, I am solving the equation C^{-1} * d  where C denotes a matrix, and d is a vector-array. For the sake of discussion, the dimensions of C are shape (1000,1000) and d is shape (1,1000). 
numpy.linalg.solve(A, b) solves the equation A*x=b for x, i.e. x = A^{-1} * b. Therefore, I could either solve this equation by
(1)
or (2)
Method (2) gives far more precise results. Why is this?  
What exactly is happening such that one ""works better"" than the other? 
",152
31256269,31257236,2,"[Updated answer]
The problem comes in when the 'request' object's url is used by webapp2:
In this case you are overriding the RequestHandler's 'self.request' attribute with your own (to call GitHub):
I surmise that there is no 'url' on this new request object.
I suggest that you use a different variable name, or don't store the Github request on 'self'.
=======================
[Old answer]
Going out on a limb here based on the given info, but is your handler class extending webapp2.RequestHandler? If not the url attribute may not exist in 'self'.
Please include your handler class definition and your (minimal) handler method if this is not the case.
ie:
",135
31256269,31256269,1,"I'm writing a small web app on AppEngine/Python for accessing the GitHub API. 
I've successfully performed the Oauth2 flow (ie. I can access the logged user info). What I'd like to do now is that as the user goes back to my webpage uri which I specified as the redirect_uri for GitHub, having authorized the app, I make the request for obtaining the access token and then redirect the user to the homepage. 
Now, if I perform the redirect with self.redirect(""/"") at the end of the handler for the redirect_uri, Python trows the error AttributeError: url. 
What am I doing wrong? 
Here's the class definition of the handler for redirect_uri
Here's the full stack trace
",138
31256286,31256286,1,"I have a complex document that I am trying to structure most conveniently and efficiently with JSON in Python.  I would like to be able to retrieve one of the items in my document with one line (i.e. not via a for loop)
A demo of the structure looks like this:
I am trying to be able to easily fetch a movie based on its id field.  To do this, right now, I have made the index the same as the key to the movies object.   So when I json.load the object to find movie 1's name I can just do movie[(id)]['name']
It seems like I should have a list of movies in the json file but it also seems like that would be more complicated. It could look like this:
but if that were the case I would have to loop through the entire array like this:
Is there a more effiecient way of doing this?
",180
31256286,31256350,2,"Let 'movies' be a dict and not a list:
and you can access movie by id with yourjson['movies'][str(id)]
",30
31256360,31256360,1,"So, recently, I've been experimenting with the multiprocessing module. I wrote this script to test it:
However, when I try to run it, it throws this error:
...And I can't tell what's going on.
Does anyone know what happened, and how I can fix it?
",59
31256360,31256426,2,"Pass arguments to the function that is ran by a Process is done differently - looking at the documentation it shows:
Or in your case:
Addition:
Good catch by Luke (in the comments below) - you're overriding the function a with the variable name a when doing:
You should use a different name.
",61
31256397,31256397,1,"I have data of the following form:
Now whenever two lines are equal, like: _:question1 <http#responseCode> ""200""^^<http://integer> ., then I want to delete the equal lines (lines which match with each other character by character are equal lines) along with (i). the subsequent line (which ends with a fullstop) (ii). line previous to the equal line (which begins with #@).
Now one way to do this is to store all these lines in a set in python and whenever two lines are equal (i.e. they match character by character) the previous and subsequent two lines are deleted. However, the size of my dataset is 100GB (and I have RAM of size 64GB), therefore I can not keep this information in set form in main-memory. Is there some way by which I can delete the duplicate lines along with their previous and subsequent two lines in python with limited main-memory space (RAM size 64 GB)
",197
31256397,31256671,2,"Keep a boolean hashtable of hash codes of lines already seen.
For each line:
if line hash()es to something you have already seen, you have a potential match: scan the file to check if it really is a duplicate.
if line hash()es to a new hash, just mark the hash for the first time.
Dedicate as much memory you can to this hashtable, and the false positive rate will be low (i.e. less times you will have to scan for duplicates and found none).
Example:
As it has been pointed out, since you cannot store all the lines in memory you don't have many options, but at least this option doesn't require to scan the file for every single line, because most of the entries in the table will be False, i.e. the algorithm is sub-quadratic if the tabe is not full; it will degenerate to O(n2) once the table is full.
You can make a very memory-efficient implementation of the hash table, that requires only 1 bit for each hash code (e.g. make it an array of bytes, where each byte can store 8 boolean values)
See also Bloom Filters for more advanced techniques.
",233
31256397,31256687,2,"One fairly straightforward way - make a version of your data such that each line includes a field with its line number. Use unix 'sort' to sort that new file, excluding the line number field. The sort utility will merge sort the file even if it exceeds the size of available memory. Now you have a new file in which the duplicates are ordered, along with their original line numbers. Extract the line numbers of the duplicates and then use that as input for linearly processing your original data. 
In more detailed steps. 
Make a new version of your file such that each line is prepended by its line number. So, ""someline"" becomes ""1, someline"" 
sort this file using the unix sort utility  - sort -t"","" -k2,2 file
Scan the new file for consecutive duplicate entries in the second field
the line numbers (first field) of such entries are the line numbers of duplicate lines in your original file - extract these and use them as input to remove duplicates in the original data. Since you know exactly where they are, you need not read in the entire file or create a giant in-memory structure for duplicates
The advantage of this method compared to some of the others suggested - it always works, regardless of the size of the input and the size of your available memory and it does not fail due to hash collisions or other probabilistic artifacts. You are leveraging the merge sort in unix sort where the hard stuff - dealing with larger-than-memory input - 
 has been done for you. 
",290
31256397,31257069,2,"Here's an outline of how I'd do it using UNIX sort/uniq:
Modify the data format so that each record is a single line. You could do this using the methods here.
Sort the data with the sort command. Note the you can specify which fields are important with the --key option, you might need to exclude the #@ <abc> part by selecting all the other fields as keys (I wasn't entirely sure from your description).
Apply the uniq command to the sorted output to get only the unique lines.
This should all work fine on out-of-core data as far as I know.
",119
31256444,31256444,1,"My goal: to extract all of the transcripts in this url, and clean them for my particular use.
I need to recursively extract links which follow a pattern. I am a newbie and am having trouble formulating the full code that will work.
Here are some examples of how the URLs will look:
so all begin with http://tvmegasite.net/transcripts, then the show abbreviation, then main or older, etc.
What I've tried so far:
Getting urls from a particular page is easy with BeautifulSoup but I haven't figured out how to do it recursively. I was thinking of just using a scraper like Scrapy to get all the urls starting from tvmegasite.net/transcripts, and then using the re package to search for ones that match the pattern. I'm still not sure how to make this into a full code.
From what I can guess, these are possibly the kinds of regular expressions that can work:
",174
31256444,31261611,2,"If you use Scrapy you do not need regular expressions -- or at least you can limit them to a minimum. For example with the LxmlLinkExtractor you can set up which URLs to follow (allow) and in which XPath-branch (restrict_xpaths).
And you can use your regular expressions (which look fine for me at first glance) in in the allow restriction -- and for this site you do not need a restriction on XPath.
",82
31256450,31270049,2,"For oldest task you should probably use created_at rather than modified_at (the oldest task could have been modified recently, after all).
We don't have any way in the API to specifically get the older task, or order by creation time or anything like that, so for now your workaround is in fact the only way to do it. You may be able to make it a little faster by using ?opt_fields=created_at in the task query to cut down on how much data you're loading.
",95
31256450,31256450,1,"So I have a little bit of a dilemma in trying to obtain the oldest task of a user. I’m able to get the oldest task of each user in my team's workspace, but takes 1 hour and 30 minutes to run.
Here's the current workflow that I have currently:
What's taking so long is the loading of and iterating through each user's tasks. Any ideas on how to make this process faster? Would there potentially be a way to query specifically for the oldest task of a user through the API?
I’m building a tool that will track the age of the oldest task for each user for tracking purposes. I would love to find out if there’s something that I’m missing.
",144
31256469,31256469,1,"I am trying to locate a visible element that will change based on what the user enters on the website. I am successful if use the follow with a static xpath search string:
Default-Test will change arbitrarily and i have how to get this value but I have not been successful using a variable in the xpath search:
Test method 1 Does not work
Test method 2 does not work, this is the actual method for locating the value
",83
31256469,31329850,2,"This is the solution to the issue I was encountering.  Tried reworking the string and the related escapes '\' but was not successful. Did however get the following working. I am not clear why this worked.
",41
31256484,31256484,1,"I am looking for a pythonic way to query something like this:
I'd like to get a list of district (or pks) like what you get when use values_list method, but making the query to User Model. I've tried this:
But nothing works, I am using django 1.7 (depth parameter is not longer available for select_related method). Thanks in advance.
",73
31256484,31256581,2,"Queries across relationships don't use the _set syntax. Try this:
",13
31256484,31257996,2,"You could try to execute the query like this:
And if you need to query from an especified model instance:
Also if you are using User.objects.all().values('user_profile_set__district__name') and getting this output:
It might mean that your save method is not working correctly or at least for the name field, you might want to use the shell and query your District objects to verify if they have a name.
",79
31256583,31256583,1,"After the installation of motionless library, I try to run my code and the following error  message occurs. 
",20
31256583,31256661,2,"Motionless hasn't been updated since 06/08/2010 according to the PyPi Package Index.
I've downloaded it and get the same error immediately just by running:
It's also not flagged as being Python 3.4 compatible in PyPi; if you are running the latest version of Python this is likely the issue.  Have you tried running it with Python 2.7 instead?
Edit: Looking at the Python 2.7 docs; it states here that != and <> are equvilent, however <> is deprecated.  In the Python 3.4 docs it states here that only != is supported, no mention of <> so I imagine it's been removed.
You could try instead:
Raising an issue on the GitHub Repo; the author may still be updating the library and not know it's incompatible with Python 3.4
Checking out the code yourself from the GitHub Repo and manually fixing the problem (Check out 2to3 for automatically doing this.  It will convert all <> usage to != for you)
",187
31257446,31257446,1,"I'm using idle in OS X, and I'm trying to parse a file with .data extension:
but this prints an empty array:  []
I also tried Sublime but also doesn't work.  What is another method to use in Python to read this?
",51
31257446,31257483,2,"When opening a file just by filename, Python by default will first look in the current working directory.  If you're using IDLE, this may not actually be the same directory in which you're .py file is.  Try running your script from the command line in the same directory as your ""*.data"" file.
",61
31257446,31257464,2,"open it in binary mode and print the repr of its contents instead 
if this gives an empty string than you indeed have an empty file ...
im guessing ls -l tells you that the file is 0 bytes big?
",41
31257446,31257455,2,"instead of .readlines() you can cast a file pointer to a list which automatically gets all the lines. So you can do this:
or this:
",30
31257457,31257521,2,"How about this:
output is:
",7
31257457,31257457,1,"This is a noob question asked by a noob. I need to create a function to check whether or no all the character in the string can be found in 'scraps', if true, just print out the whole string once, else, print the error message below. I am sure the real solution is pretty simple but at the moment I feel like I am solving calculus problem with grade school math. please advise. thanks~
def fix_it(scraps, recycled):
The function should be able to produce:
print fix_it('AbCdEfG', 'AhK') ==> ""Give me something that's not useless next time.""
print fix_it('AbCdEfG', 'CdE') ==> 'CdE'
",135
31257478,31257478,1,"I'm going through Miguel Grinbergs Flask book and at one point he uses the following line:
I know [:5] is a slice operation, but I'm not sure why it is being used here.  What does the list consist of that we only want elements 0-5?
",53
31257478,31257977,2,"
What does the list consist of that we only want elements 0-5?
To be precise, request.endpoint is not a list, it's a string. And it doesn't matter what the rest of it contains, the code is only concerned with it beginning with 'auth.':
I don't have much experience with Flask so I can't specify what possible auth.* values exist, but it's probably values like auth.username and auth.password.
If you're curious about what value it contains, you can add a debugging breakpoint to the code and inspect it:
Then run and test the code. When it hits that point, it'll pause execution and give you a pdb shell, which will let you look at the request object and its endpoint attribute.
",145
31257489,31257489,1,"What's going on
I'm collecting data from a few thousand network devices every few minutes in Python 2.7.8 via package netsnmp. I'm also using fastsnmpy so that I can access the (more efficient) Net-SNMP command snmpbulkwalk.
I'm trying to cut down how much memory my script uses. I'm running three instances of the same script which sleeps for two minutes before re-querying all devices for data we want. When I created the original script in bash they would use less than 500MB when active simultaneously. As I've converted this over to Python, however, each instance hogs 4GB each which indicates (to me) that my data structures need to be managed more efficiently. Even when idle they're consuming a total of 4GB.
Code Activity
My script begins with creating a list where I open a file and append the hostname of our target devices as separate values. These usually contain 80 to 1200 names.
From there I set up the SNMP sessions and execute the requests
Because of how both SNMP packages behave, the device responses are parsed up into lists and stored into one giant data structure. For example,
I'm having to iterate through each response, combine related data, then output each device's complete response. This is a bit difficult For example,
Each device has a varying number of responses. I can't loop through expecting every device having a uniform arbitrary number of values to combine into a string to write out to a CSV.
How I'm handling the data
I believe it is here where I'm consuming a lot of memory but I cannot resolve how to simplify the process while simultaneously removing visited data.
Currently I'm creating a new dictionary, checking that the device response is not null, then appending the response value to a string that will be used to write out to the CSV file.
The problem with this is that I'm essentially cloning the existing dictionary, meaning I'm using twice as much system memory. I'd like to remove values that I've visited in expandresults when I move them to expandarrays so that I'm not using so much RAM. Is there an efficient method of doing this? Is there also a better way of reducing the complexity of my code so that it's easier to follow?
The Culprit
Thanks to those who answered. For those in the future that stumble across this thread due to experiencing similar issues: the fastsnmpy package is the culprit behind the large use of system memory. The multiwalk() function creates a thread for each host but does so all at once rather than putting some kind of upper limit. Since each instance of my script would handle up to 1200 devices that meant 1200 threads were instantiated and queued within just a few seconds. Using the bulkwalk() function was slower but still fast enough to suit my needs. The difference between the two was 4GB vs 250MB (of system memory use).
",556
31257489,34982060,2,"The memory consumption was due to instantiation of several workers in an unbound manner.
I've updated fastsnmpy (latest is version 1.2.1 ) and uploaded it to
  PyPi.  You can do a search from PyPi for 'fastsnmpy', or grab it
  directly from my PyPi page here at FastSNMPy
Just finished updating the docs, and posted them to the project page at fastSNMPy DOCS
What I basically did here is to replace the earlier model of unbound-workers with a process-pool from multiprocessing. This can be passed in as an argument, or defaults to 1.
You now have just 2 methods for simplicity. 
snmpwalk(processes=n) and snmpbulkwalk(processes=n)
You shouldn't see the memory issue anymore. If you do, please ping me on github.
",139
31257489,31257791,2,"You might have an easier time figuring out where the memory is going by using a profiler: 
https://pypi.python.org/pypi/memory_profiler
Additionally, if you're already already tweaking the fastsnmpy classes, you can just change the implementation to do the dictionary based results merging for you instead of letting it construct a gigantic list first. 
How long are you hanging on to the session? The result list will grow indefinitely if you reuse it. 
",79
31257489,31257738,2,"If the device responses are in order and are grouped together by host, then you don't need a dictionary, just three lists:
",26
31260814,31260814,1,"How would I go about writing a recursive function that consumes two lists and then produces a list with the elements that are in both lists? If both lists have a number twice in each, the produced list will also have that number in it twice.
This is what I have so far:
def merge(L1, L2, i, j, R):
        if L1[i] == L2[j]:
                R.append(L1[i])
                R.append(L2[j])
                merge(L1, L2, i, j+1, R)
        else:
                merge(L1, L2, i+1, j, R)
def sorted_intersection(lst1, lst2):
        R = []
        return merge(lst1, lst2, lst1[0], lst2[0], R)
Nvm, figured out the code. Thanks for all the help!
",166
31260814,31262754,2,"Recursive sounds like a non-pythonistic way of doing this. What about something as simple as:
produces: c = [1, 2, 3, 2, 3, 4]
",34
31260814,31260954,2,"Please see the documentation of the module collections, which contains helpers for many functional tasks:
In here a and b are both iterated once.
",27
31260814,31260886,2,"Why recursive? 
",3
31260899,31260966,2,"
You never change integer1, so if the condition is true, it is always true and it loops forever.
Assign to integer1 rather than replace1. Similarly for the second.
",33
31260899,31260899,1,"
",0
31260899,31260959,2,"I think this what you expected:
Since the value of integer1 is not changed in first while loop it will act as a infinite loop
Instead of eval you could use int() since there are some harmful effect on using eval stick with int
Modified:
",49
31260915,31260915,1,"
I am reading the feature data from the out.csv file and storing in the x variable and i am reading the target data from the out.csv file and storing in the y variable 
after running this program i am getting the error called
X has 1 samples, but y has 10.
can any one provide me the solution for this error
",62
31260915,31261081,2,"Possible Duplicate : 
sklearn error: ""X and y have incompatible shapes.""
The above link answers your question.
",22
31260935,31261004,2,"sys.exc_info returns the tuple of 3 elements, where the third is the traceback.
The returned tuple is like - (type, value, traceback) .
You are doing - str(sys.exc_info()[0:2]) which only selects first two elements.
Try -
If you cannot use the traceback module to format the traceback. And if you just want the exception's line number and filename, you can use the following -
Please note these can be internal names, and best is to use traceback module.
",99
31260935,31260935,1,"In our production code, we log errors like this:
But it only allows to see this kind of info about the error:
Which is not enough - I want to see line number and name of the file with code. However, I could get that info with this code:
But we DO NOT use traceback module in production because it is considered too heavy. So how can I get line number and filename without using traceback?
",84
31260982,31260982,1,"I need to run a program from sudo privileges and I am running .py file inside the program. So when my program runs from sudo then it automatically calls/runs the .py file with sudo. I am running file like ""./myFile.py arg1 arg2""
My requirement is that my program need to be run from sudo but when I runs/calls .py file inside program then it should runs as normal privileges ( without sudo).
If anybody has idea then pleas let me know.
Thanks.
",90
31260982,31261059,2,"Instead of running the python file directly, run it as another user
",13
31260988,31261439,2,"When Python executes a script file, the whole file is parsed first. You can notice that when you introduce a syntax error somewhere: Regardless of where it is, it will prevent any line from executing.
So since Python parses the file first, literals can be loaded effectively into the memory. Since Python knows that these are constant, all variables that represent those constant values can point to the same object in memory. So the object is shared.
This works for ints and floats, but even for strings; even when there is a constant expression that needs to be evaluated first:
Now in IDLE, the behavior is very different: As an interactive interpreter, IDLE executes every line separately. So a = 1.1 and b = 1.1 are executed in separated contexts which makes it impossible (or just very hard) to figure out that they both share the same constant literal value and could share the memory. So instead, the interpreter will allocate two different objects, which causes the identity check using is to fail.
For small integers, the situation is a bit different. Because they are often used, CPython stores a set of integers (in the range between -5 and 256) statically and makes that every value of these points to the same int object. Thatâ€™s why you get a different result for small integers than for any other object. See also the following questions:
""is"" operator behaves unexpectedly with integers
What's with the Integer Cache inside Python?
",282
31260988,31260988,1,"In the python idle:
But when I put the code in a script and run it, I will get a different result:
Why did this happen? I know that is compares the id of two objects, so why the ids of two objects are same/unique in python script/idle?
I also found that, if I use a small int, for example 1, instead of 1.1, the result will be the same in both the python script and python idle. Why did small int and small float have different behavior?
I am using CPython 2.7.5.
",106
31261047,31261170,2,"It's not necessary to escape the space character.
",10
31261047,31261079,2,"there is no need to escape the space character.
",10
31261047,31261047,1,"I'm trying to delete a file which contains a space in its name. I'm using a Mac, and escaping the space character. However it stills throws an error.
Here's a screenshot,
How do I fix this?
",45
31261068,31261237,2,"Here every item in item_to_be_found list is a Tag type object so you can get the string inside <loc> tag using .text or .string on them. Though .text and .string have differences both will work same in this case.
this will give you a result like
",50
31261068,31261068,1,"I am crawling a sitemap.xml and my objective is to find all the url's and the incremental count of them.
Below is the structure of the xml
Below is my code
My output is like this
Need the output as links without loc and /loc. Have tried replace command but that is throwing an error.
",59
31261123,31261123,1,"I have a file that I am currently reading from using
Then by doing
I basically copy the file to a new one, but also add some text at the end of the newly created file. How would I be able to insert that line say, in between two lines separated by an empty line? I.e:
Can I tokenize different sentences by a delimiter like \n for new line?
",75
31261123,31261196,2,"First you should load the file using the open() method and then apply the .readlines() method, which splits on ""\n"" and returns a list, then you update the list of strings by inserting a new string in between the list, then simply write the contents of the list to the new file using the new_file.write(""\n"".join(updated_list))
NOTE: This method will only work for files which can be loaded in the memory.
",90
31261123,31261375,2,"For Large file
",3
31261123,31264759,2,"readlines() is not recommended because it reads the whole file into memory. It is also not needed because you can iterate over the file directly.
The following code will insert Hello at line 2 at line 2
Note the use of the with open('filename','w') as filevar idiom. This removes the need for an explicit close() because it closes the file automatically at the end of the block, and better, it does this  even if there is an exception.
",95
31261376,31261376,1,"How to read the twitter.avro files in pyspark and extract the values from it? 
rdd=sc.textFile(""twitter.asvc"") is working good
But when I do
I am getting output below 
['Obj\x01\x02\x16avro.schema\x04{""type"":""record"",""name"":""episodes"",""namespace"":""testing.hive.avro.serde"",""fields"":[{""name"":""title"",""type"":""string"",""doc"":""episode
  title""},{""name"":""air_date"",""type"":""string"",""doc"":""initial
  date""},{""name"":""doctor"",""type"":""int"",""doc"":""main actor playing the
  Doctor in episode""}]}\x00kR\x03LS\x17m|]Z^{0\x10\x04""The Eleventh
  Hour\x183 April 2010\x16""The Doctor\'s Wife\x1614 May 2011\x16&Horror
  of Fang Rock 3 September 1977\x08$An Unearthly Child 23 November
  1963\x02*The Mysterious Planet 6 September 1986\x0c\x08Rose\x1a26
  March 2005\x12.The Power of the Daleks\x1e5 November
  1966\x04\x14Castrolava\x1c4 January 1982', 'kR\x03LS\x17m|]Z^{0']
Is there a python library for reading this format?
",219
31261376,31263508,2,"You should use a FileInputFormat specific for Avro files. 
Unfortunately I am not using python so I can only link you to a solution. You can look into that: https://github.com/apache/spark/blob/master/examples/src/main/python/avro_inputformat.py
The most interesting part is this one:
",43
31261448,31262473,2,"try this once:
Try this:
",7
31261448,31261448,1,"
So here is the html for a website I am trying to use selenium on. It is a dropdown menu that lets you choose a class, and when it is selected, the website will go to that class. The url does not change, and all that changes is the spot of the selected=""selected"". How do I use selenium to change which one is selected? I want to open the menu first and somehow click on each class in order so that I can see the info on the classes. But, I don't want to use the name of the classes or their values, so that this process can be repeated for other classes.
",128
31261477,31262013,2,"I think what you are trying to achieve is something like the following:
file1.py
file2.py
Running file1.py would give:
As mentioned in the comments, file2.py really only needs to contain the following for it to work though:
file2.py
Tested in Python 2.7
",46
31261477,31261477,1,"I have a hard-coded list that I have to use across many Python files.
I tried doing something like(Say the list is in xyz.py)
How can I access it in other Python files?
",38
31261528,31936248,2,"What worked for me is as follows:
Install python-psutil: sudo apt-get install python-psutil. If you
have a previous installation of the psutil module from other
method, for example through source or easy_install, remove it first.
Run pyinstaller as you do, without the hidden-import option.
",52
31261528,31313124,2,"pyinstall is hard to configure, the cx_freeze maybe better, both support windows (you can download the exe directly) and linux. Provide the example.py, In windows, suppose you have install python in the default path (C:\\Python27):
the cxfreeze is a python script, you should run it with python, then the build files are under some_path (with a lot of xxx.pyd and xxx.dll).
In Linux, just run:
and also output a lot of files(xxx.so) under some_path.
The defect of cx_freeze is it would not wrap all libraries to target dir, this means you have to test your build under different environments. If any library missing, just copy them to target dir. A exception case is, for example, if your build your python under Centos 6, but when running under Centos 7, the missing of libc.so.6 will throw, you should compile your python both under Centos 7 and Centos 6. 
",180
31261528,31261528,1,"I want to compile my python code to binary by using pyinstaller, but the hidden import block me. For example, the following code import psutil and print the CPU count:
And I compile the code:
When I run the output under dist:
Then I tried:
Still the same error. I have read the pyinstall manual, but I still don't know how to use the hidden import. Is there a detailed example for this? Or at least a example to compile and run my example.py?
ENVs:
OS: Ubuntu 14.04
Python: 2.7.6
pyinstaller: 2.1
",109
31261528,31611824,2,"Hi hope you're still looking for an answer. Here is how I solved it:
add a file called hook-psutil.py
And then call pyinstaller --additional-hooks-dir=(the dir contain the above script) script.py
",37
31261543,31288272,2,"I found, the foreign key constraint in the column must be removed
",13
31261543,31261543,1,"With SQLAlchemy, I want inherit a model with more than one primary keys, example:
The inherit work with only one primary key column, and this not really my use case. I have two String primary key column and I use the polymorphic pattern of SQLAlchemy. I don't write all my code, the issue is the same with or without polymorphic configuration.
This code works with sqlite, but not with postgres, the __table_args__ is useless here, What do I miss ?
regards
",93
31261600,31262642,2,"This is by the way a terrible idea, since you already have begun using gitpython, and I have never tried working with that, but I just really want to let you know, that you can do it without cloning it in local, without using gitpython.
Simply run the git command, in a shell, using subprocess..
running bash commands in python
edit: added some demonstration code, of reading stdout and writing stdin.
some of this is stolen from here:
http://eyalarubas.com/python-subproc-nonblock.html
The rest is a small demo..
first two prerequisites
shell.py
nbstreamreader.py:
then the actual code:
",110
31261600,31261600,1,"I am trying to archive a remote git repo using Python code. I did it successfully using Git command line with following command.
This command fetches the required file from the repo and stores the zip in my current working directory. Note that there is no cloning of remote repo happening.
Now I have to do it using Python code. I am using GitPython 1.0.1. I guess if it is doable using command line then it should be doable using GitPython library. According to the docs, 
Above line of code will archive the repo. Here repo is the instance of Repo class. It can be initialized using
If I give path to my remote repo(Ex. ssh://path/to/my/repo) in above line it goes to find it in directory where the .py file containing this code is residing(Like, Path\to\python\file\ssh:\path\to\my\repo), which is not what I want. So to sum up I can archive a local repo but not a remote one using GitPython. I may be able to archive remote repo if I am able to create a repo instance pointing to the remote repo. I am very new to Git and Python.
Is there any way to archive a remote repo using Python code without cloning it in local?
",234
31261664,31262622,2,"We can't really diagnose this if you don't tell us what it does return. If you need any sort of login to access it, it won't work because Python doesn't have your browser cookies. Python also won't automatically follow some types of redirects and anything Javascript-dependant is right out of the question. Those would be the first things to check for.
",71
31261664,31261664,1,"I want to capture a file that is downloaded when a certain url is passed in python. The problem is that the downloaded file is NOT returned by the server. The file gets downloaded when I pass the same url in a browser, but not when I do so via urllib2.urlopen(). Is there a way to capture this seemingly triggered side effect in python ? This is what I have so far.
",79
31262702,31262792,2,"y is known only in the scope of the function fpol. You should assign the result to a variable, and only then print its value:
Note that y is a different variable here, it has nothing to do with the y inside the function. You could write:
",53
31262702,31262804,2,"y is out of scope. It was only in scope for your function call, and since the function fpol has ran and ended, the scope has died with it. We need to assign a variable that's visible to the print command. Let's reuse y for simplicity.
The key rule of thumb for python is every time you indent you have started a new scope! You must make sure your variables are in scope to use them.
",86
31262702,31262830,2,"y is a variable with scope that's local to function fpol().
it is not defined outside of that scope.
The code return y does not make y visible outside of the function in which it has been defined. It only returns the value of y to the caller as a function return value.
",60
31262702,31262854,2,"Using your example, the following would show you the value of y
But trying to print y after your function call will fail with the not defined error as the variable is defined only inside you function, i.e. it is local to that function. This is often referred to as the scope of a variable.
As soon as the function returns, y ceases to exist.
",72
31262702,31262858,2,"y is not defined outside th function
you can not do print(y)
probably you want
y = fpol(4)
print(y)
is not very good programming style, but you also can make y global variable. then it will be available after function, but pls do not do it.
finally you can do just 
return y
no need ()
",70
31262702,31262886,2,"I suspect you are trying to print(y) outside the function. The variable y is local in scope, that is only defined within fpol(). So you can print it there. You can do:,
But not:
",46
31262702,31262794,2,"The variable y is only visible from within the function you have declared. To print the result of fpol(4) you can assign the returned value to a new variable:
",34
31262702,31262702,1,"I've been having some problems with the return statement and I can't seem to figure out what's wrong. It seemed to work fine yesterday, but today no function that contains it seems to work properly. Here's an example of what's going on:
If I then type in
I'm given the answer 256 (as I would expect). If I then type in
or try to use/view y in any way, I'm told
I've also tried it with return(y) being replaced by return y . I can also insert print(y) into the original function and that works fine, so I know that during the function, y actually does have a value, it's just not being returned. Any help is much appreciated.
Edit: I've now been able to work past the issue I had with the return function. Thanks to everyone who responded.
",172
31262702,31262790,2,"I guess you need to store the value returned by the function in a variable and then print it:
",20
31266550,31266738,2,"Here's a regular expression that will find which (if any) of your title keywords appear in c_raw:
The format of (?:Dresses) etc., just means match the letters inside the parenthesis in the order they appear together but don't save them as a regex group.
As to what c_raw[0] is doing, basically c_raw is a string. In python, you can treat strings as if they are arrays of characters. So, c_raw[0] is saying, give me the 0th index of the array c_raw -- i.e. get the first character of c_raw
",113
31266550,31266550,1,"I'm new to Python AND Regex and am a little confused. I want to search through a webpage title for three different terms and if the title matches one I want that printed out (I'm using scrapy so it's printing it for each item). I'm unsure how we get it to search through the three terms? Eg if title has ""Dresses|Wallets|Pumps"" print it out.
For now I just got it to print the first term in the title 
I tried
but some of the c's printed None. I'm also unsure of what c_raw[0] is doing? What does the 0 do?
Any help would be appreciated!
Edit: While the below answer helped - adding brackets around Dresses|Wallets|Pumps also worked
",140
31266550,31266918,2,"try this demo from scrapy shell,
I have made an input html to selector object since you haven't provide any specific start-url
",24
31266658,31266658,1,"I use django_celery with connecting to Amazon Redshift. To migrate database, after ""makemigrations"" I used command ""python manage.py migrate"" and error message show up as shown below. 
The reason is Redshift does not support data type 'serial' but the 'django_migrations' table that contain 'serial' type is automatically created. 
How to stop Django Migrations create this table or avoid using serial on 'django_migrations' table.
",76
31266658,43531445,2,"Are you trying to use Redshift as the backend database for your web application? That's a bad idea, Redshift is a data warehouse and as such individual query performance and latency are far from great, not to mention that Redshift doesn't enforce primary keys, which almost surely Django expects. 
My recommendation, use PostgreSQL.
",62
31266687,31285962,2,"if your ticker file is large
(judging by the path name, it may be)
using the csv reader is a waste of time.
it does not suport seek, so the only way you can get to the last line is 
since after this ""row"" will contain the last row in the file..
you can see here: Most efficient way to search the last x lines of a file in python
that it's possible to retrieve only the last lines in the file, and then parse it with the csv module afterward..
This will save some computation time..
",107
31266687,31266687,1,"I am running this code below using multiprocessing to run ticker_list through a request and parsing program faster.  The following code works, but it is very slow.  I am not so sure that this is the correct usage of multiprocessing.  If there is a more efficient way to do this then please let me know.
",60
31266760,31266827,2,"in python pickle refers to a module that provides (a specific) serialization of python objects.
serialization itself is a more general term. python objects can also be serialized into json for example.
https://en.wikipedia.org/wiki/Serialization
",40
31266760,31267072,2,"You are misreading the article. Pickling and serialisation are not synonymous, nor does the text claim them to be.
Paraphrasing slighly, the text says this:
This module implements an algorithm for turning an object into a series of bytes. This process is also called serializing the object.
I removed the module name, pickle, deliberately. The module implements a process, an algorithm, and that process is commonly known as serialisation.
There are other implementations of that process. You could use JSON or XML to serialise data to text. There is also the marshal module. Other languages have other serialization formats; the R language has one, so does Java. Etc.
See the WikiPedia article on the subject:
In computer science, in the context of data storage, serialization is the process of translating data structures or object state into a format that can be stored (for example, in a file or memory buffer, or transmitted across a network connection link) and reconstructed later in the same or another computer environment.
Python picked the name pickle because it modelled the process on how this was handled in Modula-3, where it was also called pickling. See Pickles: Why are they called that?
",229
31266760,31266760,1,"I have came across these two terms so often while reading about python objects. However, there is a confusion between pickling and serialization since at one place I read
The pickle module implements an algorithm for turning an arbitrary
  Python object into a series of bytes. This process is also called
  serializing” the object.
If serializing and pickling is same process, why use different terms for them?
",74
31266768,31267980,2,"I would take a look at itertools. 
https://docs.python.org/3.5/library/itertools.html#itertools.permutations
",13
31266768,31266768,1,"Lets say I have two lists of lists and I wand to build various permutations from it. Look at these two lists:
The two lists represent mappings, that I already have. So I know some parts of my permutation but not all of them. As I see it, there are only four permutations possible:
After thinking about it I have the following idea. I would generate all four possibilities of l1 (being a tuple in the end) make l2 into a tuple, do a dictionary zip, order that dictionary and convert these four dicts into a list of tuples. Sound reasonable?
I have a hard time generating the four tuples. This is what I have so far.
Which prints [(0, 8, 8, 0, 4, 6, 7, 7, 6, 1, 3, 5, 2)]
How do I generate another tuple when I get to an entry in my list where more than one permutation of that list entry is possible?
I also know that print(list(itertools.product(*l1))) does almost what I want but not quite. It yields [(0, 4, 6, 1, 3, 5, 2), (0, 4, 7, 1, 3, 5, 2), (8, 4, 6, 1, 3, 5, 2), (8, 4, 7, 1, 3, 5, 2)]. Maybe there is a way to modify that.
It is not a homework question.
To provide context: I am building a program to test graph isomorphism. The lists are the color classes of the nodes. With these color classes I want to limit all possible permutations of a graph to just a few to make the brute-force quicker.  
",346
31266868,33099661,2,"You can also use php to have an URL like this:
Then you can define any start and end time to create a dynamic m3u8
Here is an example of index.php
",32
31266868,31266868,1,"I have created the following config for hls in application 'live':
Now the index.m3u8 contains timestamp-named chunks and plays fine.
My question is - is it possible to create a timeshifted version of the index.m3u8, which would play the stream shifted back in time ? 
So http://server/live/test.stream/index.m3u8 plays recorded 6 hours earlier.
I heard that i might need a python script/C that select ts.chunks 6 hours. So how would i achieve this.
",81
31266899,31267007,2,"You can create the directory with :
See the doc : https://docs.python.org/2/library/os.html
",14
31266899,31266899,1,"Suppose I have a function:
As you can see I am saving a file called M_+str(c)+.dat in the path /home/user/'+str(a)+'<z<'+str(b)+'/mass/.
The problem I have is that, both str(a)+'<z<'+str(b) and mass folders don't exist. 
I have to create them inside the function and save the file inside these folders. How do I achieve this task?
",90
31266930,31267109,2,"
could we call it from cPython
No, netTcpBinding is interoperable only with WCF clients. 
From here:
The default configuration for the NetTcpBinding is faster than the
  configuration provided by the WSHttpBinding, but it is intended only
  for WCF-to-WCF communication. 
From comments:
does wsHttpBinding work or only basicHttpBinding work?
Short answer is no, basicHttpBinding is the only binding (except for the web bindings) which support interoperability with non-wcf clients. Even then you may have difficulty consuming it from non-windows.
Long answer is that the reason this is the case is because basicHttpBinding supports communication over SOAP 1.1, which is a relatively simple protocol, and most vendors have implemented it in very similar ways. Therefore, these different implementations tend to be interoperable. However, wsHttpBinding is Microsoft's attempt to support the SOAP 1.2 protocol and WS-* web service extensions, which is a much larger and more complex set of standards. So there is a much larger scope for interpretation between the various vendors, leading normally to non-interoperability between implementations. It is theoretically possible, therefore, to call an endpoint exposed over wsHttpBinding from a non-wcf (or even non-windows) client, but you would have to overcome all the niggles.
A much better approach would be to move away from SOAP completely if possible, and just use HTTP/POX or HTTP/REST services. 
",246
31266930,31266930,1,"I've read this question WCF and Python.
But in case, the wcf service use netTcpBinding, could we call it from cPython. If it's possible, please help to give an simple example ?
",39
31266964,31266964,1,"Hello I am new to Python. I have over 5000 .csv.gz files to be loaded on vertica data base. The server disconnects after 10 minutes, thus all 5000 files cannot be copied without re-setting the server connection.
I have two basic problems here:
How can I keep track of copy commands successfully executed?
How can I re-set the connection and restart from last failed command?
The code I am using is:
",80
31266964,31267634,2,"I suggest you use the STREAM NAME option when you load your data.
Then run a query on the stream loads that were successful and remove them from your list. 
Also it's good to see the rejected_row_count column value in the load_streams table as it will tell you how many rows were rejected.
You can use CURRENT_LOAD_SOURCE() as well, with this you will need to add a new column to your table.
But this is used when I do loads from different locations(parallel), and I can identify better which file failed during load.
",106
31266969,31266969,1,"I'm having some troubles and I hope you can help me. I've configured the RPi with the Cam Web Interface, who listens to port 80 in the apache server. 
I also have to use python to get some information from the RPi to the web interface (using Flask) but it cannot use the port 80, so it's using the port 5000. Everything working fine.
I had done a simple scrip with ajax to request data in json to the python module.
But I'm receiving the following error:
Here is my code:
html:
error from the console:
python code:
",117
31266969,31267472,2,"Take a look at this link, it explains how Access-Control-Allow-Origin works.
You might be hosting it on one url and directing ajax to another, and browsers have a same origin policy preventing this from working.
",39
31267030,31267126,2,"Assuming you want to allow only one hyphenated section then you can do this using an optional group
Demonstration: https://regex101.com/r/wV6zP7/1
For example, this will match ""0123-456789"" but not ""0123-456-789"".
",38
31267030,31267030,1,"I have an address string like this
Currently, I'm using regex to extract phone number from the end of the string like this:
I just realized that there are some phone numbers like:
How can I alter this regex to get the numbers before the hyphen? Any help?
Please note that the number of digits before the hyphen vary erratically and I also have numbers without any hyphens which I need as well.
",80
31267030,31267288,2,"In case your string always contains Phone: with the phone number following it at the end, you do not need the regex. Also, note that 1-800-MALL is also a valid phone number.
I suggest this:
Or, in case regex is still preferable, another solution:
",53
31267030,31267257,2,"If you always have a space before the phone number, why not simply:
",15
31267030,31267216,2,"You could have your digit pattern to include optional minus sign and expect the group to be repeated 1 or 2 times.
",23
31267030,31267158,2,"
You can simply modify your regex to this.If there is always a possiblity of only   1 - use
",18
31267030,31267122,2,"Just put -, \d inside a char class.
If the phonenumber startswith with a optional + then you may try this,
",24
31267073,31267073,1,"How can I set IncludeExceptionDetailInFaults with Python/SUDS? For one of my requests, I am getting this error message:
There is no obvious error in the request, so I would like to obtain additional information, but can't find a way using SUDS API. Is there any way?
",54
31267073,31879325,2,"As I understand it now, the answer is no. The IncludeExceptionDetailInFaults setting seems to be only accessible on the server, there doesn't seem to be a way to change it from the client side.
",39
31267080,31267080,1,"can't copy xlsx and xlsm files with xlrd as it says ""formatting info= True"" not yet implemented and
openpyxl runs out of memory when doing the following:
",31
31267080,31268669,2,"If you just want to copy the file then just do that using shutil. There are still several things that openpyxl doesn't support such as images and charts that will be lost. And, as you're seeing, memory is an issue. Each cell uses about 45 kB of memory.
The openpyxl documentation is pretty clear about the different options used when opening workbooks: data_only only read the results of any formulae and ignore the formulae.
See https://bitbucket.org/openpyxl/openpyxl/issue/171/copy-worksheet-function if you want to copy worksheets.
Otherwise you can use two workbooks, one in read-only mode and the other in write-only mode. But if you want to copy, this is best done in the file system.
If you only want to copy the values from one workbook to another then you can combine read-only and write-only modes to reduce the memory footprint. The following pseudo-code should give you a basis.
",165
31267147,31267147,1,"I'm testing a web application using Django-nose to monitor the code coverage. At first it worked perfectly well, but when trying to generate HTML it fails with the error:
Imput error: No module named copy_reg
It happened after a few times (until then in worked). I tried it on a computer with newly installed django, django-nose and coverage and the very same code works fine. Re-installing django and django-nose didn't help.
Any suggestions? Should I re-install any library or something?
Thank you in advance!
",99
31267147,31290685,2,"I fixed this by uninstalling coverage.py with pip and installing it using easy_install.
",14
31267148,31267171,2,"Just pass param for usecols:
Here I generate the list [0,1,2,3] to indicate which columns I'm interested in.
",23
31267148,31267148,1,"How do I ignore last whitespace in a line when converting to Pandas DataFrame?
I have a CSV file in the following format:
I loop through the 'Column #' lines to create my column names first (so 4 columns), then I parse the following lines to create my DataFrame using ';' as the separator.  However some of my files contain a trailing ';' on the end of each line as shown above, so my Pandas DataFrame thinks there is a 5th column containing whitespace, and consequently throws an error to say there aren't enough column names specified
Is there a mechanism in Pandas to remove/ignore the trailing ';', or whitespace when creating a DataFrame?  I am using read_csv to create the DataFrame.
Thanks.
",143
31267174,31267174,1,"I'm currently doing this tutorial here: http://nbviewer.ipython.org/urls/bitbucket.org/hrojas/learn-pandas/raw/master/lessons/01%20-%20Lesson.ipynb
I am currently on the last section where you have to plot a graph. I am running the code in my own idle and not iPython. Here is my code:
The output I am getting is:
How do I get it to actually show the graph?
",66
31267174,31268992,2,"Adding plt.show() should do the trick, although if I may make a recomendation..this data would be better represented by a bar chart. Something like the following should do the trick:
",35
31267174,31267403,2,"If you are not in interactive mode, you might need to add plt.figure() before your code, and plt.show() after.
",26
31268331,31269337,2,"Figured out what was wrong thanks to the user Weeble's comment. When he said 'something is causing your main.py to run twice' I remembered that Bottle has an argument that is called 'reloader'. When set to True, this will make the application load twice, and thus the thread creation is run twice as well.
",62
31268331,31268331,1,"I'm having an issue with threading that I can't solve in any way I've tried. I searched in StackOverflow too, but all I could find was cases that didn't apply to me, or explanations that I didn't understand.
I'm trying to build an app with BottlePy, and one of the features I want requires a function to run in background. For this, I'm trying to make it run in a thread. However, when I start the thread, it runs twice. 
I've read in some places that it would be possible to check if the function was in the main script or in a module using if __name__ == '__main__':, however I'm not able to do this, since __name__ is always returning the name of the module.
Below is an example of what I'm doing right now.
The main script:
The class:
So what I intend here is to have check_list running in a thread all the time, doing something and waiting some seconds to run again. All this so I can have the list updated, and be able to read it with the main script.
Can you explain to me what I'm doing wrong, why the thread is running twice, and how can I avoid this?
",243
31268331,31269060,2,"This works fine:
",4
31268359,31268359,1,"I'm following the implementation of a python script example shown here.
Basically, this does what I need it to do except that we now want to run the script with the domain credentials without prompting the user for a username and password.  Ideally, we'd want to run this during maintenance windows, and we would rather have a domain account's credentials handle the authentication instead of baking it in to the script for anyone to stumble upon.  
Since I'm not really well versed with python, I'm not entirely sure what the capabilities/best practices are in these scenarios.  
",109
31268359,31269439,2,"If all you're looking for is a way to pass in arguments without prompting the user, you could use the library argparse to pass in command line arguments, see https://docs.python.org/3/library/argparse.html
Another source of information would be an .ini file, which is handled by the module ConfigParser or any other file-based source like XML, shelve, json.
All these issues have the weakness, that you have to apply access protection for the file containing the credentials.
",85
31268364,31269128,2,"Your code fails as you have already consumed the iterator on the first call, if you call none_context() in the with block the original code would work:
You can see using your original code that if you add a None for each open then the code will work as expected:
",55
31268364,31268364,1,"I read in this answer of Is it possible to have an optional with/as statement in python? that you can have a dummy file writer with contextmanager. I want, however, to open multiple dummy file writers in a with statement context.
Say I create two dummy files: touch a and touch b.
Given the first part of the script:
This addition works with a single dummy file writer (it prints 2):
This also works, because we are indeed reading files (it prints 1):
However, it doesn't work if we are using two dummy file writers:
It shows the error:
Why is this happening? And also: how can I make this multiple with open commands work with a dummy file writer?
",142
31268373,31268373,1,"I need to generate PDF from html tables, and was looking for a library that allows to take html tables with full CSS and make pdf. I'm trying to do this with PDFKit. I installed and I tested some simple examples that are explained in its documentation,it works. Documentation link:PDFKit
I have some tables like this,this is table that i want to convert to pdf: 
In form action of table above i called this method, I created a route method:
My problem is that when i try to generate pdf i don't know how to pass arguments, for example, how to make know that i want to generate to pdf table with id=""table_to_pdf""?
 Any help?
",137
31268373,31269006,2,"It looks like you are using flask as a framework:
Have a look at some of the example here and here. 
You can pass the form ID in as a URL parameter.
",35
31268453,31268453,1,"I am trying to use a CSV in order to fill a 34 columns SQL database by using Python, even though I can't.
I've been following many indications, although it is my first time pythoning and I don't know how to do that.
Could you please help me with this? Thanks a lot in advanced.
Pd: In case it is not inferable, the csv file is without a header, and I am trying to fill column by column at once.
",93
31268453,31268940,2,"If the CSV elements are positionally correct, can you not do something more straight forward i.e. as an example with the following data
use the following;
",29
31268453,31269661,2,"This works. I used a few short cuts to save on typing.
Note its bad practice to use string format operations to build up sql query strings. It can lead to sql injection attacks, if used with unknown input data. I am doing so here because the strings are only being built from known values and unknown input (that from the file) is built properly using the standard '?' placeholder with tuple passed to execute method.
Note also you have far too many parameters in one table. It should be more normalised across multiple tables, but I guess you will learn that at some point.
",118
31268494,31268494,1,"I have a use case where I have to send_email to user in my views. Now the user who submitted the form will not receive an HTTP response until the email has been sent . I do not want to make the user wait on the send_mail. So i want to send the mail asynchronously without caring of the email error.  I am using using celery for sending mail async but i have read that it may be a overkill for simpler tasks like this. How can i achieve the above task without using celery
",99
31268494,31272086,2,"I'm assuming you don't want to wait because you are using an external service (outside of your control) for sending email. If that's the case then setup a local SMTP server as a relay. Many services such as Amazon SES, SendGrid, Mandrill/Mailchimp have directions on how to do it. The application will only have to wait on the delivery to localhost (which should be fast and is within your control). The final delivery will be forwarded on asynchronously to the request/response. STMP servers are already built to handle delivery failures with retries which is what you might gain by moving to Celery.
",117
31272409,31272562,2,"Add an optional quantifier ? to the value part so that it is matched zero or one time
Changes made
Moved the = to a non capturing group (?:..)
(?:=(?P<val>.+))? Matched zero or one time. This is ensured by the ?. That is it checks if =value can be matched (capturing only the value part). If not None is captured.
",83
31272409,31272651,2,"Try this:
Edited the regex
Test 1 :  'x=y' , then key='x' and val='y'
Test 2 :  'x=' , then key='x' and val=''
Test 3 :  'x' , then key='x' and val=None
",40
31272409,31272409,1,"I want to match either key or key=val in python such that the resulting groupdict() from the Match object will either have val as None or have a value. 
The following is close:
But I want val to be None in the second case. I tried moving the optional = into the second group:
That gives me the None, but then attaches the = to val. I also tried using the lookbehind with (?<==) but that didn't work for either expression. Is there a way to achieve this?
",103
31272427,31272427,1,"I have to apologise in advance 'cause this question is quite general and may be not clear enough. The question is: how would you run in parallel a Python function that itself uses a pool of processes for some subtasks and does lots of heavy I/O operations? Is it even a valid task?
I will try to provide some more information. I've got a procedure, say test_reduce(), that I need to run in parallel. I tried several ways to do that (see below), and I seem to lack some knowledge to understand why all of them fail.
This test_reduce() procedure does lots of things. Some of those are more relevant to the question than others (and I list them below):
It uses the multiprocessing module (sic!), namely a pool.Pool instance,
It uses a MongoDB connection,
It relies heavily on numpy and scikit-learn libs,
It uses callbacks and lambdas,
It uses the dill lib to pickle some stuff.
First I tried to use a multiprocessing.dummy.Pool (which seems to be a thread pool). I don't know what is specific about this pool and why it is, eh, ""dummy""; the whole thing worked, and I got my results. The problem is CPU load. For parallelized sections of test_reduce() it was 100% for all cores; for synchronous sections it was around 40-50% most of the time. I can't say there was any increase in overall speed for this type of ""parallel"" execution. 
Then I tried to use a multiprocessing.pool.Pool instance to map this procedure to my data. It failed with the following:
I made a guess that cPickle is to blame, and found the pathos lib that uses a far more advanced pickler dill. However it also fails:
Now, this error is something I don't understand at all. I've got no output to stdout from my procedure when it works in a pool, so it's hard to guess what's going on. The only thing I know is that test_reduce() runs successfully when no multiprocessing is used.
So, how would you run in parallel something that heavy and complicated?
",414
31272427,31293713,2,"So, thanks to @MikeMcKerns' answer, I found how to get the job done with the pathos lib. I needed to get rid of all pymongo cursors, which (being generators) could not be pickled by dill; doing that solved the problem and I managed to run my code in parallel.
",59
31769514,31769514,1,"I have a xml file ""sample.xml"" as:
and respective dtd as ""sample.dtd"":
I want ""sample.xml"" to get validated against ""sample.dtd"" by making use of python script. How will i achieve this? kindly help.
",46
31769514,31769680,2,"The lxml lib is well suited for this:
With sample.txt and sample.dtd in the current working directory, you can simply run:
Results in:
See here for more detail.  Also, a related question
",38
31769525,31769543,2,"You can do it with another loop, and using a comma after the print:
Note: The comma will avoid printing a new-line character.
",27
31769525,31769537,2,"You're trying to mutate the list's respresenting string instead of using it's members to build your specific
representation. This is not the way to go.
Use map to create strings and str.join to join the strings with your favorite separator:
An alternative with a generator expression:
",53
31769525,31769525,1,"I have the list :: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
created by a loop. 
I want it to print
I've used the .strip('[]') to get rid of the parentheses but I cannot get rid of the commas. 
",64
32491545,32492220,2,"Your php script is executed by the www user.
You could check if the python script interpreter is correctly called, it is usually in one of the directory in the PATH environment variable (like /usr/bin/python), but the www user don't have a PATH environment variable set.
Solution specify the whole path to your python interpreter in your shell_exec call ( also specify the full path to your script when you're at it )
What about the path the nltk library is installed, you could check if the python interpreter would correctly look for it by looking at the sys.path while running python with the www user.
Diagnostic : use the shell_exec call to run a python script to print the sys.path values
Solution : append the library path to the sys.path in your python script before the import nltk
These would be the most obvious solutions considering the information provided in the question.
Update :
As there is 2 version version of python installed (on that haven't got the library installed ), it is recommended to specify the path to the desired interpreter. The first solution help correct the issue.
In unix like system I would recommend using which python command to determine the path of your default python interpreter.
",227
32491545,32492406,2,"Option 1
Setup a simple python httpserver listening on localhost.  This old answer might help but there are plenty of howtos out there. The advantage is that you don't have the overhead of starting the python interpreter each time the ntlk stuff needs to be executed and you don't have to worry about shell script executions, permissions etc. Disadvantage is a little of extra work and a little overhead.
Option 2
Using a task queue. Whatever said and done it's not safe to execute commands from your web facing PHP scripts. If you are already using RabbitMQ or something similar you can use that here. Or else if you are using redis you can use the lpush, rpop methods to make redis behave like a queue. Disadvantage: the result is not immidiately available.
Option 3
Anbother strategy for your php script to enter the data into a table and setup your python script to run as a cron job to check the table once a minute. Disadvantage: the result is not immidiately available.
Option 4
Your current choice but please make sure that you have escaped the data properly by @lafor if this option is chosen @dvhh 's answer ought to work.
",223
32491545,32491545,1,"Im trying to call a python file containing a sentence/word tokenizer from my php file like this:
I've tried single exec, full paths to python and tokenizer.py, wrapping $sentence in double quotes. But logically, It should not be the problem because calling print(1) at the beginning of python the python code before actually using any nltk packages makes $output equal to '1'. So I came to conclusion that the problem here is the nltk itself, like the path to the modules is not correct or something...
But, calling python from the shell using the same command as above gives me fully tokenized output! To conclude: looks like when calling python from php magically 'turns off' nltk, while it fully works when executed from the shell.
Here's the part of the python code I am using:
The server's running on CentOS (Linux), I am accessing it via SSH.
Obvious question: What am I doing wrong here with PHP? Or generally? Any alternatives?
EDIT
As visible in dvhh's answer and its comments, the situation happened because there were two versions installed on the server (2.6 and 2.7), while the www user had access to 2.6 and through console, the default version was 2.7. The solution was to change the default python to 2.7 for both cases and to put the nltk modules to one of the dependency folders. (Or append the dependency directory using sys.path.append)
",275
38957908,38957908,1,"noob programmer here. I am trying to build a small program in 2.7 which generates a prime number, asks the user to continue or not, and then continues generating primes until the user tells it to stop. Unfortunately my program isn't outputting anything at all, and I can't figure out why.
Here is my code:
First, the part which checks for primes. I know that this part is functioning properly, because the exact same code works properly for my prime factor finder.
And second, the part which iterates through all the numbers, prints result, and asks to continue or not. The error must be in here somewhere:
",125
38957908,38958001,2,"n should be incremented unconditionally. If it isn't the program gets stuck in an infinite loop the first time it encounters a non-prime.
",26
38957908,38958593,2,"Your code is a bit chaotic, I would rewrite it like this:
",14
38957961,38957961,1,"I have a Player class with a score attribute:
This score increases/decreases as the player succeeds/fails to do objectives. Now I need to tell the player his rank out of the total amount of players with something like
First I thought of having a list of all the players, and whenever anything happens to a player:
I check if his score increased or decreased
find him in the list
move him until his score is in the correct place
But this would be extremely slow. There can be hundreds of thousands of players, and a player can reset his own score to 0 which would mean that I'd have to move everyone after him in the stack. Even finding the player would be O(n).
What I'm looking for is a high performance solution. RAM usage isn't quite as important, although common sense should be used. How could I improve the system to be a lot faster?
Updated info: I'm storing a player's data into a MySQL database with SQLAlchemy everytime he leaves the gameserver, and I load it everytime he joins the server. These are handled through 'player_join' and 'player_leave' events:
Also, the player's score is updated upon 'player_kill' event:
",230
38957961,39002736,2,"That kind of information can be pulled using SQLAlchemy's sort_by function. If you perform a Query like:
You will have the list of Players sorted by their score. Keep in mind that every time you do this you do an I/O with the database which can be rather slow instead of saving the data python variables. 
",61
38957961,39043000,2,"Redis sorted sets help with this exact situation (the documentation uses leader boards as the example usage) http://redis.io/topics/data-types-intro#redis-sorted-sets
The key commands you care about are ZADD (update player rank) and ZRANK (get rank for specific player). Both operations are O(log(N)) complexity.
Redis can be used as a cache of player ranking. When your application starts, populate redis from the SQL data. When updating player scores in mysql also update redis.
If you have multiple server processes/threads and they could trigger player score updates concurrently then you should also account for the mysql/redis update race condition, eg:
only update redis from a DB trigger; or
serialise player score updates; or
let data get temporarily out of sync and do another cache update after a delay; or
let data get temporarily out of sync and do a full cache rebuild at fixed intervals
",168
38957961,39000851,2,"The problem you have is that you want real-time updates against a database, which requires a db query each time. If you instead maintain a list of scores in memory, and update it at a more reasonable frequency (say once an hour, or even once a minute, if your players are really concerned with their rank), then the players will still experience real-time progress vs a score rank, and they can't really tell if there is a short lag in the updates.
With a sorted list of scores in memory, you can instantly get the player's rank (where by instantly, I mean O(lg n) lookup in memory) at the cost of the memory to cache, and of course the time to update the cache when you want to. Compared to a db query of 100k records every time someone wants to glance at their rank, this is a much better option.
Elaborating on the sorted list, you must query the db to get it, but you can keep using it for a while. Maybe you store the last_update, and re-query the db only if this list is ""too old"". So you update quickly by not trying to update all the time, but rather just enough to feel like real-time.
In order to find someone's rank nearly instantaneously, you use the bisect module, which supports binary search in a sorted list. The scores are sorted when you get them.
This says that a 7 score is rank 4, which is correct.
",289
38957964,38957964,1,"this is my first post and I would like to thanks the amazing community of Stack Overflow for helping me during all this years! 
However, after extensive research, i couldn't find a solution to my problem. I have the file generated by QtCreator which contain a progressbar.
In my code, I have 2 class, and 1 is a Thread. This Thread must change the value of the progressbar, but I completely fail to do so.
I can't access the variable from my Thread, but i can from the init of Mainwindow. I think the problem is the nature of the ""self"" variable in setprogressBar, but I'm literally stuck finding out what it is.. 
When i tried to execute this code, here is the result :   
File A, generated with QtCreator : 
File B, my code :
Thx a lot for the help guys !
",165
38957964,38958044,2,"MainWindow is a class, not an object. What you should do instead is something like:
",18
38958048,38958092,2,"Something like this:
That should match any contiguous run of word characters containing at least one digit.
",19
38958048,38958048,1,"It's easy to remove all digits from the following string:
But I want to remove every standalone digit (e.g. 321 by itself) and every combination of letters and numbers in the string (e.g. 843aa943 and asprx12303).
This is what I have so far:
So, these two patterns work pretty well, but I'm left with 2ci at the end. How can I make an all-encompassing regex for this issue? My solution is ok so far, but is not quite what I need.
",98
38958114,38958451,2,"You may want to use facts.d and place your python script there to be available as a fact.
Or write a simple action plugin that returns json object to eliminate the need in stdout->from_json conversion.
",39
38958114,38958114,1,"i am very new to ansible and would like to test a few things.
I have a couple of Amazon EC2 instances and would like to install different software components on them. I don't want to have the (plaintext) credentials of the technical users inside of ansible scripts or config files. I know that it is possible to encrypt those files, but I want to try keepass for a central password management tool. So my installation scripts should read the credentials from a .kdbx (Keepass 2) database file before starting the actual installation.
Till now i wrote a basic python script for reading the .kdbx file. The script outputs a json object via:
The ouput looks like the following: 
Now I want to achieve, that the python script is executed by ansible and the key value pairs of the output are included/registered as ansible variables. So far my playbook looks as follows:
The first debug generates the correct json output, but i am not able to include the variables in ansible, so that I can use them via jinja2 notation. set_fact doesn't throw an exception, but the last debug just returns a ""Hello world"" - message? So my question is: How do I properly include the json key value pairs as ansible variables via task?
",242
38958212,39005037,2,"As already said in the comments:
See a demo on regex101.com.
",13
38958212,38964290,2,"I hope you find this solution satisfactory:
",8
38958212,38958212,1,"
How to get 2.x-dev, 6.x-2.x, 6.x-2.3, 7.x-1.x-dev, 6.x-1 6.x-1.6.
Thanks
",15
38958233,38958233,1,"I was following a tutorial called ""Black Hat Python"" and got a ""the requested address is not valid in its context"" error. I'm Python IDE version: 2.7.12
This is my code:
and this is my error:
",45
38958233,38958939,2,"You are trying to bind to an IP address that is not actually assigned to your network interface:
See the Windows Sockets Error Codes documentation:
WSAEADDRNOTAVAIL 10049
Cannot assign requested address.
The requested address is not valid in its context. This normally results from an attempt to bind to an address that is not valid for the local computer.
That may be an IP address that your router is listening to before using NAT (network address translation) to talk to your computer, but that doesn't mean your computer sees that IP address at all.
Either bind to 0.0.0.0, which will use all available IP addresses (both localhost and any public addresses configured):
or use any address that your computer is configured for; run ipconfig /all in a console to see your network configuration.
You probably also don't want to use ports < 1024; those are reserved for processes running as root only. You'll have to pick a higher number than that if you want to run an unprivileged process (and in the majority of tutorials programs, that is exactly what you want):
I believe the specific tutorial you are following uses BIND_IP = '0.0.0.0' and BIND_PORT = 9090.
",225
38960631,38960694,2,"You need to convert the list to numpy array in order to use vectorized operation such as == and &:
Shorter version (if you are sure that x and y are numpy arrays):
",37
38960631,38960631,1,"I have two numpy lists:
How can I find indexes when simulataneously x equals 'A' and y equals '2'?
I expect to get indexes [1, 5].
I tried to use:
np.where(x == 'A' and y == '2') but it didn't help me. 
",58
38960631,38960855,2,"If you want to work with lists:
",8
38960631,38960750,2,"pure python solution:
",4
38960708,38960708,1,"Trying to deploy my project on the server, and i'm stuck in migrations becouse there is some error:
And here is my models.py file:
Just trying to make some fresh app on heroku, making new migrations and deleting olders but it's still didn't work. Can anybody know what's wrong with it? Thanks for the answer.
",66
38960708,38961132,2,"Here's the culprit:
You can't set a Model class as a Foreignkey default. If you're thinking of setting an hardcoded default then you should use an int and be sure the selected value exists as a key in your Profile model.
",47
38960714,38960864,2,"Here's one way to use itertools for this problem.
",11
38960714,38961307,2,"This is a minor variation on @David's answer.
If we look at itertools.permutations([1,2,3,4])
You'll notice that for a tuple (a,b,c,d), (c,d,a,b) also appears.  If a number a == 9*b, then b != a*9.  We'll use this to our advantage.
Also, note that if a = 9*b, a must be bigger than b, unless we're using negative numbers or non-integers.
You'll see that as we look at the results, splitting the tuples in half and turning them into numbers gives, initially, a small number followed by a larger number.  This is a side effect of passing permutations a sorted list.  Again, we can use this to our advantage.
If you're taking user input, you should be able to get the same result simply by sorting your input:
",174
38960714,38960714,1,"Recently, I read a math problem inspiring me to write a program. It asked to arrange the digits 0-9 once each so that xx xxx / xx xxx = 9. I wrote a python program to find the solutions and had a bit of trouble making sure the digits were different. I found a way using nested whiles and ifs, but I'm not quite happy with it.
As you can see, the code is very long and repetitive, even the shortened form. Running it on my laptop takes almost a minute (and my laptop is new). I have searched for answers, but I only found ways to generate random numbers. I tried using itertools.permutations as well, but that only shows the permutations, not creating a number. 
Generating all ten digits takes too long, and I want to know if there is a faster, simpler way, with an explanation, using python 3..
Thanks
",174
38960714,38961548,2,"Adapting Wayne Werner's solution you can do this to add the digit uniqueness constraint (assuming Python 3):
This runs in 1.5 ms on my machine.
Note, that you can only check numbers between 10000 and 100000 / 9 = 11111. 
And if you want to allow preceding zeros, you can do it like this:
And this one takes 15 ms.
",70
38960714,38961088,2,"Take advantage of algebra:
Knowing that, you only have to bother generating the values:
If you need to filter things out by some criteria, you can easily write a filter function:
If you wanted to shorten things a bit, you could re-write your function so that it returns the valid pair, or None otherwise.
Or, just create a generator and iterate over that:
If you want to allow zero padded numbers in b then you can use this filter:
",90
38960714,38961197,2,"Runs in 0.7 secs. Faster than most solutions mentioned, though bit clumsy.
The solution printed is:
",20
38961251,38961806,2,"I believe that the cause of this problem is coalesce(), which despite the fact that it avoids a full shuffle (like repartition would do), it has to shrink the data in the requested number of partitions.
Here, you are requesting all the data to fit into one partition, thus one task (and only one task) has to work with all the data, which may cause its container to suffer from memory limitations.
So, either ask for more partitions than 1, or avoid coalesce() in this case.
Otherwise, you could try the solutions provided in the links below, for increasing your memory configurations:
Spark java.lang.OutOfMemoryError: Java heap space
Spark runs out of memory when grouping by key
",138
38961251,39106641,2,"The problem for me was indeed coalesce(). 
What I did was exporting the file not using coalesce() but parquet instead using df.write.parquet(""testP""). Then read back the file and export that with coalesce(1).
Hopefully it works for you as well.
",55
38961251,38961251,1,"I'm invoking Pyspark with Spark 2.0 in local mode with the following command:
The input dataframe is being read from a tsv file and has 580 K x 28 columns. I'm doing a few operation on the dataframe and then i am trying to export it to a tsv file and i am getting this error.
Any pointers how to get rid of this error. I can easily display the df or count the rows.
The output dataframe is 3100 rows with 23 columns
Error:
",93
38961251,41681999,2,"In my case the driver was smaller than the workers. Issue was resolved by making the driver larger.  
",20
38961252,38961252,1,"I am working with a big set of data (240, 131000) and bigger. I am currently using the code below to plot this.
However, it's taking a very long time (30 min+) and the plot still hasn't shown up yet. A quick breakpoint check says the code gets to the spectrum= line, but doesn't go past. Looking at the memory on my computer, it hasn't even gotten close to the limit. 
Does anyone have a better way of doing this?
",98
38961252,38975543,2,"pcolorfast works best for large arrays and updates quickly.
",10
38961284,38961284,1,"I'm trying to divvy up the task of looking up historical stock price data for a list of symbols by using Pool from the multiprocessing library.  
This works great until I try to use the data I get back.  I have my hist_price function defined and it outputs to a list-of-dicts pcl.  I can print(pcl) and it has been flawless, but if I try to print(pcl) after the if __name__=='__main__': block, it blows up saying pcl is undefined.  I've tried declaring global pcl in a couple places but it doesn't make a difference.
I've also tried removing the if __name__=='__main__': block but it gives me a RunTimeError telling me specifically to put it back.  Is there some other way to call variables to use outside of the if block?  
",152
38961284,38961389,2,"I think there are two parts to your issue. The first is ""what's wrong with pcl in the current code?"", and the second is ""why do I need the if __name__ == ""__main__"" guard block at all?"".
Lets address them in order. The problem with the pcl variable is that it is only defined in the if block, so if the module gets loaded without being run as a script (which is what sets __name__ == ""__main__""), it will not be defined when the later code runs.
To fix this, you can change how your code is structured. The simplest fix would be to guard the other bits of the code that use pcl within an if __name__ == ""__main__"" block too (e.g. indent them all under the current block, perhaps). An alternative fix would be to put the code that uses pcl into functions (which can be declared outside the guard block), then call the functions from within an if __name__ == ""__main__"" block. That would look something like this:
As for why the issue came up in the first place, the ultimate cause is using the multiprocessing module on Windows. You can read about the issue in the documentation.
When multiprocessing creates a new process for its Pool, it needs to initialize that process with a copy of the current module's state. Because Windows doesn't have fork (which copies the parent process's memory into a child process automatically), Python needs to set everything up from scratch. In each child process, it loads the module from its file, and if you the module's top-level code tries to create a new Pool, you'd have a recursive situation where each of the child process would start spawning a whole new set of child processes of its own.
The multiprocessing code has some guards against that, I think (so you won't fork bomb yourself out of simple carelessness), but you still need to do some of the work yourself too, by using if __name__ == ""__main__"" to guard any code that shouldn't be run in the child processes.
",410
38961293,38961293,1,"I'm trying to connect to website with python requests, but not with my real IP. So, I found some proxy on the internet and wrote this code:
When I get output, proxy simple don't work. Site returns my real IP Address. What I did wrong? Thanks.
",57
38961293,38961568,2,"The answer is quite simple. Although it is a proxy service, it doesn't guarantee 100% anonymity. When you send the HTTP GET request via the proxy server, the request sent by your program to the proxy server is:
Now, when the proxy server sends this request to the actual destination, it sends:
As you can see, it throws your IP (in my case, 122.126.64.43) in the HTTP header: X-Forwarded-For and hence the website knows that the request was sent on behalf of 122.126.64.43
Read more about this header at: https://tools.ietf.org/html/rfc7239
If you want to host your own squid proxy server and want to disable setting X-Forwarded-For header, read: http://www.squid-cache.org/Doc/config/forwarded_for/
",131
38961327,38961327,1,"We are creating a python program that executes specific macros within Polyworks based on user input into the program. Right now the code is:
However this assumes that our program is always installed in C:\RotoWorks. Ideally, our app is portable. I'm sure theres a way to retrieve the filepath that Rotoworks is stored in, then just concatenate the rest of the filepath to the end. How do I do this?
",81
38961327,38961378,2,"You can retrieve the path from the __file__ attribute of the file. Use os.path.abspath on that attribute to retrieve the absolute path of the file and then os.path.dirname to retrieve the containing directory:
Use os.path.dirname recursively to move out as many directories as you want.
",48
38961360,38961360,1,"I am working on a project that requires me to read a spreadsheet provided by the user and I need to build a system to check that the contents of the spreadsheet are valid. Specifically I want to validate that each column contains a specific datatype.
I know that this could be done by iterating over every cell in the spreadsheet, but I was hoping there is a simpler way to do it.
",77
38961360,39088757,2,"I ended up just manually looking at each cell. I have to read them all into my data structures before I can process anything anyways so it actually made sense to check then.
",35
38961360,39077066,2,"In openpyxl you'll have to go cell by cell.
You could use Excel's builtin Data Validation or Conditional Formatting, which openpyxl supports, for this. Let Excel do the work and talk to it using xlwings.
",42
38963631,38963631,1,"I'm using Sphinx version 1.4.5.
My project structure is the following:
+ src > main.py
+ docs (generated with sphinx-quickstart)
Even after adding the path to the src folder in docs/conf.py:
And generating the rst file for src/main.py (i.e. docs/src.rst and docs/modules.rst) with:
When I try to build the html webpages with:
It couldn't find both the src module and src/main.py:
WARNING: autodoc: failed to import module u'src.main'; the following exception was raised
",90
38963631,38963810,2,"Try doing this for your path insertion instead:
Also consider a better name for your directory than src.
",20
38963631,38963769,2,"Your current working directory should be the directory of your makefile, which should be docs.
",17
38963653,38964103,2,"This has nothing to do with the parser, you'll see the same behavior just from mktime() alone, since datetime.timetuple() doesn't have any time zone offset information, and mktime() is the inverse of localtime. You can correct this by converting it to localtime before calling timetuple():
Note that there is a chart on the documentation for time() (python 2.x docs) that tells you how to convert between these representations:
My personal preference would be to convert the parsed date to UTC, in which case calendar.timegm() would be the appropriate function:
",113
38963653,38963653,1,"I have looked at many possible ways to parse python times. Using parse seems link the only method that should work. While trying to use datetime.strptime causes an error because %z does not work with python 2.7. But using parse.parse incorrectly recognizes the time zone.
I parse both Fri Nov 9 09:04:02 2012 -0500 and Fri Nov 9 09:04:02 2012 -0800 and get the exact same timestamp in unix time. 1352480642
My version of python 2.7.10 
My version of dateutil 1.5
Here is my code that runs the test.
Output
Expected output
",99
38963698,38970460,2,"The refactored ""Version 2"" code in the question suffers from a concurrency / timing problem.
sse_request() is called for each of the web-clients  (in the test case 3 instances). We thus have 3 instances looping in event_stream().
These calls happen ""more or less"" in parallel: which actually means in random sequence.
However the list change_objects is shared, so the first web-client that spots a change will update the ""old"" copy in the shared WalkReporter instance to the latest state, and may do so before the other clients spot the change. i.e. the first successful web-client effectively hides the change from the other web-clients.
This is easily fixed, by giving each web-client its own copy of change_objects. 
i.e. change_objects is moved into sse_request() as shown below.
With this minor change, each instance of sse_request() can spot the changes, and thus all the web-clients receive the sse-events as expected.
",180
38963698,38963698,1,"I have a Flask web-server which generates server-sent-events (sse) which should be received by all connected web-clients.
In ""Version 1"" below this works. All web-clients receive the events, and update accordingly.
In ""Version 2"" below, which is a refactoring of Version 1, this no longer works as expected:
Instead I get:
mostly only one of the web-clients gets the event, or 
rarely
multiple web-clients get the event, or 
rarely none of the
web-clients get the event
As far as I can make out, the server is always generating the events, and normally at least one client is receiving.
My initial test hosted the web-server on a Raspberry Pi 3, with the web-clients on the Pi, on Windows and OSX using a variety of browsers.
To eliminate any possible network issues I repeated the same test with the web-server and 3 instances of Chrome all hosted on the same OSX laptop.
This gave the same results: Version 1 ""OK"", Version 2 ""NOT OK"".
The client that successfully receives seemingly varies randomly from event to event: so far I can't discern a pattern.
Both Version 1 and Version 2 have a structure change_objects containing ""things that should be tracked for changes""
In Version 1 change_objects is a dict of dicts.
In Version 2 I refactored change_objects to be a list of instances of the class Reporter, or sub classes of Reporter.
The changes to the ""things"" are triggered based on web-services received elsewhere in the code.
Version 1 (OK: sse events received by all web-clients)
Version 2 (NOT OK: sse events NOT always received by all web-clients)
Full disclosure: This question is a follow on to the question: Refactor a (multi)generator python function
which focussed on refactoring the event_stream() function when tracking changes to multiple ""things"".
However the problem here is clearly outside the scope of the original question, hence a new one.
",370
38963711,38963782,2,"Okay, i've used the API docs and found the problem.
The parameter you need to use to order the data is: ""order=asc|desc"", and not ""sort_order"" as previously thought.
Please use this function:
Note:
The way you are using the api, by simple http request, altough works, is the not the ideal way to use their API.
There is a python package called Quandl, you can install like so:
On your system.
Also then you would have a single (and not multiple using auth_token=YOUR_TOKEN in each request) auth call like so:
And then each api call will be simple and elegent using their package instead or creating an http request manually, like so:
I will advise using the second method of using the API, but both will work perfectly.
Cheers, Or.
",158
38963711,38963711,1,"I have the following code:
The code runs without error, however it is plotting in the wrong direction. Seems to be plotting from new to old dates. I would like to plot the oldest data first.
How do I achieve this? I thought changing the params = ""?sort_order=asc"" to params = ""?sort_order=desc"", which only changes the .csv file order not the plot.
Any ideas?
",79
38963734,38963734,1,"How to change the marker sizes in pandas.scatter_matrix() using python 3.5.2 and pandas 0.18.0? 
",17
38963734,38964654,2,"use the s parameter.
",5
38963751,38963870,2,"This is not about django but about html in general. This is your template:
Your checkbox, when unchecked, will not fly because it will not make a {{ form.primal.name }}=True in the url or post body.
To solve your problem, you should ensure a way to add {{ form.primal.name }}=False to the url. The standard solution involves a fixed additional field (a hidden one) like this:
Which will generate a query string part like {{ form.primal.name }}=False if checkbox is unchecked, or {{ form.primal.name }}=False&{{ form.primal.name }}=True if checkbox is checked. In this case, only the latter occurrence counts, so you will have ""True"" when checked and ""False"" when unchecked.
",146
38963751,38963751,1,"I am developing a django application. In my forms. I have a checkbox in my form. My forms submits fine when the box is checked, but when the box is unchecked, the form fails to submit. The field I am using is Boolean.
Here is my code:
Does anyone have a solution?
",61
38963816,38963816,1,"I have tried to build exceptions for this problem like asked in the problem below. Unfortunately I can't make it work. I would greatly appreciate any input whatsoever. Thank you in advance.
Compute 2^x where x is the user input. x should be greater than or equal to 5 and less than or equal to 25. If the user input is not an integer then raise an exception. Create custom exceptions and raise if x is less than 5 and greater than 25. Then add the digits of 2x. For example if user inputs 6, then find 26 = 64, so the sum of the digits is 6 + 4 = 10.
Again, Thank you so much for giving this your time. 
",137
38963816,38963918,2,"Here's a working example, it's written to be executed on python 2.x:
One advice though, try carefully to read & understand all the code and start
tweaking it here and there to make it yours. You won't learn too much using other's code 'as it is', next time try to be more specific asking which specific parts of your code don't understand :)
Have fun learning python!
",80
38963816,38964486,2,"The way you define custom exceptions in python is as shown below. You need to define each custom exception as a subclass of the Exception class. You can then catch your own custom exceptions with a catch-except block.
",41
38963822,38963822,1,"I've an html page, I'd like to display the search bar only when the passed in table object is NOT empty. But my check is not working properly. Here's the code:
Here's what I see on the browser:
 
item_info is blank, I think it should go to else branch, however, it entered if branch, any help is greatly appreciated!  
Edit after elethan's answer:
I've printed it out to debug, here's the screenshot:
So, looks like this item_info is really empty, I didn't see any item_info object gets printed out.
Also, to help debug, here's my view code:
And here's my table definition:
And here's the ItemInfo table it refers to:
",142
38963822,38963940,2,"If item_info is a RawQuerySet, try {% if item_info.all %} instead of {% if item_info %}. RawQuerySet does not define a __bool__() method, so the instances are always considered True. See the warnings in this section of the docs, repeated below, just in case this link dies in the future:
While a RawQuerySet instance can be iterated over like a normal
  QuerySet, RawQuerySet doesnâ€™t implement all methods you can use with
  QuerySet. For example, bool() and len() are not defined in
  RawQuerySet, and thus all RawQuerySet instances are considered True.
  The reason these methods are not implemented in RawQuerySet is that
  implementing them without internal caching would be a performance
  drawback and adding such caching would be backward incompatible.
",142
38963838,38963838,1,"In my python script I need to execute a command over SSH that also takes a heredoc as an argument. The command calls an interactive script that can be also called as follows:
I also found this Q&A that describes how to do it using subprocess but I really like pexpect.pxssh convenience.
Code example would be greatly appreciated 
",63
38963838,39381252,2,"I don't have pexpect handy to test my answer to your question, but I have a suggestion that should work and, if not, may at least get you closer.  
Consider this command:
What is happening?  The entire quoted string is passed as a single argument to ssh, where it is ""executed"" on the remote.  While ssh isn't explicit about what that means, exactly, we know what execv(2) does: if execve(2) fails to execute its passed arguments, the execv function will invoke /bin/sh with the same arguments (in this case, our quoted string).  The shell then evaluates the quoted string as separate arguments, detects the HereDoc redirection, and executes per usual.  
Using that information, and taking a quick look at the pexpect.pxssh documentation, it looks like you want:
If that doesn't work, something is munging your data.  Five minutes with strace(1) will tell you what happened to it, and you can start pointing fingers.  ;-)
HTH.  
",198
38963841,38963841,1,"I am trying to play around with Seaborn on Spyder (installed as part of Anaconda). 
returns:
ImportError: No module named seaborn
This despite the Anaconda website listing seaborn as one of the default packages and the seaborn site saying that Anaconda is the easiest way to get the package. 
What am I doing wrong? 
",61
38963841,42792862,2,"For those using GUI (navigator), select Environments > root. On the right is a list of packages - installed, not installed, etc. 
Select ""not installed"", search for seaborn. If it appears, then click the row, and select Apply button at bottom of that page.
If it doesn't appear, then something else is wrong with your install.
Hope that helps.
",77
38963841,42807727,2,"Since version 4.3.0 dated 2017-01-31, Anaconda comes with seaborn installed by default. Try upgrading your Anaconda installation.
",20
38963841,45656342,2,"Just do conda install seaborn. If its installed it will updated it.
",14
38963841,40393774,2,"Usually Seaborn is imported as 'sns' so if you use that, that will be much easier for you initially (because all the code examples use that), and for others later when you share your code.
Have fun with Seaborn, it is an amazing package. 
",52
38963857,45093065,2,"My 2 cents: if you want to build&test your lambda function in the environment as similar to actual lambda as possible but still under your control, I would suggest using LambdaCI's Docker images. They are based on dumps of original lambda filesystem. Also they have build-specific variants (tags build-python2.7 and build-python3.6 are most interesting for us).
These images are not very small - more than 500mb - but they allow you to avoid any headache when building.
Important benefit over Amazon Linux is that all package versions etc are the same as on the real lambda.
Here is how I did building myself:
For automating it with GitLab CI, just instruct it to use that same docker image
and put these commands in deploy script section:
",141
38963857,38966482,2,"The zip commands in that tutorial are missing a parameter. I ran into this exact problem today with pysftp, which is built on paramiko. libffi-72499c49.so.6.0.4 is in a hidden dot directory inside lib64/python2.7/site-packages/.libs_cffi_backend. Depending on how you zipped up the dependencies in your virtualenv, you may have inadvertantly excluded this directory.
First, make sure libffi-devel and openssl-devel are installed on your Amazon Linux instance, otherwise the cryptography module may not be compiling correctly.
If those packages were not installed before, delete and rebuild your virtualenv.
Make sure that when you are zipping up your site-packages that you use '.' instead of '*', otherwise you will not be including files and directories that are hidden because their names begin with a period.
",137
38963857,38963857,1,"So I'm trying to create an aws lambda function, to log in to an instance and do some stuff. And the script works fine outside of lambda, but when I package it using the same instructions as this https://aws.amazon.com/blogs/compute/scheduling-ssh-jobs-using-aws-lambda/ it doesn't work. It throws this error.
",55
38963882,38964596,2,"Recursive groupby and apply
",4
38963882,38963882,1,"I'm trying to achieve a table with subtotals as shown here, but either that code doesn't work with the latest pandas version (0.18.1) or the example is wrong for multiple columns instead of one. My code here results in the following table
and my desired output would be something like:
Any help on how to achieve this is appreciated.
",67
38963882,38965198,2,"Consider running three level pivot_tables with stack and concatenate them for a final groupby object. As mentioned, the docs does work if you see the use of .stack() on the corresponding pivot_table columns value:
Output
",40
38964528,38964528,1,"Strange error, and I don't understand where is the mistake. The traceback shows nothing relevant:
It seems like something is wrong in the urls.py file:
",30
38964528,38964560,2,"There's a trailing tuple lurking somewhere between those lines; the url(r'^accounts/register/$'...) line:
You intend to have that as a url pattern not a tuple:
",34
38964578,38971474,2,"Yes. Why didn't you simply try? ;)
Fixtures are put in conftest.py files to be able to use them in multiple test files.
",28
38964578,38964578,1,"I am new to Python and I have a doubt in pytest
test_client.py 
conftest.py
Is it possible to move the fixture defined in conftest.py to the test_client.py thereby eliminating having conftest.py
",31
38967478,38967478,1,"I am trying to plot the values of several arrays in separate plots of a figure using imshow.
When I plot one image only, and use the plt.imshow() command with the correct extents, the figure comes out perfectly.
However, when I try to create multiple plots of this image in the same figure, using plt.subplot(), each of these plots ends up with incorrect x-axis settings, and there are white margins. I tried correcting the x-axis range with the set_xlim() command, but it has no effect (which I also don't understand). 
The minimal working sample is below - any help would be appreciated!
",123
38967478,38967970,2,"I believe the reason for the whitespace is the size of the window. You can either change the window size (you'd have to figure out the numbers) or you can adjust the subplot. I found this out by playing with the ""configure subplots"" button in the image popup.
With this line the plot will have no whitespace, but still some empty space (which you can fix by adjusting the window size).
",83
38967478,39013209,2,"So the options are:
Remove the sharex/sharey keywords - seems to clash with imshow in the subplot environment. (Suggested by xnx)
Use plt.subplots_adjust with appropriate settings, in combination with plt.gcf().tight_layout() (Suggested by mwormser)
Use pcolormesh instead of imshow in the subplot environment.
",55
38967533,38967533,1,"Just looking for a simple api return, where I can input a ticker symbol and receive the full company name:
ticker('MSFT')
will return
""Microsoft""
",32
38967533,38968465,2,"You need to first find a website / API which allows you to lookup stock symbols and provide information. Then you can query that API for information. 
I came up with a quick and dirty solution here: 
This website only provides company name. I didn't put any error checks. And you need the requests module for it to work. Please install it using pip install requests. 
Update: Here's the code sample using Yahoo! Finance API: 
",87
38967533,40244249,2,"Here's another Yahoo API call. @masnun's call will return all results that contain the search param, for example trying AMD (Advanced Micro Devices):
http://d.yimg.com/autoc.finance.yahoo.com/autoc?query=amd&region=1&lang=en
gives you AMD (Advanced Micro Devices, Inc.), AMDA (Amedica Corporation), DOX (Amdocs Limited), etc.
If you know the ticker, you can try either of these Yahoo APIs:z
http://finance.yahoo.com/d/quotes.csv?s=amd&f=nb4t8 (well documented, this particular call asks for n=name; b4=book value; t8=1yr target price).
https://query2.finance.yahoo.com/v7/finance/options/amd (not very well documented but new...see more info here about this API: https://stackoverflow.com/a/40243903/933972)
Forgot to include the Google API, which seems ok for stock quotes, but not reliable for full data on option chains:
'https://www.google.com/finance?q=nyse:amd&output=json'
",164
38967581,38968045,2,"Just add [0].text, hope this help !
",11
38967581,38967581,1,"I have this code:
But the returned result includes the  tag as well. 
How do I remove them? Thanks for your help.
",26
38967599,38967621,2,"SOLUTION 1
Just try to concatenate your queryset using | 
In your example
UPDATED:
| does not works using calculated attributes, so, we can select our queryset first and then mapping our extra attributes 
SOLUTION 2 
Using chain built-in function 
There is an issue with this approach, you won't get a queryset, you will get a list containing instances. 
",66
38967599,38967599,1,"Suppose I have the following models
Now, suppose I want to get the number of awards for all users and the number of awards used for all users (ie, a queryset containing both). I prefer to do it one query for each calculation - when I combined it in my code I had some unexpected results. Also for some of my queries it won't be possible to do it one query, since the query will get too complex - I'm calculating 8 fields. This is how I solved it so far:
I'm sure there must be a way to solve this without the dictionary approach? For instance, something like this: awards_used.get(award=award), but this causes a db lookup every loop.
Or some other fancy way to join the querysets?
Note this is a simplified example and I know for this example the DB structure can be improved, I'm just trying to illustrate my question.
",177
38967666,38984224,2,"If I make all fields varchar(255) then it reads missing fiels as ''. sqlalchemy cannot force a '' from the csv file into another datatype.
Best is to use varchar to purely reads the csv file and then afterwards convert it to the proper formats
",51
38967666,38967666,1,"i try to load data from a csv file into a mysql table using odo in python.
the csv file contains blank cells. The odo command files when it encounters blank cells.
how can  I use the odo command to load the data and insert a null value by default for missing data.
I'm trying to import a simple CSV file that I downloaded from Quandl into a MySQL table with the odo python package
The rsow look like this in the CSV. The second line has a value missing.
The MySQL table is defined as follows:
It throws an exception 1366 with the following info:
sqlalchemy.exc.InternalError: (pymysql.err.InternalError) (1366, ""Incorrect decimal value: '' for column 'High' at row 185"") [SQL: 'LOAD DATA  INFILE %(path)s\n            INTO TABLE QUANDL_DATA_WIKI\n            CHARACTER SET %(encoding)s\n            FIELDS\n                TERMINATED BY %(delimiter)s\n                ENCLOSED BY %(quotechar)s\n                ESCAPED BY %(escapechar)s\n            LINES TERMINATED BY %(lineterminator)s\n            IGNORE %(skiprows)s LINES\n            '] [parameters: {'quotechar': '""', 'encoding': 'utf8', 'path': 'C:\ProgramData\MySQL\MySQL Server 5.6\Uploads\WIKI_20160725.partial.csv', 'lineterminator': '\n', 'escapechar': '\', 'skiprows': 0, 'delimiter': ','}]
Does anyone know how to configure ODO so I can upload missing values as NULL values with the simple command?
",271
38967678,38967678,1,"I am writing a piece of code to recursively processing *.py files. The code block is as the following:
When I run the script with a directory path as argument, it only runs the first layer of the directory, the line:
seems never get called. I'm using Python 3.5.
",57
38967678,38968143,2,"As temp_dir has the filename only without parent path, you should change
to
",14
38967678,38967808,2,"The recursion is happening fine, but temp_dir is not a directory so it passes control to your stub else block. You can see this if you put print(file_path) outside your if block.
temp_dir is the name of the next directory, not its absolute path. ""C:/users/adsmith/tmp/folder"" becomes just ""folder"". Use os.path.abspath to get that
Although the canonical way to do this (as mentioned in my comment on the question) is to use os.walk.
",90
38967706,38967706,1,"I'm trying to write a python program that finds and updates a document in mongodb:
But I'm getting the following error message (apparently I'm not using return_document correctly but all pymongo documentation says I am)
NameError: global name 'ReturnDocument' is not defined
",50
38967706,38967956,2,"You need to import the ReturnDocument class first. Add this to the top of your script: 
Detailed docs: http://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.ReturnDocument 
",26
38967729,38967729,1,"I was wondering if there's a library that I can use to create bar codes in Django using just HTML + CSS rather than producing the bar codes as images (like in reportlab & pybarcode). I'm reluctant to use images because I'm creating many bar codes on the same page and I feel images could be a little slow. 
P.S This technique has been used by  dinesh/barcode as a laravel library in php 
",80
38967729,38970441,2,"I have decided to use jquery-barcode which is completely works at client end 
",13
38967833,38967833,1,"I’m trying to use Python’s requests library to automatically get my grades from a university website. The URL is https://acorn.utoronto.ca/sws/transcript/academic/main.do?main.dispatch, but there are several redirects. I have the following simple code but it doesn't seem to be doing what I want.
The output is as follows:
   You do not have Javascript turned on,   please click the button to continue.     
Am I going about this the right way? I feel like I should be trying to pass a cookie instead, but how would I get the cookie?
Thanks in advance. 
Edit: this is the stuff I get from Firefox:
Network tab
Does this mean I need to fill out the entire form as parameters in the request?
",139
38967833,38971901,2,"You can try logging in then getting whatever page you want, there is more more data to be posted which you can get with bs4:
If we run the code and just print the data dict, you can see bs4 populates the required fields:
",48
38967979,38968056,2,"You need to serialize the data to a common format that is accessible from both C# and Python. For example - XML or JSON. I would recommend using JSON. 
Then you have several options: 
Use sockets to transfer the data. 
Use http to transfer the data. 
Write to a file from C# and read that file from Python
Sockets would probably the faster. Using http might be easier. With files, you will need to have some sort of scheduling or notification system to let your Python program know when you have written to the file.  
",107
38967979,38967979,1,"How can i send this list to list in Python script? This is so big for sending as arguments. Thank you. 
",24
38967979,38968841,2,"Since your C# program runs the python script, I guess the easiest solution would be to redirect the standard input of the python process:
At the python script side, you just need to use built-in function raw_input like below (please note the function has been renamed to raw_input in 3.x):
",57
38971898,38971898,1,"I have used SimpleDocTemplate for making a table and now I want to draw a rect on this same page, but I don't know how to do.
I have tried this :
But it don't works...
",41
38971898,39016545,2,"The reason you code is not working is mostly likely because you are creating a Drawing of just 1 pixel high and 100 pixels wide. Which could never fit a Rect of 500 by 100 pixels.
So your code should be something like this:
",47
38971926,38971926,1,"What I'm trying to do is apply a dynamic background color to black text.
The color is calculated as a result of a hash of the text.
The problem is, all too often the color comes out too dark to be able to read the text.
How can I lighten the color to keep it in a decent visual range (not too dark, not too light)?
The color can't be brighter than beige or darker than teal.
(Keep in mind that Blue at 255 is darker than Green at 255, because the human eye is most sensitive to Green and the least sensitive to Blue)
",119
38971926,38985310,2,"You can pick the color in the HSL space
",9
38971926,38983335,2,"QColor supports the HSL representation. You want to limit the range of lightness:
",15
38972011,38972011,1,"I have the following code: (simplified)
The main dictionary (dic) is quite big, and the for loops goes for 500 million iterations. I want to use multiprocessing to parallelize the loop on a multi-core machine. I have read several SO questions and multiprocessing lib documentation, and this very helpful video and still cannot figure it out. 
I want the program to fork into several threads when it reaches this loop, run in parallel and then after all processes have finished it should continue the program on single process from the line after the loop section. 
func_A received the dictionary value and key from dic, calculates some simple operations, and updates the anotherDic data. This is an independent process, as long as all the same i[0] keys are handles by same process. So, I cannot use pool map function which automatically divides the data between cores. I am going to sort the keys by the first element of key tuple, and then divide them manually between the threads. 
How can i pass/share the very big dictionary (dic) between the processes? Different process will read and write to different keys (i.e. keys that each process deals with are different from the rest of processes)
If I cannot find answer to this, I will just use smaller temporary dic for each process, and in the end just join the dics.
Then question is, how I can force process to fork and go muliprocessor just for the loop section, and after the loop all the processes join before continuing with rest of the code on a single thread?
",303
38972011,38972273,2,"A general answer involves using a Manager object. Adapted from the docs:
Output:
Original answer: Python multiprocessing: How do I share a dict among multiple processes?
",32
38972052,44072944,2,"The
is actually the way to go. But it is possible that there are pending conflicts. Conda usually warns of those.
It is still possible to upgrade the packages by hand but I expect a warning (for breaking a certain dependency) to be shown.
That's why you 'cannot' upgrate them all.
Considering your update: I think you can upgrade them each separately, but doing so will not only include an upgrade but also a downgrade of another package as well.
So you still cannot upgrade them all by doing the upgrades separately; the dependencies are just not satisfiable.
",115
38972052,38972052,1,"I tried the conda search --outdated, there are lots of outdated packages, for example the scipy is 0.17.1 but the latest is 0.18.0. However, when I do the conda update --all. It will not update any packages.
update 1
update 2
I can update those packages separately. I can do conda update scipy. But why I cannot update all of them in one go?
",76
38972209,38972209,1,"I use conda created an environment called testEnv and activated it, after that I use the command jupyter notebook to call the jupyter editor. It works, but the problem is that, I can only create file in the root environment. How can I create file in testEnv environment?
Here are the steps what I have done:
Here are the result, which shows I can only create file with in ""root"" but not in ""testEnv"" (There is only Root, but no testEnv):
In the Tab Conda, I can see the testEnv, but how can I switch to it?
",116
38972209,38972519,2,"The answer is that you probably shouldn't do this. Python virtualenvs and Conda environments are intended to determine the resources available to the Python system, which are completely independent of your working directory.
You can use the same environment to work on multiple projects, as long as they have the same dependencies. The minute you start tweaking the environment you begin messing with something that is normally automatically maintained.
So perhaps the real question you should ask yourself is ""why do I think it's a good idea to store my notebooks inside the environment used to execute them.""
",109
38972209,38982381,2,"You have two options. You can install the Jupyter Notebook into each environment, and run the Notebook from that environment:
or you need to install the IPython kernel from testEnv into the environment from which you want to run Jupyter Notebook. Instructions are here: http://ipython.readthedocs.io/en/stable/install/kernel_install.html#kernels-for-different-environments To summarize:
",57
38972245,38972647,2,"It's a shame when you find something ready to go but very time consuming to make it work on windows. My advice would be avoid the hazzle of installing that non-ready-to-go-on-windows library and just looking for another alternative, there are few ones dealing with spherical harmonics. What about this one? pyspharm
Also, posting an issue in the library's github issues could speed it up things.
",73
38972245,38972245,1,"We need a tool to work with 3D-Harmonics and we've come across https://github.com/SHTOOLS/SHTOOLS - which fits all of our needs, but could not be installed properly on our windows computers (as it's intended for linux\osx).
When we tried to run pip install . in the directory SHTOOLS-3.3 (we use anaconda for managing packages and it includes pip), we at first got an error saying that we need a Fortran compiler (gfortran) - which we fixed by installing gcc with conda install -c r gcc. Afterwards, we got an error saying we need to install visual C++ compiler - which we downloaded as suggested from https://www.microsoft.com/en-gb/download/details.aspx?id=44266.
Alas, running the command again, this time from the visual C++ 2008 command prompt, we still get a fatal error and are still stuck with installing the library.
Some of the errors we get: 
Followed by
and 
The full output of the installation attempt can be found here and here.
We've tried to download the lib files of the FFTW3, LAPACK and BLAS libraries but couldn't build them properly.
We would appreciate any help (suggesting a similar library that is compatible with windows \ helping with the install of SHTOOlS).
",228
38972380,39922584,2,"The best way to achieve this seems to be to create a new generator class expanding the one provided by Keras that parses the data augmenting only the images and yielding all the outputs.
",35
38972380,41872896,2,"The example below might be self-explanatory!
The 'dummy' model takes 1 input (image) and it outputs 2 values. The model computes the MSE for each output.
The function below generates batches to feed the model during training. It takes the training data x and the label y where y=[y1, y2]
Finally, we call the fit_generator()
",69
38972380,44907847,2,"If you have separated both mask and binary value you can try something like this:
So, you use the same generator for both input and mask with the same seed to define the same operation. You may change the binary value or not depending on your needs (Y2). Then, you call the fit_generator():
",63
38972380,38972380,1,"In a Keras model with the Functional API I need to call fit_generator to train on augmented images data using an ImageDataGenerator.
The problem is my model has two outputs: the mask I'm trying to predict and a binary value
I obviously only want to augment the input and the mask output and not the binary value.
How can I achieve this?
",67
38972419,38972588,2,"UPDATE:
After trying to reproduce the error on linux it's showing a similar behavior, working fine with the first file but with the second is returning Errno32.
Traceback:
Update:
Some calls of the SG_bin return that the -n parameter is the wrong  type.
This parameter comes from the window variable that is passed to the SavitzkyGolay function.
Surrounding the stdin.write with a trycatch block reveals that it breaks a hadnfull of times.
",81
38972419,38972419,1,"The script, originally taken and modified from (http://globplot.embl.de/):
My input and output files are here: link
This script takes a input (input1.fa)  and gives following output output1.txt 
But when I try to run this script with similar type but larger input file (input2.fa)  .. It shows following error:
I have no idea where the problem is. Any type of suggestion is appriciated.
I am using python 2.7 in windows 7 machine. I have also attached the Savitzky Golay module  which is needed to run the script.
Thanks
",103
38972452,38986144,2,"The quickstart.py example sets the scope to:
To update the spreadsheet you need to set the scope to:
You can do this by first deleting the existing authentication file in ~/.credentials (that is the location on a raspberry.). It will likely be called ""sheets.googleapis.com-python-quickstart.json.
After you removed it you will need to re-authenticate, which should happen automatically when you re-run the script.
",71
38972452,38972452,1,"I'm following the tutorial from this official link : https://developers.google.com/sheets/quickstart/python
I did execute 'quickstart.py' to authenticated.
After that, I ran 'quickstart.py' again and saw the data from 'https://docs.google.com/spreadsheets/d/1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms/edit#gid=0' as this tutorial gets.
I did change spreadsheet ID to my own id and make it to get the data from my spreadsheet by the method :service.spreadsheets().values().get().execute()
But my goal is to add data to my spreadsheet, so I used  the method 'update' as below:
Then I got an error : 
googleapiclient.errors.HttpError: https://sheets.googleapis.com/v4/spreadsheets/MY_SPREADSHEET_ID/values/A2%3AD?alt=json  returned ""Request had insufficient authentication scopes."">
I don't know why this erorr occurs when trying to update my sheet and why this error doesn't occur when trying to get data from my sheet.(If it is cause by authentication, the method 'get' should cause it too!)
Thank you.
",177
38972493,38995575,2,"I found a way to copy/insert data from list to flex table (in vertica) using python:
For list
For JSON
For CSV
",25
38972493,38989666,2,"Vertica-Python supports INSERT INTO. 
Unless you need frequent and very small inserts, writing your data to a file and using COPY would most likely give better performance. If you do it through python, does that still not meet your idea of 'programmatically' ?
https://github.com/uber/vertica-python
https://pypi.python.org/pypi/vertica-python/
",54
38972493,38972493,1,"I am planning to save my unstructured data in flex tables in vertica. I am receiving lists of data (type of data in list may vary in every call) from client, i want to save this in vertica flex table using python 3.
How this can be done?
I found stuff on google, but there data is being loaded in flex table directly using csv or json file, not programmatically. I want to save it programmatically using python.
Thanks in advance for help -:)
",96
38972503,43039300,2,"
Does anyone know the historic reasons for the default behaviour?
Guido van Rossum commented that he liked update() better and thinks a + operator wouldn't read clearly in code.
FWIW, he did approve PEP 448 which gives your another way to do it using star-unpacking generalizations:
There are several reasons why + might not be a good idea.  Usually, we expect addition to be commutative, but dict addition would fail whenever there were overlapping keys with distinct values.   The ""normal"" use case is to update only dict in-place, but the usual semantics of + would copy the contents of both inputs to create a new dict (which is somewhat wasteful).
In addition, Python has collections.ChainMap which replaces the expense of copying with a new expense of potentially having multiple lookups.
",149
38972503,38972503,1,"I've been musing as to why the Python language's standard dict class doesn't support addition/subtraction operators such as '+' or '+=', e.g.
My wishful thinking would be for the following result instead:
Equally why doesn't __radd__(self, other) result in the same as self.__update__(other)?
Does anyone know the historic reasons for the default behaviour? 
(I concede that it could be ambiguous which value should be used in scenarios where foo and bar have the same key but different key values)
",99
38973433,38973433,1,"I have a table of moves that decide whether or not the player wins based on their selection against the AI. Think Rock, Paper, Scissors with a lot more moves.
I'll be eventually coding it in Python, but before I begin, I want to know if there is a better way of doing this rather than LOTS and LOTS of IF statements?
The table looks like this:
I'm thinking that the moves will need to be assigned numbers, or something like that? I don't know where to start...
",102
38973433,38973575,2,"You could use a dict? Something like this:  
I didn't map out all the moves, but you get the gist
",24
38973433,38973624,2,"Yes, store the decisions as key/value pairs in a dictionary with all the possible combinations as the key and decision as result. Basically make a lookup table for the possible moves. 
This speeds up decision making at the expense of having to store all possible combinations.
",50
38973433,38973828,2,"You could use a python dictionary to map moves to numbers:
And then use a matrix to determine the outcome. Numpy can be used for that
",28
38973433,38973896,2,"I'd use a 2-dimensional list for this. Each attack is decoded to an index 0 to 5 and win tie and loss are decoded as 1, 0 and -1.
So the list will look something like this (not based on your example, I just put some random numbers):
And you will retrieve it like this:
",64
38973433,38973897,2,"You can create a map of attacks similar to your table above like this
Here, 0 is a draw, 1 is a win and -1 is a loss.
Now create an array of attacks where the places of the attacks corresponds with the map above.
Now you can easily find out if one attack beats another
Stab wins over punch
",64
38973433,38973682,2,"You could try a dictionary of dictionaries (nested dictionary). Keep values and keys in text form, rather than map to numbers, to improve readability.
",30
38973452,38974250,2,"The call to test.keywords.create(...) doesn't call the keyword, it merely creates one to be called later. If you want the results to be assigned to a variable, use the assign attribute when calling create. This argument takes a list of variable names. 
For example, given this line in plain text format:
... you would create it like this using the API:
",73
38973452,38973452,1,"I hope you can help me, I am quite stuck with this issue :(
I am trying to create all the tests using the robot api with python, I followed the example in the documentation, but I need to capture the output from a keyword and I dont find how can I do it
I tried as usual in rf-ride syntax:
It says: No keyword with name '${grep}=          grep file' found.
I tried: 
but the variable output is having just the keyword name, not the output from kw
I dont know where to look for more info, all the examples are creating kw which dont return any value...
",125
38976893,38976893,1,"I'm trying to write a cross-platform tool that runs specific commands, expects certain output for verification, and sends certain output (like username/password) for authentication.
On Unix, I have been successful in programming a Python tool that uses the pexpect library (via pip install pexpect). This code works perfectly and is exactly what I am trying to do. I've provided a small excerpt of my code for proof-of-concept below:
I tried running the same source on Windows (changing /usr/bin/ctf to c:/ctf.exe) and I receive an error message:
According to the pexpect documentation:
pexpect.spawn and pexpect.run() are not available on Windows, as they rely on Unix pseudoterminals (ptys). Cross platform code must not use these.
That led me on my search for a Windows equivalent. I have tried the popular winpexpect project here and even a more recent (forked) version here, but neither of these projects seem to work. I use the method:
only to sit and watch the Command Prompt do nothing (it seems as though it's trapped inside the winspawn method). I was wondering what other means I could go about programming a Python script to interact with the command line to achieve the same effect as I have been able to in Unix? If a suitable working Windows-version pexpect script does not exist, what other means could I use to go about this?
",263
38976893,46219607,2,"Instead of using pexpect.spawn you can use pexpect.popen_spawn.PopenSpawn for windows.
",11
38977005,38977005,1,"Consider the following code, which generates a (basic) GUI:
The resulting GUI looks like this:
I'd like to achieve the same effect but move the setting of the window title to the class definition. (I've tried self.root.title = ""This is a game window"" in the __init__ but this seemed to have no effect). Is this possible?
",70
38977005,38977181,2,"Sure. You need to call the .title method. Doing 
doesn't set the title, it overwrites the method with the string.
You could also do self.root.title(""This is a game window"") but it's more typing, and using self.root is slightly less efficient than using the  root parameter that was passed to the __init__ method, since self.root requires an attribute lookup but root is a simple local variable.
",79
38977008,38977069,2,"Your if excludes the elif condition:
This matches if position == 18 too. Python ignores all following elif conditions when a if or elif branch has matched.
If you want to run additional code for the == 18 case, use a new if statement:
Alternatively, fix your conditions to not overlap:
or
",59
38977008,38977059,2,"
Both the if and elif branches are catching position if position == 18 (note the <= in the if statement), so the elif branch will never be executed.
",33
38977008,38977079,2,"Change
by 
And leave your elif the same.
By having the condition <=8 your code always enter the if (When position is <=8), but when is 9 (>8) it will enter the elif. So if you want that the code enters the elif statement when position = 8, the if can not be true when position = 8.
",70
38977008,38977008,1,"The aim is to open the page get url nr 18, swap the url with url in position 18 and rerun 7 times but my code is stuck after getting position 18, why is the elif not running in line 24 ? (no traceback given, program just sitting there)
",54
38977041,38984902,2,"I've found the string.count() method to be very fast since it's implemented in C. Anything that avoids for loops will generally be faster, even if you iterate the string multiple times. This is probably the fastest solution:
",44
38977041,38977361,2,"
it is required to split either by a delimiter that occurs first in the string or by a delimiter that is most frequent in the string.
So you can first find all the delimiters and preserve them in a proper container with their frequency, then find the most common and first one, then split your string based on them.
Now for finding the delimiters, you need to separate them from the plain text based on a particular feature, for example if they are none word characters, and for preserving them we can use a dictionary in order to preserve the count of similar delimiters (in this case collections.Counter() will do the job).
Demo:
Note that if you are dealing with delimiters that are more than one characters you can use re.escape() in order to escape the special regex characters (like *).
",159
38977041,38977041,1,"First of all the question is tagged with Python and regex but it is not really tied to those - an answer can be high level.
At the moment I'm splitting a string with multiple delimiters with the following pattern. There are actually more delimiting patterns and they are more complex but let's keep it simple and limit them to 2 characters - # and *:
parts = re.split('#|*', string)
Which such approach a string aaa#bbb*ccc#ddd is split to 4 substrings aaa, bbb, ccc, ddd. But it is required to split either by a delimiter that occurs first in the string or by a delimiter that is most frequent in the string. aaa#bbb*ccc#ddd should be split to aaa, bbb*ccc, ddd and aaa*bbb#ccc*ddd should be split to aaa, bbb#ccc, ddd.
I know a straightforward way to achieve that - to find what delimiter occurs first or is the most frequent in a string and then split with that single delimiter. But the method has to be efficient and I'm wondering if it is possible to achieve that by a single regex expression. The question is mostly for splitting with the first occurance of the set of delimiters - for most frequent delimiter case almost for sure it will be required to calculate occurrence count in advance.
Update:
The question does not ask to split by first occurrence or most frequent delimiter simultaneously - any of this methods individually will be sufficient. I do understand that splitting by most frequent delimiter is not possible with regex without preliminary determination of the delimiter but I think there's a chance that splitting by first occurrence is possible with regex and lookahead without preparation made in advance.
",324
38977169,38977169,1,"I'm trying to click on a particular button on a webpage with Selenium in Python. How do I select and click on the button ""Replace"", with the following HTML?
There is another instance of a ""Replace"" button within the HTML which should be ignored:
Update: 
Unsure if this is helpful. How the buttons HTML updates when hovering / clicking.
When hovering over the button:
When clicking the button:
When I run: 
I'm presented with the following: 
",93
38977169,38977374,2,"
",0
38977169,38977366,2,"Try xpath:
",3
38977169,38977261,2,"You should try using xpath as below :-
Or if there is multiple button with same text Replace try as below :-
Or
Edited : If you are unable to click on element due to overlay of other element, you can try to click using execute_script as below :-
",53
38977184,38977184,1,"I have a data.frame looking similar to this (except much longer and with way more colornames):
I want the data.frame to look like this:
I tried the following:
In my longer data.frame I typically get the error: 
ValueError: cannot reindex from a duplicate axis
Can anyone help?
Thanks very much in advance!
",62
38977184,38977377,2,"There is problem with duplicates in index. You can replace all values of index by reset_index to Regular Index (0,1,2..len(df)-1). Old values are removed by parameter drop=True:
Test:
",38
38977285,38977553,2,"Instead of the spaces, I would highly recommend the following:
If you absolutely need to accept spaces, 
use DeepSpace's answer.
",25
38977285,38977345,2,"If a worker name can't include a space, you can check that and change the behavior accordingly.
This will even work if the user combine the 2 options (e.g. entering 'a' and then 'b c'), although it is not perfect (it is possible to get more than n names, for example if n == 3 and inputting 'a b' and 'c d')
",76
38977285,38977285,1,"I am not sure about input format that user will enter. There are two possibities:
code:
possibility 1:
enter the no of workers 
4
enter the names of workers
name1
name2
name3
name4
['name1', 'name2', 'name3', 'name4']
working great...!!
possibility 2:
input will be : 
enter the no of workers 
4
enter the names of workers
name1 name2 name3 name4
in this case my code will fail. I'll need to write different code( i know how to write that ;) ) to accept this format of input.
So is there anyway that one code will work for both input formats. By treating spaces as Enter. Thanks
",130
38977344,39014240,2,"Based on the answer of Simon Visser on question : 
Cannot install numpy from wheel format
the solution is to replace ""cp26mu"" in the name of the file by none. 
",34
38977344,38977344,1,"I am trying to install numpy from a wheel package (that I have generated in my virtualenv) on a Redhat 6.5 with python version =2.6.6:
I am getting the following error: 
Any way to fix that? thanks :) 
",44
38977360,38977360,1,"I have a class called run_c which is being used to initialize and execute the run of a kinematics simulation. I assign default values to the attributes of run_c, such as x, before run_c.__init__() is executed. All __init__() is doing is extracting user-input values, aggregated into a dictionary, and assigning them to corresponding attributes in run_c if they exists. For example...
run_c.states is a list of lists that is being used to record the values of run_c attributes as they change with timesteps. Later, within run_c.execute(), I am storing x values in states[1], incrementing the timestep dt, and updating x using velocity and timestep. It's pretty simple stuff, right?...
Here's where the problem begins, though. The first time that the instance of run_c is created, initialized, and executed, it runs perfectly. However, I am operating this simulation by creating, initializing, and executing multiple runs based off of list read from a JSON file. As such, in the driver module...
What happens is that all the values that were stored in run_c.states do not get wiped after each iteration of the loop. I thought that new instances of run_c are being created every time I run the loop so as to execute __init__() fresh with new information. Why are the data values, such as old values for x, being retained after the end of each loop?
Update: When I add in a line of code to reset the values of states back to empty lists within __init__(), the problem goes away. But this shouldn't be a necessary step in what I want to do...should it?
",320
38977360,38977584,2,"In addition to the existing answer: you can initialize these variables in the __new__ method.
If you remove the __new__ method, the output'd be 1 1 and otherwise 1 2. 
",35
38977360,38977521,2,"dt, x, and states are defined as class variables, not instance variables. Every instance of that class that you make will share them. If you need them to be initialized each time you generate a new instance, that's exactly what __init__() is for:
",53
38977390,38977390,1,"i run my python file on web browser but i have some error can u help me solve them
The server encountered an internal error or misconfiguration and was unable to complete your request.
Please contact the server administrator at webmaster@localhost to inform them of the time this error occurred, and the actions you performed just before this error.
More information about this error may be available in the server error log.
Apache/2.4.18 (Ubuntu) Server at localhost Port 80
",87
38977390,38977559,2,"you can find your apache logs in this directory /var/log/apache/. Error 500 usually means there is a server error. If you can't find the error in these logs, then try to use verbose logging.
",39
38977401,38977401,1,"I have an array of team names from NCAA, along with statistics associated with them. The school names are often shortened or left out entirely, but there is usually a common element in all variations of the name (like Alabama Crimson Tide vs Crimson Tide). These names are all contained in an array in no particular order. I would like to be able to take all variations of a team name by fuzzy matching them and rename all variants to one name. I'm working in python 2.7 and I have a numpy array with all of the data. Any help would be appreciated, as I have never used fuzzy matching before.
I have considered fuzzy matching through a for-loop, which would (despite being unbelievably slow) compare each element in the column of the array to every other element, but I'm not really sure how to build it.
Currently, my array looks like this:
{Names , info1, info2, info 3} 
The array is a few thousand rows long, so I'm trying to make the program as efficient as possible.
",205
38977401,38986394,2,"The Levenshtein edit distance is the most common way to perform fuzzy matching of strings.  It is available in the python-Levenshtein package.  Another popular distance is Jaro Winkler's distance, also available in the same package.
Assuming a simple array numpy array:
We define helpers to give us indexes of Levenshtein and Jaro distances, between a string we have and all strings in the array.
Now, note that Levenshtein distance is an integer value counted in number of characters, whilst Jaro's distance is a floating point value that normally varies between 0 and 1.  Let's test this using np.where:
And we get:
",116
38977457,44454088,2,"Shobhit Bhatnagar, try this one - pip install -e git+https://github.com/dvska/gdata-python3#egg=gdata
",15
38977457,38977457,1,"I'm installing google api python client on python 3 using pip from the GitHub repository.
But getting the following error:
Microsoft Visual C++ 10.0 is required. Get it with ""Microsoft Windows SDK 7.1"": www.microsoft.com/download/details.aspx?id=8279
I already installed microsoft windows sdk 7.1 and net framework 4.0.
But it's not working.
Please check the snapshot for more details.
",69
38977457,38977627,2,"gdata is deprecated and does not have support for Python 3.x in the first place. Use any of these APIs.
",22
38977472,38977472,1,"I have the following code:
How can I let the main thread know that the TestWorker has finished (call on_work_done()), without blocking calls to do_something_else() as thread.join() would?
",39
38977472,38978075,2,"
",0
38977472,38980087,2,"
OUT:
",2
38977472,38978438,2,"You can give your thread instance an optional callback function to call when it's finished.
Note I added a Lock to prevent concurrent printing (which does block).
Output:
",34
38977525,38984338,2,"Some details of what you must do may depend on what you want to do with IDLE's Shell once you have it running.  I would like to know more about that.  But let us start simple and make the minimum changes to pyshell.main needed to make it run with other code.  
Note that in 3.6, which I use below, PyShell.py is renamed pyshell.py. Also note that everything here amounts to using IDLE's private internals and is 'use at your own risk'.
I presume you want to run Shell in the same process (and thread) as your tkinter code.  Change the signature to
Change root creation (find # setup root) to
In current 3.6, there are a couple more lines to be indented under if not tkroot:
Guard mainloop and destroy (at the end) with
The above adds 'dependency injection' of a root window to the function.  I might add it in 3.6 to make testing (an example of 'other code') easier.
The follow tkinter program now runs, displaying the both the root window and an IDLE shell.
You should be able to call pyshell.main whenever you want.
",215
38977525,38977525,1,"I need to embed an interative python interpreter into my tkinter program. Could anyone help me out as to how to integrate it?
I have already looked at the main() function, but it's way to complex for my needs, but I can't seem to reduce it without breaking it.
",58
38978073,44909825,2,"Using tf.metrics did the trick for me : 
Result : 
Note : for Accuracy I would use :
As it is simpler and already compute in the evaluate.
Also call variables_initializer if you don't want cumulative result.
",40
38978073,38978073,1,"I was wondering if there was a simple solution to get recall and precision value for the classes of my classifier? 
To put some context, I implemented a 20 classes CNN classifier using Tensorflow with the help of  Denny Britz code : https://github.com/dennybritz/cnn-text-classification-tf .
As you can see at the end of text_cnn.py he implements a simple function to compute the global accuracy : 
Any ideas on how i could do something similar to get the recall and precision value for the differents categories? 
Maybe my question will sound dumb but I'm a bit lost with this to be honest. Thanks for the help.
",112
38978084,38978161,2,"If I understand correctly, I think this is what you're looking for:
UPDATE
There are other options, assuming you control the code for Agents. E.g., you could redefine the method like this:
and then always pass all arguments:
or perhaps keep the definition of update the same and just initialize your parameters to the string 'none'.
",66
38978084,38978084,1,"Ok I am not even sure the proper terminology to use to describe what I am trying to do. Anyway, I want to know if it is possible to programmatically or dynamically build a function call in python.
Let me explain.
I have a function inside a class that is defined with optional parameters like so:
So when I when I go to use that function, I may be updating just one, two or all 3 of the optional parameters. Sometimes it may be full_name and role but not status... or status and role but not name, or just status, or well you get the idea.
So I could handle this with a big block of if elif statements to account for all the permutations but that strikes me as really clumsy.
Currently I am calling the function like so:
Is there anyway to construct that function call programmatically, like appending the arguments to the call before making it. So that I can account for multiple scenarios like so:
or 
Anyway, what is the best way to approach this?
",198
38978160,38989657,2,"Martijn answer was right with this
But you do not need to use urllib to get the args with flask. I would instead on my endpoint just use the following.
Also I would look at using the flask_testing extension that way you can setup reproducible test cases that exist within the context of the running application.
https://pythonhosted.org/Flask-Testing/
",62
38978160,38978160,1,"I am implementing a REST API in Python using Flask.
I have to get parameters to perform a query and return resources. To be aligned with REST principles, I am going to use a GET request for this operation.
Given that there can be a lot of parameters, I want to send them through a conf.json file, for instance:
I perform the request through curl:
$ curl -H ""Content-Type: application/json"" --data @conf.json -G http://localhost:8080/resources/
The request is redirected to the route with these operations:
what I get back is:
Somebody knows how to get the json data and properly handle it in the request?
",122
38978160,38978254,2,"request.get_json() looks for JSON data in the request body (e.g. what a POST request would include). You put the JSON data in the URL query string of a GET request instead.
Your curl command sends your JSON un-escaped, and produces an invalid URL, so the server rightly rejects that:
You can't have spaces in a URL, for example. You'd have to use --data-urlencode instead for this to be escaped properly:
Note that the Content-Type header is not needed here; you don't have any request body to record the content of.
The adjusted curl command now sends a properly encoded URL:
Access that data with request.query_string. You will also have to decode the URL encoding again before passing this to json.loads():
Take into account that many webservers put limits on how long a URL they'll accept. If you are planning on sending more than 4k characters in a URL this way, you really need to reconsider and use POST requests instead. That's 4k with the JSON data URL encoded, which adds a considerable overhead.
",204
38978163,38984401,2,"Seems -2 returns 254 on echo $? in bash.
The Bash is interpreting the -2 as 254 probably due to how it handles negative numbers which I believe is by returning 8-bit unsigned integers.  
Soln: Use a postive integer number > 0 but < 256.  Bash will see a return code > 0 as an error.
Does the lock below need to be global? I believe so, in order that it stays in scope until the program finishes.
Ans: If there are no other conflicts with locking the file than I would say that the global lock is fine.
",110
38978163,38978163,1,"I am setting up cron job for single instance.
Does the lock below need to be global? I believe so, in order that it stays in scope until the program finishes. Or at least outside of try/except block. Also, return values should be positive from Python? Seems -2 returns 254 on echo $? in bash. 
",64
38978174,38998694,2,"Thanks for your help. The final solution is kind of stupid. I started spyder via the anaconda GUI. If I do so the above code does not work.
If I run this directly via the console or start spyder via the console everything is fine. It seems that the bash_profile is not loaded when spyder is loaded but requires the console to do so
",69
38978174,38978267,2,"With shell=True the cmd needs to be a string.
subprocess.check_call(command, shell=True)
where command is of type str
",22
38978174,38978174,1,"I am using am using Python 2.7 on MacOS and want to use a bash command within a python script.
I had to include the path of this program in my bash_profile in order to run it. I tested so far:
and
Neither worked. The latter threw error 127 and the first one only returned 32512. A google search told me that this occurs when the command is not known. 
If I now start this command within the terminal everything works perfectly fine.
Do I have to include something such that python can find this command? Why is this behavior?
",109
38978182,39120531,2,"Since I wasn't able to implement an Observer to watch widgets like the ttk.Combobox, I've decided to create a workaround. Here are the steps I took, in order to achieve a MVC architecture from Bryan Oakleys example (link is in the question), which refreshes its model class via the controller class, whenever a user takes an action in the view (GUI). 
Step 1: Add a model class
First, in order to use a MVC architecture, we have to seperate the code into model, view and control. In this example, model is class Model:, control is class PageControl(tk.Tk): and view are the pages class StartPage(tk.Frame), PageOne(tk.Frame) and PageTwo(tk.Frame).
Step 2: Set up your model class
Now we have to decide on which variables we want to have in the model class. In this example, we have directories and keys (status of the comboboxes), which we want to save in dictionaries. After setting them up empty, all we have to do is add setters and getters for each variable, so we can refresh data in model and also retrieve some, if we want. Additionally, we could implement delet methods for each variable, if we wanted to.
Step 3: Add push and pull methods to the control class
Now that there is a model class, we can refrence it via e. g. self.model = Model() in PageControl(tk.Tk) (control). Now we have the basic tools to set data in Model via e. g. self.model.set_keys(self.shared_keys) and also get data from Model. Since we want our control class to do that, we need some methods, that can achieve this. So we add the push and pull methods to the PageControl (e. g. def push_key(self)), which in turn can be refrenced from view (StartPage, PageOne, PageTwo) via controller.
Step 4: Add your widgets to the view class
Now we have to decide on which widgets shall be on which page and what you want them to do. In this example, there are buttons for navigation, which for the sake of the task can be ignored, two comboboxes and a button, which opens a file dialog. 
Here, we want the comboboxes to refresh their status whenever it is changed and send the new status via controller to the model. Whereas the Open button of PageOne shall open a file dialog, where the user then selects files he/she wants to open. The directories we got from this interaction then shall be send via controller to model.
Step 5: Get all your functionality into the controller class
Since there is a controller variable, we can use it to refrence methods, which are in the controller class. This way, we can outsource all our methods from the pages into the controller and reference them via self.controller.function_of_controller_class. But we have to be aware, that methods, which are bound to commands via lambda: can't return any values, but they are also not called on programme startup. So keep that in mind.
Step 6: Set up your bindings and wrappers
Here we have to set up the .bind() for our comboboxes. Since the controller allready is set up to store data and the comboboxes have a textvariable, we can use this to gather information about the status of the comboboxes via combobox.bind(<<ComboboxSelect>>). All we have to do is to set up a wrapper which is called, whenever combobox.bind(<<ComboboxSelect>>) is throwing an event.
Closing statement
Now we have it, a programme based on Bryan Oakleys example of ""How to get variable data from a class"", which utilises a model, which is updated via controller whenever the user takes a corresponding action in the view. Unfortunately it doesn't utilise a Observer class, as first intended, but I'll keep working on it and update this, when I've found a satisfying solution.
",761
38978182,38978182,1,"I'm currently working on a GUI which is based on the thread How to get variable data from a class. Since there will be a lot of data to handle, I would like to use a Model-Class, which get's its updates via Observer.
Right now, changes in the ttk.Combobox on Page One are registered via <<ComboboxSelect>>, pulled into the variable self.shared_data of the Controller and passed to the Model. This way, no Oberserver/Observable logic is used. Instead, the data in Model is changed, whenever the user takes a corresponding action in the GUI.
I, however, would love not to have to use bindings like <<ComboboxSelect>> to change the corresponding data in the Model, but an Observer/Observable logic, which detects, that i.e. the entry ""Inputformat"" in the dictionary self.shared_data within the Controller was changed, which in turn refreshes the data in the Model, i.e. self.model_data, where the actual state of the ttk.Combobox is saved.
In short, I want to achieve the following, by using an Observer: 
User selects i.e. ""Entry 01"" in the ttk.Combobox --> self.shared_data[""Inputformat""] in the Controller is now filled with ""Entry 01"" --> an Observer/Observable logic detects this --> the corresponding variable in the Model is beeing changed.
For you to have something to work with, here is the code.
",266
38981503,38981503,1,"I have created a user registration form in Django but every time I test it, the user is not being saved to the database because when the password is entered it raises ""this field can not be null. I do not understand why this is happening since the form was working before, and it follows the same procedures as other registration forms. I have been trying to figure out the cause of the issue for days and I am still not  understanding how the field can be null and still wont work. However in the terminal I am not getting any error messages, and the login form is working because users are created via the admin. Thanks for the help and  here is the code I am using.
the user form
My view registration function
My html
",146
38981503,38981919,2,"If you clean a django form field (using def clean_myfield) you need to return that field
",18
38981700,38981700,1,"I need to override createsuperuser.py's handle method in Django Command class.
I created myapp\management\commands\createsuperuser.py:
When I do in terminal ./manage.py createsuperuser I do not see any changes. If I change the name of my file to lets say mycmd.py and do ./manage.py mycmd everything starts to work as I expect.
How to get changes I need using ./manage.py createsuperuser?
",65
38981700,38982236,2,"Put your application name on top in the INSTALLED_APPS list. 
",11
38981765,38981765,1,"EDIT
This is my code:
It's working as it should within python, so I compiled it to .exe using Pyinstaller.
I managed to have an exe file now but I'm struggling with the import of pyperclip.
the exe works except for that last command. I tried -p dir and also --hidden-import pyperclip.
I ran it with -d and the debug I get is:
I'm not sure how to proceed right now :\ any input?
",87
38981765,39013958,2,"In the end I managed to import all the dependencies by using the command 'pyi-makespec' which then required an additional step to make the actual exe file. The documentation for pyinstaller is really extensive and accurate. Thanks @Repiklis for your inputs
",45
38981814,38981814,1,"I'm new to using Classes in Python, and could use some guidance on what resources to consult/how to use a class in a loop.
Sample data:
here's the code outside of a class:
I need to translate this into a class, like so:
I think there must be a better way to structure it or run through the three different values in the styles list.  But I don't even know what to search for to improve this.  Any suggestions, pointers, etc. would be appreciated!  
",99
38981814,38982181,2,"An elegant way to do it is to iterate through both lists at the same time using zip
As for the design it could be improved
For instance, define your model files as a list directly:
Which gives:
",41
38981814,38981970,2,"You could just enumerate the files like so
Does this answer the question?
",14
38981847,38981959,2,"The easy way to ensure that dependent modules are installed is through pip -r.
Essentially, make a requirements.txt along with your script for users to install the correct modules and versions. 
Inside the text file should look like this:
You can use:
To find which modules you have installed
EDIT: This is implied your script is small and not packaged inside a .dmg or .exe file. 
",72
38981847,38982133,2,"numpy, scipy takes a long time to install in a virtualenv.  For this reason I would not recommend virtualenv like others have.  You could try pyinstaller to create an OS specific executable.  Haven't tried it with numpy or scipy myself though.
http://www.pyinstaller.org
",50
38981847,38981847,1,"I apologise in advance, as I am rather new to Python programming, but I was curious as to how this system works. My question is: if I write a Python script and make it distributable, but my program imports other external libraries such as numpy or scipy (which is what I am working on currently) how does it all work together? It is my understanding that the user would still have to install the libraries separately or I need to write a separate makefile that runs a script to install it while my distributable is being installed. Am  I right in this opinion? Advice would be greatly appreciated. Also a explanation how it works internally given your answer. Thanks a lot! Appreciate your time! 
",138
38981885,38982172,2,"You can use agg function after the groupby:
Or a shorter version according to @root and @Jon Clements:
",22
38981885,38981885,1,"I have some entries in dataframe like :
Here is what I am trying to achieve as result of pivot table:
I was trying something like :
however I want the phone numbers to be concatenated as part of aggfunc. 
Any Suggestions ?
",45
38981912,38983726,2,"Here's a compact vectorized approach without those error checks -
Runtime test and verification -
",16
38981912,38981912,1,"Lets say one has 600 annotated semantic segmentation mask images, which contain 10 different colors, each representing one entity. These images are in a numpy array of shape (600, 3, 72, 96), where n = 600, 3 = RGB channels, 72 = height, 96 = width. 
How to map each RGB-pixel in the numpy array to a color-index-value? For example, a color list would be [(128, 128, 0), (240, 128, 0), ...n], and all (240, 128, 0) pixels in the numpy array would be converted to index value in unique mapping (= 1).
How to do this efficiently and with less code? Here's one solution I came up with, but it's quite slow.
Testing it:
",157
38981915,38981915,1,"I've installed a virtualenv with pyenv using Python v2.7.12. Inside this virtualenv, I installed matplotlib v1.5.1 via:
with no issues. The problem is that a simple
script fails to produce a plot window. The backend that I see in the virtualenv using:
is agg, which is apparently the root cause of the issue. If I check the backend in my system-wide installation, I get Qt4Agg (and the above script when run shows a plot window just fine).
There are already several similar questions in SO, and I've tried the solutions given in all of them.
Matplotlib plt.show() isn't showing graph
Tried to create the virtualenv with the --system-site-packages option. No go.
How to ensure matplotlib in a Python 3 virtualenv uses the TkAgg backend?
Installed sudo apt install tk-dev, then re-installed using pip --no-cache-dir install -U --force-reinstall matplotlib. The backend still shows as agg.
Matplotlib doesn't display graph in virtualenv
Followed install instructions given in this answer, did nothing (the other answer involves using easy_install, which I will not do)
matplotlib plot window won't appear
The solution given here is to ""install a GUI library (one of Tkinter, GTK, QT4, PySide, Wx)"". I don't know how to do this. Furthermore, if I use:
I get:
meaning that all those backends are available in my system (?).
matplotlib does not show my drawings although I call pyplot.show()
My matplotlibrc file shows the line:
I don't know how to make the virtualenv aware of this?
Some of the solutions involve creating links to the system version of matplotlib (here and here), which I don't want to do. I want to use the version of matplotlib installed in the virtualenv. 
If I try to set the backend with:
I get ImportError: Gtk* backend requires pygtk to be installed (same with GTK). But if I do sudo apt-get install python-gtk2 python-gtk2-dev, I see that they are both installed.
Using:
(or Qt5Agg) results in ImportError: Matplotlib qt-based backends require an external PyQt4, PyQt5, or PySide package to be installed, but it was not found. Not sure if I should install some package?
Using:
results in ImportError: No module named _tkinter, but sudo apt-get install python-tk says that it is installed.
Using:
results in ImportError: No module named gtk. So I try sudo apt-get install libgtk-3-dev but it says that it already installed.
How can I make the virtualenv use the same backend that my system is using?
",489
38981915,38988308,2,"You can consider changing your backend to TkAgg in the Python 2 virtualenv by running the following:
To confirm the backend is indeed TkAgg, run
and you should see TkAgg.
",33
38981977,38981977,1,"code to make test data:
data looks like
how to shift column A backwards by number of months as value from column B
to the effect of 
",28
38981977,38982372,2,"You can try:
",4
38981977,38983098,2,"Here is a vectorized way to compose arrays of dates (NumPy datetime64s) out of
date components (such as years, months, days):
",28
38982002,38982002,1,"I have am writing a program that accepts arguements in the following form.
I am currently parsing these options using the following (messy) code
Is there a better way to do this with either argparse or docopt?
",41
38982002,38984485,2,"One argument could be
and later
others:
",8
38982763,38982763,1,"I wrote this very simple code, it doesn't work and I can't understand why. I answer 'good' but if part doesn't work!
",29
38982763,38982810,2,"have you tried
the double equal means the same as.
Also, if you are looking for a helpful free beginner course, try codecademy. They helped me loads learn python
",33
38982763,38982857,2,"You have to use:
If p == 'good':
The == compares the value of the two. The Is keyword checks for identity by comparing  the memory address. 
Hope this explains it enough. 
Hannes
",39
38982776,38983968,2,"
",0
38982776,38982776,1,"I have a list of lists m which I need to modify
I need that the sum of each row to be greater than A and the sum of each column to be lesser than B 
I have something like this
My problem is: I need to satisfy both conditions and, this way, after the second for I won't be sure if the first condition is still met.
Any suggestions on how to satisfy both conditions and, of course, with the best execution? I could definitely consider the use of numpy
Edit (an example)
I may need to add 
This is for the generation of the random initial population of a genetic algorithm, the restrictions are to make them a possible solution, and I would need to run this like 80 times to get different possible solutions
",149
38982776,38983999,2,"Something like this should to the trick:
Caveats
I use <=, not < as indicated in your question, because that's what numpy supports.
This minimizes the total sum of all values in the target vector.
For your use case, you probably want to minimize the distance
to the original sample, which the linear program cannot handle, since neither the squared error nor the absolute difference can be expressed using a linear combination (which is what c stands for). For that, you will probably need to go to full minimize(). 
Still, this should get you rough idea.
",116
38982776,38983417,2,"A NumPy solution:
I chose to ignore the original content of m - it's all zero in your example anyway.
",23
38982784,38982984,2,"Edit: Also, your server isn't accepting/listing for connections
You should make the server multithreaded so that it can send and receive at the same time. Here's how it might look:
When you send a string over TCP you need to encode it to bytes. So your client file should look like this instead:
I much prefer the .encode() and .decode() syntax as it makes the code a little more readable IMO.
",84
38982784,38982784,1,"I am writing a multi-chat which consists of the Client handler, the server and chat record. The Client should allow multiple chats. At the moment it doesn't allow for even one chat, I get an error message after the name has been entered. 
This is the Client handler file
This is the server file
This is the Chartrecord
This is the error message I get when running the chat record. I can enter a name then after that I get the error message below
Please assist
",93
38982807,38984367,2,"Presumably you already have a Flask app object and routes set up, but if you create the app like this:
then set up your @app.route()s, and then when you want to start the app:
Then you can just run your application directly rather than having to tell gunicorn or uWSGI or anything else to run it for you.
I had a case where I wanted the utility of flask to build a web application (a REST API service) and found the inability to compose flask with other non-flask, non-web-service elements a problem. I eventually found gevent.wsgi.WSGIServer and it was just what I needed. After the call to app_server.serve_forever(), you can call app_server.stop() when your application wants to exit.
In my deployment, my application is listening on localhost: using flask and gevent, and then I have nginx reverse-proxying HTTPS requests on another port and forwarding them to my flask service on localhost.
",175
38982807,38982989,2,"When you ""run Flask"" you are actually running Werkzeug's development WSGI server, and passing your Flask app as the WSGI callable.
The development server is not intended for use in production.  It is not designed to be particularly efficient, stable, or secure.
Replace the Werkzeug dev server with a production-ready WSGI server such as Gunicorn or uWSGI when moving to production, no matter where the app will be available.
The answer is similar for ""should I use a web server"".  WSGI servers happen to have HTTP servers but they will not be as good as a dedicated production HTTP server (Nginx, Apache, etc.).
Flask documents how to deploy in various ways. Many hosting providers also have documentation about deploying Python or Flask.
",143
38982807,38982807,1,"Setting up Flask with uWSGI and Nginx is quite difficult, and even with buildout scripts it takes quite some time, and has to be recorded to instructions to be reproduced later.
If I don't plan a big load on server (it's hidden from public), does it make sense to run it without uWSGI? (Flask can listen to a port. Can Nginx just forward requests?)
Does it make sense to not use even Nginx, just running bare flask app on a port?
",96
38986113,38986113,1,"I try to make Async ping process using subprocess.Popen , I try to understand how i implement it in this case 
",21
38986113,39173449,2,"
edited a bit class i have found also used threads instead of Async & await
Credit: http://blog.boa.nu/2012/10/python-threading-example-creating-pingerpy.html
",20
38986117,38986117,1,"I was using Jupyter in Windows and just switched to Ubuntu. I found the colour of the code is very weird in the firefox browser. E.g. it highlights the variables in every other line.
I tried to solve this problem by installing a custom theme and the effect should be like
Instead, it still highlights every other variable on my side, like
This just makes my eyes very tired when try to debug the code.
I also tried disabling all the add-ons in Firefox which didn't help. Is there any setting that I can change to restore to the default colour display? 
",112
38986117,39112179,2,"I sometimes get this if I'm copying/pasting from a source that has a different indentation size than that of the jupyter notebook. In your screenshot it looks like a small indent size so this seems like the likely culprit. Try highlighting the full block of indented code and hitting ctrl+[ then ctrl+] (this unindents the selected lines of code, then reindents them using the jupyter indent size). 
If this doesn't work, you might try checking to see if there are any custom indentation settings specified in either "".jupyter/nbconfig/notebook.json"" or "".jupyter/custom/custom.js"" (... or whatever the Windows equivalents are).
In "".jupyter/nbconfig/notebook.json"", I have the indentUnit set to 4 spaces (and have also enabled linewrapping).
Most editors allow you to set your indent size (Atom, sublime text, etc.) so you can avoid this issue in the future by making sure you have the same indent size everywhere you're swapping code to/from (assuming this is what's causing the red highlighting). Python's default is 4 so def recommend sticking with that. 
",202
38986202,38986284,2,"As I understand it, you are matching on the first field and the file is sorted.  In that case, try:
How it works
NR==1{printf ""%s"",$0}
For the first line, we print it with no trailing newline.
NR>1{printf ""%s%s"",($1==last?"" "":""\n""),$0}
For lines after the first, we print a space if the first fields match or a newline if they don't, followed by the line.
The tricky-looking part here is the ternary statement $1==last?"" "":""\n"".  This just tests to see if the first field is equal to the last first field.  If it is, it returns the string after the ?.  If it isn't, it returns the string after the :.
last=$1
We update the variable last to the most recent first field.
END{print""""}
After we have finished reading the file and to make sure that we have a complete final line, we print a newline.
",211
38986202,38986768,2,"another awk
accumulate records with the same key, print at the end and sort (by the key), column for prettying.  Doesn't require the keys to be contiguous or sorted.
",36
38986202,38993617,2,"This could be approached in Python as follows:
This would display:
",13
38986202,38986202,1,"I am working with a data set:
Using AWK and I need lines that match, to be printed onto the same line.
i.e. 
I've been trying all sorts of different ideas but cannot locate code to do this.
I have been trying to use AWK, as instructed by my superior. Would be interested to see if it would be easier in Python?
(note white space between lines to preserve structure)
",82
38986205,38986230,2,"You can use re.split() using a positive lookbehind to AM or PM having an optional - and a space character as a delimiter:
",26
38986205,38986205,1,"An example of the python string is '8:30 AM- 10:00 PM Subject: Math'. In general, the string contains a start time, end time, and the subject I want to separate this string into 3 components: the start time, end time, and subject. For example 8:30 AM, 10:00 PM, and Subject: Math.
How can I do this using regex in python?
",75
38986217,38986217,1,"Using Python I am trying to install a library called yappi through easy_install. However I am getting this error below on Windows 7 Command Shell:
I explored alternative installations. I tried previously 'pip install yappi' but this didn't work due to a separate error (can't build wheel) which is a separate question. 
",61
38986217,39050639,2,"Try downloading the appropriate wheel from here.
Then use pip install [package].
",16
38986225,38986225,1,"I have a create form in which I'm submitting the field ['created_at'] as a hidden datetimefield. I'm importing from django.utils import timezone and have this working on another model. Does anyone have any insight into why the value is blank? Thanks for any help!
form:
However when the templates renders the hidden input is there, but it doesn't have a value. 
Here is the template code snippet:
",81
38986225,39009987,2,"Rather than continue banging my head on the wall with this, I opted to set the created_at time in the model rather than worry about it elsewhere. 
Set the attribute to have (default=timezone.now) and that cured my woes.
",43
38986235,44858602,2,"They've updated the spacy version to spacy-alpha V2.0.0.
You can check it here for the new documentation Here
",20
38986235,38986235,1,"So lately I've been playing around with a WikiDump.
I preprocessed it and trained it on Word2Vec + Gensim
Does anyone know if there is only one script within Spacy that would generate
tokenization, sentence recognition, part of speech tagging, lemmatization, dependency parsing, and named entity recognition all at once
I have not been able to find clear documentation
Thank you 
",68
38986235,39058506,2,"Spacy gives you all of that with just using en_nlp = spacy.load('en'); doc=en_nlp(sentence). The documentation gives you details about how to access each of the elements.
An example is given below:
Sentences can be obtained by using doc.sents:
Noun chunks are given by doc.noun_chunks:
Named entity is given by doc.ents:
Tokenization: You can iterate over the doc to get tokens. token.orth_ gives str of the token.
POS is given by token.tag_:
Lemmatization:
Dependency parsing. You can traverse the parse tree by using token.dep_ token.rights or token.lefts. You can write a function to print dependencies:
For more details please consult the spacy documentation.
",125
38986244,38986244,1,"In Python, what are the running time and space complexities if a list is converted to a set?
",20
38986244,38986278,2,"You have to iterate through the entire list, which is O(n) time, and then insert each into a set, which is O(1) time. So the overall time complexity is O(n), where n is the length of the list.
No other space other than the set being created or the list being used is needed.
",70
38986244,38986289,2,"Converting a list to a set requires that every item in the list be visited once, O(n). Inserting an element into a set is O(1), so the overall time complexity would be O(n).
Space required for the new set is less than or equal to the length of the list, so that is also O(n).
Here's a good reference for Python data structures.
",83
38986261,38986261,1,"I understand that in order to find a nested document that I am to use the dot notation, but I need what follows the dot be in variable form. I am doing the following:
and getting 
I don't want ""src_id"" in the offset document I want to add to the a3c1b98d5606be7c5f0c5d14ffb0b741 key. I am using python3.5 and pymongo version 3.2.2. Thanks!
",70
38986261,38986305,2,"You can just have a nested dictionary:
Or, you can dynamically make the field via string formatting or concatenation:
",22
38986341,38986421,2,"At the time of passing the parameters, the parameters are not initialised
so you need to send len of myByteArray as another variable.
So what you could do is,
",32
38986341,38986430,2,"Python default arguments are evaluated when the function is defined. Rather, you want something like this:
",19
38986341,38986341,1,"I have a problem with the fact that I am calling len(myByteArray) in the input arguments to a function I am declaring. I'd like that to be a default argument, but Python doesn't seem to like it. myByteArray is of type bytearray. See documentation on bytearray here. I am accessing its built-in find function, documented here (see ""bytes.find"").
My function:
Examples to attempt to use the function above:
Error: 
NameError: name 'myByteArray' is not defined
If I just remove my default arguments (=0 and =len(myByteArray)), however, it works fine. But...I really want those default arguments there so that the start and end arguments are optional. What do I do? 
In C++ this would be easy, as argument types are specified when you write functions.
",159
38986342,38986374,2,"Something like this should do the trick:
",8
38986342,38986375,2,"You can create a dict to map from characters to indices and then do lookups into that. This will avoid repeatedly searching the string as other answers are suggesting (which is O(n)) and instead give O(1) lookup time with respect to the alphabet:
At that point you can easily wrap it in a function:
Or use the bound method directly
But you don't want to change the name that refers to a builtin function, that'll confuse everyone else trying to use it that can see your change. If you are really trying to hurt yourself you can replace my_ord with ord in the above.
",120
38986342,38986376,2,"You don't.
You're going about this the wrong way: you're making the mistake
This existing thing doesn't meet my needs. I want to make it meet my needs!
instead, the way to go about the problem is
This existing thing doesn't meet my needs. I need a thing that does meet my needs!
Once you realize that, the problem is now pretty straightforward. e.g.
",79
38986342,38986388,2,"If i've understood correctly, this is what you want:
",12
38986342,38986342,1,"For example, in python, when I type in ord(""a"") it returns 97 because it refers to the ascii list. I want ord(""a"") to return zero from a string that I created such as 
so ord(""b"") would be 1 and ord(""c"") would be 2 ect.
How would I go about doing this?
",76
38986343,38986385,2,"Assuming both Latitude and Longitude are Integers:
Try This.
Your mistake is that you are not concatenating the strings properly.
",23
38986343,38986343,1,"I'm trying to create a hyper link in python that includes string and variables but i keep getting a syntax error, I think its probably because of quotations but I cant seen to figure it out. Thanks for the help.
",44
38986344,38986563,2,"First of all, I'm guessing that you didn't really want to print that y value of 10; that you really wanted the base-10 reduction to 0.  Note that you have an extra character in the pyramid base.
Do not change the value of a loop parameter while you're inside the loop.  Specifically, don't change y within the for y loop.
Get rid of c; you can derive it from the other values.
For flexibility, make your upper limit a parameter: you have two constants (6 and 7) that depend on one concept (row limit).
Here's my version:
Output:
If you really want to push things, you can shorten the loops with string concatenation and comprehension, but it's likely harder to read for you.
Each of the loops is turned into a list comprehension, such as:
Then, this list of characters is joined with no interstitial character; this forms half of the row.  The second half of the row is the other loop (counting down).  In front of all this, I concatenate the proper number of spaces.
Frankly, I prefer my first form.
",221
38986344,38986584,2,"Here's my implementation.
Python 2:
Python 3:
Input:
Output:
",15
38986344,38986344,1,"Can you help to simplify this code and make it more efficient? Mine seems like it's not the best version; what can I improve?
This is the code I made:
",35
38986369,38986369,1,"I'm trying to build a client and a server using xmlrpc in python, I HAVE to use a class named FunctionWrapper which has a method and the client use it, the method's name is sendMessage_wrapper(self, message), and the server is declared in another class, I'm trying to register the method in the server but when i call the method from de client I raise and error, can you help me, please?
Cliente:
Server:
",89
38986369,38986383,2,"Here are the constants in case you need it 
",9
38986403,39010456,2,"The line json=data should have been data=data. The json attribute accepts a dictionary, which that data string is not. Here is what working code looks like:
",30
38986403,38986403,1,"Short version:
This Python request doesn't work. The Javascript version does. Why? 
Longer version:
I have this Javascript that successfully makes an Ajax call. I print the response to the console and can see an array of suggested characters from the input I send.
https://jsbin.com/wufesifasa/1/edit?js,console,output
Output:
[""SUCCESS"", [[""fb02254b519a9da2"", [""+"", ""十"", ""t"", ""T"", ""ナ"", ""f"", ""子"",
  ""干"", ""1"", ""千""], [], [object Object] {   is_html_escaped: false }]]]
Now I'm trying to replicate that in Python. I've tried using the above code and many variations of it, but every time I receive the response 'FAILED_TO_PARSE_REQUEST_BODY'. What's different between the Ajax and Python calls that makes my request fail? 
This question is similar to this and this, but they deal with using the same key multiple times and incorrect data encoding, which I do not think applies in this case. 
",213
38986427,38989448,2,"This is the best I could come up with for now.
",12
38986427,38986732,2,"Simplest way is to form them into another DataFrame. Use pd.concat
",12
38986427,38986427,1,"My dataframe looks like
If I apply a groupby like 
I get group names and groups like:
So my question is how can I resample the group names formed above and fill non existent groups with None to get something like: 
Can this be formed as a Dataframe as well?
Regards
",54
38986525,38986525,1,"I'm working on a simple text-based trivia game as my first python project, and my program won't terminate once the score limit is reached.
",28
38986525,38986607,2,"It looks like the points variable is not increased. Something like this might work in your inner loop:
I'm assuming that quest_list is a list of functions, and you're passing the points value as an argument? To make this example work, you'll also want to return the points from the function returned by the quest_list that's called. A perhaps cleaner way to build this would be to return only the points generated by the quest. Then you could do something like:
Unless points is a mutable data structure, it won't change the value. You can read more about that in this StackOverflow question. 
",119
38986527,38986527,1,"I'm currently working on a IoT project using NanoPi M1 and it's expected to notify the users when the amount of light received is not adequate during the day time. I've done the part related to light. Hence, I need to find an effective way to retrieve sunrise and sunset time in python, since the whole script is written in python. I know there are several libraries out there for other languages, I wonder what is the most convenient way to do this in python.
It will seem pretty much like this, I suppose:
I'd appreciate any help here, have a good one.
",118
38986527,38986561,2,"Check out astral. Here's a slightly modified example from their docs:
If you use something like this example, please remember to change the city_name and date provided to city.sun.
",34
39267793,39268611,2,"I suspect that your inner class should be called Meta, not meta.
",14
39267793,39267793,1,"Hi i am creating a simple Sign Up form with django framework and mongodb. Following is my view:
Following is my model:
Following is my forms.py
Following is the traceback ValueError recieved on loading SignUpview
Traceback:
File ""C:\Program Files\Python35\lib\site-packages\django\core\handlers\exception.py"" in inner
    39.             response = get_response(request)
File ""C:\Program Files\Python35\lib\site-packages\django\core\handlers\base.py"" in _get_response
    187.                 response = self.process_exception_by_middleware(e, request)
File ""C:\Program Files\Python35\lib\site-packages\django\core\handlers\base.py"" in _get_response
    185.                 response = wrapped_callback(request, *callback_args, **callback_kwargs)
File ""C:\Program Files\Python35\lib\site-packages\django\views\generic\base.py"" in view
    68.             return self.dispatch(request, *args, **kwargs)
File ""C:\Program Files\Python35\lib\site-packages\django\views\generic\base.py"" in dispatch
    88.         return handler(request, *args, **kwargs)
File ""C:\Program Files\Python35\lib\site-packages\django\views\generic\edit.py"" in get
    174.         return self.render_to_response(self.get_context_data())
File ""C:\Program Files\Python35\lib\site-packages\django\views\generic\edit.py"" in get_context_data
    93.             kwargs['form'] = self.get_form()
File ""C:\Program Files\Python35\lib\site-packages\django\views\generic\edit.py"" in get_form
    45.         return form_class(**self.get_form_kwargs())
File ""C:\Program Files\Python35\lib\site-packages\mongodbforms\documents.py"" in init
    353.                 raise ValueError('A document class must be provided.')
Exception Type: ValueError at /signup/
  Exception Value: A document class must be provided.
I am not able to find root of this problem. I am new to django and this is my first project. Also is their anyother way for creating model forms for mongo documents??
",260
39660132,41006261,2,"first, do: pip install pygame
second: make sure you have a correct import statement on top of your main app module.
",25
39660132,39660132,1,"I want to compile a game I made in Python. I searched around for compilers and I prefer Nuitka because it is cross-platform. But whenever I try to compile my code with Nuitka using nuitka --recurse-all --standalone myappname.py I get this error:
I have pygame installed, could anyone please help me?
PS: I dont want to use ""compilers"" like cx_freeze
Thanks in advance
",73
39867462,39867462,1,"How would I replace the the hard coded 'python' with snippet['language'] from a for loop in my view?
",24
39867462,39869400,2,"You can simply put your variable in place of the hardcoded string like this:
You haven't showed us your for-loop, but in principle you can do the same thing in the for-loop too:
",37
39867464,39867464,1,"I have a project which is making a simple breakout game with python. I am having a problem with making a button on a graphic window.
Here, How can I make those rectangles as buttons which represent the movements ""Left"",& ""Right""??
",52
39867464,41554451,2,"Below is a simple solution for a red left button, a green right button and an ""Exit"" button to quit the program.  I've rearranged the rectangles that represent the buttons such that P1 is the lower left corner and P2 is the upper right corner.  This simplifies the test to see if the clicked point was inside the button.  (You can make the code more sophisticated to remove this assumption.)
If you click the red or green buttons, you'll get ""left"" or ""right"" printed in the center of the window, otherwise no text appears:
",112
40018348,40018429,2,"Have a look at BeautifulSoup: https://www.crummy.com/software/BeautifulSoup/
You can request a website and then read the HTML source code from it:
If you want to read JavaScript, look into Headless Browsers.
",36
40018348,40018348,1,"What I had try are as following:
1)
In this way, I can't open the url in browser.
2)
In this way, I can't get source code of the url.
So, how can I open an URL and get source code at the same time?
Thanks for your help.
",61
40018351,40018351,1,"I'm running on debian jessie. I'm trying to parse my pdf with tabula-py library but I get this error 
How can do to fix that ?
Here is my code :
The error will be happened at this line 
df = read_pdf_table('ed.pdf', area=(top,left,bottom,right))
",60
40018351,41790252,2,"I'm author of tabula-py. I guess you would like to extract image based PDF, but tabula-py isn't a tool for OCR. It is assumed to extract text embedded PDF.
I think you should try OCR tools such as Google Cloud Vision API.
",49
40018351,41791796,2,"Just to add to what Chezou has said: Google Could Vision OCR does not support PDFs directly. You would first need to extract the pages (as images) with a tool like Ghostscript and then send the image of each page to the API. But if your PDF has three pages or less, you can use the free OCR.space PDF OCR api, which can take the whole PDF document as input.
",78
