After,some,experimentation,",",my,current,best,solution,is,to,have,a,main,graph,featuring,training,inputs,and,a,separate,graph,with,just,evaluation,data,operations,.,I,open,a,separate,session,to,get,evaluation,data,and,feed,this,to,the,training,graph,when,I,want,to,evaluate,.,Highly,inelegant,(,and,evaluation,runs,take,longer,than,they,ideally,would,as,they,have,to,come,ot,of,one,session,only,to,be,fed,to,another,),",",but,assuming,evaluation,runs,are,rare,compared,to,training,runs,",",this,seems,preferable,to,the,original,version,...,Results,:,Update,:,when,using,this,approach,in,training,problems,with,tf.contrib.layers,and,regularization,",",I,find,the,regularization,losses,go,to,infinity,if,the,DataSupplier,graph,is,on,the,same,device,as,the,training,graph,.,I,cannot,for,the,life,of,me,explain,why,this,is,the,case,",",but,explicitly,setting,the,device,of,the,DataSupplier,to,the,CPU,(,given,the,training,graph,is,on,my,GPU,),seems,to,work,...
