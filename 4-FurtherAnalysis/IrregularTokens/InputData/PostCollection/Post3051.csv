If,you're,using,a,queue,when,loading,your,data,and,follow,it,up,with,a,batch,input,then,this,shouldn't,be,a,problem,as,you,can,specify,the,max,amount,to,have,loaded,or,stored,in,the,queue,.,See,here,for,more,details,:,https://www.tensorflow.org/versions/r0.10/api_docs/python/io_ops.html#batch,Also,there's,an,alternative,approach,that,skips,the,tf.cond,completely,.,Just,define,two,losses,one,that,follows,the,data,through,the,autoencoder,and,discrimator,and,the,other,that,follows,the,data,through,just,the,discriminator,.,Then,it,just,becomes,a,matter,of,calling,or,In,this,way,the,graph,will,only,run,through,which,ever,loss,was,called,upon,.,Let,me,know,if,this,needs,more,explanation,.,Lastly,I,think,to,make,variant,one,work,you,need,to,do,something,like,this,if,you're,using,preloaded,data,.,https://www.tensorflow.org/versions/r0.10/how_tos/reading_data/index.html#preloaded-data,Otherwise,I'm,not,sure,what,the,issue,is,to,be,honest,.
